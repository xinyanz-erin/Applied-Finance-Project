{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/European_Call_jax_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "49f9dd38-660e-4110-b885-6050e61b59bb"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 3\n",
        "numsteps = 50\n",
        "numpaths = 100000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([100.]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 110.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3851213\n",
            "100 loops, best of 5: 5.32 ms per loop\n",
            "[0.09405017 0.09377991 0.09379512]\n",
            "10 loops, best of 5: 45.1 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06baa1ba-5396-4c61-b82d-a6ecd97312f4"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 1.0)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "          drift = jnp.array(np.random.random(self.N_STOCKS) * 0.1)\n",
        "\n",
        "          T = self.T\n",
        "          K = np.random.random(1) * 1.0\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 10000, batch = 2, seed = 15, stocks=3) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "00e04c14-70cd-4dee-dae1-3a34f074a764"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*3, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1) # 4 outputs: price, delta1, delta2, delta3\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1, 1.0, 1.0, 0.3, 0.1, 0.1]*3)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "9272c759-0915-4d5d-d81f-4ac1bc1cdcfd"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.7)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "S3CyULkENYKb",
        "outputId": "16219a20-4a31-4268-ef7f-addd07745e05"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    y_pred = model(x)\n",
        "    # print(y_pred)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2,8,14]]\n",
        "\n",
        "    # print(torch.unbind(x))\n",
        "    # print([compute_deltas(x) for x in torch.unbind(x)])\n",
        "    # print(torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0))\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # print(y_pred)\n",
        "\n",
        "    loss_weight = 1/(y.mean(axis=0)**2)\n",
        "    # print(y.mean(axis=0))\n",
        "    # print((y.mean(axis=0)**2))\n",
        "    # print(1/(y.mean(axis=0)**2))\n",
        "    \n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    # print(loss_weight)\n",
        "    # print(loss_weight_normalized)\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean() # compute weighted MSE between the 2 arrays\n",
        "    # print((y_pred - y) ** 2)\n",
        "    # print((y_pred - y) ** 2 * loss_weight_normalized)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 49.021514132618904 average time 0.08911899334993904 iter num 20\n",
            "loss 46.344297006726265 average time 0.060328774799972965 iter num 40\n",
            "loss 46.41591105610132 average time 0.05114368869996421 iter num 60\n",
            "loss 34.83923617750406 average time 0.04642080802500459 iter num 80\n",
            "loss 46.445601619780064 average time 0.04389973148003264 iter num 100\n",
            "loss 50.489846616983414 average time 0.08542299254986574 iter num 20\n",
            "loss 40.714540518820286 average time 0.058695979674917 iter num 40\n",
            "loss 36.078246776014566 average time 0.04994600836660842 iter num 60\n",
            "loss 35.97424132749438 average time 0.04546079827497351 iter num 80\n",
            "loss 40.46469461172819 average time 0.04286929183997927 iter num 100\n",
            "loss 34.987195394933224 average time 0.08645505044996753 iter num 20\n",
            "loss 27.211173437535763 average time 0.05945551572503973 iter num 40\n",
            "loss 15.933963004499674 average time 0.050283291800027045 iter num 60\n",
            "loss 10.47053374350071 average time 0.04575959580001836 iter num 80\n",
            "loss 11.821053922176361 average time 0.04292805607000446 iter num 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-c8c30e4018d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m           \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m           \u001b[0mEuropean_Call_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m           \u001b[0mgooptionvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptionvalueavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m           \u001b[0mDeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooptionvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36moptionvalueavg\u001b[0;34m(key, initial_stocks, numsteps, drift, r, cov, K, T, keys)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m###################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   6551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6552\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6553\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6554\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeferring_binary_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a40050a-cd45-4d56-b0a2-dc7b4237eec2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_test_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed9bb43-0e48-40eb-9c68-9b2c159543f9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eff326d-4853-478b-cb00-a3b9dac2f8d2"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_test_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e746fa3b-76e0-42c3-ed3e-1ba9670d606b"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=18, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34c1a7f-e80c-42d5-f51d-2a84f816f129"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 8, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    y_pred = model(x)\n",
        "    # print(y_pred)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2,8,14]]\n",
        "\n",
        "    # print(torch.unbind(x))\n",
        "    # print([compute_deltas(x) for x in torch.unbind(x)])\n",
        "    # print(torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0))\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # print(y_pred)\n",
        "\n",
        "    loss_weight = 1/(y.mean(axis=0)**2)\n",
        "    # print(y.mean(axis=0))\n",
        "    # print((y.mean(axis=0)**2))\n",
        "    # print(1/(y.mean(axis=0)**2))\n",
        "    \n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    # print(loss_weight)\n",
        "    # print(loss_weight_normalized)\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean() # compute weighted MSE between the 2 arrays\n",
        "    # print((y_pred - y) ** 2)\n",
        "    # print((y_pred - y) ** 2 * loss_weight_normalized)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 10\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 50)\n",
        "\n",
        "model_save_name = 'jax_european_test_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 1.489993737777695 average time 0.14872073680016912 iter num 10\n",
            "loss 0.40073315176414326 average time 0.07942319760004465 iter num 20\n",
            "loss 0.26352327040513046 average time 0.056397417633221876 iter num 30\n",
            "loss 0.4649558832170442 average time 0.044976566524928784 iter num 40\n",
            "loss 0.22839494704385288 average time 0.03791268223998486 iter num 50\n",
            "loss 0.06467154889833182 average time 0.03325133311664104 iter num 60\n",
            "loss 0.30129565857350826 average time 0.029946736514362523 iter num 70\n",
            "loss 0.08773660738370381 average time 0.02750182762501936 iter num 80\n",
            "loss 0.23859687644289806 average time 0.025600441822219485 iter num 90\n",
            "loss 0.5628457074635662 average time 0.024113024350008345 iter num 100\n",
            "loss 0.4427204839885235 average time 0.0660500273006619 iter num 10\n",
            "loss 0.15784524293849245 average time 0.03780793875012023 iter num 20\n",
            "loss 0.07773603101668414 average time 0.028544101333318396 iter num 30\n",
            "loss 0.4490707578952424 average time 0.023841018750044896 iter num 40\n",
            "loss 0.7900097989477217 average time 0.021155707560174052 iter num 50\n",
            "loss 0.3223051317036152 average time 0.019298077666887063 iter num 60\n",
            "loss 0.42266994569217786 average time 0.017997601085880888 iter num 70\n",
            "loss 0.2671937727427576 average time 0.01696715451266755 iter num 80\n",
            "loss 0.3574705624487251 average time 0.01618849312225292 iter num 90\n",
            "loss 0.33572032407391816 average time 0.01553075654002896 iter num 100\n",
            "loss 0.260265023825923 average time 0.06392168930033222 iter num 10\n",
            "loss 0.12591908671311103 average time 0.03693904100055079 iter num 20\n",
            "loss 0.4557822467177175 average time 0.02808969830027005 iter num 30\n",
            "loss 1.5956156130414456 average time 0.023674021175338567 iter num 40\n",
            "loss 0.30034168958081864 average time 0.02098379116017895 iter num 50\n",
            "loss 0.10070527423522435 average time 0.01922167556695058 iter num 60\n",
            "loss 0.8507525490131229 average time 0.017924606428849594 iter num 70\n",
            "loss 1.5383401478175074 average time 0.01701962031270341 iter num 80\n",
            "loss 0.46630277211079374 average time 0.016218549489106712 iter num 90\n",
            "loss 0.12900438377982937 average time 0.015562833630210661 iter num 100\n",
            "loss 0.16517216863576323 average time 0.06452874440001324 iter num 10\n",
            "loss 0.337754245265387 average time 0.03748538730014843 iter num 20\n",
            "loss 0.02405634859314887 average time 0.02848567373342424 iter num 30\n",
            "loss 0.1183818130812142 average time 0.023801972000183014 iter num 40\n",
            "loss 0.1678803891991265 average time 0.021083573240102852 iter num 50\n",
            "loss 0.5043290002504364 average time 0.01924004298349852 iter num 60\n",
            "loss 0.05611109372694045 average time 0.017916213557327865 iter num 70\n",
            "loss 0.8300723857246339 average time 0.016966629287662727 iter num 80\n",
            "loss 0.2622076135594398 average time 0.01619369920012509 iter num 90\n",
            "loss 0.2608649629110005 average time 0.015613665870114346 iter num 100\n",
            "loss 0.7394413114525378 average time 0.06490950899969902 iter num 10\n",
            "loss 0.2619717088236939 average time 0.03757174689999374 iter num 20\n",
            "loss 0.1278770469070878 average time 0.028378454633275395 iter num 30\n",
            "loss 0.330805269186385 average time 0.023741314849758055 iter num 40\n",
            "loss 1.3024057261645794 average time 0.020945329079695513 iter num 50\n",
            "loss 0.12346511539362837 average time 0.019164106182870455 iter num 60\n",
            "loss 0.12575555956573226 average time 0.01787837554246445 iter num 70\n",
            "loss 0.2582935667305719 average time 0.016861230649647042 iter num 80\n",
            "loss 0.053279400162864476 average time 0.016108528599741274 iter num 90\n",
            "loss 0.23145799787016585 average time 0.015489242199728324 iter num 100\n",
            "loss 0.8264431380666792 average time 0.06401490640018892 iter num 10\n",
            "loss 0.3321781332488172 average time 0.036858585250229225 iter num 20\n",
            "loss 0.18156493752030656 average time 0.027977002733314292 iter num 30\n",
            "loss 0.21838932298123837 average time 0.023446238874930712 iter num 40\n",
            "loss 0.16763387975515798 average time 0.02081091672000184 iter num 50\n",
            "loss 0.24289332941407338 average time 0.018952901949887746 iter num 60\n",
            "loss 0.2873345874832012 average time 0.01766217369991604 iter num 70\n",
            "loss 0.11791689757956192 average time 0.016696424712381485 iter num 80\n",
            "loss 2.16424698010087 average time 0.01592356355540687 iter num 90\n",
            "loss 0.1581926699145697 average time 0.015310022349804059 iter num 100\n",
            "loss 0.7763031317153946 average time 0.06496955420006997 iter num 10\n",
            "loss 0.2158374263672158 average time 0.037293413849874925 iter num 20\n",
            "loss 0.740067771403119 average time 0.028567557866699643 iter num 30\n",
            "loss 0.11904788152605761 average time 0.023822077275235644 iter num 40\n",
            "loss 0.5270779729471542 average time 0.021051499560126103 iter num 50\n",
            "loss 0.061311666286201216 average time 0.019193463599973863 iter num 60\n",
            "loss 0.4328859722591005 average time 0.0178936236285933 iter num 70\n",
            "loss 0.13233507161203306 average time 0.016910515737572497 iter num 80\n",
            "loss 0.4378844460006803 average time 0.01613364815570498 iter num 90\n",
            "loss 0.4850808181799948 average time 0.015547392240150656 iter num 100\n",
            "loss 0.5022864934289828 average time 0.06481318630030727 iter num 10\n",
            "loss 0.14916853615432046 average time 0.03756604985010199 iter num 20\n",
            "loss 0.46370536438189447 average time 0.02821220466697317 iter num 30\n",
            "loss 1.1379016359569505 average time 0.023635485425074876 iter num 40\n",
            "loss 0.0754588518248056 average time 0.020894538900029146 iter num 50\n",
            "loss 0.12330052413744852 average time 0.019187036433383278 iter num 60\n",
            "loss 0.0867945982463425 average time 0.017833821028606117 iter num 70\n",
            "loss 0.33855576475616544 average time 0.01685291155004052 iter num 80\n",
            "loss 0.8265612268587574 average time 0.016063891511172792 iter num 90\n",
            "loss 0.14364388334797695 average time 0.015454689630059875 iter num 100\n",
            "loss 0.10629328244249336 average time 0.06455059319923748 iter num 10\n",
            "loss 0.6449114880524576 average time 0.03733330014947569 iter num 20\n",
            "loss 0.5199649604037404 average time 0.028316950033210258 iter num 30\n",
            "loss 0.20841718651354313 average time 0.02370563314998435 iter num 40\n",
            "loss 0.3981200643465854 average time 0.020913623519954853 iter num 50\n",
            "loss 0.3260131779825315 average time 0.019092092266691906 iter num 60\n",
            "loss 1.4036054199095815 average time 0.017828834857209586 iter num 70\n",
            "loss 0.6845806638011709 average time 0.016827264400035347 iter num 80\n",
            "loss 1.6193330520763993 average time 0.016071538311209426 iter num 90\n",
            "loss 0.12143756066507194 average time 0.015435001290061336 iter num 100\n",
            "loss 0.136555927383597 average time 0.06569588069942256 iter num 10\n",
            "loss 0.4323979374021292 average time 0.03785485155003698 iter num 20\n",
            "loss 0.6154138827696443 average time 0.028698376366567874 iter num 30\n",
            "loss 0.8281829650513828 average time 0.02409797777499989 iter num 40\n",
            "loss 0.8999580313684419 average time 0.021285757459918387 iter num 50\n",
            "loss 0.31051455152919516 average time 0.019401876300010674 iter num 60\n",
            "loss 0.9427946497453377 average time 0.01809482727143664 iter num 70\n",
            "loss 0.3298672891105525 average time 0.017124249237485855 iter num 80\n",
            "loss 0.29295977583387867 average time 0.01631658217783196 iter num 90\n",
            "loss 0.724044002708979 average time 0.015656455280040973 iter num 100\n",
            "loss 0.34938482713187113 average time 0.06403967549958907 iter num 10\n",
            "loss 0.11413360880396795 average time 0.03695550974989601 iter num 20\n",
            "loss 0.14332660612126347 average time 0.02802799913309476 iter num 30\n",
            "loss 0.6331675103865564 average time 0.024079527499816324 iter num 40\n",
            "loss 3.009116044268012 average time 0.02127995059963723 iter num 50\n",
            "loss 2.0689109805971384 average time 0.019429545266333056 iter num 60\n",
            "loss 0.3152993303956464 average time 0.018074365871143527 iter num 70\n",
            "loss 0.20126764866290614 average time 0.01706613949977509 iter num 80\n",
            "loss 0.06696345735690556 average time 0.01628081549974417 iter num 90\n",
            "loss 0.09179313565255143 average time 0.015664249279834622 iter num 100\n",
            "loss 0.4665878077503294 average time 0.06396987870029988 iter num 10\n",
            "loss 1.4552251377608627 average time 0.03711337260028813 iter num 20\n",
            "loss 0.8778886694926769 average time 0.028203225067045423 iter num 30\n",
            "loss 0.4559247463475913 average time 0.023850055875300313 iter num 40\n",
            "loss 0.44759188313037157 average time 0.021085408440339962 iter num 50\n",
            "loss 0.8181476732715964 average time 0.01930191491695344 iter num 60\n",
            "loss 0.28507325623650104 average time 0.018005569314534245 iter num 70\n",
            "loss 0.1954567778739147 average time 0.01703098808775394 iter num 80\n",
            "loss 0.3738615123438649 average time 0.01622355213350804 iter num 90\n",
            "loss 1.066239201463759 average time 0.015598221190120967 iter num 100\n",
            "loss 0.5243459600023925 average time 0.0644288711999252 iter num 10\n",
            "loss 0.742292613722384 average time 0.03756578039956367 iter num 20\n",
            "loss 0.05071053237770684 average time 0.02838681766637213 iter num 30\n",
            "loss 0.34696495276875794 average time 0.02390432312467965 iter num 40\n",
            "loss 0.5138607957633212 average time 0.02115778411985957 iter num 50\n",
            "loss 1.028267652145587 average time 0.019336048233283994 iter num 60\n",
            "loss 0.25894754799082875 average time 0.01804622992863837 iter num 70\n",
            "loss 0.1989138763747178 average time 0.017020137662530032 iter num 80\n",
            "loss 0.5223598054726608 average time 0.016208081799939263 iter num 90\n",
            "loss 2.3451172455679625 average time 0.015692245959908178 iter num 100\n",
            "loss 0.9226547990692779 average time 0.06518139719992178 iter num 10\n",
            "loss 0.24015964299906045 average time 0.0377960459996757 iter num 20\n",
            "loss 0.28687889425782487 average time 0.02869781956663549 iter num 30\n",
            "loss 0.39331120206043124 average time 0.024274798774786176 iter num 40\n",
            "loss 0.21573127014562488 average time 0.02155392305998248 iter num 50\n",
            "loss 0.43033312977058813 average time 0.019597503516646006 iter num 60\n",
            "loss 0.09686655175755732 average time 0.018245038257009582 iter num 70\n",
            "loss 0.5964762385701761 average time 0.01719199853732789 iter num 80\n",
            "loss 0.145527101267362 average time 0.016401074066501074 iter num 90\n",
            "loss 0.01751328909449512 average time 0.015807557109837944 iter num 100\n",
            "loss 1.3033449067734182 average time 0.06537576879964035 iter num 10\n",
            "loss 0.4199231989332475 average time 0.037590214749980075 iter num 20\n",
            "loss 0.23718213924439624 average time 0.02856742896644088 iter num 30\n",
            "loss 0.1932123450387735 average time 0.024055933424915566 iter num 40\n",
            "loss 0.20964325813110918 average time 0.021222835639928234 iter num 50\n",
            "loss 0.3247835411457345 average time 0.019452305166547983 iter num 60\n",
            "loss 0.06480747288151179 average time 0.01810889618562734 iter num 70\n",
            "loss 0.05369598511606455 average time 0.017167326537401096 iter num 80\n",
            "loss 0.47130950406426564 average time 0.01637878229998427 iter num 90\n",
            "loss 0.14728740097780246 average time 0.015763908409935537 iter num 100\n",
            "loss 0.08578476808906998 average time 0.06586808590072905 iter num 10\n",
            "loss 0.47195106162689626 average time 0.037899223050226284 iter num 20\n",
            "loss 0.0805838226369815 average time 0.028536103666677567 iter num 30\n",
            "loss 0.08425901796726976 average time 0.023918119849986395 iter num 40\n",
            "loss 0.06289631528488826 average time 0.021182178419912816 iter num 50\n",
            "loss 0.10534017746977042 average time 0.019400812066548194 iter num 60\n",
            "loss 0.33465959859313443 average time 0.018193018628517167 iter num 70\n",
            "loss 0.0723254015611019 average time 0.017203129437393726 iter num 80\n",
            "loss 0.2763146039796993 average time 0.016449309044320317 iter num 90\n",
            "loss 0.2060988663288299 average time 0.01578971973995067 iter num 100\n",
            "loss 0.1348787554888986 average time 0.06691662779994659 iter num 10\n",
            "loss 0.5080534901935607 average time 0.03848409865022404 iter num 20\n",
            "loss 1.3107541599310935 average time 0.029177410633443892 iter num 30\n",
            "loss 0.3514856507536024 average time 0.024431307050144825 iter num 40\n",
            "loss 0.07448654287145473 average time 0.02159517859996413 iter num 50\n",
            "loss 0.2522844079066999 average time 0.019799956283410816 iter num 60\n",
            "loss 0.2866403156076558 average time 0.0184706571287409 iter num 70\n",
            "loss 0.09969934581022244 average time 0.017525748912612472 iter num 80\n",
            "loss 0.04708115284302039 average time 0.016719539900037085 iter num 90\n",
            "loss 0.5270651308819652 average time 0.01610596105005243 iter num 100\n",
            "loss 0.4794176493305713 average time 0.06512282740004594 iter num 10\n",
            "loss 1.7851351003628224 average time 0.037717755700214184 iter num 20\n",
            "loss 0.15392415662063286 average time 0.02861163126702498 iter num 30\n",
            "loss 0.25920382540789433 average time 0.023945404325240817 iter num 40\n",
            "loss 0.1772562973201275 average time 0.021129848500349907 iter num 50\n",
            "loss 0.22197227735887282 average time 0.019284845850173347 iter num 60\n",
            "loss 0.07910070962680038 average time 0.018055020771550648 iter num 70\n",
            "loss 0.02770830633380683 average time 0.017214314037664734 iter num 80\n",
            "loss 0.5106939352117479 average time 0.01643798221125002 iter num 90\n",
            "loss 0.11561049177544191 average time 0.01577089986003557 iter num 100\n",
            "loss 0.2821937596308999 average time 0.06503154800120683 iter num 10\n",
            "loss 0.7642596028745174 average time 0.037661820550601986 iter num 20\n",
            "loss 0.3498363366816193 average time 0.02840792353369276 iter num 30\n",
            "loss 3.859670541714877 average time 0.023831666975274855 iter num 40\n",
            "loss 0.5715752558899112 average time 0.02109770450020733 iter num 50\n",
            "loss 0.17393558664480224 average time 0.019237910033552907 iter num 60\n",
            "loss 0.09167849384539295 average time 0.017950368671621877 iter num 70\n",
            "loss 0.06196818958414951 average time 0.017009872887638267 iter num 80\n",
            "loss 1.6469749971292913 average time 0.01629020735571329 iter num 90\n",
            "loss 0.26956142392009497 average time 0.015807685160216352 iter num 100\n",
            "loss 0.13885331100027543 average time 0.06438756290044693 iter num 10\n",
            "loss 0.2865703936549835 average time 0.03720625110017863 iter num 20\n",
            "loss 0.8737416646908969 average time 0.028250870233387106 iter num 30\n",
            "loss 0.09818497346714139 average time 0.02374002892502176 iter num 40\n",
            "loss 1.7703589401207864 average time 0.02108644116007781 iter num 50\n",
            "loss 2.7727868291549385 average time 0.019268317950203104 iter num 60\n",
            "loss 0.1437568698747782 average time 0.017978308185930863 iter num 70\n",
            "loss 0.08351196811418049 average time 0.016997624925170384 iter num 80\n",
            "loss 0.08500607691530604 average time 0.01630890997795278 iter num 90\n",
            "loss 0.4570545206661336 average time 0.015696272380082518 iter num 100\n",
            "loss 0.2883457455027383 average time 0.0649469197003782 iter num 10\n",
            "loss 0.2144127392966766 average time 0.03739370120001695 iter num 20\n",
            "loss 0.16416339349234477 average time 0.028510662833529446 iter num 30\n",
            "loss 0.13360806406126358 average time 0.02385529682505876 iter num 40\n",
            "loss 0.22164080291986465 average time 0.021176303480096977 iter num 50\n",
            "loss 0.06004345777910203 average time 0.01942237245020806 iter num 60\n",
            "loss 0.15648509361199103 average time 0.018093780857296744 iter num 70\n",
            "loss 3.7844825419597328 average time 0.01725321676276508 iter num 80\n",
            "loss 0.1920471913763322 average time 0.01646216482250667 iter num 90\n",
            "loss 0.19865930880769156 average time 0.01580481026023335 iter num 100\n",
            "loss 0.43494677811395377 average time 0.06532277500045894 iter num 10\n",
            "loss 0.7808983355062082 average time 0.03793052980036009 iter num 20\n",
            "loss 0.2666742875589989 average time 0.028712048633557665 iter num 30\n",
            "loss 0.6286613643169403 average time 0.024128401200050574 iter num 40\n",
            "loss 4.442059143912047 average time 0.021395282080120523 iter num 50\n",
            "loss 0.12373547178867739 average time 0.019491044150102728 iter num 60\n",
            "loss 0.5221458923188038 average time 0.018117735771582894 iter num 70\n",
            "loss 0.4914939563605003 average time 0.017146431887658763 iter num 80\n",
            "loss 0.1054304993886035 average time 0.016387055833466647 iter num 90\n",
            "loss 0.08596784937253688 average time 0.015741356060098043 iter num 100\n",
            "loss 0.11482190529932268 average time 0.06479152319952845 iter num 10\n",
            "loss 0.93461261712946 average time 0.03730574549990706 iter num 20\n",
            "loss 1.5606354281771928 average time 0.028309427466835284 iter num 30\n",
            "loss 0.15091902241692878 average time 0.02386809727504442 iter num 40\n",
            "loss 0.12979437087778933 average time 0.02104998696013354 iter num 50\n",
            "loss 0.29436057957354933 average time 0.019182885650055444 iter num 60\n",
            "loss 0.18620488845044747 average time 0.017894024200099272 iter num 70\n",
            "loss 0.21940941223874688 average time 0.01695708647507672 iter num 80\n",
            "loss 0.283098215732025 average time 0.016208860311194763 iter num 90\n",
            "loss 0.1010717824101448 average time 0.01562550264003221 iter num 100\n",
            "loss 1.1829488357761875 average time 0.06447213599931274 iter num 10\n",
            "loss 1.1780553177231923 average time 0.03752920084989455 iter num 20\n",
            "loss 1.308511127717793 average time 0.028433383766484136 iter num 30\n",
            "loss 1.1894475028384477 average time 0.023893340449922106 iter num 40\n",
            "loss 0.9527698421152309 average time 0.021303634459982276 iter num 50\n",
            "loss 0.14784276572754607 average time 0.01945544225000049 iter num 60\n",
            "loss 0.05183786925044842 average time 0.018121196414306595 iter num 70\n",
            "loss 0.026753275506052887 average time 0.017123790750019907 iter num 80\n",
            "loss 0.16596035493421368 average time 0.016365451533339286 iter num 90\n",
            "loss 0.8513146167388186 average time 0.01575278303997038 iter num 100\n",
            "loss 0.17373871742165647 average time 0.06396482909949555 iter num 10\n",
            "loss 0.11437419743742794 average time 0.03720002964946616 iter num 20\n",
            "loss 1.2808633618988097 average time 0.028226780899300745 iter num 30\n",
            "loss 0.8090107439784333 average time 0.023689500874479564 iter num 40\n",
            "loss 0.13413303349807393 average time 0.021019314219374793 iter num 50\n",
            "loss 0.27307098207529634 average time 0.019191676432698538 iter num 60\n",
            "loss 0.3300647222204134 average time 0.01790154461362233 iter num 70\n",
            "loss 0.20172723452560604 average time 0.016915610624391775 iter num 80\n",
            "loss 0.0913875010155607 average time 0.016119911566096965 iter num 90\n",
            "loss 0.06837733508291421 average time 0.015521764089462521 iter num 100\n",
            "loss 0.23782968128216453 average time 0.06424249470073846 iter num 10\n",
            "loss 1.79085778654553 average time 0.03719661645045562 iter num 20\n",
            "loss 0.17840065993368626 average time 0.02807950610028153 iter num 30\n",
            "loss 1.1974151129834354 average time 0.02361530557527658 iter num 40\n",
            "loss 0.8257681474788114 average time 0.021080175300303382 iter num 50\n",
            "loss 0.2060632687062025 average time 0.019350866033588925 iter num 60\n",
            "loss 0.6394220690708607 average time 0.018002095571708714 iter num 70\n",
            "loss 0.8010031888261437 average time 0.016992566462795365 iter num 80\n",
            "loss 0.5557723488891497 average time 0.01622088752244761 iter num 90\n",
            "loss 1.0823705815710127 average time 0.015591849130141782 iter num 100\n",
            "loss 0.44645319576375186 average time 0.06425003450021904 iter num 10\n",
            "loss 0.37989742850186303 average time 0.03729064369999833 iter num 20\n",
            "loss 0.3481229941826314 average time 0.028132306333403297 iter num 30\n",
            "loss 1.2456804688554257 average time 0.023785693575064215 iter num 40\n",
            "loss 0.7512105366913602 average time 0.020980553399931524 iter num 50\n",
            "loss 0.25967656256398186 average time 0.01921436734986249 iter num 60\n",
            "loss 0.22244008505367674 average time 0.017930964671229177 iter num 70\n",
            "loss 0.4927454574499279 average time 0.016920275074835445 iter num 80\n",
            "loss 0.14038025256013498 average time 0.016172218022095168 iter num 90\n",
            "loss 0.26680831069825217 average time 0.015557614649842434 iter num 100\n",
            "loss 0.5697647429769859 average time 0.06470740539989492 iter num 10\n",
            "loss 0.29005228498135693 average time 0.037300219350254335 iter num 20\n",
            "loss 2.6452686870470643 average time 0.028346198066841074 iter num 30\n",
            "loss 0.32960593671305105 average time 0.023791551625254214 iter num 40\n",
            "loss 0.23203196178656071 average time 0.02101204528022208 iter num 50\n",
            "loss 0.5040545875090174 average time 0.01911939603354161 iter num 60\n",
            "loss 0.12558159141917713 average time 0.017818824214555207 iter num 70\n",
            "loss 0.5517819954548031 average time 0.01683616992518182 iter num 80\n",
            "loss 0.1945427356986329 average time 0.01604700250015109 iter num 90\n",
            "loss 0.48762776714283973 average time 0.015444487940003455 iter num 100\n",
            "loss 0.9775829676073045 average time 0.06419292720020167 iter num 10\n",
            "loss 0.6468139326898381 average time 0.03728674324993335 iter num 20\n",
            "loss 0.47595531214028597 average time 0.028264073566363852 iter num 30\n",
            "loss 0.7461882341885939 average time 0.023717483324708154 iter num 40\n",
            "loss 0.13306596883921884 average time 0.021055032299846062 iter num 50\n",
            "loss 0.25131190341198817 average time 0.01917668904970924 iter num 60\n",
            "loss 0.2815726838889532 average time 0.017861850556853045 iter num 70\n",
            "loss 0.22597880160901695 average time 0.016932039699804592 iter num 80\n",
            "loss 0.12551199688459747 average time 0.01613105006650181 iter num 90\n",
            "loss 3.4740488626994193 average time 0.015575468959868886 iter num 100\n",
            "loss 0.196359287656378 average time 0.06572966339954292 iter num 10\n",
            "loss 0.43123247451148927 average time 0.03795226234997244 iter num 20\n",
            "loss 0.12465075087675359 average time 0.028619626433161707 iter num 30\n",
            "loss 0.7492885924875736 average time 0.024006130899851996 iter num 40\n",
            "loss 0.04581414941640105 average time 0.0212650398400001 iter num 50\n",
            "loss 0.06087524525355548 average time 0.01955764261662504 iter num 60\n",
            "loss 0.11853173418785445 average time 0.018203243742703178 iter num 70\n",
            "loss 0.21716836272389628 average time 0.017187315062437848 iter num 80\n",
            "loss 0.1995101774809882 average time 0.016370631188854328 iter num 90\n",
            "loss 0.4855018050875515 average time 0.01584783863996563 iter num 100\n",
            "loss 0.16085043171187863 average time 0.06518620209935762 iter num 10\n",
            "loss 0.11151621947647072 average time 0.03772626669924648 iter num 20\n",
            "loss 0.17458292859373614 average time 0.02842862406614586 iter num 30\n",
            "loss 0.14792264664720278 average time 0.0238810570995156 iter num 40\n",
            "loss 0.1517733107903041 average time 0.02113655791952624 iter num 50\n",
            "loss 0.17022462998284027 average time 0.019245877116312236 iter num 60\n",
            "loss 0.30552895623259246 average time 0.017976180042595453 iter num 70\n",
            "loss 0.0837650804896839 average time 0.017007254099780766 iter num 80\n",
            "loss 0.2433186637063045 average time 0.016233724921969245 iter num 90\n",
            "loss 0.37289184547262266 average time 0.015600637989809912 iter num 100\n",
            "loss 0.27970687369816005 average time 0.06406512830071734 iter num 10\n",
            "loss 0.5660016540787183 average time 0.037415560650879344 iter num 20\n",
            "loss 0.41446590330451727 average time 0.028242826767260944 iter num 30\n",
            "loss 0.2094090450555086 average time 0.023658410375446692 iter num 40\n",
            "loss 0.07543126230302732 average time 0.020970510740444297 iter num 50\n",
            "loss 0.20591349311871454 average time 0.0191925343169108 iter num 60\n",
            "loss 0.22572616217075847 average time 0.017976752185859368 iter num 70\n",
            "loss 0.14486788131762296 average time 0.01704588137522478 iter num 80\n",
            "loss 0.4356665158411488 average time 0.016242136600259173 iter num 90\n",
            "loss 0.3361424751346931 average time 0.01567063461028738 iter num 100\n",
            "loss 0.2545669303799514 average time 0.064716404500723 iter num 10\n",
            "loss 0.4249103949405253 average time 0.0371897330005595 iter num 20\n",
            "loss 0.3645896867965348 average time 0.02822562516691202 iter num 30\n",
            "loss 0.1770764538377989 average time 0.02372628782532047 iter num 40\n",
            "loss 0.0352177494278294 average time 0.021001552560264827 iter num 50\n",
            "loss 0.12105502719350625 average time 0.019145740616841066 iter num 60\n",
            "loss 0.2688317545107566 average time 0.017831717728716154 iter num 70\n",
            "loss 0.166083118529059 average time 0.016893328212563574 iter num 80\n",
            "loss 0.8759775664657354 average time 0.01610547881112628 iter num 90\n",
            "loss 0.25512181309750304 average time 0.015476905719988282 iter num 100\n",
            "loss 0.3827166074188426 average time 0.06527608770047663 iter num 10\n",
            "loss 0.18079368601320311 average time 0.037885498300420296 iter num 20\n",
            "loss 0.05396002052293625 average time 0.02862556900011744 iter num 30\n",
            "loss 0.6475313421105966 average time 0.02395516779988611 iter num 40\n",
            "loss 0.1055582379194675 average time 0.021114306939853122 iter num 50\n",
            "loss 0.5802558371215127 average time 0.01931876519981112 iter num 60\n",
            "loss 0.14904511772328988 average time 0.017978794742625075 iter num 70\n",
            "loss 0.49048532673623413 average time 0.01698333244985406 iter num 80\n",
            "loss 0.258979998761788 average time 0.016255787810951005 iter num 90\n",
            "loss 0.10271567589370534 average time 0.015681555239862064 iter num 100\n",
            "loss 0.26900748707703315 average time 0.06494803550049255 iter num 10\n",
            "loss 0.04142068974033464 average time 0.0375445457006208 iter num 20\n",
            "loss 0.12872034858446568 average time 0.02836892023345475 iter num 30\n",
            "loss 0.5650854654959403 average time 0.02386554687500393 iter num 40\n",
            "loss 0.21483225282281637 average time 0.021105631120008184 iter num 50\n",
            "loss 0.038414023038058076 average time 0.019493482899997618 iter num 60\n",
            "loss 0.25506615202175453 average time 0.018216379028516323 iter num 70\n",
            "loss 0.08953328688221518 average time 0.017200699475051807 iter num 80\n",
            "loss 0.5727234020014293 average time 0.016392565788959878 iter num 90\n",
            "loss 0.3968954843003303 average time 0.015726306310025393 iter num 100\n",
            "loss 0.17457175999879837 average time 0.06424769110053603 iter num 10\n",
            "loss 0.14147588444757275 average time 0.03716670265039283 iter num 20\n",
            "loss 1.1693258420564234 average time 0.028165499100092955 iter num 30\n",
            "loss 0.48022990085883066 average time 0.023699627125097322 iter num 40\n",
            "loss 0.752908963477239 average time 0.02105933790000563 iter num 50\n",
            "loss 0.17823494999902323 average time 0.019272773266614727 iter num 60\n",
            "loss 0.46427303459495306 average time 0.017987845342837056 iter num 70\n",
            "loss 0.612345029367134 average time 0.016944110049871598 iter num 80\n",
            "loss 0.11427588106016628 average time 0.016242731044379375 iter num 90\n",
            "loss 0.054820093282614835 average time 0.01565771928988397 iter num 100\n",
            "loss 0.1724825233395677 average time 0.06698780760016235 iter num 10\n",
            "loss 0.2690389737836085 average time 0.03864326790062478 iter num 20\n",
            "loss 0.7814023410901427 average time 0.02905458466714966 iter num 30\n",
            "loss 0.9079409937839955 average time 0.02435159167544043 iter num 40\n",
            "loss 0.11340007404214703 average time 0.02150077554033487 iter num 50\n",
            "loss 0.04977000571670942 average time 0.019573571316990033 iter num 60\n",
            "loss 0.06230372491700109 average time 0.018182453985978748 iter num 70\n",
            "loss 0.04718301170214545 average time 0.01716994933776732 iter num 80\n",
            "loss 0.2652513467182871 average time 0.016455603333654128 iter num 90\n",
            "loss 0.9399984264746308 average time 0.015790855540326446 iter num 100\n",
            "loss 0.1849501313699875 average time 0.0646516338998481 iter num 10\n",
            "loss 0.272085344477091 average time 0.03716483639982471 iter num 20\n",
            "loss 1.367716322420165 average time 0.02825838810009979 iter num 30\n",
            "loss 0.0915217515284894 average time 0.02376607297510418 iter num 40\n",
            "loss 0.6942063191672787 average time 0.02101788052015763 iter num 50\n",
            "loss 0.1354309824819211 average time 0.019282233150079264 iter num 60\n",
            "loss 0.1635270928090904 average time 0.017972631857160845 iter num 70\n",
            "loss 0.1599004644958768 average time 0.01695362000004934 iter num 80\n",
            "loss 0.05462160970637342 average time 0.016203436655592263 iter num 90\n",
            "loss 1.7665540508460253 average time 0.01557848559004924 iter num 100\n",
            "loss 0.28144451789557934 average time 0.06458186459967692 iter num 10\n",
            "loss 0.6379120168276131 average time 0.03732352519982669 iter num 20\n",
            "loss 0.07958343303471338 average time 0.028227869499460212 iter num 30\n",
            "loss 0.09193844562105369 average time 0.023736376749548073 iter num 40\n",
            "loss 0.11003932741004974 average time 0.02102211131968943 iter num 50\n",
            "loss 1.305382902501151 average time 0.019206578383151887 iter num 60\n",
            "loss 0.7675575034227222 average time 0.017837447328461816 iter num 70\n",
            "loss 0.16111005606944673 average time 0.01683253726237126 iter num 80\n",
            "loss 0.10537333764659707 average time 0.01612301567769868 iter num 90\n",
            "loss 0.3904279219568707 average time 0.015516301159914292 iter num 100\n",
            "loss 0.5456149665405974 average time 0.06601973279939558 iter num 10\n",
            "loss 0.5372191299102269 average time 0.03807870229957189 iter num 20\n",
            "loss 0.42159783333772793 average time 0.028839635966263207 iter num 30\n",
            "loss 1.204752188641578 average time 0.024113094224685484 iter num 40\n",
            "loss 0.2862212568288669 average time 0.021267202879607795 iter num 50\n",
            "loss 1.0626553557813168 average time 0.019463743382887817 iter num 60\n",
            "loss 0.8069846080616117 average time 0.018167545485422515 iter num 70\n",
            "loss 0.11587428161874413 average time 0.017205675399827668 iter num 80\n",
            "loss 0.5334220622899011 average time 0.016464004510999074 iter num 90\n",
            "loss 0.044035054997948464 average time 0.015912183369910054 iter num 100\n",
            "loss 0.16269339539576322 average time 0.06542343940091086 iter num 10\n",
            "loss 0.6186614336911589 average time 0.03786006225054735 iter num 20\n",
            "loss 0.33676285966066644 average time 0.028797950967054932 iter num 30\n",
            "loss 0.1829019674914889 average time 0.02437288792552863 iter num 40\n",
            "loss 0.7841959450161085 average time 0.021571258240364842 iter num 50\n",
            "loss 0.05397621862357482 average time 0.01965704828350378 iter num 60\n",
            "loss 0.10566845958237536 average time 0.01835192632877027 iter num 70\n",
            "loss 0.7949238352011889 average time 0.017317149725158743 iter num 80\n",
            "loss 0.20678089640568942 average time 0.016550483933396108 iter num 90\n",
            "loss 0.254408223554492 average time 0.016024591190034698 iter num 100\n",
            "loss 0.12931666788063012 average time 0.06522793839976657 iter num 10\n",
            "loss 0.4039616760564968 average time 0.0382024601496596 iter num 20\n",
            "loss 0.7400113099720329 average time 0.028822323400042173 iter num 30\n",
            "loss 1.806831278372556 average time 0.024168282249956975 iter num 40\n",
            "loss 3.5552948247641325 average time 0.021387588520010468 iter num 50\n",
            "loss 1.688197662588209 average time 0.019613909600108552 iter num 60\n",
            "loss 0.6691555608995259 average time 0.018253647957213355 iter num 70\n",
            "loss 0.8886274736141786 average time 0.017251103175158277 iter num 80\n",
            "loss 0.274118847300997 average time 0.016450196222366584 iter num 90\n",
            "loss 0.39511396607849747 average time 0.01583535183017375 iter num 100\n",
            "loss 3.7124764639884233 average time 0.06466246730014973 iter num 10\n",
            "loss 0.6574981671292335 average time 0.03721678769998107 iter num 20\n",
            "loss 0.804947194410488 average time 0.02825719910021386 iter num 30\n",
            "loss 0.9084428893402219 average time 0.02373269932531912 iter num 40\n",
            "loss 0.49163012590724975 average time 0.021083888600114734 iter num 50\n",
            "loss 0.28697018933598883 average time 0.01931179481671279 iter num 60\n",
            "loss 0.41466319089522585 average time 0.017976653457091225 iter num 70\n",
            "loss 0.06472516815847484 average time 0.016942920537485407 iter num 80\n",
            "loss 0.5177705315873027 average time 0.016173370944387797 iter num 90\n",
            "loss 0.16447564121335745 average time 0.015570405749931524 iter num 100\n",
            "loss 0.21771469619125128 average time 0.06422899129975121 iter num 10\n",
            "loss 0.3755135548999533 average time 0.03707293719980953 iter num 20\n",
            "loss 0.13627657608594745 average time 0.028171503766498063 iter num 30\n",
            "loss 0.19363167666597292 average time 0.023743365174868813 iter num 40\n",
            "loss 0.8986394095700234 average time 0.02104687773993646 iter num 50\n",
            "loss 0.14083598216529936 average time 0.019327728116574388 iter num 60\n",
            "loss 0.0755994051360176 average time 0.01796423969998224 iter num 70\n",
            "loss 0.4638370228349231 average time 0.017045288462486496 iter num 80\n",
            "loss 2.4864290026016533 average time 0.01631314802216366 iter num 90\n",
            "loss 0.1588975646882318 average time 0.015744081730008474 iter num 100\n",
            "loss 0.688934960635379 average time 0.06478121640066092 iter num 10\n",
            "loss 0.323295098496601 average time 0.037219227900459374 iter num 20\n",
            "loss 0.9530162787996233 average time 0.028196610400361047 iter num 30\n",
            "loss 0.7112461025826633 average time 0.023681331075204072 iter num 40\n",
            "loss 0.17178954294649884 average time 0.021263105400139468 iter num 50\n",
            "loss 0.16003750715753995 average time 0.01940129531679607 iter num 60\n",
            "loss 0.22915435692993924 average time 0.018059768871483226 iter num 70\n",
            "loss 0.4165302379988134 average time 0.01703428621249259 iter num 80\n",
            "loss 0.022899976102053188 average time 0.016282336688891518 iter num 90\n",
            "loss 0.22355483451974578 average time 0.01567589427995699 iter num 100\n",
            "loss 0.2614580989757087 average time 0.06477854020013182 iter num 10\n",
            "loss 0.3526735963532701 average time 0.037305080849910154 iter num 20\n",
            "loss 0.8734392031328753 average time 0.028188698833279582 iter num 30\n",
            "loss 2.4555486743338406 average time 0.02364858054997967 iter num 40\n",
            "loss 0.19267219613539055 average time 0.02101359219988808 iter num 50\n",
            "loss 0.08544478987460025 average time 0.019169840566731485 iter num 60\n",
            "loss 0.0942098722589435 average time 0.01786614701439768 iter num 70\n",
            "loss 0.17105954611906782 average time 0.016915686725133126 iter num 80\n",
            "loss 1.1209040530957282 average time 0.01615914251120153 iter num 90\n",
            "loss 0.17900920283864252 average time 0.015562754740058153 iter num 100\n",
            "loss 0.07719157110841479 average time 0.06376945459996933 iter num 10\n",
            "loss 0.6193103763507679 average time 0.03688953955024772 iter num 20\n",
            "loss 0.5228259396972135 average time 0.02809617476668791 iter num 30\n",
            "loss 0.4151389657636173 average time 0.023688876350024656 iter num 40\n",
            "loss 0.32433039450552315 average time 0.020941487380041507 iter num 50\n",
            "loss 0.3905035919160582 average time 0.019108709966591655 iter num 60\n",
            "loss 0.2213390871474985 average time 0.0178161996712983 iter num 70\n",
            "loss 0.11794169040513225 average time 0.016799111799809908 iter num 80\n",
            "loss 0.16698160834494047 average time 0.016061113299898958 iter num 90\n",
            "loss 0.10772626410471275 average time 0.01547884241990687 iter num 100\n",
            "loss 0.043974496293230914 average time 0.0648863293004979 iter num 10\n",
            "loss 0.1716570113785565 average time 0.03771210730028542 iter num 20\n",
            "loss 0.15063889804878272 average time 0.028584157133567108 iter num 30\n",
            "loss 0.14718683814862743 average time 0.02403009332520014 iter num 40\n",
            "loss 0.08662809705128893 average time 0.021190713460091503 iter num 50\n",
            "loss 0.2754344131972175 average time 0.019388448283461913 iter num 60\n",
            "loss 0.10327577911084518 average time 0.018009775800185576 iter num 70\n",
            "loss 0.9356201917398721 average time 0.01707512453772324 iter num 80\n",
            "loss 0.04139796601521084 average time 0.016278356633564625 iter num 90\n",
            "loss 0.3983436181442812 average time 0.015707654270227068 iter num 100\n",
            "loss 0.9500621672486886 average time 0.06415455579954141 iter num 10\n",
            "loss 0.48899171815719455 average time 0.03689962599983119 iter num 20\n",
            "loss 1.0223290155408904 average time 0.027969281233405734 iter num 30\n",
            "loss 0.2943125036836136 average time 0.023566559525170304 iter num 40\n",
            "loss 1.413283753208816 average time 0.020796014300067327 iter num 50\n",
            "loss 0.16872836567927152 average time 0.019008828083436433 iter num 60\n",
            "loss 0.27495068934513256 average time 0.01773315840010972 iter num 70\n",
            "loss 0.016548469830013346 average time 0.016813811700103543 iter num 80\n",
            "loss 0.14772220310987905 average time 0.016063708133394052 iter num 90\n",
            "loss 0.19085440726485103 average time 0.015469953060091938 iter num 100\n",
            "loss 0.17394399037584662 average time 0.0649257150998892 iter num 10\n",
            "loss 0.3527983062667772 average time 0.03742707474993949 iter num 20\n",
            "loss 0.15913152310531586 average time 0.02831736726645128 iter num 30\n",
            "loss 0.18714212274062447 average time 0.023807645099805087 iter num 40\n",
            "loss 0.15027243534859736 average time 0.02107240897981683 iter num 50\n",
            "loss 0.1289823467232054 average time 0.019192825016458907 iter num 60\n",
            "loss 0.24022241632337682 average time 0.017880728728362424 iter num 70\n",
            "loss 0.288913488475373 average time 0.01689091083731 iter num 80\n",
            "loss 0.09577868695487268 average time 0.016153048299949863 iter num 90\n",
            "loss 0.053045173444843385 average time 0.015514765139923838 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872bee0c-e34b-4887-fff7-a935f0579411"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 0.8, 0.8, 0.25, 0.05, 0.05]*3]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2,8,14]]\n",
        "\n",
        "# price, delta1, delta2, delta3\n",
        "# should be around (0.067710705, 0.22125466 , 0.22136934 , 0.22104672)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.0582]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2135, 0.2105, 0.2070], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_2AXrPt7bNj",
        "outputId": "60f10c94-351e-4dbf-bb67-90334f8c7f03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "numstocks = 3\n",
        "numsteps = 50\n",
        "numpaths = 100000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([0.8]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.8\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.067794845\n",
            "[0.22087786 0.22019869 0.220483  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "outputId": "02093add-ecc3-42ae-fd3a-67e2d3a71a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.8, S, 0.25, 0.05, 0.05] + ([1, 0.8, 0.8, 0.25, 0.05, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(0, 1, 0.01)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1e1a4bff90>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}