{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「Grid Test_Knock Out Call 1stock Monte Carlo」的副本",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Judy/Grid_Test_Knock_Out_Call_1stock_Monte_Carlo_SGD_Judy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYigDkiy0HU9",
        "outputId": "e0c3b860-737f-466a-875b-80441b966805"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 8)\n",
        "K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "B_range = jnp.linspace(0.5, 1.0, 5)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "\n",
        "print(S_range)\n",
        "print(K_range)\n",
        "print(B_range)\n",
        "print(sigma_range)\n",
        "print(r_range)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75       0.82142854 0.89285713 0.9642857  1.0357143  1.1071429\n",
            " 1.1785713  1.25      ]\n",
            "[0.75  0.875 1.    1.125 1.25 ]\n",
            "[0.5   0.625 0.75  0.875 1.   ]\n",
            "[0.15       0.25       0.35000002 0.45      ]\n",
            "[0.01  0.025 0.04 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQxpJqK6OZr"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "\n",
        "goptionvalueavg = jax.grad(optionvalueavg, argnums=1)\n",
        "\n",
        "#################################################################### Adjust all parameters here (not inside class)\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 8)\n",
        "K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "B_range = jnp.linspace(0.5, 1.0, 5)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "T = 1.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "####################################################################\n",
        "\n",
        "call = []\n",
        "count = 0\n",
        "\n",
        "for S in S_range:\n",
        "  for K in K_range:\n",
        "    for B in B_range:\n",
        "      for r in r_range:\n",
        "        for sigma in sigma_range:    \n",
        "\n",
        "          initial_stocks = jnp.array([S]*numstocks) # must be float\n",
        "          r_tmp = jnp.array([r]*numstocks)\n",
        "          drift = r_tmp\n",
        "          cov = jnp.identity(numstocks)*sigma*sigma\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "          Deltas = goptionvalueavg(keys, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "          call.append([T, K, B, S, sigma, r, r, European_Call_price] + list(Deltas)) #T, K, B, S, sigma, mu, r, price, delta\n",
        "          \n",
        "          count += 1\n",
        "          print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "e_OUtP8GUwj5",
        "outputId": "9c73750d-5f32-428c-9697-23ecc7102976"
      },
      "source": [
        "Thedataset = pd.DataFrame(call)\n",
        "Thedataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.04829822</td>\n",
              "      <td>0.555157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.07776406</td>\n",
              "      <td>0.563845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.10653644</td>\n",
              "      <td>0.572229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.13306102</td>\n",
              "      <td>0.569618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.053997554</td>\n",
              "      <td>0.594301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2395</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.19972013</td>\n",
              "      <td>0.471407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.10002679</td>\n",
              "      <td>0.631672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2397</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.1442415</td>\n",
              "      <td>0.583131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2398</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.17972831</td>\n",
              "      <td>0.529840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2399</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.20697877</td>\n",
              "      <td>0.482881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2400 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6            7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010   0.04829822  0.555157\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010   0.07776406  0.563845\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010   0.10653644  0.572229\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010   0.13306102  0.569618\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.053997554  0.594301\n",
              "...   ...   ...  ...   ...   ...    ...    ...          ...       ...\n",
              "2395  1.0  1.25  1.0  1.25  0.45  0.025  0.025   0.19972013  0.471407\n",
              "2396  1.0  1.25  1.0  1.25  0.15  0.040  0.040   0.10002679  0.631672\n",
              "2397  1.0  1.25  1.0  1.25  0.25  0.040  0.040    0.1442415  0.583131\n",
              "2398  1.0  1.25  1.0  1.25  0.35  0.040  0.040   0.17972831  0.529840\n",
              "2399  1.0  1.25  1.0  1.25  0.45  0.040  0.040   0.20697877  0.482881\n",
              "\n",
              "[2400 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQKnflf6peX"
      },
      "source": [
        "# save to csv\n",
        "Thedataset.to_csv('Knock_Out_Call_1stock_MC_Datset_v2.csv', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "skGWSSsG8TGG",
        "outputId": "f589689f-1293-4cd3-c7d4-de8fa9501885"
      },
      "source": [
        "# read csv\n",
        "import pandas as pd\n",
        "\n",
        "Thedataset = pd.read_csv('Knock_Out_Call_1stock_MC_Datset.csv', header=None)\n",
        "Thedataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.048510</td>\n",
              "      <td>0.556664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.078121</td>\n",
              "      <td>0.565542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.107045</td>\n",
              "      <td>0.574232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.133747</td>\n",
              "      <td>0.572047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.054222</td>\n",
              "      <td>0.595631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.200693</td>\n",
              "      <td>0.473159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.100417</td>\n",
              "      <td>0.632826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.144892</td>\n",
              "      <td>0.584785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.180591</td>\n",
              "      <td>0.531644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.207978</td>\n",
              "      <td>0.484591</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1536 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6         7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010  0.048510  0.556664\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010  0.078121  0.565542\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010  0.107045  0.574232\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010  0.133747  0.572047\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.054222  0.595631\n",
              "...   ...   ...  ...   ...   ...    ...    ...       ...       ...\n",
              "1531  1.0  1.25  1.0  1.25  0.45  0.025  0.025  0.200693  0.473159\n",
              "1532  1.0  1.25  1.0  1.25  0.15  0.040  0.040  0.100417  0.632826\n",
              "1533  1.0  1.25  1.0  1.25  0.25  0.040  0.040  0.144892  0.584785\n",
              "1534  1.0  1.25  1.0  1.25  0.35  0.040  0.040  0.180591  0.531644\n",
              "1535  1.0  1.25  1.0  1.25  0.45  0.040  0.040  0.207978  0.484591\n",
              "\n",
              "[1536 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4HV-G44th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1896761-96bb-4783-f4f7-bede4a71c3ef"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch\n",
        "torch.set_printoptions(precision=6)\n",
        "Thedataset = pd.read_csv('Knock_Out_Call_1stock_MC_Datset.csv', header=None)\n",
        "Thedataset_X = Thedataset.iloc[:,:7]\n",
        "Thedataset_Y = Thedataset.iloc[:,7:]\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.X = cupy.array(Thedataset_X)\n",
        "        self.Y = cupy.array(Thedataset_Y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(self.X.toDlpack()), from_dlpack(self.Y.toDlpack()))\n",
        "\n",
        "# print\n",
        "# ds = OptionDataSet(max_len = 1)\n",
        "# for i in ds:\n",
        "#     print(i[0])\n",
        "#     print(i[0].shape)\n",
        "#     print(i[1])\n",
        "#     print(i[1].shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "3728162b-f5f9-40b7-9613-e72d852e8c8b"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(7*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.5, 0.3, 0.03, 0.03]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, B, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.5, 0.75, 0.15, 0.01, 0.01]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "2b229fa7-6055-4b3c-fd5e-05adc388a5d3"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 11.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeLVZiiaDS4y",
        "outputId": "de36f93e-0689-4c4e-e704-68c01b110ecb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CyULkENYKb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94ed8f5d-469e-4b37-9738-ec153e88c410"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import SGD\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = SGD(model.parameters(), lr=1e-3) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    # print(x.shape)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "    y_pred = model(x.float())\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]  # Now index 3 is stock price, not 2\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_SGD_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.16174215223005395 average time 2.2934526660999976 iter num 20\n",
            "loss 0.1617256477824784 average time 2.299380876074996 iter num 40\n",
            "loss 0.16171492063456486 average time 2.2979287415333287 iter num 60\n",
            "loss 0.1617101558535418 average time 2.302521453024993 iter num 80\n",
            "loss 0.161709307860244 average time 2.302753612559995 iter num 100\n",
            "loss 0.16169058947380333 average time 2.3104833722499962 iter num 20\n",
            "loss 0.16167429650592863 average time 2.314886290949997 iter num 40\n",
            "loss 0.16166370761925225 average time 2.305634159400002 iter num 60\n",
            "loss 0.16165900470564598 average time 2.305852455274997 iter num 80\n",
            "loss 0.16165817057276746 average time 2.3070976614699963 iter num 100\n",
            "loss 0.16163969375786463 average time 2.3321345370500013 iter num 20\n",
            "loss 0.16162360449203816 average time 2.3266342223249894 iter num 40\n",
            "loss 0.16161314989061254 average time 2.334613928066659 iter num 60\n",
            "loss 0.1616085070680938 average time 2.3296324320999973 iter num 80\n",
            "loss 0.16160768634108144 average time 2.325242324119996 iter num 100\n",
            "loss 0.16158944238107206 average time 2.3166773734500112 iter num 20\n",
            "loss 0.16157355691399078 average time 2.3312035212250066 iter num 40\n",
            "loss 0.16156323279695944 average time 2.334412614200005 iter num 60\n",
            "loss 0.16155865030211575 average time 2.3240930742500012 iter num 80\n",
            "loss 0.16155784305596704 average time 2.3178488040199934 iter num 100\n",
            "loss 0.16153982954569804 average time 2.326503148300037 iter num 20\n",
            "loss 0.1615241433643929 average time 2.3415248308750165 iter num 40\n",
            "loss 0.1615139495214748 average time 2.3393579573166865 iter num 60\n",
            "loss 0.16150942594929615 average time 2.3317658314250225 iter num 80\n",
            "loss 0.16150863098928958 average time 2.3313663889500185 iter num 100\n",
            "loss 0.16149084413430398 average time 2.3371640092500003 iter num 20\n",
            "loss 0.16147535126106854 average time 2.3384389399500036 iter num 40\n",
            "loss 0.16146528232432605 average time 2.3346818056333367 iter num 60\n",
            "loss 0.16146082024226732 average time 2.3398499697000004 iter num 80\n",
            "loss 0.16146003654375873 average time 2.3422327278999955 iter num 100\n",
            "loss 0.16144246823486458 average time 2.2929645751500174 iter num 20\n",
            "loss 0.16142716585119948 average time 2.3093815299749907 iter num 40\n",
            "loss 0.16141722005972323 average time 2.3150282488666636 iter num 60\n",
            "loss 0.1614128134368777 average time 2.312550786712498 iter num 80\n",
            "loss 0.16141204247777394 average time 2.313342219039985 iter num 100\n",
            "loss 0.16139468430362847 average time 2.295055047599999 iter num 20\n",
            "loss 0.1613795739651861 average time 2.2931917355999643 iter num 40\n",
            "loss 0.16136974961746828 average time 2.3277000812666455 iter num 60\n",
            "loss 0.16136539834747288 average time 2.3216247948124815 iter num 80\n",
            "loss 0.16136464218805754 average time 2.3181979258599723 iter num 100\n",
            "loss 0.16134749928738296 average time 2.326587772500011 iter num 20\n",
            "loss 0.1613325694556049 average time 2.3106373310750086 iter num 40\n",
            "loss 0.16132286617727148 average time 2.3013532882333343 iter num 60\n",
            "loss 0.16131856691229338 average time 2.2950365023375014 iter num 80\n",
            "loss 0.161317820507699 average time 2.2960204822200057 iter num 100\n",
            "loss 0.1613008917225808 average time 2.307961612300005 iter num 20\n",
            "loss 0.16128614078435607 average time 2.2761171984749806 iter num 40\n",
            "loss 0.16127655459994683 average time 2.2798274206999545 iter num 60\n",
            "loss 0.16127231075892776 average time 2.277030992487448 iter num 80\n",
            "loss 0.16127157587753546 average time 2.2794187274599746 iter num 100\n",
            "loss 0.16125485279474297 average time 2.2595739021500094 iter num 20\n",
            "loss 0.16124027583563413 average time 2.2631507800500117 iter num 40\n",
            "loss 0.16123080548256916 average time 2.2804275440500303 iter num 60\n",
            "loss 0.16122661617160627 average time 2.278189561125015 iter num 80\n",
            "loss 0.16122589357486067 average time 2.288109692390017 iter num 100\n",
            "loss 0.16120937216186065 average time 2.3038136737000285 iter num 20\n",
            "loss 0.1611949673888699 average time 2.299931949450013 iter num 40\n",
            "loss 0.16118561197756856 average time 2.2901254052000164 iter num 60\n",
            "loss 0.1611814753544852 average time 2.287616174825007 iter num 80\n",
            "loss 0.16118076237573226 average time 2.2955547704900026 iter num 100\n",
            "loss 0.1611644358774207 average time 2.3004923900000223 iter num 20\n",
            "loss 0.1611502041025616 average time 2.3061142271250334 iter num 40\n",
            "loss 0.16114095850654864 average time 2.2868125723000126 iter num 60\n",
            "loss 0.16113686824442183 average time 2.2782480456000087 iter num 80\n",
            "loss 0.1611361672325402 average time 2.2729096728400098 iter num 100\n",
            "loss 0.16112002980980006 average time 2.248203399849922 iter num 20\n",
            "loss 0.1611059623244572 average time 2.2688345924749798 iter num 40\n",
            "loss 0.16109682710195528 average time 2.2869534361499975 iter num 60\n",
            "loss 0.1610927859984775 average time 2.2895007160625083 iter num 80\n",
            "loss 0.16109209469816713 average time 2.287945774269988 iter num 100\n",
            "loss 0.16107613696190187 average time 2.2639952034500084 iter num 20\n",
            "loss 0.1610622333594057 average time 2.273282361799988 iter num 40\n",
            "loss 0.16105320284762464 average time 2.2799155198500025 iter num 60\n",
            "loss 0.16104921003025055 average time 2.279874683924999 iter num 80\n",
            "loss 0.16104852810399065 average time 2.28468183244001 iter num 100\n",
            "loss 0.16103275122394362 average time 2.302938446049984 iter num 20\n",
            "loss 0.1610190073085307 average time 2.2729671290500164 iter num 40\n",
            "loss 0.1610100824735399 average time 2.2730317350833427 iter num 60\n",
            "loss 0.1610061397396728 average time 2.2755355020124965 iter num 80\n",
            "loss 0.16100546821891254 average time 2.2688302468300026 iter num 100\n",
            "loss 0.16098986894311304 average time 2.3331564198999786 iter num 20\n",
            "loss 0.16097628509298914 average time 2.3120755924750482 iter num 40\n",
            "loss 0.1609674617998071 average time 2.2944974244334086 iter num 60\n",
            "loss 0.1609635640657282 average time 2.2845895649125394 iter num 80\n",
            "loss 0.16096290217312603 average time 2.2866126731300485 iter num 100\n",
            "loss 0.16094748030067924 average time 2.269545644600066 iter num 20\n",
            "loss 0.16093405288924081 average time 2.260386433125018 iter num 40\n",
            "loss 0.16092533099488573 average time 2.2647356602333124 iter num 60\n",
            "loss 0.16092147875665858 average time 2.269728830024974 iter num 80\n",
            "loss 0.1609208284166027 average time 2.270527763669979 iter num 100\n",
            "loss 0.16090557834915292 average time 2.3255415836000792 iter num 20\n",
            "loss 0.1608923056372666 average time 2.2907010984750285 iter num 40\n",
            "loss 0.16088367929239905 average time 2.2889017275000243 iter num 60\n",
            "loss 0.16087987143362276 average time 2.2846595984375426 iter num 80\n",
            "loss 0.16087922867588336 average time 2.2826923715300564 iter num 100\n",
            "loss 0.16086415439422858 average time 2.2799250357499203 iter num 20\n",
            "loss 0.16085102896075523 average time 2.284949942599951 iter num 40\n",
            "loss 0.16084249531221298 average time 2.291639943516672 iter num 60\n",
            "loss 0.160838733236328 average time 2.2969382343499545 iter num 80\n",
            "loss 0.16083810109875174 average time 2.3033649205399525 iter num 100\n",
            "loss 0.16082319345647458 average time 2.294132112249963 iter num 20\n",
            "loss 0.16081021294115877 average time 2.282200624925008 iter num 40\n",
            "loss 0.16080177765544204 average time 2.280758001999963 iter num 60\n",
            "loss 0.16079805830235316 average time 2.2754411644249783 iter num 80\n",
            "loss 0.16079743691676163 average time 2.281233580299977 iter num 100\n",
            "loss 0.16078269125034808 average time 2.3405709175498486 iter num 20\n",
            "loss 0.1607698534504222 average time 2.3254582910499266 iter num 40\n",
            "loss 0.16076150991384555 average time 2.31564457123327 iter num 60\n",
            "loss 0.1607578330434281 average time 2.3093164792249694 iter num 80\n",
            "loss 0.16075722081418156 average time 2.3070529454599726 iter num 100\n",
            "loss 0.1607426352940873 average time 2.2673753774500254 iter num 20\n",
            "loss 0.1607299374347292 average time 2.288571116999992 iter num 40\n",
            "loss 0.160721684768577 average time 2.29074189738335 iter num 60\n",
            "loss 0.1607180466470966 average time 2.2946476302500174 iter num 80\n",
            "loss 0.16071744285435585 average time 2.2854587922300196 iter num 100\n",
            "loss 0.16070301487981592 average time 2.334583689100009 iter num 20\n",
            "loss 0.160690457460505 average time 2.306456067299996 iter num 40\n",
            "loss 0.16068229364840392 average time 2.3074478306166837 iter num 60\n",
            "loss 0.16067869746571004 average time 2.3077666774750014 iter num 80\n",
            "loss 0.16067810330614474 average time 2.2947080818500125 iter num 100\n",
            "loss 0.16066383312428534 average time 2.2215341487998104 iter num 20\n",
            "loss 0.16065141188099308 average time 2.23510275787487 iter num 40\n",
            "loss 0.16064333803043562 average time 2.2397238199332605 iter num 60\n",
            "loss 0.1606397829565865 average time 2.255274548287434 iter num 80\n",
            "loss 0.16063919560471315 average time 2.259910109709972 iter num 100\n",
            "loss 0.16062507926751665 average time 2.252298791049816 iter num 20\n",
            "loss 0.16061279080052454 average time 2.2713086634749287 iter num 40\n",
            "loss 0.1606047992802033 average time 2.2788476701665785 iter num 60\n",
            "loss 0.16060128288672373 average time 2.2846587144373984 iter num 80\n",
            "loss 0.16060070447567046 average time 2.2822182297899234 iter num 100\n",
            "loss 0.16058674072310544 average time 2.2872010086500723 iter num 20\n",
            "loss 0.16057457986509535 average time 2.2832563836750976 iter num 40\n",
            "loss 0.16056667323286097 average time 2.2919828098834385 iter num 60\n",
            "loss 0.1605631928858072 average time 2.2798444040250727 iter num 80\n",
            "loss 0.16056262349247946 average time 2.275500946360062 iter num 100\n",
            "loss 0.1605488085531631 average time 2.2642491996500214 iter num 20\n",
            "loss 0.1605367767387013 average time 2.2481963039000674 iter num 40\n",
            "loss 0.160528955274437 average time 2.263280159566693 iter num 60\n",
            "loss 0.16052551307012716 average time 2.268160342737519 iter num 80\n",
            "loss 0.16052495050161403 average time 2.2620157496399953 iter num 100\n",
            "loss 0.1605112860101836 average time 2.2842527299500945 iter num 20\n",
            "loss 0.16049937844018866 average time 2.270873855075115 iter num 40\n",
            "loss 0.1604916373495196 average time 2.2732441320833914 iter num 60\n",
            "loss 0.16048823320990008 average time 2.272339588162538 iter num 80\n",
            "loss 0.16048767872385145 average time 2.2714174370000273 iter num 100\n",
            "loss 0.16047415380325403 average time 2.2856273657000656 iter num 20\n",
            "loss 0.1604623679454144 average time 2.2680548140250947 iter num 40\n",
            "loss 0.16045470738659126 average time 2.265050955416761 iter num 60\n",
            "loss 0.16045134153390725 average time 2.2629372476875576 iter num 80\n",
            "loss 0.1604507940018904 average time 2.261421645860055 iter num 100\n",
            "loss 0.1604374066010672 average time 2.254825915250194 iter num 20\n",
            "loss 0.16042573993343204 average time 2.2711923300001673 iter num 40\n",
            "loss 0.16041815781934848 average time 2.2817792433667363 iter num 60\n",
            "loss 0.16041482623524017 average time 2.2844189788375386 iter num 80\n",
            "loss 0.16041428908111763 average time 2.2835839087500425 iter num 100\n",
            "loss 0.1604010332960072 average time 2.264135863249885 iter num 20\n",
            "loss 0.1603894884847717 average time 2.276067210624933 iter num 40\n",
            "loss 0.1603819805588479 average time 2.2748295869332953 iter num 60\n",
            "loss 0.16037868340882475 average time 2.2835555420624702 iter num 80\n",
            "loss 0.16037815303617386 average time 2.281953729689976 iter num 100\n",
            "loss 0.16036503237411226 average time 2.273026477199983 iter num 20\n",
            "loss 0.160353601476102 average time 2.299004659250045 iter num 40\n",
            "loss 0.16034616933551385 average time 2.304196659850019 iter num 60\n",
            "loss 0.16034290433315163 average time 2.3005490388375165 iter num 80\n",
            "loss 0.16034238048474836 average time 2.29710773285 iter num 100\n",
            "loss 0.16032938808884512 average time 2.2950404204999812 iter num 20\n",
            "loss 0.16031807083757948 average time 2.2841457368750526 iter num 40\n",
            "loss 0.16031071307623604 average time 2.301226731483378 iter num 60\n",
            "loss 0.1603074841413754 average time 2.296416982250025 iter num 80\n",
            "loss 0.1603069695442878 average time 2.2955547021900564 iter num 100\n",
            "loss 0.16029410847209216 average time 2.2501627301000555 iter num 20\n",
            "loss 0.16028290369298434 average time 2.246740575099966 iter num 40\n",
            "loss 0.1602756179526126 average time 2.256165814649921 iter num 60\n",
            "loss 0.16027242154612217 average time 2.2558386468498837 iter num 80\n",
            "loss 0.16027191228918344 average time 2.261748044289925 iter num 100\n",
            "loss 0.16025917674773085 average time 2.2815793477002444 iter num 20\n",
            "loss 0.1602480841506258 average time 2.288399419574989 iter num 40\n",
            "loss 0.1602408665701757 average time 2.2814552142165363 iter num 60\n",
            "loss 0.1602377035796019 average time 2.275233193362419 iter num 80\n",
            "loss 0.16023720240025469 average time 2.2676070315499417 iter num 100\n",
            "loss 0.16022459115080318 average time 2.312933308949687 iter num 20\n",
            "loss 0.16021360336861107 average time 2.298717698849896 iter num 40\n",
            "loss 0.16020645202382336 average time 2.287758268766538 iter num 60\n",
            "loss 0.16020332063220344 average time 2.295874521374958 iter num 80\n",
            "loss 0.16020282716855883 average time 2.296559770459953 iter num 100\n",
            "loss 0.1601903374087012 average time 2.2824026939998476 iter num 20\n",
            "loss 0.16017945345276563 average time 2.282651373974841 iter num 40\n",
            "loss 0.16017237522397434 average time 2.2771650945165676 iter num 60\n",
            "loss 0.16016927338747192 average time 2.2773269634373947 iter num 80\n",
            "loss 0.16016878754824607 average time 2.2827705936199165 iter num 100\n",
            "loss 0.1601564143980034 average time 2.286715554950206 iter num 20\n",
            "loss 0.16014563264170634 average time 2.263436873525143 iter num 40\n",
            "loss 0.1601386182658106 average time 2.2790105547834174 iter num 60\n",
            "loss 0.1601355472027792 average time 2.279235505237557 iter num 80\n",
            "loss 0.16013506596221636 average time 2.2737704177300837 iter num 100\n",
            "loss 0.16012280852281469 average time 2.257252033200075 iter num 20\n",
            "loss 0.1601121279811522 average time 2.25476967970003 iter num 40\n",
            "loss 0.1601051805333669 average time 2.270449727283388 iter num 60\n",
            "loss 0.16010213864102968 average time 2.2747293113750855 iter num 80\n",
            "loss 0.16010166428828843 average time 2.2732815799900528 iter num 100\n",
            "loss 0.16008952413852323 average time 2.2805432922499675 iter num 20\n",
            "loss 0.16007893945611748 average time 2.3109705963750002 iter num 40\n",
            "loss 0.1600720609949798 average time 2.2937308224333415 iter num 60\n",
            "loss 0.1600690501390628 average time 2.2844635143250343 iter num 80\n",
            "loss 0.16006858249864447 average time 2.2759145479700784 iter num 100\n",
            "loss 0.1600565558334315 average time 2.2690855596502844 iter num 20\n",
            "loss 0.1600460665475935 average time 2.2687998753500325 iter num 40\n",
            "loss 0.16003924970763517 average time 2.272496890383324 iter num 60\n",
            "loss 0.16003626958356704 average time 2.28542239423748 iter num 80\n",
            "loss 0.1600358076900835 average time 2.293803149939995 iter num 100\n",
            "loss 0.1600238870286437 average time 2.3058347932001197 iter num 20\n",
            "loss 0.16001349186461541 average time 2.290047415375011 iter num 40\n",
            "loss 0.16000673708004104 average time 2.2858737273166603 iter num 60\n",
            "loss 0.1600037847358653 average time 2.2831861834750726 iter num 80\n",
            "loss 0.16000332989604735 average time 2.2866550024500976 iter num 100\n",
            "loss 0.15999151833098987 average time 2.274656274300014 iter num 20\n",
            "loss 0.15998121661041118 average time 2.297048236974979 iter num 40\n",
            "loss 0.15997452461839523 average time 2.2798264886333226 iter num 60\n",
            "loss 0.1599716004124877 average time 2.2811364059624792 iter num 80\n",
            "loss 0.15997115145691967 average time 2.2884149994299516 iter num 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-89d0e220eeb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_MC_oneDS_SGD_1.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-89d0e220eeb8>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# print(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-89d0e220eeb8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# print(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-89d0e220eeb8>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_fEzULvKwR-"
      },
      "source": [
        "# 2hr 30min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3be0cc-7ec7-4b6f-f8b8-dc8109dcbf5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_SGD_5.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df2b39b-c8f3-4553-87df-67ffdfc6dc90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78146a25-794a-47c8-b198-22b22a40a5a7"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_SGD_4.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67f1c04-d3bc-4570-b363-e2e8c33f7350"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=7, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd285d7f-b1c7-4954-8bfb-bec0285e7509"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import SGD\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = SGD(model.parameters(), lr=1e-3) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_SGD_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.15995504071374575 average time 2.2831595504999314 iter num 20\n",
            "loss 0.15994484471269058 average time 2.304582312550019 iter num 40\n",
            "loss 0.15993822204433816 average time 2.310298074066668 iter num 60\n",
            "loss 0.15993532686431042 average time 2.310493552337448 iter num 80\n",
            "loss 0.15993488526681204 average time 2.3114961413299717 iter num 100\n",
            "loss 0.15992329688617815 average time 2.2744791073999293 iter num 20\n",
            "loss 0.15991319145485988 average time 2.276531232374964 iter num 40\n",
            "loss 0.15990662736296946 average time 2.283996195199961 iter num 60\n",
            "loss 0.15990376008997123 average time 2.2919434673499834 iter num 80\n",
            "loss 0.15990332323640477 average time 2.2933436218300085 iter num 100\n",
            "loss 0.15989184081389507 average time 2.2939314872998695 iter num 20\n",
            "loss 0.15988182380219002 average time 2.3117554854998614 iter num 40\n",
            "loss 0.15987531777649905 average time 2.3219858975499847 iter num 60\n",
            "loss 0.15987248059239306 average time 2.316892886612391 iter num 80\n",
            "loss 0.1598720492473847 average time 2.3162921540498975 iter num 100\n",
            "loss 0.15986067192987524 average time 2.2870992366496465 iter num 20\n",
            "loss 0.15985074434978713 average time 2.2896302364998062 iter num 40\n",
            "loss 0.15984429689671634 average time 2.2922122390831947 iter num 60\n",
            "loss 0.15984148443187238 average time 2.2988630441499254 iter num 80\n",
            "loss 0.15984105850353544 average time 2.303971400589926 iter num 100\n",
            "loss 0.1598297845947043 average time 2.283750419900116 iter num 20\n",
            "loss 0.1598199431038948 average time 2.2900469699500263 iter num 40\n",
            "loss 0.15981355201205502 average time 2.2861253888832835 iter num 60\n",
            "loss 0.15981076533294794 average time 2.287702309449969 iter num 80\n",
            "loss 0.1598103439268819 average time 2.2958492282899714 iter num 100\n",
            "loss 0.15979917169571256 average time 2.317497447450023 iter num 20\n",
            "loss 0.15978941473419928 average time 2.3146685409500605 iter num 40\n",
            "loss 0.15978307821311408 average time 2.2983504824499503 iter num 60\n",
            "loss 0.15978031874765292 average time 2.3080822594500203 iter num 80\n",
            "loss 0.15977990343156476 average time 2.2993372677900332 iter num 100\n",
            "loss 0.15976882782572863 average time 2.3333426136498927 iter num 20\n",
            "loss 0.1597591568042298 average time 2.332785325449959 iter num 40\n",
            "loss 0.15975287398179486 average time 2.338645238483241 iter num 60\n",
            "loss 0.159750139124469 average time 2.3353771411124624 iter num 80\n",
            "loss 0.15974973120300148 average time 2.3312054141000225 iter num 100\n",
            "loss 0.15973874251921186 average time 2.3121593675999974 iter num 20\n",
            "loss 0.1597291558516925 average time 2.3122418893749455 iter num 40\n",
            "loss 0.15972292716875108 average time 2.3157670448666674 iter num 60\n",
            "loss 0.15972021814748044 average time 2.3218272403375066 iter num 80\n",
            "loss 0.15971981409799338 average time 2.32131475967999 iter num 100\n",
            "loss 0.15970891852506183 average time 2.3250010262498106 iter num 20\n",
            "loss 0.15969941518891337 average time 2.3256342935998875 iter num 40\n",
            "loss 0.15969323853831316 average time 2.319150218249918 iter num 60\n",
            "loss 0.1596905509323981 average time 2.3153872532749573 iter num 80\n",
            "loss 0.15969015208410475 average time 2.3167861617199925 iter num 100\n",
            "loss 0.1596793457769439 average time 2.3236309507497936 iter num 20\n",
            "loss 0.15966991948531462 average time 2.3208249030499246 iter num 40\n",
            "loss 0.15966379568736175 average time 2.3307648531832457 iter num 60\n",
            "loss 0.15966113010840544 average time 2.330086693387352 iter num 80\n",
            "loss 0.15966073797566566 average time 2.3271053606999157 iter num 100\n",
            "loss 0.1596500177178327 average time 2.3392119481497504 iter num 20\n",
            "loss 0.15964066869971202 average time 2.3348331323748424 iter num 40\n",
            "loss 0.15963459466221483 average time 2.331365927933287 iter num 60\n",
            "loss 0.15963195187640186 average time 2.32359311849998 iter num 80\n",
            "loss 0.1596315656536621 average time 2.321345978219997 iter num 100\n",
            "loss 0.15962093350841175 average time 2.3112805854505494 iter num 20\n",
            "loss 0.1596116580191915 average time 2.3090368185502483 iter num 40\n",
            "loss 0.15960563152271018 average time 2.320317802233573 iter num 60\n",
            "loss 0.15960301311261985 average time 2.3140323946626724 iter num 80\n",
            "loss 0.1596026305217647 average time 2.322929799090143 iter num 100\n",
            "loss 0.1595920783309053 average time 2.2934059018498374 iter num 20\n",
            "loss 0.1595828770788555 average time 2.3007095557498816 iter num 40\n",
            "loss 0.15957689929765462 average time 2.295862596083225 iter num 60\n",
            "loss 0.15957430363937633 average time 2.3088178584874415 iter num 80\n",
            "loss 0.15957392586171526 average time 2.315393787799894 iter num 100\n",
            "loss 0.15956345498721244 average time 2.3961701323501075 iter num 20\n",
            "loss 0.1595543241583024 average time 2.357303801350099 iter num 40\n",
            "loss 0.1595483944394986 average time 2.3410482651500693 iter num 60\n",
            "loss 0.1595458205539598 average time 2.341920105075019 iter num 80\n",
            "loss 0.1595454468567769 average time 2.3345934882099755 iter num 100\n",
            "loss 0.15953505504321505 average time 2.323577159049819 iter num 20\n",
            "loss 0.15952599265366998 average time 2.317169469275041 iter num 40\n",
            "loss 0.15952010838188652 average time 2.31987181630005 iter num 60\n",
            "loss 0.15951755564790987 average time 2.3216838811750677 iter num 80\n",
            "loss 0.15951718643998217 average time 2.322507143630046 iter num 100\n",
            "loss 0.15950687449333636 average time 2.3033127549998427 iter num 20\n",
            "loss 0.1594978793982616 average time 2.3155487050000376 iter num 40\n",
            "loss 0.15949204154082236 average time 2.316063014616581 iter num 60\n",
            "loss 0.1594895106921675 average time 2.3174898739249556 iter num 80\n",
            "loss 0.15948914548306237 average time 2.331738666950023 iter num 100\n",
            "loss 0.15947891246969848 average time 2.3362629847997596 iter num 20\n",
            "loss 0.15946998804021234 average time 2.3445866668249438 iter num 40\n",
            "loss 0.15946419299696318 average time 2.3363561971332274 iter num 60\n",
            "loss 0.15946167886540458 average time 2.3404139267374147 iter num 80\n",
            "loss 0.15946131925714044 average time 2.340483891519889 iter num 100\n",
            "loss 0.15945116414034763 average time 2.3289737313996737 iter num 20\n",
            "loss 0.1594423072996755 average time 2.3189601551748638 iter num 40\n",
            "loss 0.15943655694437578 average time 2.315278109383265 iter num 60\n",
            "loss 0.15943406548525363 average time 2.316138568749943 iter num 80\n",
            "loss 0.15943370995884235 average time 2.318111731959907 iter num 100\n",
            "loss 0.1594236305766776 average time 2.3083760739502397 iter num 20\n",
            "loss 0.1594148426475017 average time 2.319076453050138 iter num 40\n",
            "loss 0.15940913386662098 average time 2.3270512562333656 iter num 60\n",
            "loss 0.159406663093071 average time 2.3279431574875162 iter num 80\n",
            "loss 0.15940631171253192 average time 2.3297672814999713 iter num 100\n",
            "loss 0.15939630862850493 average time 2.3269745339999646 iter num 20\n",
            "loss 0.15938758368882014 average time 2.336703682524967 iter num 40\n",
            "loss 0.15938191981299854 average time 2.348003306199947 iter num 60\n",
            "loss 0.15937946840907075 average time 2.3468503762749835 iter num 80\n",
            "loss 0.15937912241860722 average time 2.3426654515199696 iter num 100\n",
            "loss 0.15936919084277912 average time 2.3051019054501922 iter num 20\n",
            "loss 0.15936053166275635 average time 2.348816128450244 iter num 40\n",
            "loss 0.15935491077636327 average time 2.350185546983448 iter num 60\n",
            "loss 0.1593524773940282 average time 2.3427926940250474 iter num 80\n",
            "loss 0.1593521355314902 average time 2.334461503190032 iter num 100\n",
            "loss 0.15934227733059797 average time 2.334396627899878 iter num 20\n",
            "loss 0.15933368092712025 average time 2.3383590927246587 iter num 40\n",
            "loss 0.15932810091508445 average time 2.3385156056830967 iter num 60\n",
            "loss 0.1593256864257714 average time 2.345930404299952 iter num 80\n",
            "loss 0.15932534636526147 average time 2.341228004959994 iter num 100\n",
            "loss 0.15931555994994753 average time 2.3424113312499686 iter num 20\n",
            "loss 0.15930702542721978 average time 2.31488701674989 iter num 40\n",
            "loss 0.15930148483219736 average time 2.335287784300029 iter num 60\n",
            "loss 0.1592990882466055 average time 2.330731402862466 iter num 80\n",
            "loss 0.15929875288387552 average time 2.336301122979967 iter num 100\n",
            "loss 0.15928903612120235 average time 2.3306074820004143 iter num 20\n",
            "loss 0.1592805609033859 average time 2.34185825472523 iter num 40\n",
            "loss 0.15927505958410348 average time 2.330816803500117 iter num 60\n",
            "loss 0.15927268033175676 average time 2.342817566637632 iter num 80\n",
            "loss 0.15927235022033687 average time 2.343245609400219 iter num 100\n",
            "loss 0.15926270595460432 average time 2.3719045004500003 iter num 20\n",
            "loss 0.15925428796371655 average time 2.3732981852251216 iter num 40\n",
            "loss 0.15924882616097108 average time 2.372946722583462 iter num 60\n",
            "loss 0.15924646784263485 average time 2.3545768419623982 iter num 80\n",
            "loss 0.1592461402484081 average time 2.349797828569972 iter num 100\n",
            "loss 0.1592365628851044 average time 2.30423478594912 iter num 20\n",
            "loss 0.15922820412747818 average time 2.322567436699592 iter num 40\n",
            "loss 0.15922277994977752 average time 2.3296936748663937 iter num 60\n",
            "loss 0.1592204379481438 average time 2.330554019624833 iter num 80\n",
            "loss 0.15922011574841305 average time 2.3332912069799567 iter num 100\n",
            "loss 0.15921060287192332 average time 2.3500138417995915 iter num 20\n",
            "loss 0.15920230081705322 average time 2.3617449502996353 iter num 40\n",
            "loss 0.15919691506768618 average time 2.363284200116383 iter num 60\n",
            "loss 0.1591945880413029 average time 2.3635844441622793 iter num 80\n",
            "loss 0.15919426956324928 average time 2.3679688911798804 iter num 100\n",
            "loss 0.15918481932721254 average time 2.310728475200267 iter num 20\n",
            "loss 0.1591765728246265 average time 2.3273523891000876 iter num 40\n",
            "loss 0.1591712232134243 average time 2.3410487164834195 iter num 60\n",
            "loss 0.15916891569464597 average time 2.341376946437458 iter num 80\n",
            "loss 0.1591685994991402 average time 2.3466947553299904 iter num 100\n",
            "loss 0.15915921081113246 average time 2.3086834631005333 iter num 20\n",
            "loss 0.15915102056674793 average time 2.314397692000057 iter num 40\n",
            "loss 0.15914570483328327 average time 2.3264994084499753 iter num 60\n",
            "loss 0.15914341633449233 average time 2.3258439289999844 iter num 80\n",
            "loss 0.15914310257669514 average time 2.3196350650200475 iter num 100\n",
            "loss 0.1591337753758694 average time 2.3574454414001593 iter num 20\n",
            "loss 0.15912563956320447 average time 2.3192431223501444 iter num 40\n",
            "loss 0.15912035919607317 average time 2.3230454156999256 iter num 60\n",
            "loss 0.15911808459042115 average time 2.332887282412412 iter num 80\n",
            "loss 0.15911777660432708 average time 2.3382803148399036 iter num 100\n",
            "loss 0.15910850822185305 average time 2.3797185423503833 iter num 20\n",
            "loss 0.1591004245187628 average time 2.345920734050105 iter num 40\n",
            "loss 0.1590951805711567 average time 2.3471291433334045 iter num 60\n",
            "loss 0.15909292242400094 average time 2.328621684850077 iter num 80\n",
            "loss 0.1590926170921805 average time 2.3123716166199664 iter num 100\n",
            "loss 0.1590834085044463 average time 2.259500752399981 iter num 20\n",
            "loss 0.15907537872611985 average time 2.264993403624794 iter num 40\n",
            "loss 0.15907016690119707 average time 2.275288677349878 iter num 60\n",
            "loss 0.15906792600291558 average time 2.281867211749841 iter num 80\n",
            "loss 0.1590676233448486 average time 2.279433448179807 iter num 100\n",
            "loss 0.1590584677110672 average time 2.322328996950091 iter num 20\n",
            "loss 0.15905048650420878 average time 2.3280418222997468 iter num 40\n",
            "loss 0.15904531061445346 average time 2.3144820454829946 iter num 60\n",
            "loss 0.1590430858547094 average time 2.3167837474871704 iter num 80\n",
            "loss 0.1590427873115752 average time 2.3148759653696835 iter num 100\n",
            "loss 0.15903368678607985 average time 2.323802333650201 iter num 20\n",
            "loss 0.1590257546205304 average time 2.3171466883754874 iter num 40\n",
            "loss 0.15902061063783407 average time 2.323659821200211 iter num 60\n",
            "loss 0.15901840153696972 average time 2.3265401124375784 iter num 80\n",
            "loss 0.15901810629374546 average time 2.331112986600019 iter num 100\n",
            "loss 0.15900906418419825 average time 2.302898285300034 iter num 20\n",
            "loss 0.15900117712402065 average time 2.3187922752503254 iter num 40\n",
            "loss 0.15899606554085088 average time 2.3226669627336376 iter num 60\n",
            "loss 0.15899387135849083 average time 2.324556445237795 iter num 80\n",
            "loss 0.15899357936068634 average time 2.3242199384000926 iter num 100\n",
            "loss 0.15898459548169677 average time 2.30985614969959 iter num 20\n",
            "loss 0.15897675564441027 average time 2.3157141076246264 iter num 40\n",
            "loss 0.1589716770874498 average time 2.3280978805163612 iter num 60\n",
            "loss 0.15896949785901168 average time 2.313062307549944 iter num 80\n",
            "loss 0.1589692072229097 average time 2.311457885249947 iter num 100\n",
            "loss 0.1589602835142825 average time 2.313748799300265 iter num 20\n",
            "loss 0.15895249032390457 average time 2.2993851257750064 iter num 40\n",
            "loss 0.1589474441722874 average time 2.3041794467665873 iter num 60\n",
            "loss 0.15894527752559706 average time 2.2972834366749795 iter num 80\n",
            "loss 0.15894499184974178 average time 2.296433687029894 iter num 100\n",
            "loss 0.15893612288119435 average time 2.2525844310996037 iter num 20\n",
            "loss 0.15892837794019732 average time 2.2533616989495386 iter num 40\n",
            "loss 0.1589233589536591 average time 2.2485001072496136 iter num 60\n",
            "loss 0.15892120783876532 average time 2.255872760287275 iter num 80\n",
            "loss 0.1589209244701143 average time 2.2558549367698286 iter num 100\n",
            "loss 0.15891210828960006 average time 2.1937879313501982 iter num 20\n",
            "loss 0.15890440904350175 average time 2.2099856221751906 iter num 40\n",
            "loss 0.15889942038874688 average time 2.2070686406335276 iter num 60\n",
            "loss 0.15889728350323112 average time 2.2024211290626683 iter num 80\n",
            "loss 0.15889700290745867 average time 2.2014251456502096 iter num 100\n",
            "loss 0.15888824123843318 average time 2.235242433199892 iter num 20\n",
            "loss 0.15888058789518245 average time 2.2402444710499365 iter num 40\n",
            "loss 0.15887562836781952 average time 2.217670600883139 iter num 60\n",
            "loss 0.1588735053493685 average time 2.214577149837305 iter num 80\n",
            "loss 0.15887322631464013 average time 2.2224208120298137 iter num 100\n",
            "loss 0.15886451464670806 average time 2.261636508999982 iter num 20\n",
            "loss 0.15885690591799423 average time 2.2574064335999537 iter num 40\n",
            "loss 0.1588519744019686 average time 2.253646162033268 iter num 60\n",
            "loss 0.1588498667725117 average time 2.238324070375029 iter num 80\n",
            "loss 0.1588495918547837 average time 2.2253348771800665 iter num 100\n",
            "loss 0.15884093050011844 average time 2.189941052850372 iter num 20\n",
            "loss 0.15883336717655133 average time 2.1896617740250806 iter num 40\n",
            "loss 0.15882846386599347 average time 2.190210232700095 iter num 60\n",
            "loss 0.158826368902817 average time 2.1932318567375204 iter num 80\n",
            "loss 0.15882609543698037 average time 2.202042318180138 iter num 100\n",
            "loss 0.15881748758335143 average time 2.2314376218004326 iter num 20\n",
            "loss 0.15880996928423402 average time 2.2600103220751406 iter num 40\n",
            "loss 0.15880509183873362 average time 2.262751943283365 iter num 60\n",
            "loss 0.1588030110056744 average time 2.2592043613875377 iter num 80\n",
            "loss 0.15880274198933747 average time 2.2736436029700418 iter num 100\n",
            "loss 0.15879418554227168 average time 2.23035050005019 iter num 20\n",
            "loss 0.15878670839690368 average time 2.260290301600162 iter num 40\n",
            "loss 0.15878185894656466 average time 2.2673834896333576 iter num 60\n",
            "loss 0.15877978995628111 average time 2.2634748678749474 iter num 80\n",
            "loss 0.15877952232004391 average time 2.2657365369598845 iter num 100\n",
            "loss 0.15877101333014926 average time 2.29112561649963 iter num 20\n",
            "loss 0.15876358197964707 average time 2.2790528532998904 iter num 40\n",
            "loss 0.15875876003831818 average time 2.2860029285166092 iter num 60\n",
            "loss 0.15875670212934592 average time 2.2901469714500307 iter num 80\n",
            "loss 0.15875643925197463 average time 2.2956856422200507 iter num 100\n",
            "loss 0.1587479784231326 average time 2.2724994621001313 iter num 20\n",
            "loss 0.1587405891885318 average time 2.289424472224982 iter num 40\n",
            "loss 0.1587357945790913 average time 2.295455606299826 iter num 60\n",
            "loss 0.15873374877589266 average time 2.2968651656122803 iter num 80\n",
            "loss 0.15873348690973293 average time 2.2957772131897944 iter num 100\n",
            "loss 0.15872507437393615 average time 2.2946020338999005 iter num 20\n",
            "loss 0.15871772747828014 average time 2.2945462700251484 iter num 40\n",
            "loss 0.15871295735914526 average time 2.3070935733667284 iter num 60\n",
            "loss 0.15871092744486717 average time 2.315669045200093 iter num 80\n",
            "loss 0.15871066689363902 average time 2.320101593510044 iter num 100\n",
            "loss 0.15870229722256624 average time 2.319001297900468 iter num 20\n",
            "loss 0.1586949913805901 average time 2.3347596173000964 iter num 40\n",
            "loss 0.1586902466027406 average time 2.3298700260166396 iter num 60\n",
            "loss 0.1586882286133476 average time 2.3272651089375813 iter num 80\n",
            "loss 0.15868797090807268 average time 2.3214152197000657 iter num 100\n",
            "loss 0.15867964830237485 average time 2.265779332600141 iter num 20\n",
            "loss 0.1586723809600783 average time 2.2799043956249987 iter num 40\n",
            "loss 0.1586676641856804 average time 2.279746843349979 iter num 60\n",
            "loss 0.158665658734145 average time 2.2924939270500544 iter num 80\n",
            "loss 0.1586654037060849 average time 2.2998693792600533 iter num 100\n",
            "loss 0.15865712765396844 average time 2.307570565999595 iter num 20\n",
            "loss 0.1586498972962757 average time 2.3239176416748704 iter num 40\n",
            "loss 0.15864520344054975 average time 2.333930712333252 iter num 60\n",
            "loss 0.15864321074725313 average time 2.3323198976373534 iter num 80\n",
            "loss 0.158642957634685 average time 2.3342316130298424 iter num 100\n",
            "loss 0.15863472330974404 average time 2.295891985000708 iter num 20\n",
            "loss 0.1586275334781597 average time 2.3328710549002607 iter num 40\n",
            "loss 0.158622866730706 average time 2.322399923550135 iter num 60\n",
            "loss 0.15862088372256852 average time 2.3198574659252245 iter num 80\n",
            "loss 0.15862063296085016 average time 2.3221338497901525 iter num 100\n",
            "loss 0.15861243934538616 average time 2.3308762916005437 iter num 20\n",
            "loss 0.15860528701045318 average time 2.3443495623250783 iter num 40\n",
            "loss 0.15860064399784557 average time 2.344055694083242 iter num 60\n",
            "loss 0.15859867389106605 average time 2.3442195262249244 iter num 80\n",
            "loss 0.1585984253842955 average time 2.343883414469892 iter num 100\n",
            "loss 0.15859027174586068 average time 2.2919350795498756 iter num 20\n",
            "loss 0.1585831549418974 average time 2.327873315799843 iter num 40\n",
            "loss 0.15857853372972602 average time 2.323810392999864 iter num 60\n",
            "loss 0.15857657362304584 average time 2.324946043424961 iter num 80\n",
            "loss 0.15857632912453626 average time 2.3207848254500276 iter num 100\n",
            "loss 0.15856821701471546 average time 2.3452019235995976 iter num 20\n",
            "loss 0.15856113643972675 average time 2.3511708867496965 iter num 40\n",
            "loss 0.15855653921785676 average time 2.3464610429329698 iter num 60\n",
            "loss 0.15855458978558196 average time 2.344989966374669 iter num 80\n",
            "loss 0.1585543461961337 average time 2.3578321893497196 iter num 100\n",
            "loss 0.15854627324329565 average time 2.3255415427000115 iter num 20\n",
            "loss 0.15853922787386437 average time 2.3221516690247883 iter num 40\n",
            "loss 0.15853465392097116 average time 2.3205602430164314 iter num 60\n",
            "loss 0.15853271600518612 average time 2.3234294563747424 iter num 80\n",
            "loss 0.158532474223174 average time 2.323681564069702 iter num 100\n",
            "loss 0.15852443752064035 average time 2.355500439599382 iter num 20\n",
            "loss 0.15851742803256885 average time 2.3574132629749327 iter num 40\n",
            "loss 0.1585128783713648 average time 2.348555235699799 iter num 60\n",
            "loss 0.15851095124584438 average time 2.335626944374917 iter num 80\n",
            "loss 0.15851071100625197 average time 2.3422253907698907 iter num 100\n",
            "loss 0.15850271051652667 average time 2.3455243955497282 iter num 20\n",
            "loss 0.15849573465297023 average time 2.326168795499689 iter num 40\n",
            "loss 0.15849120803988442 average time 2.3214688003498547 iter num 60\n",
            "loss 0.1584892909939497 average time 2.327969353024855 iter num 80\n",
            "loss 0.15848905320790166 average time 2.33088093888 iter num 100\n",
            "loss 0.15848109157925616 average time 2.2942879079506384 iter num 20\n",
            "loss 0.1584741482524177 average time 2.3078440961752675 iter num 40\n",
            "loss 0.15846964282721202 average time 2.3293240449001433 iter num 60\n",
            "loss 0.15846773828469898 average time 2.3411557668251133 iter num 80\n",
            "loss 0.15846750143375907 average time 2.3416771788400363 iter num 100\n",
            "loss 0.1584595755872128 average time 2.329377443099838 iter num 20\n",
            "loss 0.15845266796390162 average time 2.3266646445998957 iter num 40\n",
            "loss 0.1584481846559728 average time 2.329035894166494 iter num 60\n",
            "loss 0.1584462877554489 average time 2.3356155589748595 iter num 80\n",
            "loss 0.1584460558046569 average time 2.3371343326299394 iter num 100\n",
            "loss 0.15843816777339914 average time 2.3078023087000474 iter num 20\n",
            "loss 0.15843129072286743 average time 2.3429054543001255 iter num 40\n",
            "loss 0.1584268293002519 average time 2.3497399289166423 iter num 60\n",
            "loss 0.15842494345872266 average time 2.3424515796623835 iter num 80\n",
            "loss 0.15842471079699436 average time 2.345233268349948 iter num 100\n",
            "loss 0.15841686174438946 average time 2.325281246999657 iter num 20\n",
            "loss 0.1584100154903505 average time 2.349291220674604 iter num 40\n",
            "loss 0.15840557387856333 average time 2.3479336670662936 iter num 60\n",
            "loss 0.15840369844184315 average time 2.34213433347486 iter num 80\n",
            "loss 0.1584034678587817 average time 2.335759367509854 iter num 100\n",
            "loss 0.15839565435014658 average time 2.3035348947505554 iter num 20\n",
            "loss 0.158388838560277 average time 2.322481411700392 iter num 40\n",
            "loss 0.15838441951767201 average time 2.335677845300294 iter num 60\n",
            "loss 0.15838255376173677 average time 2.3470760867126046 iter num 80\n",
            "loss 0.1583823259756518 average time 2.365135360150016 iter num 100\n",
            "loss 0.158374547581533 average time 2.3652199721498617 iter num 20\n",
            "loss 0.15836776074838663 average time 2.3444189781000206 iter num 40\n",
            "loss 0.1583633631187314 average time 2.3229964683498716 iter num 60\n",
            "loss 0.15836150796361217 average time 2.305805956762515 iter num 80\n",
            "loss 0.158361280788726 average time 2.304374717910032 iter num 100\n",
            "loss 0.15835353710000896 average time 2.328439191399775 iter num 20\n",
            "loss 0.15834677839396263 average time 2.3233570029749897 iter num 40\n",
            "loss 0.1583424009090718 average time 2.333114047216683 iter num 60\n",
            "loss 0.15834055419976545 average time 2.3264225977875412 iter num 80\n",
            "loss 0.15834033032971992 average time 2.323782939940138 iter num 100\n",
            "loss 0.15833261930950074 average time 2.343557463350044 iter num 20\n",
            "loss 0.15832589058494578 average time 2.3366841774248313 iter num 40\n",
            "loss 0.1583215310986749 average time 2.3360071137833076 iter num 60\n",
            "loss 0.1583196947636256 average time 2.3302478288875137 iter num 80\n",
            "loss 0.15831947232101454 average time 2.334968920210049 iter num 100\n",
            "loss 0.1583117963323662 average time 2.3400742644002093 iter num 20\n",
            "loss 0.15830509660525344 average time 2.3338199211503707 iter num 40\n",
            "loss 0.15830075751874384 average time 2.3324762567502453 iter num 60\n",
            "loss 0.1582989316371661 average time 2.3328846092752884 iter num 80\n",
            "loss 0.15829870974230234 average time 2.3409553161502483 iter num 100\n",
            "loss 0.15829106796779818 average time 2.292112049099887 iter num 20\n",
            "loss 0.15828439513001355 average time 2.320248283699766 iter num 40\n",
            "loss 0.15828007543930855 average time 2.332100932683473 iter num 60\n",
            "loss 0.1582782574829315 average time 2.3359437628377235 iter num 80\n",
            "loss 0.15827803759526496 average time 2.33681954290023 iter num 100\n",
            "loss 0.15827042990255175 average time 2.37906636050011 iter num 20\n",
            "loss 0.15826378423798695 average time 2.3680415875498513 iter num 40\n",
            "loss 0.15825948453281954 average time 2.3777253333331827 iter num 60\n",
            "loss 0.15825767343496355 average time 2.3679293736498495 iter num 80\n",
            "loss 0.15825745552071 average time 2.35594794154993 iter num 100\n",
            "loss 0.1582498807780768 average time 2.3524131052005033 iter num 20\n",
            "loss 0.15824326412229756 average time 2.3455679390254773 iter num 40\n",
            "loss 0.15823898201891118 average time 2.3538475954336415 iter num 60\n",
            "loss 0.15823718110713994 average time 2.3563808994752433 iter num 80\n",
            "loss 0.15823696500490866 average time 2.3514918852701885 iter num 100\n",
            "loss 0.15822942133457035 average time 2.326740625000821 iter num 20\n",
            "loss 0.15822282972184126 average time 2.319926814900282 iter num 40\n",
            "loss 0.15821856584586985 average time 2.31529553883344 iter num 60\n",
            "loss 0.158216771828746 average time 2.3218277324376686 iter num 80\n",
            "loss 0.1582165569565933 average time 2.3293445819701084 iter num 100\n",
            "loss 0.15820904652399975 average time 2.312361359749957 iter num 20\n",
            "loss 0.15820248112098534 average time 2.324990565524968 iter num 40\n",
            "loss 0.15819823878413672 average time 2.326048917349969 iter num 60\n",
            "loss 0.15819645223377607 average time 2.335692622099941 iter num 80\n",
            "loss 0.1581962389190634 average time 2.331254733490023 iter num 100\n",
            "loss 0.1581887529515879 average time 2.3204957357998865 iter num 20\n",
            "loss 0.15818221724603868 average time 2.3457585363246833 iter num 40\n",
            "loss 0.15817799043071804 average time 2.3336386811496292 iter num 60\n",
            "loss 0.15817621231731074 average time 2.3328600466746594 iter num 80\n",
            "loss 0.15817600172129156 average time 2.3323618173697103 iter num 100\n",
            "loss 0.15816854385934295 average time 2.308666844700201 iter num 20\n",
            "loss 0.15816203662545802 average time 2.3203900417754086 iter num 40\n",
            "loss 0.15815782598282815 average time 2.3340286222003974 iter num 60\n",
            "loss 0.1581560565643059 average time 2.3469624514752923 iter num 80\n",
            "loss 0.1581558452165588 average time 2.3442307474502377 iter num 100\n",
            "loss 0.15814841727697224 average time 2.3486979375502415 iter num 20\n",
            "loss 0.15814193531345316 average time 2.327346902050067 iter num 40\n",
            "loss 0.15813774176782444 average time 2.31856109474993 iter num 60\n",
            "loss 0.15813598026051695 average time 2.325769059125014 iter num 80\n",
            "loss 0.15813577173761798 average time 2.3354950653299964 iter num 100\n",
            "loss 0.1581283719727469 average time 2.3525277889499194 iter num 20\n",
            "loss 0.1581219136132681 average time 2.323125105075087 iter num 40\n",
            "loss 0.15811773840569754 average time 2.331130739916504 iter num 60\n",
            "loss 0.15811598469448057 average time 2.3242470955497536 iter num 80\n",
            "loss 0.15811577690725376 average time 2.323551732299784 iter num 100\n",
            "loss 0.15810840417065994 average time 2.333222071449927 iter num 20\n",
            "loss 0.15810197145697638 average time 2.319073668125293 iter num 40\n",
            "loss 0.158097812902961 average time 2.3261550980666166 iter num 60\n",
            "loss 0.1580960665658082 average time 2.3229152757499376 iter num 80\n",
            "loss 0.15809586152652885 average time 2.329762689030031 iter num 100\n",
            "loss 0.15808851984579425 average time 2.307045220250802 iter num 20\n",
            "loss 0.15808211304786882 average time 2.312893467075628 iter num 40\n",
            "loss 0.15807796996308063 average time 2.322440942183736 iter num 60\n",
            "loss 0.1580762320203669 average time 2.3179536015878055 iter num 80\n",
            "loss 0.15807602743921825 average time 2.3172783602401976 iter num 100\n",
            "loss 0.15806871448099097 average time 2.3708876016999056 iter num 20\n",
            "loss 0.1580623308271928 average time 2.343506545924993 iter num 40\n",
            "loss 0.15805820223533937 average time 2.3403845618665704 iter num 60\n",
            "loss 0.1580564704891611 average time 2.337839338087315 iter num 80\n",
            "loss 0.15805626887241447 average time 2.3370996595797986 iter num 100\n",
            "loss 0.15804898438212947 average time 2.3596048381003127 iter num 20\n",
            "loss 0.15804262327448368 average time 2.346695899875067 iter num 40\n",
            "loss 0.15803851421071705 average time 2.335716282083316 iter num 60\n",
            "loss 0.1580367901398952 average time 2.333320133612415 iter num 80\n",
            "loss 0.15803658779398289 average time 2.3313652916999126 iter num 100\n",
            "loss 0.15802932967761 average time 2.336521973949857 iter num 20\n",
            "loss 0.1580229927618231 average time 2.3430631284250922 iter num 40\n",
            "loss 0.15801890044860709 average time 2.3366855363001378 iter num 60\n",
            "loss 0.1580171826643956 average time 2.3262336427001173 iter num 80\n",
            "loss 0.15801698145391274 average time 2.33140104643011 iter num 100\n",
            "loss 0.15800974933033868 average time 2.35741342959991 iter num 20\n",
            "loss 0.15800343506976602 average time 2.3329444348250945 iter num 40\n",
            "loss 0.1579993572342273 average time 2.343729554249997 iter num 60\n",
            "loss 0.1579976473222819 average time 2.3444325072747687 iter num 80\n",
            "loss 0.15799744758375192 average time 2.341112436239855 iter num 100\n",
            "loss 0.15799023854930563 average time 2.3333894305993454 iter num 20\n",
            "loss 0.15798394691400688 average time 2.3333531007498096 iter num 40\n",
            "loss 0.1579798867972669 average time 2.328041381549883 iter num 60\n",
            "loss 0.15797818244562262 average time 2.3320625338749323 iter num 80\n",
            "loss 0.15797798424792417 average time 2.329750630209928 iter num 100\n",
            "loss 0.15797080116869042 average time 2.3666359384000315 iter num 20\n",
            "loss 0.1579645319006411 average time 2.334786914149754 iter num 40\n",
            "loss 0.15796048473646182 average time 2.3451711638499546 iter num 60\n",
            "loss 0.15795878783575829 average time 2.339290180337457 iter num 80\n",
            "loss 0.15795859265652312 average time 2.3483202535098826 iter num 100\n",
            "loss 0.15795143734610867 average time 2.34535160500036 iter num 20\n",
            "loss 0.15794519076303357 average time 2.333116969575167 iter num 40\n",
            "loss 0.1579411567641053 average time 2.3188217969666463 iter num 60\n",
            "loss 0.15793946764896755 average time 2.3140783621875927 iter num 80\n",
            "loss 0.1579392718639099 average time 2.3169727256700208 iter num 100\n",
            "loss 0.1579321455430204 average time 2.333377072350231 iter num 20\n",
            "loss 0.15792591877543852 average time 2.327541023050162 iter num 40\n",
            "loss 0.15792190020461302 average time 2.329583371249949 iter num 60\n",
            "loss 0.15792021718136476 average time 2.3234339288248065 iter num 80\n",
            "loss 0.15792002336575423 average time 2.3173138368199218 iter num 100\n",
            "loss 0.15791292411533056 average time 2.3941181403497467 iter num 20\n",
            "loss 0.15790671993928365 average time 2.3779043190250375 iter num 40\n",
            "loss 0.15790271467909606 average time 2.3549522677999146 iter num 60\n",
            "loss 0.1579010395704477 average time 2.3484685716375227 iter num 80\n",
            "loss 0.15790084596951562 average time 2.3522317864400977 iter num 100\n",
            "loss 0.157893771384171 average time 2.3437014648496186 iter num 20\n",
            "loss 0.1578875869271677 average time 2.3373988167999413 iter num 40\n",
            "loss 0.1578835941954386 average time 2.331439454150071 iter num 60\n",
            "loss 0.15788192532742776 average time 2.344539716750023 iter num 80\n",
            "loss 0.15788173326644675 average time 2.337571299290066 iter num 100\n",
            "loss 0.15787468175340721 average time 2.3526060106494695 iter num 20\n",
            "loss 0.1578685187712799 average time 2.3399572674246882 iter num 40\n",
            "loss 0.157864543321243 average time 2.3440872390163956 iter num 60\n",
            "loss 0.15786287949479952 average time 2.3378021919497316 iter num 80\n",
            "loss 0.15786268822370664 average time 2.327378264479776 iter num 100\n",
            "loss 0.1578556620830024 average time 2.324911725399943 iter num 20\n",
            "loss 0.15784952060527452 average time 2.332738187425002 iter num 40\n",
            "loss 0.157845554449703 average time 2.3324794474998876 iter num 60\n",
            "loss 0.15784389766228218 average time 2.3352476296500297 iter num 80\n",
            "loss 0.15784370789999286 average time 2.3299493542099663 iter num 100\n",
            "loss 0.15783670488571439 average time 2.3118275992002966 iter num 20\n",
            "loss 0.15783058293569488 average time 2.300790262675309 iter num 40\n",
            "loss 0.15782663087176 average time 2.3033664907001365 iter num 60\n",
            "loss 0.1578249801601455 average time 2.313255157950107 iter num 80\n",
            "loss 0.1578247919726848 average time 2.3209391276901443 iter num 100\n",
            "loss 0.15781781082286445 average time 2.320038307500181 iter num 20\n",
            "loss 0.15781170715600915 average time 2.3354165868248855 iter num 40\n",
            "loss 0.15780776775358438 average time 2.3236164159834516 iter num 60\n",
            "loss 0.15780612288678572 average time 2.322341375500173 iter num 80\n",
            "loss 0.1578059358152877 average time 2.312706865010223 iter num 100\n",
            "loss 0.15779897628462386 average time 2.3136243892991843 iter num 20\n",
            "loss 0.1577928884174934 average time 2.302329627199197 iter num 40\n",
            "loss 0.15778895985249974 average time 2.31542038913279 iter num 60\n",
            "loss 0.15778732190268227 average time 2.3216347038369802 iter num 80\n",
            "loss 0.15778713649095052 average time 2.322635826209553 iter num 100\n",
            "loss 0.15778019195031046 average time 2.3140056181004183 iter num 20\n",
            "loss 0.15777412362018725 average time 2.337372862774828 iter num 40\n",
            "loss 0.15777020914090795 average time 2.3350986486332355 iter num 60\n",
            "loss 0.15776857558172538 average time 2.3336246327998196 iter num 80\n",
            "loss 0.15776839185960384 average time 2.3473192657500475 iter num 100\n",
            "loss 0.157761469718344 average time 2.360305692399197 iter num 20\n",
            "loss 0.15775542081860852 average time 2.3427104150243396 iter num 40\n",
            "loss 0.1577515195177163 average time 2.3223981815493366 iter num 60\n",
            "loss 0.15774989115771712 average time 2.311942335911863 iter num 80\n",
            "loss 0.1577497079302137 average time 2.314183292879461 iter num 100\n",
            "loss 0.15774280557727433 average time 2.332761655699869 iter num 20\n",
            "loss 0.15773677650020837 average time 2.3880776597003206 iter num 40\n",
            "loss 0.15773288802992527 average time 2.38849266398344 iter num 60\n",
            "loss 0.15773126483102812 average time 2.3610161554624938 iter num 80\n",
            "loss 0.1577310834389363 average time 2.3695558148697455 iter num 100\n",
            "loss 0.15772420116967253 average time 2.4459987778496726 iter num 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fdc890eac869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_MC_oneDS_SGD_2.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fdc890eac869>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fdc890eac869>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fdc890eac869>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQs-OZHGEwac"
      },
      "source": [
        "# 6hr 8 min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRRhtcpLJP-v"
      },
      "source": [
        "## Continued Continued to train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MuZLR6wnJQQV",
        "outputId": "5c510f07-1910-44b5-8444-633c7eebcf43"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import SGD\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = SGD(model.parameters(), lr=1e-3) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_SGD_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.15771566553396593 average time 2.3652323660004186 iter num 20\n",
            "loss 0.15770966257319627 average time 2.3419441313748393 iter num 40\n",
            "loss 0.1577057917298616 average time 2.3224139882330443 iter num 60\n",
            "loss 0.15770417837201653 average time 2.3207987988622336 iter num 80\n",
            "loss 0.15770399792329626 average time 2.31961428754963 iter num 100\n",
            "loss 0.15769714524055517 average time 2.322695706949162 iter num 20\n",
            "loss 0.15769115886632412 average time 2.3343088916743 iter num 40\n",
            "loss 0.15768730143791276 average time 2.323199258382859 iter num 60\n",
            "loss 0.15768569190628015 average time 2.334309038574702 iter num 80\n",
            "loss 0.15768551187720256 average time 2.322863323199781 iter num 100\n",
            "loss 0.15767867937335941 average time 2.250401875799798 iter num 20\n",
            "loss 0.1576727135561756 average time 2.286921439324942 iter num 40\n",
            "loss 0.15766886769860777 average time 2.2905765453331695 iter num 60\n",
            "loss 0.15766726359972724 average time 2.293932866212617 iter num 80\n",
            "loss 0.15766708380667305 average time 2.2982436000300366 iter num 100\n",
            "loss 0.15766027087555426 average time 2.2929289275496556 iter num 20\n",
            "loss 0.15765432282121394 average time 2.292871295149598 iter num 40\n",
            "loss 0.15765048769060822 average time 2.299750105133232 iter num 60\n",
            "loss 0.15764889061180512 average time 2.301112144912531 iter num 80\n",
            "loss 0.15764871283512571 average time 2.308321640349968 iter num 100\n",
            "loss 0.15764191653219378 average time 2.2816954882000573 iter num 20\n",
            "loss 0.1576359856778574 average time 2.2939968495507856 iter num 40\n",
            "loss 0.1576321633848949 average time 2.292837102550402 iter num 60\n",
            "loss 0.15763056940630168 average time 2.291843326900289 iter num 80\n",
            "loss 0.1576303921350329 average time 2.2928789081703145 iter num 100\n",
            "loss 0.1576236158901216 average time 2.3404090335494403 iter num 20\n",
            "loss 0.1576177025297711 average time 2.308403721949799 iter num 40\n",
            "loss 0.1576138912491993 average time 2.293310953533849 iter num 60\n",
            "loss 0.15761230457753356 average time 2.2940217334629778 iter num 80\n",
            "loss 0.15761212842202718 average time 2.2841633399302372 iter num 100\n",
            "loss 0.1576053691162826 average time 2.302442863048782 iter num 20\n",
            "loss 0.15759947333706747 average time 2.352957753224655 iter num 40\n",
            "loss 0.15759567232006366 average time 2.3306752617166544 iter num 60\n",
            "loss 0.15759409017640696 average time 2.329900246737725 iter num 80\n",
            "loss 0.15759391444096407 average time 2.3249820084103705 iter num 100\n",
            "loss 0.15758717562619354 average time 2.355629587549629 iter num 20\n",
            "loss 0.15758129441067747 average time 2.3226803346242377 iter num 40\n",
            "loss 0.15757750463184123 average time 2.3412148263158694 iter num 60\n",
            "loss 0.15757592875582688 average time 2.3414020490867187 iter num 80\n",
            "loss 0.15757575334661317 average time 2.343916328369305 iter num 100\n",
            "loss 0.15756903360381708 average time 2.2664037851500325 iter num 20\n",
            "loss 0.15756316765923345 average time 2.2892267965757127 iter num 40\n",
            "loss 0.15755939276579015 average time 2.3093993968831152 iter num 60\n",
            "loss 0.15755781877998623 average time 2.300151548287522 iter num 80\n",
            "loss 0.15755764543641237 average time 2.302861860000121 iter num 100\n",
            "loss 0.15755094469450073 average time 2.2897494716002256 iter num 20\n",
            "loss 0.15754509543800024 average time 2.307001462225162 iter num 40\n",
            "loss 0.15754133070618975 average time 2.287906985533603 iter num 60\n",
            "loss 0.15753976261226954 average time 2.304065503025231 iter num 80\n",
            "loss 0.15753958950579958 average time 2.316234643840289 iter num 100\n",
            "loss 0.15753290147544582 average time 2.341847542350661 iter num 20\n",
            "loss 0.15752706573608982 average time 2.371990527525668 iter num 40\n",
            "loss 0.15752330962004954 average time 2.3461557995337596 iter num 60\n",
            "loss 0.15752174651409756 average time 2.334466110350513 iter num 80\n",
            "loss 0.15752157545904782 average time 2.3303526273803437 iter num 100\n",
            "loss 0.15751490617384903 average time 2.279873924099957 iter num 20\n",
            "loss 0.15750908411236275 average time 2.309975086774648 iter num 40\n",
            "loss 0.15750533777273562 average time 2.3136467114668147 iter num 60\n",
            "loss 0.15750378017269603 average time 2.324648156137573 iter num 80\n",
            "loss 0.1575036088792641 average time 2.328922001100218 iter num 100\n",
            "loss 0.15749695706351294 average time 2.3305602382995856 iter num 20\n",
            "loss 0.15749114903324868 average time 2.323031360674031 iter num 40\n",
            "loss 0.1574874112170659 average time 2.3219229781322306 iter num 60\n",
            "loss 0.15748585939851173 average time 2.3236977212868624 iter num 80\n",
            "loss 0.15748568910782895 average time 2.322480710159434 iter num 100\n",
            "loss 0.15747905297502532 average time 2.3046514573499737 iter num 20\n",
            "loss 0.15747326044939886 average time 2.316263782499482 iter num 40\n",
            "loss 0.15746953218053575 average time 2.3053500366165585 iter num 60\n",
            "loss 0.1574679831470161 average time 2.3005939285627393 iter num 80\n",
            "loss 0.15746781302846696 average time 2.303828248930295 iter num 100\n",
            "loss 0.15746119414997056 average time 2.331570256999112 iter num 20\n",
            "loss 0.15745541497039844 average time 2.319202633774694 iter num 40\n",
            "loss 0.15745169874436662 average time 2.3098627148832747 iter num 60\n",
            "loss 0.15745015298694204 average time 2.2940762535878094 iter num 80\n",
            "loss 0.1574499842958366 average time 2.289476134600045 iter num 100\n",
            "loss 0.15744338272932024 average time 2.273567085150353 iter num 20\n",
            "loss 0.15743762189236818 average time 2.272921566799778 iter num 40\n",
            "loss 0.1574339113326055 average time 2.2779959526331366 iter num 60\n",
            "loss 0.15743237060917056 average time 2.2877914170249825 iter num 80\n",
            "loss 0.15743220312522235 average time 2.2861552609799403 iter num 100\n",
            "loss 0.15742561625543866 average time 2.2699135643495536 iter num 20\n",
            "loss 0.15741986878559502 average time 2.283966646175031 iter num 40\n",
            "loss 0.15741616871111347 average time 2.2845095476665542 iter num 60\n",
            "loss 0.15741463289871063 average time 2.2809755301250334 iter num 80\n",
            "loss 0.15741446482599197 average time 2.278154331200203 iter num 100\n",
            "loss 0.15740789560076524 average time 2.3362424069498955 iter num 20\n",
            "loss 0.15740216269521623 average time 2.3256558692748515 iter num 40\n",
            "loss 0.1573984723544775 average time 2.303883046399763 iter num 60\n",
            "loss 0.15739694016587133 average time 2.299103681962424 iter num 80\n",
            "loss 0.15739677341164904 average time 2.3014389105600275 iter num 100\n",
            "loss 0.15739021476220202 average time 2.322825653250402 iter num 20\n",
            "loss 0.15738449970954904 average time 2.276588342549803 iter num 40\n",
            "loss 0.15738081740032112 average time 2.2878690956497545 iter num 60\n",
            "loss 0.15737928887010144 average time 2.284005519999846 iter num 80\n",
            "loss 0.15737912367045162 average time 2.280841605349924 iter num 100\n",
            "loss 0.1573725808577379 average time 2.2952605257498364 iter num 20\n",
            "loss 0.15736687832790117 average time 2.282644257200263 iter num 40\n",
            "loss 0.15736320682836769 average time 2.2521255044500625 iter num 60\n",
            "loss 0.1573616848460897 average time 2.2569833267372816 iter num 80\n",
            "loss 0.1573615189028395 average time 2.264383213099718 iter num 100\n",
            "loss 0.15735498927023447 average time 2.3154576718505266 iter num 20\n",
            "loss 0.15734930111927425 average time 2.31055438805015 iter num 40\n",
            "loss 0.15734563701061832 average time 2.2920679351841198 iter num 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a121ea7f866f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_MC_oneDS_SGD_3.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a121ea7f866f>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a121ea7f866f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a121ea7f866f>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# normalize the parameter to range [0-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN1289EbbdX-"
      },
      "source": [
        "#1hr 16min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyNIY9Yvbfk1"
      },
      "source": [
        "## Continued Continued Continued Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H_uvwLVqbfuU",
        "outputId": "b1783405-7ed9-466e-9b20-aea9976369a7"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import SGD\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = SGD(model.parameters(), lr=1e-3) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 200)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_SGD_4.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.15733865396808083 average time 2.2630760662006653 iter num 20\n",
            "loss 0.15733297865382898 average time 2.3097834692003745 iter num 40\n",
            "loss 0.15732932184406864 average time 2.320877319767048 iter num 60\n",
            "loss 0.15732780719064482 average time 2.3217156298003827 iter num 80\n",
            "loss 0.1573276424550325 average time 2.3173264616005325 iter num 100\n",
            "loss 0.15732114961906601 average time 2.309592571050234 iter num 20\n",
            "loss 0.15731548459343023 average time 2.2918997797756675 iter num 40\n",
            "loss 0.1573118363721942 average time 2.2884598610334552 iter num 60\n",
            "loss 0.1573103263776297 average time 2.2874198805127888 iter num 80\n",
            "loss 0.1573101632905118 average time 2.2762016343503637 iter num 100\n",
            "loss 0.15730368310509996 average time 2.2491593338501845 iter num 20\n",
            "loss 0.15729803182966218 average time 2.2633440056997642 iter num 40\n",
            "loss 0.15729439229779213 average time 2.250903064083468 iter num 60\n",
            "loss 0.1572928858409673 average time 2.255341844525174 iter num 80\n",
            "loss 0.1572927233416173 average time 2.260387919720088 iter num 100\n",
            "loss 0.15728625550773417 average time 2.333604821650442 iter num 20\n",
            "loss 0.1572806202086223 average time 2.300457950325472 iter num 40\n",
            "loss 0.15727698939034576 average time 2.3008296053337594 iter num 60\n",
            "loss 0.15727548560956803 average time 2.285718176037426 iter num 80\n",
            "loss 0.15727532263309804 average time 2.2786407244401925 iter num 100\n",
            "loss 0.157268868316646 average time 2.248787362049188 iter num 20\n",
            "loss 0.1572632443531642 average time 2.2608854021993467 iter num 40\n",
            "loss 0.1572596212528473 average time 2.262269250883037 iter num 60\n",
            "loss 0.15725811956155808 average time 2.269647465449907 iter num 80\n",
            "loss 0.15725795832526082 average time 2.265005549899797 iter num 100\n",
            "loss 0.15725151528925527 average time 2.289397563050079 iter num 20\n",
            "loss 0.15724590257727408 average time 2.2707951227503145 iter num 40\n",
            "loss 0.15724228868386325 average time 2.2651876703667466 iter num 60\n",
            "loss 0.15724079081763256 average time 2.2704735110623004 iter num 80\n",
            "loss 0.15724062925905238 average time 2.2681012518496573 iter num 100\n",
            "loss 0.15723419889598136 average time 2.2591062841995155 iter num 20\n",
            "loss 0.15722859804833197 average time 2.276106884024921 iter num 40\n",
            "loss 0.15722499271638496 average time 2.286938059133172 iter num 60\n",
            "loss 0.15722349917494072 average time 2.3045899979746536 iter num 80\n",
            "loss 0.15722333900125435 average time 2.29373464402961 iter num 100\n",
            "loss 0.15721692181306832 average time 2.340962212399245 iter num 20\n",
            "loss 0.1572113311642225 average time 2.3073419733244007 iter num 40\n",
            "loss 0.15720773322738868 average time 2.308058789649658 iter num 60\n",
            "loss 0.15720624468884203 average time 2.2953432688120303 iter num 80\n",
            "loss 0.15720608435123964 average time 2.2842060000096533 iter num 100\n",
            "loss 0.15719968166638446 average time 2.2328177656498154 iter num 20\n",
            "loss 0.15719410189743735 average time 2.274201563149654 iter num 40\n",
            "loss 0.1571905121802078 average time 2.2888964078830516 iter num 60\n",
            "loss 0.15718902534904328 average time 2.301008033799826 iter num 80\n",
            "loss 0.15718886508870022 average time 2.304826112009905 iter num 100\n",
            "loss 0.15718247269576063 average time 2.406708229051219 iter num 20\n",
            "loss 0.15717690469517126 average time 2.382099821225893 iter num 40\n",
            "loss 0.15717332360884742 average time 2.359008554367271 iter num 60\n",
            "loss 0.15717184024394734 average time 2.3705918840378217 iter num 80\n",
            "loss 0.15717168099199927 average time 2.3650411702400014 iter num 100\n",
            "loss 0.15716530150635763 average time 2.357666660249015 iter num 20\n",
            "loss 0.15715974396274662 average time 2.3746964054740602 iter num 40\n",
            "loss 0.1571561700613485 average time 2.379118698549549 iter num 60\n",
            "loss 0.15715468974004748 average time 2.37453719094965 iter num 80\n",
            "loss 0.15715453267474044 average time 2.3701239888498096 iter num 100\n",
            "loss 0.1571481657712248 average time 2.3826183345998286 iter num 20\n",
            "loss 0.1571426215496827 average time 2.401171875824912 iter num 40\n",
            "loss 0.15713905416208648 average time 2.390481913033727 iter num 60\n",
            "loss 0.15713757869576833 average time 2.37559237583755 iter num 80\n",
            "loss 0.15713742105866269 average time 2.364615246670146 iter num 100\n",
            "loss 0.15713106664219917 average time 2.396126081499824 iter num 20\n",
            "loss 0.15712553142688496 average time 2.4201451093502326 iter num 40\n",
            "loss 0.15712197226312896 average time 2.416827864767159 iter num 60\n",
            "loss 0.1571205015741081 average time 2.4036179024249575 iter num 80\n",
            "loss 0.1571203423231554 average time 2.4049474375899447 iter num 100\n",
            "loss 0.1571140021151736 average time 2.341565770100715 iter num 20\n",
            "loss 0.15710847851944448 average time 2.3641657849502735 iter num 40\n",
            "loss 0.15710492709818488 average time 2.367828371384045 iter num 60\n",
            "loss 0.15710345667959832 average time 2.3571647038750596 iter num 80\n",
            "loss 0.1571032999650671 average time 2.359000218029978 iter num 100\n",
            "loss 0.15709697288283908 average time 2.364816947151121 iter num 20\n",
            "loss 0.15709146158333614 average time 2.3438562374252796 iter num 40\n",
            "loss 0.15708791551265233 average time 2.354746130533628 iter num 60\n",
            "loss 0.1570864483236368 average time 2.362921085125163 iter num 80\n",
            "loss 0.15708629258511017 average time 2.374158049030157 iter num 100\n",
            "loss 0.15707997808415122 average time 2.403104099850316 iter num 20\n",
            "loss 0.15707447647717443 average time 2.3810364086752087 iter num 40\n",
            "loss 0.15707093728170451 average time 2.367014464816748 iter num 60\n",
            "loss 0.1570694724524926 average time 2.360724961874894 iter num 80\n",
            "loss 0.15706931753962797 average time 2.3514509535700925 iter num 100\n",
            "loss 0.15706301587515 average time 2.3146045985504315 iter num 20\n",
            "loss 0.1570575250112319 average time 2.310709313650295 iter num 40\n",
            "loss 0.15705399310231138 average time 2.3073112850666804 iter num 60\n",
            "loss 0.15705253314860446 average time 2.2989349304751157 iter num 80\n",
            "loss 0.15705237755054327 average time 2.2960108848399976 iter num 100\n",
            "loss 0.15704608991484678 average time 2.2610805793490725 iter num 20\n",
            "loss 0.15704060804709297 average time 2.254945415049588 iter num 40\n",
            "loss 0.1570370845042314 average time 2.2540439561661816 iter num 60\n",
            "loss 0.1570356266533327 average time 2.2532135571494107 iter num 80\n",
            "loss 0.15703547142129895 average time 2.258798672089615 iter num 100\n",
            "loss 0.1570291962099088 average time 2.311940783449245 iter num 20\n",
            "loss 0.15702372753395297 average time 2.307256776174836 iter num 40\n",
            "loss 0.15702020914243325 average time 2.309024528599548 iter num 60\n",
            "loss 0.15701875476005756 average time 2.322582322424478 iter num 80\n",
            "loss 0.15701860030283724 average time 2.319754632079639 iter num 100\n",
            "loss 0.15701233629971925 average time 2.389324388148816 iter num 20\n",
            "loss 0.15700687956769505 average time 2.3576586855242567 iter num 40\n",
            "loss 0.1570033657287183 average time 2.3394125997497515 iter num 60\n",
            "loss 0.15700191281796183 average time 2.3412455066752047 iter num 80\n",
            "loss 0.15700175939976674 average time 2.336706649900443 iter num 100\n",
            "loss 0.15699550730377168 average time 2.3078531775994633 iter num 20\n",
            "loss 0.15699005945893932 average time 2.3140696556250986 iter num 40\n",
            "loss 0.15698655017362045 average time 2.313735385283265 iter num 60\n",
            "loss 0.15698510313424235 average time 2.3221778625748812 iter num 80\n",
            "loss 0.15698494984158773 average time 2.3299961538500793 iter num 100\n",
            "loss 0.1569787105671173 average time 2.3542257045501174 iter num 20\n",
            "loss 0.15697327077497075 average time 2.3468783875992814 iter num 40\n",
            "loss 0.15696976888884448 average time 2.335779074782598 iter num 60\n",
            "loss 0.15696832259219307 average time 2.3407457691245326 iter num 80\n",
            "loss 0.15696816983095746 average time 2.352905771849328 iter num 100\n",
            "loss 0.1569619393188766 average time 2.377379722751357 iter num 20\n",
            "loss 0.15695651096937965 average time 2.3692162390503655 iter num 40\n",
            "loss 0.15695301350794294 average time 2.3628786862000806 iter num 60\n",
            "loss 0.15695157208736718 average time 2.376402891012276 iter num 80\n",
            "loss 0.1569514190099252 average time 2.3775504520197863 iter num 100\n",
            "loss 0.15694519837167595 average time 2.388909325100758 iter num 20\n",
            "loss 0.15693977834134654 average time 2.3655670515250677 iter num 40\n",
            "loss 0.15693628891600236 average time 2.3634834306331567 iter num 60\n",
            "loss 0.15693484717112835 average time 2.3667322152626182 iter num 80\n",
            "loss 0.15693469555751113 average time 2.3700372325701755 iter num 100\n",
            "loss 0.1569284858128362 average time 2.3838026207515215 iter num 20\n",
            "loss 0.15692307591357907 average time 2.377304807375549 iter num 40\n",
            "loss 0.15691959091802488 average time 2.382123998016808 iter num 60\n",
            "loss 0.15691815174106646 average time 2.3794853272750514 iter num 80\n",
            "loss 0.1569179996483604 average time 2.3813639983801114 iter num 100\n",
            "loss 0.15691180084230244 average time 2.3507433903007042 iter num 20\n",
            "loss 0.15690639905627846 average time 2.351107999800297 iter num 40\n",
            "loss 0.15690292040703824 average time 2.358510589183546 iter num 60\n",
            "loss 0.15690148567596957 average time 2.356632147162782 iter num 80\n",
            "loss 0.15690133471454307 average time 2.3612074244004178 iter num 100\n",
            "loss 0.1568951436955111 average time 2.403366123399974 iter num 20\n",
            "loss 0.15688975257868326 average time 2.3964993893994686 iter num 40\n",
            "loss 0.15688627924880277 average time 2.389648103249662 iter num 60\n",
            "loss 0.15688484616466836 average time 2.3837732123249227 iter num 80\n",
            "loss 0.15688469510343728 average time 2.379317802349833 iter num 100\n",
            "loss 0.15687851617149554 average time 2.3622660887485836 iter num 20\n",
            "loss 0.15687313206521183 average time 2.3460176941242024 iter num 40\n",
            "loss 0.15686966514277612 average time 2.337728935832638 iter num 60\n",
            "loss 0.15686823596481314 average time 2.3276514253243477 iter num 80\n",
            "loss 0.15686808437212083 average time 2.3314081501993495 iter num 100\n",
            "loss 0.1568619163003558 average time 2.312632605500403 iter num 20\n",
            "loss 0.15685654186530082 average time 2.3132589609505887 iter num 40\n",
            "loss 0.15685307834907794 average time 2.313660991050468 iter num 60\n",
            "loss 0.15685165162097636 average time 2.3300405264378243 iter num 80\n",
            "loss 0.1568515012990377 average time 2.327510107040289 iter num 100\n",
            "loss 0.15684534142612633 average time 2.302460384299411 iter num 20\n",
            "loss 0.15683997384842405 average time 2.332821647249875 iter num 40\n",
            "loss 0.15683651671475957 average time 2.3324288107663356 iter num 60\n",
            "loss 0.15683509207694185 average time 2.345680282174635 iter num 80\n",
            "loss 0.15683494227902683 average time 2.3359129693994327 iter num 100\n",
            "loss 0.15682879245194428 average time 2.3960362333000376 iter num 20\n",
            "loss 0.1568234330101487 average time 2.358232203249827 iter num 40\n",
            "loss 0.15681998255110605 average time 2.348963632633483 iter num 60\n",
            "loss 0.1568185600896582 average time 2.3415965989753205 iter num 80\n",
            "loss 0.15681841083641504 average time 2.339510117370446 iter num 100\n",
            "loss 0.15681227075535856 average time 2.371452175450031 iter num 20\n",
            "loss 0.15680692073095495 average time 2.344272066875237 iter num 40\n",
            "loss 0.15680347449575613 average time 2.3412850802332588 iter num 60\n",
            "loss 0.15680205530748234 average time 2.3470646414498333 iter num 80\n",
            "loss 0.15680190669850916 average time 2.341284895179997 iter num 100\n",
            "loss 0.15679577672461464 average time 2.330194039350681 iter num 20\n",
            "loss 0.15679043408229487 average time 2.3474446617254214 iter num 40\n",
            "loss 0.15678699522945624 average time 2.3311955841335776 iter num 60\n",
            "loss 0.15678557766089293 average time 2.33286996752513 iter num 80\n",
            "loss 0.15678542865347608 average time 2.3274829798300924 iter num 100\n",
            "loss 0.15677930866268308 average time 2.302749067100012 iter num 20\n",
            "loss 0.15677397481266106 average time 2.3049251150752754 iter num 40\n",
            "loss 0.15677053978603128 average time 2.3088513793005405 iter num 60\n",
            "loss 0.15676912472967675 average time 2.3298550162501668 iter num 80\n",
            "loss 0.15676897741293472 average time 2.324849870539838 iter num 100\n",
            "loss 0.1567628687564078 average time 2.287927731799937 iter num 20\n",
            "loss 0.15675754063277925 average time 2.2970247498746175 iter num 40\n",
            "loss 0.15675411115795282 average time 2.2904107625832695 iter num 60\n",
            "loss 0.1567527001356098 average time 2.3002866728120352 iter num 80\n",
            "loss 0.15675255241663177 average time 2.3085497203596606 iter num 100\n",
            "loss 0.15674645260460265 average time 2.2952566735493747 iter num 20\n",
            "loss 0.156741135131487 average time 2.314115865149688 iter num 40\n",
            "loss 0.15673771017137272 average time 2.3189615515996898 iter num 60\n",
            "loss 0.15673629976416756 average time 2.3172025917876455 iter num 80\n",
            "loss 0.15673615263090881 average time 2.3266721900900302 iter num 100\n",
            "loss 0.15673006204464465 average time 2.326769851299832 iter num 20\n",
            "loss 0.15672475256323606 average time 2.328240755849765 iter num 40\n",
            "loss 0.15672133301395574 average time 2.3161308466165793 iter num 60\n",
            "loss 0.15671992512807478 average time 2.302797904837189 iter num 80\n",
            "loss 0.15671977843422027 average time 2.291195986269668 iter num 100\n",
            "loss 0.1567136993960512 average time 2.2866287340999407 iter num 20\n",
            "loss 0.1567083964341206 average time 2.2946299119003015 iter num 40\n",
            "loss 0.1567049813866982 average time 2.2984951828839257 iter num 60\n",
            "loss 0.15670357517679753 average time 2.3012002766255137 iter num 80\n",
            "loss 0.15670342800941162 average time 2.298113001850361 iter num 100\n",
            "loss 0.15669735698361933 average time 2.3199358744986966 iter num 20\n",
            "loss 0.15669206192180302 average time 2.2904778054993584 iter num 40\n",
            "loss 0.15668865368232154 average time 2.2837778736991217 iter num 60\n",
            "loss 0.15668724988226365 average time 2.2836081774617925 iter num 80\n",
            "loss 0.1566871018323526 average time 2.282166115749569 iter num 100\n",
            "loss 0.15668103953607776 average time 2.251474773499285 iter num 20\n",
            "loss 0.15667575337273731 average time 2.2769066073240536 iter num 40\n",
            "loss 0.1566723479043481 average time 2.289916325182518 iter num 60\n",
            "loss 0.15667094644977153 average time 2.2870066834869247 iter num 80\n",
            "loss 0.15667080069090927 average time 2.305514363999464 iter num 100\n",
            "loss 0.15666474401158187 average time 2.3502056985002127 iter num 20\n",
            "loss 0.156659466634046 average time 2.3785599582752184 iter num 40\n",
            "loss 0.15665606578156277 average time 2.359431683783866 iter num 60\n",
            "loss 0.15665466842568562 average time 2.3556593705879094 iter num 80\n",
            "loss 0.15665452152984405 average time 2.3559117365406563 iter num 100\n",
            "loss 0.15664847470394855 average time 2.34688893950115 iter num 20\n",
            "loss 0.15664320308239296 average time 2.33776591490041 iter num 40\n",
            "loss 0.15663980747526915 average time 2.3582940926336353 iter num 60\n",
            "loss 0.1566384107910584 average time 2.3567031418127042 iter num 80\n",
            "loss 0.1566382654666508 average time 2.3672312926800805 iter num 100\n",
            "loss 0.1566322254893428 average time 2.391586261500561 iter num 20\n",
            "loss 0.1566269630434152 average time 2.3890898057758023 iter num 40\n",
            "loss 0.15662357122395026 average time 2.392946361317202 iter num 60\n",
            "loss 0.15662217475990065 average time 2.3761518422628796 iter num 80\n",
            "loss 0.15662202897502303 average time 2.3777575637604604 iter num 100\n",
            "loss 0.15661599701349554 average time 2.3099644428009922 iter num 20\n",
            "loss 0.1566107411252205 average time 2.332496220500252 iter num 40\n",
            "loss 0.1566073530481689 average time 2.3505647885668925 iter num 60\n",
            "loss 0.15660596005284608 average time 2.358502923787364 iter num 80\n",
            "loss 0.15660581500282358 average time 2.3598778857300204 iter num 100\n",
            "loss 0.15659979149309217 average time 2.3758774908506894 iter num 20\n",
            "loss 0.15659454318902785 average time 2.3846799318504055 iter num 40\n",
            "loss 0.15659116057146769 average time 2.37735347520041 iter num 60\n",
            "loss 0.15658976826651624 average time 2.3584542412379053 iter num 80\n",
            "loss 0.1565896232067446 average time 2.3686469417801708 iter num 100\n",
            "loss 0.15658361019788836 average time 2.335888841900305 iter num 20\n",
            "loss 0.15657836730583657 average time 2.33072160680058 iter num 40\n",
            "loss 0.15657498874662254 average time 2.344745354733216 iter num 60\n",
            "loss 0.15657359773549706 average time 2.34085004007502 iter num 80\n",
            "loss 0.15657345378083112 average time 2.3476634147202278 iter num 100\n",
            "loss 0.15656744508889398 average time 2.3855569861007098 iter num 20\n",
            "loss 0.15656220985848254 average time 2.380196406625328 iter num 40\n",
            "loss 0.15655883594078757 average time 2.3823139236003046 iter num 60\n",
            "loss 0.156557448395795 average time 2.38859382508781 iter num 80\n",
            "loss 0.156557305323628 average time 2.3848002235302554 iter num 100\n",
            "loss 0.15655130385655144 average time 2.3477811870005096 iter num 20\n",
            "loss 0.15654607528183018 average time 2.34887678064988 iter num 40\n",
            "loss 0.15654270567715614 average time 2.3342091148498487 iter num 60\n",
            "loss 0.15654131969484655 average time 2.3375100012248367 iter num 80\n",
            "loss 0.15654117680111268 average time 2.3375984659599633 iter num 100\n",
            "loss 0.15653518421704837 average time 2.3909432834501785 iter num 20\n",
            "loss 0.15652996391243074 average time 2.3636879501002115 iter num 40\n",
            "loss 0.15652659602463329 average time 2.3598040840499985 iter num 60\n",
            "loss 0.15652521204311348 average time 2.346112980587532 iter num 80\n",
            "loss 0.1565250692362032 average time 2.345678945170002 iter num 100\n",
            "loss 0.1565190861858538 average time 2.3118425116503203 iter num 20\n",
            "loss 0.15651386956561217 average time 2.314893094299987 iter num 40\n",
            "loss 0.15651050671943711 average time 2.3291242203002183 iter num 60\n",
            "loss 0.15650912517865376 average time 2.339478235887691 iter num 80\n",
            "loss 0.15650898282799938 average time 2.3523546732301472 iter num 100\n",
            "loss 0.15650300424264113 average time 2.3803867217004155 iter num 20\n",
            "loss 0.15649779532635472 average time 2.3502894848752476 iter num 40\n",
            "loss 0.15649443549349148 average time 2.358714047083777 iter num 60\n",
            "loss 0.15649305697185584 average time 2.365278454262989 iter num 80\n",
            "loss 0.15649291432862436 average time 2.3592669587703856 iter num 100\n",
            "loss 0.15648694222129694 average time 2.3780385761998333 iter num 20\n",
            "loss 0.15648174109641896 average time 2.3510220645996016 iter num 40\n",
            "loss 0.15647838619841523 average time 2.353058419083148 iter num 60\n",
            "loss 0.15647700792134275 average time 2.341667773937297 iter num 80\n",
            "loss 0.15647686668671645 average time 2.351014924849878 iter num 100\n",
            "loss 0.15647090340169814 average time 2.3552657932490546 iter num 20\n",
            "loss 0.15646570902222293 average time 2.35196016859918 iter num 40\n",
            "loss 0.15646235613757986 average time 2.3620618615993103 iter num 60\n",
            "loss 0.1564609828142085 average time 2.3533293586371657 iter num 80\n",
            "loss 0.15646083965307817 average time 2.3439677715697327 iter num 100\n",
            "loss 0.1564548888477777 average time 2.268456174799212 iter num 20\n",
            "loss 0.1564496992073501 average time 2.2768105320497853 iter num 40\n",
            "loss 0.1564463512134974 average time 2.2945281808999427 iter num 60\n",
            "loss 0.15644497844293065 average time 2.2919547308749317 iter num 80\n",
            "loss 0.15644483588670088 average time 2.283143264640021 iter num 100\n",
            "loss 0.15643889442404915 average time 2.2561002392983935 iter num 20\n",
            "loss 0.15643370981529012 average time 2.2708397223490464 iter num 40\n",
            "loss 0.1564303665579017 average time 2.258790658015884 iter num 60\n",
            "loss 0.1564289961437627 average time 2.26647185207421 iter num 80\n",
            "loss 0.1564288523410843 average time 2.2689387737891957 iter num 100\n",
            "loss 0.15642291887678603 average time 2.280469512399577 iter num 20\n",
            "loss 0.15641774180106705 average time 2.286998394499642 iter num 40\n",
            "loss 0.1564144042780306 average time 2.2967562879497563 iter num 60\n",
            "loss 0.15641303332657527 average time 2.28539272479984 iter num 80\n",
            "loss 0.15641289142058676 average time 2.2745803769497432 iter num 100\n",
            "loss 0.15640696716561134 average time 2.2883146611497067 iter num 20\n",
            "loss 0.15640179695663964 average time 2.269819132925295 iter num 40\n",
            "loss 0.15639846032717897 average time 2.2722528702837734 iter num 60\n",
            "loss 0.15639709318716405 average time 2.264462550362805 iter num 80\n",
            "loss 0.15639695060775083 average time 2.2669717734302686 iter num 100\n",
            "loss 0.1563910327905671 average time 2.2624856381004066 iter num 20\n",
            "loss 0.15638586853124445 average time 2.261388116499802 iter num 40\n",
            "loss 0.1563825366153378 average time 2.278279654066864 iter num 60\n",
            "loss 0.15638117112129826 average time 2.284656835587339 iter num 80\n",
            "loss 0.15638102834372536 average time 2.2799140121698294 iter num 100\n",
            "loss 0.15637511847964283 average time 2.3447610907998753 iter num 20\n",
            "loss 0.15636995991381383 average time 2.34396472212502 iter num 40\n",
            "loss 0.1563666313076446 average time 2.326953680499719 iter num 60\n",
            "loss 0.1563652662879071 average time 2.31837411866245 iter num 80\n",
            "loss 0.15636512624743315 average time 2.3142925441898115 iter num 100\n",
            "loss 0.15635922267832295 average time 2.247880499049643 iter num 20\n",
            "loss 0.15635407372899363 average time 2.26184308915017 iter num 40\n",
            "loss 0.15635074738512827 average time 2.250749651249983 iter num 60\n",
            "loss 0.1563493847569725 average time 2.2572300260249905 iter num 80\n",
            "loss 0.156349243393259 average time 2.2555059811702085 iter num 100\n",
            "loss 0.15634334651579074 average time 2.200915621750755 iter num 20\n",
            "loss 0.1563381999481211 average time 2.2491914174001066 iter num 40\n",
            "loss 0.15633487832606358 average time 2.2263884989333746 iter num 60\n",
            "loss 0.15633351800727896 average time 2.226441947674721 iter num 80\n",
            "loss 0.15633337709611256 average time 2.2362239028097246 iter num 100\n",
            "loss 0.15632748793711673 average time 2.2597632331995556 iter num 20\n",
            "loss 0.15632234649674134 average time 2.263329590699686 iter num 40\n",
            "loss 0.15631902909173362 average time 2.24534251194976 iter num 60\n",
            "loss 0.15631767048945655 average time 2.245990757987238 iter num 80\n",
            "loss 0.15631752962889647 average time 2.2429639256697556 iter num 100\n",
            "loss 0.15631164605327372 average time 2.2216806922508114 iter num 20\n",
            "loss 0.15630651102270318 average time 2.2281004973001473 iter num 40\n",
            "loss 0.1563031973725394 average time 2.235739257816992 iter num 60\n",
            "loss 0.1563018401786993 average time 2.2332759978879038 iter num 80\n",
            "loss 0.15630169960749352 average time 2.2382838490302674 iter num 100\n",
            "loss 0.15629582124757138 average time 2.2491561921007817 iter num 20\n",
            "loss 0.15629069341689705 average time 2.2401057149249026 iter num 40\n",
            "loss 0.15628738060794878 average time 2.240185430250373 iter num 60\n",
            "loss 0.1562860262222152 average time 2.2409345012627453 iter num 80\n",
            "loss 0.15628588655871387 average time 2.24384923019039 iter num 100\n",
            "loss 0.15628001528165017 average time 2.2661878493996483 iter num 20\n",
            "loss 0.15627489209172324 average time 2.261586769549649 iter num 40\n",
            "loss 0.15627158438898753 average time 2.2660417553165946 iter num 60\n",
            "loss 0.15627022983955147 average time 2.2617709262376593 iter num 80\n",
            "loss 0.15627009000106984 average time 2.257076066760128 iter num 100\n",
            "loss 0.1562642254380513 average time 2.235989869301193 iter num 20\n",
            "loss 0.15625911012234622 average time 2.2418768868503323 iter num 40\n",
            "loss 0.15625580409647033 average time 2.252012605450121 iter num 60\n",
            "loss 0.15625444985601478 average time 2.248390790175108 iter num 80\n",
            "loss 0.15625431058417455 average time 2.2543874289800443 iter num 100\n",
            "loss 0.1562484535695816 average time 2.254104249001466 iter num 20\n",
            "loss 0.15624334152730496 average time 2.249210343450977 iter num 40\n",
            "loss 0.15624003966700134 average time 2.2580930085170015 iter num 60\n",
            "loss 0.15623868853344475 average time 2.260187652750301 iter num 80\n",
            "loss 0.15623854911834228 average time 2.255086521650228 iter num 100\n",
            "loss 0.15623269715874744 average time 2.244126794001204 iter num 20\n",
            "loss 0.15622758985221025 average time 2.2425001145009444 iter num 40\n",
            "loss 0.1562242942460429 average time 2.2479099667007785 iter num 60\n",
            "loss 0.15622294346052512 average time 2.2502999303509568 iter num 80\n",
            "loss 0.15622280445359313 average time 2.252540645330737 iter num 100\n",
            "loss 0.1562169559767898 average time 2.2555240323497854 iter num 20\n",
            "loss 0.15621185710350177 average time 2.2478774772996983 iter num 40\n",
            "loss 0.15620856303854652 average time 2.2600438677163766 iter num 60\n",
            "loss 0.1562072134731546 average time 2.2597361641249334 iter num 80\n",
            "loss 0.15620707527695596 average time 2.2623255090598833 iter num 100\n",
            "loss 0.1562012344973029 average time 2.3351326069503555 iter num 20\n",
            "loss 0.15619613740648858 average time 2.2955976319753972 iter num 40\n",
            "loss 0.15619284778907477 average time 2.2623109089169398 iter num 60\n",
            "loss 0.15619149953725586 average time 2.2514681525628477 iter num 80\n",
            "loss 0.1561913624148918 average time 2.2469925293204143 iter num 100\n",
            "loss 0.15618552547789696 average time 2.194889182249608 iter num 20\n",
            "loss 0.1561804354749395 average time 2.193209297274734 iter num 40\n",
            "loss 0.1561771493045559 average time 2.2000415375834566 iter num 60\n",
            "loss 0.1561758017669868 average time 2.2161383309503435 iter num 80\n",
            "loss 0.15617566504900599 average time 2.2201637622202544 iter num 100\n",
            "loss 0.15616983250362887 average time 2.1807287161998827 iter num 20\n",
            "loss 0.1561647485597892 average time 2.210064745275486 iter num 40\n",
            "loss 0.1561614645258126 average time 2.2122708030334857 iter num 60\n",
            "loss 0.15616011921519302 average time 2.2140543667877863 iter num 80\n",
            "loss 0.15615998202457793 average time 2.2180128914301895 iter num 100\n",
            "loss 0.15615415699848403 average time 2.2109262733498327 iter num 20\n",
            "loss 0.1561490771366577 average time 2.2131328564499197 iter num 40\n",
            "loss 0.15614579684370472 average time 2.2145613147335097 iter num 60\n",
            "loss 0.15614445424741877 average time 2.215546908475153 iter num 80\n",
            "loss 0.15614431715386226 average time 2.214908492310278 iter num 100\n",
            "loss 0.15613849862908868 average time 2.2266120577998665 iter num 20\n",
            "loss 0.15613342417003734 average time 2.2019760980745557 iter num 40\n",
            "loss 0.1561301464423024 average time 2.19261310981589 iter num 60\n",
            "loss 0.15612880574060875 average time 2.190107740799431 iter num 80\n",
            "loss 0.15612866722124882 average time 2.1921603914294976 iter num 100\n",
            "loss 0.15612285577357463 average time 2.232231592800235 iter num 20\n",
            "loss 0.15611778726545278 average time 2.246932358225058 iter num 40\n",
            "loss 0.15611451173901517 average time 2.2364165882503584 iter num 60\n",
            "loss 0.156113172179843 average time 2.2160725275627557 iter num 80\n",
            "loss 0.15611303485710978 average time 2.2129543108802316 iter num 100\n",
            "loss 0.15610723036242333 average time 2.238370395349193 iter num 20\n",
            "loss 0.1561021658029681 average time 2.2308642274247177 iter num 40\n",
            "loss 0.15609889403636001 average time 2.2419167571832075 iter num 60\n",
            "loss 0.15609755488143381 average time 2.233636400499654 iter num 80\n",
            "loss 0.1560974179999276 average time 2.238881449009932 iter num 100\n",
            "loss 0.15609161795435822 average time 2.3010370063002483 iter num 20\n",
            "loss 0.15608655931539261 average time 2.2947382804501104 iter num 40\n",
            "loss 0.15608329115200578 average time 2.2790073887499602 iter num 60\n",
            "loss 0.1560819545681026 average time 2.2794802411874118 iter num 80\n",
            "loss 0.15608181755376718 average time 2.2730471483496513 iter num 100\n",
            "loss 0.1560760222529366 average time 2.3457758967491826 iter num 20\n",
            "loss 0.15607096617857164 average time 2.3466781839499165 iter num 40\n",
            "loss 0.15606770262876368 average time 2.320685258316856 iter num 60\n",
            "loss 0.15606636731026996 average time 2.314220480499807 iter num 80\n",
            "loss 0.15606622896291356 average time 2.300353548720232 iter num 100\n",
            "loss 0.1560604411288032 average time 2.30317189280031 iter num 20\n",
            "loss 0.15605538998064322 average time 2.294879362524989 iter num 40\n",
            "loss 0.1560521281362179 average time 2.2907985035165135 iter num 60\n",
            "loss 0.1560507941479521 average time 2.2877226603246528 iter num 80\n",
            "loss 0.15605065815362357 average time 2.280731887179863 iter num 100\n",
            "loss 0.15604487366262323 average time 2.238958941549208 iter num 20\n",
            "loss 0.15603982782965292 average time 2.257867073599118 iter num 40\n",
            "loss 0.1560365697444498 average time 2.2498600200495273 iter num 60\n",
            "loss 0.1560352370041439 average time 2.2491997360497407 iter num 80\n",
            "loss 0.15603510062023435 average time 2.242759277749792 iter num 100\n",
            "loss 0.1560293216986592 average time 2.222781955699247 iter num 20\n",
            "loss 0.1560242809411479 average time 2.2474730815496513 iter num 40\n",
            "loss 0.15602102642840504 average time 2.2411268550832273 iter num 60\n",
            "loss 0.15601969532477672 average time 2.2525818461873315 iter num 80\n",
            "loss 0.1560195588855251 average time 2.2470047231898933 iter num 100\n",
            "loss 0.15601378589616022 average time 2.2261345087990776 iter num 20\n",
            "loss 0.15600874828402025 average time 2.2271454111749334 iter num 40\n",
            "loss 0.15600549649323597 average time 2.235790503566386 iter num 60\n",
            "loss 0.1560041671957208 average time 2.241547153837291 iter num 80\n",
            "loss 0.15600403096267845 average time 2.2480895506597154 iter num 100\n",
            "loss 0.15599826288007806 average time 2.2056562779998785 iter num 20\n",
            "loss 0.15599323130757964 average time 2.209298374275022 iter num 40\n",
            "loss 0.1559899841428462 average time 2.216207389750222 iter num 60\n",
            "loss 0.15598865425924344 average time 2.2129269675127943 iter num 80\n",
            "loss 0.15598851802022315 average time 2.2159371381103847 iter num 100\n",
            "loss 0.15598275629471545 average time 2.2407119252507983 iter num 20\n",
            "loss 0.15597773002337129 average time 2.22743619840021 iter num 40\n",
            "loss 0.15597448536909334 average time 2.2279648439499096 iter num 60\n",
            "loss 0.15597315781764592 average time 2.227100186674943 iter num 80\n",
            "loss 0.15597302191392828 average time 2.2206909965699744 iter num 100\n",
            "loss 0.1559672649710564 average time 2.203488548500536 iter num 20\n",
            "loss 0.15596224337155337 average time 2.1967267815503875 iter num 40\n",
            "loss 0.15595900131982826 average time 2.2143071859000583 iter num 60\n",
            "loss 0.1559576745797242 average time 2.2146944712127152 iter num 80\n",
            "loss 0.15595753955618094 average time 2.2200487800701376 iter num 100\n",
            "loss 0.15595178717203995 average time 2.216215385050236 iter num 20\n",
            "loss 0.15594676940897226 average time 2.222959493149574 iter num 40\n",
            "loss 0.15594352941547732 average time 2.229499514733349 iter num 60\n",
            "loss 0.15594220531659997 average time 2.2310844770996483 iter num 80\n",
            "loss 0.15594206997222448 average time 2.227025336959705 iter num 100\n",
            "loss 0.15593632300344648 average time 2.2925709923016258 iter num 20\n",
            "loss 0.1559313076688276 average time 2.2451084112763056 iter num 40\n",
            "loss 0.15592807099554953 average time 2.2374409979005576 iter num 60\n",
            "loss 0.15592674784356744 average time 2.2530851543255266 iter num 80\n",
            "loss 0.15592661236609476 average time 2.249744756390355 iter num 100\n",
            "loss 0.155920870335349 average time 2.21256151439884 iter num 20\n",
            "loss 0.15591585938437036 average time 2.22348001487444 iter num 40\n",
            "loss 0.15591262648244264 average time 2.2211683433827907 iter num 60\n",
            "loss 0.15591130443772383 average time 2.2188324444746286 iter num 80\n",
            "loss 0.15591116948317416 average time 2.215755173329453 iter num 100\n",
            "loss 0.1559054323970933 average time 2.2048382294993645 iter num 20\n",
            "loss 0.15590042558147865 average time 2.187205803174584 iter num 40\n",
            "loss 0.1558971962687193 average time 2.1834650632662185 iter num 60\n",
            "loss 0.15589587442534977 average time 2.1840719197241922 iter num 80\n",
            "loss 0.15589574105007903 average time 2.1848768297694185 iter num 100\n",
            "loss 0.15589000773179715 average time 2.175001584900019 iter num 20\n",
            "loss 0.1558850059063887 average time 2.180070226274438 iter num 40\n",
            "loss 0.15588177894570462 average time 2.193101045032745 iter num 60\n",
            "loss 0.15588045983762533 average time 2.1923917142870777 iter num 80\n",
            "loss 0.15588032563423165 average time 2.1971464566895884 iter num 100\n",
            "loss 0.1558745970442606 average time 2.2353356504994735 iter num 20\n",
            "loss 0.15586959878968612 average time 2.231043622099969 iter num 40\n",
            "loss 0.15586637616228405 average time 2.2251796475998464 iter num 60\n",
            "loss 0.1558650582632331 average time 2.240668967212605 iter num 80\n",
            "loss 0.1558649235716318 average time 2.2438121565600158 iter num 100\n",
            "loss 0.15585920001944983 average time 2.2483038276004663 iter num 20\n",
            "loss 0.1558542059834902 average time 2.2646030674499342 iter num 40\n",
            "loss 0.15585098415285562 average time 2.2633851574497386 iter num 60\n",
            "loss 0.1558496676145537 average time 2.2677176283371407 iter num 80\n",
            "loss 0.1558495325330252 average time 2.273635276059649 iter num 100\n",
            "loss 0.15584381358957028 average time 2.2597105412991367 iter num 20\n",
            "loss 0.15583882524507747 average time 2.262812610724177 iter num 40\n",
            "loss 0.15583560525857423 average time 2.264499689349395 iter num 60\n",
            "loss 0.1558342898924978 average time 2.2580203652242745 iter num 80\n",
            "loss 0.1558341559777837 average time 2.2512457117293523 iter num 100\n",
            "loss 0.15582844113593444 average time 2.232987123651765 iter num 20\n",
            "loss 0.1558234554513855 average time 2.243117273250937 iter num 40\n",
            "loss 0.1558202400628623 average time 2.2604089273840753 iter num 60\n",
            "loss 0.15581892698389818 average time 2.2676794253382466 iter num 80\n",
            "loss 0.15581879126803805 average time 2.26625083883082 iter num 100\n",
            "loss 0.15581308223451937 average time 2.237296386599701 iter num 20\n",
            "loss 0.1558081004103158 average time 2.2693194882245735 iter num 40\n",
            "loss 0.1558048873745413 average time 2.266308610233197 iter num 60\n",
            "loss 0.1558035725045522 average time 2.2662226857873975 iter num 80\n",
            "loss 0.15580343847510233 average time 2.274118194899929 iter num 100\n",
            "loss 0.15579773409360215 average time 2.284077463700305 iter num 20\n",
            "loss 0.15579275494892966 average time 2.2647961767004743 iter num 40\n",
            "loss 0.15578954491078906 average time 2.26133814560053 iter num 60\n",
            "loss 0.15578823393862487 average time 2.259501796713084 iter num 80\n",
            "loss 0.1557880976595981 average time 2.2658661094904526 iter num 100\n",
            "loss 0.15578239692871865 average time 2.3000765892003985 iter num 20\n",
            "loss 0.15577742444907255 average time 2.2740060891501344 iter num 40\n",
            "loss 0.15577421517616444 average time 2.260983750783635 iter num 60\n",
            "loss 0.15577290310892966 average time 2.2592298885502715 iter num 80\n",
            "loss 0.1557727701418994 average time 2.2630525720398875 iter num 100\n",
            "loss 0.15576707148974592 average time 2.2844511318500738 iter num 20\n",
            "loss 0.1557621024172597 average time 2.2809457204753927 iter num 40\n",
            "loss 0.15575889552128552 average time 2.2920548332501007 iter num 60\n",
            "loss 0.15575758597801032 average time 2.292290224037606 iter num 80\n",
            "loss 0.15575745231810656 average time 2.2813398204201074 iter num 100\n",
            "loss 0.155751757536733 average time 2.233192200350095 iter num 20\n",
            "loss 0.15574679373212083 average time 2.2554512210250324 iter num 40\n",
            "loss 0.15574359015004655 average time 2.2665427747499054 iter num 60\n",
            "loss 0.15574228183618874 average time 2.2616349244874074 iter num 80\n",
            "loss 0.15574214844225903 average time 2.264701105860004 iter num 100\n",
            "loss 0.1557364579244925 average time 2.2777507021004566 iter num 20\n",
            "loss 0.15573149891581908 average time 2.267834488425251 iter num 40\n",
            "loss 0.15572829788534712 average time 2.268354451866738 iter num 60\n",
            "loss 0.155726989189149 average time 2.279962286637692 iter num 80\n",
            "loss 0.15572685644810044 average time 2.277302189790353 iter num 100\n",
            "loss 0.15572117074900776 average time 2.245734044600249 iter num 20\n",
            "loss 0.1557162135099672 average time 2.254591466250349 iter num 40\n",
            "loss 0.15571301497258347 average time 2.269127661284074 iter num 60\n",
            "loss 0.15571170759151312 average time 2.2711746970757303 iter num 80\n",
            "loss 0.15571157451569861 average time 2.267074101760518 iter num 100\n",
            "loss 0.1557058935768872 average time 2.2895493346502915 iter num 20\n",
            "loss 0.1557009400174725 average time 2.286697416125753 iter num 40\n",
            "loss 0.15569774400071373 average time 2.28877411710079 iter num 60\n",
            "loss 0.15569643820252288 average time 2.2879041351381604 iter num 80\n",
            "loss 0.15569630477641971 average time 2.29188173848037 iter num 100\n",
            "loss 0.15569062773758924 average time 2.267146422701262 iter num 20\n",
            "loss 0.15568567819774648 average time 2.259638641775564 iter num 40\n",
            "loss 0.1556824840917315 average time 2.263426869650357 iter num 60\n",
            "loss 0.15568117958604483 average time 2.2598279060503956 iter num 80\n",
            "loss 0.15568104619835335 average time 2.2553874556901428 iter num 100\n",
            "loss 0.15567537137843018 average time 2.293079966800724 iter num 20\n",
            "loss 0.15567042758458954 average time 2.274321637250614 iter num 40\n",
            "loss 0.15566723541139027 average time 2.2620056578166743 iter num 60\n",
            "loss 0.1556659315675281 average time 2.2660358576750697 iter num 80\n",
            "loss 0.15566579937472708 average time 2.2660993100199995 iter num 100\n",
            "loss 0.155660128835555 average time 2.266513279100036 iter num 20\n",
            "loss 0.1556551870332017 average time 2.2577566497498993 iter num 40\n",
            "loss 0.15565199834922555 average time 2.2668151917662422 iter num 60\n",
            "loss 0.15565069620957414 average time 2.2878037102122106 iter num 80\n",
            "loss 0.1556505638033807 average time 2.284741467999702 iter num 100\n",
            "loss 0.155644898360837 average time 2.300741717100027 iter num 20\n",
            "loss 0.1556399605168624 average time 2.2897537771499628 iter num 40\n",
            "loss 0.15563677400941678 average time 2.2778562603002985 iter num 60\n",
            "loss 0.15563547312407572 average time 2.2918916769375755 iter num 80\n",
            "loss 0.15563534070964008 average time 2.293523166849991 iter num 100\n",
            "loss 0.15562967748667894 average time 2.32169409289927 iter num 20\n",
            "loss 0.15562474496192097 average time 2.3151863342245633 iter num 40\n",
            "loss 0.15562155992610405 average time 2.303277528266214 iter num 60\n",
            "loss 0.15562025845085903 average time 2.302200983137027 iter num 80\n",
            "loss 0.15562012749645285 average time 2.298080123929467 iter num 100\n",
            "loss 0.15561446869010645 average time 2.285355604900178 iter num 20\n",
            "loss 0.15560953844023812 average time 2.260051338725134 iter num 40\n",
            "loss 0.1556063565439865 average time 2.2611477125334205 iter num 60\n",
            "loss 0.15560505837091862 average time 2.27510013502515 iter num 80\n",
            "loss 0.1556049250119856 average time 2.275352547610237 iter num 100\n",
            "loss 0.15559927046872477 average time 2.2904196259503804 iter num 20\n",
            "loss 0.15559434485789833 average time 2.262982675850617 iter num 40\n",
            "loss 0.15559116647071974 average time 2.293746574483521 iter num 60\n",
            "loss 0.15558986682506146 average time 2.2990481653125245 iter num 80\n",
            "loss 0.1555897351097002 average time 2.295531386399962 iter num 100\n",
            "loss 0.15558408463755555 average time 2.273165972449715 iter num 20\n",
            "loss 0.1555791618150512 average time 2.2735741823755236 iter num 40\n",
            "loss 0.15557598297779848 average time 2.273269160783578 iter num 60\n",
            "loss 0.15557468638672375 average time 2.278763153750242 iter num 80\n",
            "loss 0.15557455436791276 average time 2.2896222862001743 iter num 100\n",
            "loss 0.1555689075654235 average time 2.298118351049925 iter num 20\n",
            "loss 0.15556398939766153 average time 2.310299079225115 iter num 40\n",
            "loss 0.1555608140236891 average time 2.3025391664834993 iter num 60\n",
            "loss 0.1555595165920581 average time 2.305743534100111 iter num 80\n",
            "loss 0.1555593854132311 average time 2.3102416819901554 iter num 100\n",
            "loss 0.15555374406613218 average time 2.341188960999716 iter num 20\n",
            "loss 0.15554882679872123 average time 2.3455093918000784 iter num 40\n",
            "loss 0.15554565515813962 average time 2.3258716897005796 iter num 60\n",
            "loss 0.15554435902069555 average time 2.316842466237722 iter num 80\n",
            "loss 0.15554422717259203 average time 2.320565831420099 iter num 100\n",
            "loss 0.15553858961059888 average time 2.34283555764996 iter num 20\n",
            "loss 0.15553367696959466 average time 2.3171277248760815 iter num 40\n",
            "loss 0.15553050724397058 average time 2.327271219167718 iter num 60\n",
            "loss 0.15552921369220907 average time 2.3307495912258673 iter num 80\n",
            "loss 0.15552908137005078 average time 2.3336613976204537 iter num 100\n",
            "loss 0.1555234480746868 average time 2.3031933904516335 iter num 20\n",
            "loss 0.15551853798504386 average time 2.304429256376352 iter num 40\n",
            "loss 0.15551537078069264 average time 2.301844992900927 iter num 60\n",
            "loss 0.1555140771413056 average time 2.3130318264259584 iter num 80\n",
            "loss 0.1555139441810795 average time 2.319521384870677 iter num 100\n",
            "loss 0.15550831540818194 average time 2.328189493300306 iter num 20\n",
            "loss 0.15550341028588968 average time 2.3323567885747254 iter num 40\n",
            "loss 0.15550024404312904 average time 2.3425346435662506 iter num 60\n",
            "loss 0.15549895093556096 average time 2.334221935612095 iter num 80\n",
            "loss 0.15549881957728956 average time 2.330425123979512 iter num 100\n",
            "loss 0.15549319676343218 average time 2.3160788040499027 iter num 20\n",
            "loss 0.15548829486240626 average time 2.313689422600146 iter num 40\n",
            "loss 0.15548513102345315 average time 2.3138407256329327 iter num 60\n",
            "loss 0.1554838395802017 average time 2.315083028774643 iter num 80\n",
            "loss 0.15548370726874566 average time 2.311688176739699 iter num 100\n",
            "loss 0.15547808748895503 average time 2.3420336634510024 iter num 20\n",
            "loss 0.1554731897434396 average time 2.322320604875313 iter num 40\n",
            "loss 0.15547002844989324 average time 2.3363728895337164 iter num 60\n",
            "loss 0.15546873759772917 average time 2.342721325475395 iter num 80\n",
            "loss 0.15546860557406397 average time 2.3435414240502723 iter num 100\n",
            "loss 0.15546299045751527 average time 2.3137680462004937 iter num 20\n",
            "loss 0.1554580961611904 average time 2.3191898887505884 iter num 40\n",
            "loss 0.15545493664959625 average time 2.32679807046649 iter num 60\n",
            "loss 0.15545364826425973 average time 2.3334263734622254 iter num 80\n",
            "loss 0.15545351644608746 average time 2.33434928302966 iter num 100\n",
            "loss 0.15544790359068753 average time 2.3594624895515155 iter num 20\n",
            "loss 0.15544301174541128 average time 2.3224335088503723 iter num 40\n",
            "loss 0.15543985554568066 average time 2.3418481440491936 iter num 60\n",
            "loss 0.15543856723274502 average time 2.3362393464873095 iter num 80\n",
            "loss 0.15543843561163004 average time 2.333635996940284 iter num 100\n",
            "loss 0.15543282681248097 average time 2.3157417825001176 iter num 20\n",
            "loss 0.15542793928539167 average time 2.3325099483747183 iter num 40\n",
            "loss 0.15542478372828036 average time 2.3474117603332463 iter num 60\n",
            "loss 0.15542349680258366 average time 2.3400612763252866 iter num 80\n",
            "loss 0.15542336504492432 average time 2.346494341989746 iter num 100\n",
            "loss 0.15541776110409458 average time 2.290571037701011 iter num 20\n",
            "loss 0.1554128751635692 average time 2.3094499424005335 iter num 40\n",
            "loss 0.15540972253788699 average time 2.321284897150569 iter num 60\n",
            "loss 0.15540843504840815 average time 2.3234774072871005 iter num 80\n",
            "loss 0.15540830476978693 average time 2.3222874531993876 iter num 100\n",
            "loss 0.15540270319238284 average time 2.295522032300505 iter num 20\n",
            "loss 0.1553978206531898 average time 2.299079338126103 iter num 40\n",
            "loss 0.15539466891305823 average time 2.3161340831342385 iter num 60\n",
            "loss 0.15539338419512583 average time 2.317417459101125 iter num 80\n",
            "loss 0.1553932529447901 average time 2.330675643651048 iter num 100\n",
            "loss 0.15538765488399284 average time 2.3611147908013663 iter num 20\n",
            "loss 0.15538277604560202 average time 2.3773891741257103 iter num 40\n",
            "loss 0.15537962589862173 average time 2.3700851987174247 iter num 60\n",
            "loss 0.15537834233424494 average time 2.3541853890630593 iter num 80\n",
            "loss 0.15537820970014718 average time 2.3419936775007226 iter num 100\n",
            "loss 0.15537261757466939 average time 2.31205415634613 iter num 20\n",
            "loss 0.1553677409347772 average time 2.329067151149502 iter num 40\n",
            "loss 0.1553645936636398 average time 2.3236005367665107 iter num 60\n",
            "loss 0.15536330964686054 average time 2.3198413549243924 iter num 80\n",
            "loss 0.1553631795912848 average time 2.3169552461692366 iter num 100\n",
            "loss 0.1553575905819566 average time 2.361346161750407 iter num 20\n",
            "loss 0.1553527163301618 average time 2.3242218416762626 iter num 40\n",
            "loss 0.1553495713855599 average time 2.3161833807006285 iter num 60\n",
            "loss 0.15534828896263894 average time 2.312147065687532 iter num 80\n",
            "loss 0.15534815772822627 average time 2.3170107580498733 iter num 100\n",
            "loss 0.15534257268497328 average time 2.276788342150394 iter num 20\n",
            "loss 0.1553377018864617 average time 2.300217090100341 iter num 40\n",
            "loss 0.15533455804778076 average time 2.3107696534503095 iter num 60\n",
            "loss 0.15533327525209725 average time 2.3123139255370915 iter num 80\n",
            "loss 0.1553331453913878 average time 2.316509062059922 iter num 100\n",
            "loss 0.15532756330310232 average time 2.332906519999233 iter num 20\n",
            "loss 0.15532269631569812 average time 2.3399728392250836 iter num 40\n",
            "loss 0.1553195541656753 average time 2.340023261383855 iter num 60\n",
            "loss 0.15531827396910272 average time 2.3283607508634305 iter num 80\n",
            "loss 0.15531814370648353 average time 2.3257245913708173 iter num 100\n",
            "loss 0.1553125654148559 average time 2.3270608667997292 iter num 20\n",
            "loss 0.15530770148611006 average time 2.326371619398924 iter num 40\n",
            "loss 0.1553045620715619 average time 2.335269303649693 iter num 60\n",
            "loss 0.1553032811591442 average time 2.3361371083248743 iter num 80\n",
            "loss 0.15530315101541375 average time 2.3317069114495825 iter num 100\n",
            "loss 0.15529757697671498 average time 2.323851027951605 iter num 20\n",
            "loss 0.1552927164878468 average time 2.3280649238509796 iter num 40\n",
            "loss 0.15528957868308543 average time 2.317134381950018 iter num 60\n",
            "loss 0.15528829843886632 average time 2.3049379536621926 iter num 80\n",
            "loss 0.15528816805256143 average time 2.311793870440015 iter num 100\n",
            "loss 0.15528259801963068 average time 2.3003628466511143 iter num 20\n",
            "loss 0.15527773991466962 average time 2.280326545950447 iter num 40\n",
            "loss 0.155274605193475 average time 2.304574921317544 iter num 60\n",
            "loss 0.1552733266489912 average time 2.3067324322882996 iter num 80\n",
            "loss 0.15527319553505434 average time 2.3103984338906596 iter num 100\n",
            "loss 0.15526762845734782 average time 2.295526735450403 iter num 20\n",
            "loss 0.1552627729244091 average time 2.305048779050412 iter num 40\n",
            "loss 0.1552596399133105 average time 2.297334432883266 iter num 60\n",
            "loss 0.15525836258324535 average time 2.2848414974379923 iter num 80\n",
            "loss 0.15525823130698532 average time 2.2851936772507906 iter num 100\n",
            "loss 0.15525266735391935 average time 2.269021161351702 iter num 20\n",
            "loss 0.15524781670249777 average time 2.2887704627257337 iter num 40\n",
            "loss 0.15524468306628236 average time 2.2831034370666505 iter num 60\n",
            "loss 0.1552434071813224 average time 2.2714051614993878 iter num 80\n",
            "loss 0.15524327675267274 average time 2.269994092899724 iter num 100\n",
            "loss 0.15523771536998957 average time 2.256711843749508 iter num 20\n",
            "loss 0.15523286496337918 average time 2.2578074506502164 iter num 40\n",
            "loss 0.1552297346909595 average time 2.2577581642001556 iter num 60\n",
            "loss 0.15522845791007028 average time 2.2632062288626913 iter num 80\n",
            "loss 0.15522832827358132 average time 2.267151231300086 iter num 100\n",
            "loss 0.15522276940026314 average time 2.283086828798696 iter num 20\n",
            "loss 0.15521792345232574 average time 2.289617112723499 iter num 40\n",
            "loss 0.1552147956119177 average time 2.283599407849154 iter num 60\n",
            "loss 0.15521351977004988 average time 2.288560202937333 iter num 80\n",
            "loss 0.1552133902651364 average time 2.284874901669973 iter num 100\n",
            "loss 0.1552078351865694 average time 2.275873718799994 iter num 20\n",
            "loss 0.15520299314051908 average time 2.279342767225171 iter num 40\n",
            "loss 0.15519986554540494 average time 2.2779658763836776 iter num 60\n",
            "loss 0.15519859146682133 average time 2.2850643929998116 iter num 80\n",
            "loss 0.1551984623902746 average time 2.288782240830333 iter num 100\n",
            "loss 0.15519291038830324 average time 2.2742078244504227 iter num 20\n",
            "loss 0.1551880699492693 average time 2.2784452545747627 iter num 40\n",
            "loss 0.1551849446122283 average time 2.2755733321003695 iter num 60\n",
            "loss 0.1551836724181655 average time 2.2699198643000273 iter num 80\n",
            "loss 0.15518354259404202 average time 2.266986156889907 iter num 100\n",
            "loss 0.15517799296492704 average time 2.303696640700218 iter num 20\n",
            "loss 0.1551731542358347 average time 2.2936758796247885 iter num 40\n",
            "loss 0.15517003103961519 average time 2.2862261699495625 iter num 60\n",
            "loss 0.15516875819923848 average time 2.2898802375621017 iter num 80\n",
            "loss 0.15516862967333134 average time 2.286061511769949 iter num 100\n",
            "loss 0.1551630812251874 average time 2.353883970100287 iter num 20\n",
            "loss 0.1551582458203836 average time 2.314117698499831 iter num 40\n",
            "loss 0.15515512336286297 average time 2.2966720912319336 iter num 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b0519f1384c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_MC_oneDS_SGD_4.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b0519f1384c7>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b0519f1384c7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b0519f1384c7>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# normalize the parameter to range [0-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36melu\u001b[0;34m(input, alpha, inplace)\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDFGvBfCUaRG"
      },
      "source": [
        "#8hr 46min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbr6Z4A1UiU_"
      },
      "source": [
        "## Continued Continued Cotinued Continued to Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3L8m3gWsUco0",
        "outputId": "723cdd46-10cc-4dd2-edd6-efa182fde5e3"
      },
      "source": [
        "#version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import SGD\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = SGD(model.parameters(), lr=1e-3) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 200)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_SGD_5.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.1551493713217277 average time 2.312431484249828 iter num 20\n",
            "loss 0.1551445366934691 average time 2.301714130775872 iter num 40\n",
            "loss 0.15514141843443022 average time 2.299632771649825 iter num 60\n",
            "loss 0.15514014690407263 average time 2.2944758533500136 iter num 80\n",
            "loss 0.15514001780460712 average time 2.2875134562005406 iter num 100\n",
            "loss 0.15513447482661546 average time 2.291360321199318 iter num 20\n",
            "loss 0.15512964557859607 average time 2.2774691356255063 iter num 40\n",
            "loss 0.1551265281878329 average time 2.276994114849852 iter num 60\n",
            "loss 0.15512525723614856 average time 2.2676163488122256 iter num 80\n",
            "loss 0.15512512725477803 average time 2.2615672990200983 iter num 100\n",
            "loss 0.15511958873421375 average time 2.2560817708988905 iter num 20\n",
            "loss 0.15511476086118403 average time 2.2847189782485655 iter num 40\n",
            "loss 0.155111646206497 average time 2.2725993852659787 iter num 60\n",
            "loss 0.15511037642139694 average time 2.2740542812118294 iter num 80\n",
            "loss 0.15511024676305063 average time 2.2765305505195284 iter num 100\n",
            "loss 0.15510471012965105 average time 2.2637179028970422 iter num 20\n",
            "loss 0.15509988552488668 average time 2.2623801199479203 iter num 40\n",
            "loss 0.15509677150157208 average time 2.2637946796647155 iter num 60\n",
            "loss 0.15509550362260066 average time 2.262543954723333 iter num 80\n",
            "loss 0.15509537423147313 average time 2.2650807772086408 iter num 100\n",
            "loss 0.1550898395351472 average time 2.297585789048753 iter num 20\n",
            "loss 0.1550850178240818 average time 2.2827981259746593 iter num 40\n",
            "loss 0.15508190651738266 average time 2.2756528941165017 iter num 60\n",
            "loss 0.15508063695894211 average time 2.2668650751627863 iter num 80\n",
            "loss 0.15508050787862035 average time 2.2581909894799175 iter num 100\n",
            "loss 0.15507497637947878 average time 2.262276729300356 iter num 20\n",
            "loss 0.15507015771481958 average time 2.2545672021005885 iter num 40\n",
            "loss 0.1550670469844777 average time 2.2524644749503446 iter num 60\n",
            "loss 0.15506578119457884 average time 2.2520888839007966 iter num 80\n",
            "loss 0.1550656513962037 average time 2.249267218151217 iter num 100\n",
            "loss 0.1550601228221344 average time 2.256040202599979 iter num 20\n",
            "loss 0.15505530703305068 average time 2.2485371041999316 iter num 40\n",
            "loss 0.155052197209473 average time 2.272937796549377 iter num 60\n",
            "loss 0.15505093139088486 average time 2.29004578117474 iter num 80\n",
            "loss 0.15505080286611717 average time 2.294504288209573 iter num 100\n",
            "loss 0.15504527739303364 average time 2.3172187124007904 iter num 20\n",
            "loss 0.15504046327639387 average time 2.290742409775703 iter num 40\n",
            "loss 0.15503735585016049 average time 2.2768739823339272 iter num 60\n",
            "loss 0.15503609067603388 average time 2.2683021688380904 iter num 80\n",
            "loss 0.15503596213120432 average time 2.2643731782305987 iter num 100\n",
            "loss 0.15503044007504652 average time 2.2570323904983525 iter num 20\n",
            "loss 0.1550256287597333 average time 2.2563862221235467 iter num 40\n",
            "loss 0.15502252285444737 average time 2.2573304226668065 iter num 60\n",
            "loss 0.155021258590177 average time 2.258868115999758 iter num 80\n",
            "loss 0.15502112983898791 average time 2.2687582587097133 iter num 100\n",
            "loss 0.15501561121680144 average time 2.2630808354995677 iter num 20\n",
            "loss 0.15501080245336815 average time 2.266396424425693 iter num 40\n",
            "loss 0.15500769876665707 average time 2.272429409850156 iter num 60\n",
            "loss 0.1550064352816611 average time 2.281855864350109 iter num 80\n",
            "loss 0.15500630634971804 average time 2.284016407220479 iter num 100\n",
            "loss 0.1550007904004452 average time 2.3074086121509025 iter num 20\n",
            "loss 0.15499598439225892 average time 2.325154472801296 iter num 40\n",
            "loss 0.1549928818806085 average time 2.3012604508010552 iter num 60\n",
            "loss 0.15499161920684051 average time 2.306207702512802 iter num 80\n",
            "loss 0.1549914907363295 average time 2.303839789250487 iter num 100\n",
            "loss 0.15498597645360396 average time 2.333203447199048 iter num 20\n",
            "loss 0.15498117390714683 average time 2.3096372714746396 iter num 40\n",
            "loss 0.15497807315133091 average time 2.2909537823996895 iter num 60\n",
            "loss 0.1549768107420539 average time 2.290292949299328 iter num 80\n",
            "loss 0.15497668196728273 average time 2.2933248509494297 iter num 100\n",
            "loss 0.15497117158245124 average time 2.2801042058999883 iter num 20\n",
            "loss 0.15496637055773813 average time 2.286053566501141 iter num 40\n",
            "loss 0.15496327126724868 average time 2.2951069210510466 iter num 60\n",
            "loss 0.1549620092959912 average time 2.2951667391258526 iter num 80\n",
            "loss 0.15496188085203794 average time 2.2966000036205516 iter num 100\n",
            "loss 0.15495637380257332 average time 2.2997156500998246 iter num 20\n",
            "loss 0.1549515747859382 average time 2.289931212225201 iter num 40\n",
            "loss 0.15494847759745087 average time 2.2904198351667824 iter num 60\n",
            "loss 0.15494721693354407 average time 2.296072208737314 iter num 80\n",
            "loss 0.15494708768008777 average time 2.2919366515894946 iter num 100\n",
            "loss 0.15494158408737696 average time 2.292923528299434 iter num 20\n",
            "loss 0.15493678719145745 average time 2.271946440800093 iter num 40\n",
            "loss 0.1549336903994581 average time 2.2733701035162084 iter num 60\n",
            "loss 0.1549324297683933 average time 2.2781910178622637 iter num 80\n",
            "loss 0.1549323020009234 average time 2.2815745401999448 iter num 100\n",
            "loss 0.1549268000212612 average time 2.288235877799889 iter num 20\n",
            "loss 0.15492200646996784 average time 2.2870732693241735 iter num 40\n",
            "loss 0.15491891034422056 average time 2.2779103957497377 iter num 60\n",
            "loss 0.15491765120148976 average time 2.275666074786932 iter num 80\n",
            "loss 0.15491752326921146 average time 2.2737052199097523 iter num 100\n",
            "loss 0.15491202499833717 average time 2.2993358688996524 iter num 20\n",
            "loss 0.15490723396535738 average time 2.275545710675215 iter num 40\n",
            "loss 0.15490414077447462 average time 2.2823109613496246 iter num 60\n",
            "loss 0.1549028815537658 average time 2.274161876287144 iter num 80\n",
            "loss 0.1549027543900362 average time 2.270526215319551 iter num 100\n",
            "loss 0.15489725854553169 average time 2.2814830920011446 iter num 20\n",
            "loss 0.15489246994968833 average time 2.2911811211994064 iter num 40\n",
            "loss 0.15488937802855166 average time 2.2923018545504115 iter num 60\n",
            "loss 0.15488811939350167 average time 2.286247608850317 iter num 80\n",
            "loss 0.15488799071611317 average time 2.280398567329976 iter num 100\n",
            "loss 0.15488249870928214 average time 2.2752594863522972 iter num 20\n",
            "loss 0.15487771389798258 average time 2.2904148353514757 iter num 40\n",
            "loss 0.1548746222852406 average time 2.292006660668024 iter num 60\n",
            "loss 0.15487336490203468 average time 2.290467586150771 iter num 80\n",
            "loss 0.15487323718527252 average time 2.291091498510941 iter num 100\n",
            "loss 0.15486774888965577 average time 2.2987209455990523 iter num 20\n",
            "loss 0.15486296586905915 average time 2.2924848463000673 iter num 40\n",
            "loss 0.1548598765144644 average time 2.290408341133055 iter num 60\n",
            "loss 0.1548586182212622 average time 2.2926355527622944 iter num 80\n",
            "loss 0.15485849058935067 average time 2.292797150260012 iter num 100\n",
            "loss 0.1548530048587324 average time 2.285873044350592 iter num 20\n",
            "loss 0.15484822483823046 average time 2.275918413650288 iter num 40\n",
            "loss 0.15484513740885789 average time 2.279263577366267 iter num 60\n",
            "loss 0.15484388079301684 average time 2.2779698195496167 iter num 80\n",
            "loss 0.15484375149656 average time 2.281412251709844 iter num 100\n",
            "loss 0.15483827139237397 average time 2.3099921692002683 iter num 20\n",
            "loss 0.15483349313267555 average time 2.299301875348829 iter num 40\n",
            "loss 0.1548304057666894 average time 2.3056291476493547 iter num 60\n",
            "loss 0.15482915084021462 average time 2.304466973174749 iter num 80\n",
            "loss 0.15482902334433837 average time 2.3000541155197425 iter num 100\n",
            "loss 0.15482354440622054 average time 2.3141299925504426 iter num 20\n",
            "loss 0.1548187673699281 average time 2.2904788649502734 iter num 40\n",
            "loss 0.1548156817023253 average time 2.2902010309667578 iter num 60\n",
            "loss 0.1548144264727401 average time 2.2873615406751924 iter num 80\n",
            "loss 0.15481429903913513 average time 2.286207249740546 iter num 100\n",
            "loss 0.1548088237864602 average time 2.30304093470113 iter num 20\n",
            "loss 0.15480404767279907 average time 2.3126152126253148 iter num 40\n",
            "loss 0.1548009644122622 average time 2.308535419150333 iter num 60\n",
            "loss 0.15479970976459143 average time 2.315171811712753 iter num 80\n",
            "loss 0.1547995803125959 average time 2.3064909891098795 iter num 100\n",
            "loss 0.1547941083694661 average time 2.2743722915976834 iter num 20\n",
            "loss 0.1547893359157889 average time 2.3075762618736917 iter num 40\n",
            "loss 0.15478625316245695 average time 2.315170509648548 iter num 60\n",
            "loss 0.15478499871731694 average time 2.301649343898498 iter num 80\n",
            "loss 0.1547848711712335 average time 2.3002874976689056 iter num 100\n",
            "loss 0.15477940190212544 average time 2.2774911294989577 iter num 20\n",
            "loss 0.15477463098657374 average time 2.2800963691741343 iter num 40\n",
            "loss 0.15477154986662486 average time 2.2866933422995013 iter num 60\n",
            "loss 0.15477029699673361 average time 2.2849723989871564 iter num 80\n",
            "loss 0.15477017020038591 average time 2.2858240444900004 iter num 100\n",
            "loss 0.15476470249466268 average time 2.284585685399361 iter num 20\n",
            "loss 0.15475993341489835 average time 2.277664162574365 iter num 40\n",
            "loss 0.15475685415266555 average time 2.2729776951489233 iter num 60\n",
            "loss 0.1547556022839089 average time 2.277221792986893 iter num 80\n",
            "loss 0.15475547448173343 average time 2.2817059228497967 iter num 100\n",
            "loss 0.15475000944418102 average time 2.3011798015002567 iter num 20\n",
            "loss 0.15474524188540986 average time 2.2803600221504894 iter num 40\n",
            "loss 0.1547421648805428 average time 2.2747189511171504 iter num 60\n",
            "loss 0.1547409121931447 average time 2.2741288064000402 iter num 80\n",
            "loss 0.15474078403399355 average time 2.2732505575701363 iter num 100\n",
            "loss 0.1547353231615741 average time 2.2387325811519987 iter num 20\n",
            "loss 0.1547305586218865 average time 2.259403311951246 iter num 40\n",
            "loss 0.1547274813594771 average time 2.2520447104849155 iter num 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e876a1845064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_MC_oneDS_SGD_5.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-e876a1845064>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-e876a1845064>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-e876a1845064>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# normalize the parameter to range [0-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7jm7W9ythai"
      },
      "source": [
        "# 1hr 49min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0093aa-7b7e-4637-89b3-36e71b5e34a7"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 0.8, 1, 0.25, 0.02, 0.02]]).cuda() # T, K, B, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[3]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.10632345, 0.5543747)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.101666]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.591832], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqpasxVi0hx3",
        "outputId": "91788b74-ac25-43d2-d57a-9dedd3c42b76"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "        dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx2)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.02]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.]*numstocks) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "B = 0.8 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10632345\n",
            "[0.5543747]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovJwXo3-YEu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1a688e44-e63c-4519-f43f-226b45759239"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "correct_call_prices = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    correct_call_prices.append(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, correct_call_prices, label = \"correct_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(correct_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "77bf560d-5ade-49c7-938f-066562b0857a"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.0, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgD1NjP7DOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "4014d547-8d24-4a76-c5bd-3919eb48b2c9"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.775, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 0.775, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7MfujMQ7oij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "db38961a-0b40-4ffa-ff90-3d8fb8671537"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.225, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.225, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}