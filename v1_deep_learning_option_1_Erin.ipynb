{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "v1_deep_learning_option_1_Erin.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/v1_deep_learning_option_1_Erin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "2a21f3d3-9f26-479d-e985-76bca9f3d022"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0  24307      0 --:--:-- --:--:-- --:--:-- 24307\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9MB 51kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 36.1MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. \n",
        "\n",
        "Loading all the necessary libraries:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6no5JzH-B6"
      },
      "source": [
        "# !pip install cupy-cuda101"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbkx3hXWnwi8"
      },
      "source": [
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBN3YFOnwi-"
      },
      "source": [
        "The CuPy version of batched barrier option pricing simulation is as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhj4DtLnwi-"
      },
      "source": [
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRjmX5zcnwi_"
      },
      "source": [
        "Note, the parameters (K, B, S0, sigma, mu, r) are passed in as an array with length of batch size. The output array is a two dimensional array flatten to 1-D. The first dimension is for Batch and the second dimension is for Path. \n",
        "\n",
        "Testing it out by entering two sets of option parameters:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4PMo7Inwi_"
      },
      "source": [
        "# N_PATHS = 2048000\n",
        "# N_STEPS = 365\n",
        "# N_BATCH = 2\n",
        "# T = 1.0\n",
        "\n",
        "# K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
        "# B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
        "# S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
        "# sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
        "# mu = cupy.array([0.15, 0.1], dtype=cupy.float32)\n",
        "# r =cupy.array([0.05, 0.05], dtype=cupy.float32)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpWK3wcEnwjA"
      },
      "source": [
        "Put everything into a simple function to launch this GPU kernel. The option prices for each batch is the average of the corresponding path terminal values. This can be computed easily by Cupy function `mean(axis=1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAb34NTnwjA"
      },
      "source": [
        "# def batch_run():\n",
        "#     number_of_threads = 256\n",
        "#     number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
        "#     randoms_gpu = cupy.random.normal(0, 1, N_BATCH*N_PATHS * N_STEPS, dtype=cupy.float32)\n",
        "#     output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     s = time.time()\n",
        "#     cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
        "#                        (output, np.float32(T), K, B, S0, sigma, mu, r,\n",
        "#                         randoms_gpu, N_STEPS, N_PATHS, N_BATCH))\n",
        "#     v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     e = time.time()\n",
        "#     print('time', e-s, 'v',v)\n",
        "# batch_run()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRgQCelnwjC"
      },
      "source": [
        "This produces the option prices $21.22$ and $0.848$ for these two sets of option parameters in $66ms$.\n",
        "\n",
        "It works efficiently hence we will construct an `OptionDataSet` class to wrap the above code so we can use it in Pytorch. For every `next` element, it generates uniform distributed random option parameters in the specified range, launches the GPU kernel to compute the option prices, convert the CuPy array to Pytorch tensors with zero copy via the DLPack. Note how we implemented the iterable Dataset interface:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1KUra7ZnwjC"
      },
      "source": [
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo46Vf4XnwjD"
      },
      "source": [
        "Put everything related to Pytorch dataset into a file `cupy_dataset.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQUGMBlnwjE"
      },
      "source": [
        "# #%%writefile cupy_dataset.py \n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "# cupy.cuda.set_allocator(None)\n",
        "\n",
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')\n",
        "\n",
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPAsh7JnwjF"
      },
      "source": [
        "Here is a test code to sample 10 data points with batch size 16:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLKxMF05nwjF"
      },
      "source": [
        "# from cupy_dataset import OptionDataSet\n",
        "# ds = OptionDataSet(10, number_path=100000, batch=16, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlzRTD0nwjG"
      },
      "source": [
        "We can implement the same code by using Numba to accelerate the calculation in GPU:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IsfSwVwnwjG"
      },
      "source": [
        "# import numba\n",
        "# from numba import cuda\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average)\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]:\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                               X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
        "#         o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
        "#         Y = o.mean(axis = 1) \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=1, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "8df13b6c-073f-462f-a0b4-b582703a592e"
      },
      "source": [
        "################################# TEST ########################################\n",
        "#%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# batch - number of underlyings in one basket option\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "          # X = cupy.random.rand(self.N_STOCKS, 6, dtype=cupy.float32)\n",
        "          X = cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS, 6)\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          # X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32) # parameters\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op, i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "          stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "ds = NumbaOptionDataSet(2, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "for i in ds:\n",
        "    print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000],\n",
            "        [200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000],\n",
            "        [200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000]], device='cuda:0'), tensor([26.4928, 26.2970, 26.3781], device='cuda:0'))\n",
            "(tensor([[200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000],\n",
            "        [200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000],\n",
            "        [200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000]], device='cuda:0'), tensor([26.3816, 26.3763, 26.4107], device='cuda:0'))\n",
            "(tensor([[200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000],\n",
            "        [200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000],\n",
            "        [200.0000,   0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,\n",
            "           0.9900, 200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900,\n",
            "         200.0000,   0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,\n",
            "           0.4000,   0.2000,   0.2000, 200.0000,   0.9900, 200.0000,   0.4000,\n",
            "           0.2000,   0.2000]], device='cuda:0'), tensor([26.3934, 26.3903, 26.3735], device='cuda:0'))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBOv_RiBsCWa"
      },
      "source": [
        "### PUI TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BME87CgGsFrd"
      },
      "source": [
        "# %%writefile cupy_dataset.py\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n",
        "\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp2 = math.exp(-r*T)\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)    \n",
        "\n",
        "#     for i in range(ii, N_PATHS, stride): # for each path          \n",
        "#         running_average = 0.0\n",
        "\n",
        "#         for j in range(N_STOCKS): # initialize S0\n",
        "#             s_curr[j] = S0[j]\n",
        "\n",
        "#         for n in range(N_STEPS): # for each step\n",
        "#             s_curr_avg = 0.0\n",
        "\n",
        "#             for j in range(N_STOCKS): # for each stock\n",
        "#                 tmp1 = mu[j]*T/N_STEPS  \n",
        "#                 s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n",
        "#                 s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n",
        "\n",
        "#             # add stock average to running average\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n",
        "\n",
        "#             # compare to barrier\n",
        "#             if running_average <= B:\n",
        "#                 break\n",
        "\n",
        "#         payoff = running_average - K if running_average > K else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "    \n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, number_stocks = 3, batch=1, threads=512, seed=15, T=1):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_STOCKS = number_stocks\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(T)\n",
        "#         self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "\n",
        "#         ############ <new\n",
        "#         self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n",
        "#         self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#         cupy.fill_diagonal(self.Z_cov, 1)\n",
        "#         ############ new>\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "\n",
        "#         X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "\n",
        "#         for i in range(self.N_BATCH): # for each batch\n",
        "#           self.S0 = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 200\n",
        "#           self.K = 110.0\n",
        "#           self.B = 100.0\n",
        "#           self.sigma = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.mu = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.r = 0.05\n",
        "#           self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n",
        "\n",
        "#           ############ <new - add correlation between stocks\n",
        "#           all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "          \n",
        "#           single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n",
        "#                                                                                     self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n",
        "#           Y[i] = self.output.mean()\n",
        "\n",
        "#           ############ <new - combine to get X matrix\n",
        "#           X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n",
        "#                                 self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "        \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "# ds = NumbaOptionDataSet(max_len=10, number_path=100, batch=2, seed=15)\n",
        "# for i in ds:\n",
        "#   print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQt8PqinwjI"
      },
      "source": [
        "# %%writefile model.py\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6, hidden)\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0,\n",
        "#                                            198.0,\n",
        "#                                            200.0,\n",
        "#                                            0.4,\n",
        "#                                            0.2,\n",
        "#                                            0.2,]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c45064-6d21-4dc3-f236-f690171eab30"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "                                           200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "                                           200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f336f455-8e3b-4e3c-f5c4-495ae38c5d15"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.4)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486786d1-88bd-4d96-8d65-dac56cdaff4c"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.0033376929350197315 average time 0.004678051810001307\n",
            "loss 0.001502864295616746 average time 0.004562975160024507\n",
            "loss 0.19722923636436462 average time 0.004543933840013778\n",
            "loss 0.5098931789398193 average time 0.004573377949986935\n",
            "loss 0.24777474999427795 average time 0.004604062109983716\n",
            "loss 0.2487730234861374 average time 0.0045286981000253945\n",
            "loss 0.6993882656097412 average time 0.00453493981997326\n",
            "loss 0.1167130395770073 average time 0.00459125926001434\n",
            "loss 0.057976506650447845 average time 0.004585574760012605\n",
            "loss 0.2676795721054077 average time 0.004564111929980754\n",
            "loss 0.024944454431533813 average time 0.004525489510001535\n",
            "loss 0.9495897889137268 average time 0.004581687389973012\n",
            "loss 0.677587628364563 average time 0.004581583870017312\n",
            "loss 0.11046499013900757 average time 0.0045385417900206445\n",
            "loss 0.23430660367012024 average time 0.004642491090003205\n",
            "loss 0.05764923617243767 average time 0.004566168960009236\n",
            "loss 0.3661995530128479 average time 0.004545230730022922\n",
            "loss 0.222089484333992 average time 0.004584963970019089\n",
            "loss 0.44334301352500916 average time 0.004653476669996053\n",
            "loss 0.5705781579017639 average time 0.0047042225800169035\n",
            "loss 1.0963605642318726 average time 0.004669555190043866\n",
            "loss 0.03839566186070442 average time 0.004566308060029769\n",
            "loss 0.41409921646118164 average time 0.004586980729982315\n",
            "loss 0.0647880882024765 average time 0.004561994930004403\n",
            "loss 0.8968238830566406 average time 0.004539553339973281\n",
            "loss 0.06428679823875427 average time 0.0045831210299775195\n",
            "loss 0.08634192496538162 average time 0.004592786199982584\n",
            "loss 0.6346442103385925 average time 0.0046079415099802646\n",
            "loss 1.7535927295684814 average time 0.004527003629968931\n",
            "loss 0.6916801929473877 average time 0.004625451789961516\n",
            "loss 0.26972994208335876 average time 0.004544357479958307\n",
            "loss 0.013717403635382652 average time 0.004582429460024287\n",
            "loss 0.6039096117019653 average time 0.004614123869969262\n",
            "loss 0.2841812074184418 average time 0.0045389917199918275\n",
            "loss 0.2525053918361664 average time 0.0045357719800267655\n",
            "loss 0.12287183105945587 average time 0.004631157399985568\n",
            "loss 1.0670890808105469 average time 0.004579272010023488\n",
            "loss 0.015194453299045563 average time 0.004599194939955851\n",
            "loss 0.39309993386268616 average time 0.004548929880011201\n",
            "loss 0.007165689021348953 average time 0.004572227610015034\n",
            "loss 0.8549343943595886 average time 0.004532699329956813\n",
            "loss 0.49811580777168274 average time 0.004542371450024802\n",
            "loss 0.1930212825536728 average time 0.004640648369922928\n",
            "loss 0.01591353863477707 average time 0.004574943380011973\n",
            "loss 0.4111156761646271 average time 0.004599860369989983\n",
            "loss 0.03466678783297539 average time 0.004560446690029494\n",
            "loss 0.9705086946487427 average time 0.004602706640025644\n",
            "loss 0.11674778163433075 average time 0.004538739379977414\n",
            "loss 0.7059126496315002 average time 0.004537792870078192\n",
            "loss 0.39148038625717163 average time 0.004616212940027254\n",
            "loss 0.0922270342707634 average time 0.004548233049990813\n",
            "loss 0.668083667755127 average time 0.004578775959953418\n",
            "loss 0.47074025869369507 average time 0.004578941220024717\n",
            "loss 0.16693918406963348 average time 0.00456604964005237\n",
            "loss 0.613914966583252 average time 0.0045543688700036\n",
            "loss 0.4481057822704315 average time 0.004557827119897411\n",
            "loss 0.3853837251663208 average time 0.004526274809995812\n",
            "loss 0.0070278337225317955 average time 0.004570494820018212\n",
            "loss 0.1526499092578888 average time 0.004562693529969693\n",
            "loss 0.28269532322883606 average time 0.004579494430045088\n",
            "loss 0.21298183500766754 average time 0.004566173980010717\n",
            "loss 0.016684191301465034 average time 0.004627613450029457\n",
            "loss 1.1025173664093018 average time 0.0045366824100165105\n",
            "loss 0.16677407920360565 average time 0.004534982410004887\n",
            "loss 0.06625375896692276 average time 0.004624461320045157\n",
            "loss 0.17581118643283844 average time 0.004560048680014006\n",
            "loss 0.8575797080993652 average time 0.004513912869979322\n",
            "loss 0.020176008343696594 average time 0.004632613430039782\n",
            "loss 0.31353098154067993 average time 0.004515882059968135\n",
            "loss 0.5103405117988586 average time 0.004523640290008188\n",
            "loss 0.06590066105127335 average time 0.00465988189998825\n",
            "loss 0.08092010766267776 average time 0.0048994654500256725\n",
            "loss 1.1672163009643555 average time 0.004757532990024629\n",
            "loss 0.3489602208137512 average time 0.004545832640014851\n",
            "loss 1.255279779434204 average time 0.004512399779969201\n",
            "loss 0.03728292137384415 average time 0.004526882470017881\n",
            "loss 0.09082372486591339 average time 0.004607615490040189\n",
            "loss 0.15531358122825623 average time 0.004581901879982979\n",
            "loss 0.3023938238620758 average time 0.0045581936499547735\n",
            "loss 0.1550854742527008 average time 0.004551100870048686\n",
            "loss 0.3956458270549774 average time 0.004596459649965254\n",
            "loss 0.09050899744033813 average time 0.004693876549954439\n",
            "loss 0.10256664454936981 average time 0.0046241246300087364\n",
            "loss 0.18218308687210083 average time 0.004576777390047937\n",
            "loss 1.0398508310317993 average time 0.004546191569961593\n",
            "loss 0.2023182213306427 average time 0.004596827240011408\n",
            "loss 0.1718163639307022 average time 0.004519757730013225\n",
            "loss 0.09883619844913483 average time 0.004519723599996723\n",
            "loss 0.30022987723350525 average time 0.004506566209993252\n",
            "loss 0.2890219986438751 average time 0.00454836100004286\n",
            "loss 0.41358575224876404 average time 0.0045349178199649036\n",
            "loss 0.0015853444347158074 average time 0.004572714950018053\n",
            "loss 0.04240484908223152 average time 0.004599772010014931\n",
            "loss 0.3975868225097656 average time 0.004529281749992151\n",
            "loss 0.08066503703594208 average time 0.00458763708999868\n",
            "loss 0.7121344804763794 average time 0.004547630949973609\n",
            "loss 0.46200692653656006 average time 0.004544811060004577\n",
            "loss 0.022135380655527115 average time 0.004578313429992704\n",
            "loss 0.06957972049713135 average time 0.0045677796299787585\n",
            "loss 0.09467744827270508 average time 0.0045727670799988115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 10000\n",
              "\tepoch: 100\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 100\n",
              "\toutput: 0.09467744827270508\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2HokSoVnwjJ"
      },
      "source": [
        "### TensorCore mixed precision training\n",
        "\n",
        "The V100 GPUs have 640 tensor cores that can accelerate half precision matrix multiplication calculation which is the core computation done by the DL model. [Apex library](https://github.com/NVIDIA/apex) developed by NVIDIA makes mixed precision and distributed training in Pytorch easy. By changing 3 lines of code, it can use the tensor cores to accelerate the training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWionrWjPRsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf953c7-3b2e-4a08-8a5c-48fe3fab2ff7"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8054, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 8054 (delta 68), reused 97 (delta 44), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8054/8054), 14.11 MiB | 25.99 MiB/s, done.\n",
            "Resolving deltas: 100% (5467/5467), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI1mX1trPZOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002a0c65-5dd7-4ba9-9034-013487114445"
      },
      "source": [
        "cd apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/apex\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ERRfQDvPegW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc77e8c-971e-40bb-c8ec-c48aa85a7893"
      },
      "source": [
        "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-gbcnko0u\n",
            "Created temporary directory: /tmp/pip-req-tracker-d_u2hq36\n",
            "Created requirements tracker '/tmp/pip-req-tracker-d_u2hq36'\n",
            "Created temporary directory: /tmp/pip-install-rfv4_uta\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-5mizzy79\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-d_u2hq36'\n",
            "    Running setup.py (path:/tmp/pip-req-build-5mizzy79/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.9.0+cu102\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-5mizzy79/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-5mizzy79/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-5mizzy79/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-5mizzy79/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-5mizzy79/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-req-build-5mizzy79/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-5mizzy79/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-5mizzy79 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-d_u2hq36'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-ymp2swaa\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-5mizzy79/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-5mizzy79/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ymp2swaa/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.9.0+cu102\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-5mizzy79/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "    Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "    Cuda compilation tools, release 11.0, V11.0.221\n",
            "    Build cuda_11.0_bu.TC445_37.28845127_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "      File \"<string>\", line 1, in <module>\n",
            "      File \"/tmp/pip-req-build-5mizzy79/setup.py\", line 171, in <module>\n",
            "        check_cuda_torch_binary_vs_bare_metal(torch.utils.cpp_extension.CUDA_HOME)\n",
            "      File \"/tmp/pip-req-build-5mizzy79/setup.py\", line 106, in check_cuda_torch_binary_vs_bare_metal\n",
            "        \"https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  \"\n",
            "    RuntimeError: Cuda extensions are being compiled with a version of Cuda that does not match the version used to compile Pytorch binaries.  Pytorch binaries were compiled with Cuda 10.2.\n",
            "    In some cases, a minor-version mismatch will not cause later errors:  https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  You can try commenting out this check (at your own risk).\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25herror\n",
            "Cleaning up...\n",
            "  Removing source in /tmp/pip-req-build-5mizzy79\n",
            "Removed build tracker '/tmp/pip-req-tracker-d_u2hq36'\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-5mizzy79/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-5mizzy79/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ymp2swaa/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Exception information:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n",
            "    use_user_site=options.use_user_site,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/__init__.py\", line 62, in install_given_reqs\n",
            "    **kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py\", line 888, in install\n",
            "    cwd=self.unpacked_source_directory,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 275, in runner\n",
            "    spinner=spinner,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 242, in call_subprocess\n",
            "    raise InstallationError(exc_msg)\n",
            "pip._internal.exceptions.InstallationError: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-5mizzy79/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-5mizzy79/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ymp2swaa/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGGHIHS-nwjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4049528-e947-4504-c99d-2a1458f10ace"
      },
      "source": [
        "from apex import amp\n",
        "from ignite.engine import Engine, Events\n",
        "from torch.nn import MSELoss\n",
        "from ignite.handlers import Timer\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# set the AMP optimization level to O1\n",
        "opt_level = 'O1'\n",
        "# wrap the optimizer and model\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
        "#dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    # amp handles the auto loss scaling\n",
        "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
            "loss 0.06855733692646027 average time 0.006872679120006069\n",
            "loss 0.001067094737663865 average time 0.006823709189930014\n",
            "loss 0.19295565783977509 average time 0.0069034808199830875\n",
            "loss 0.5067980289459229 average time 0.006869910729992625\n",
            "loss 0.2507094442844391 average time 0.006901656980035114\n",
            "loss 0.2519429922103882 average time 0.006876953499977389\n",
            "loss 0.6993333101272583 average time 0.006906817489953028\n",
            "loss 0.11660105735063553 average time 0.006899264560006486\n",
            "loss 0.057462289929389954 average time 0.006992219789981391\n",
            "loss 0.2677612900733948 average time 0.006854169419902974\n",
            "loss 0.024004120379686356 average time 0.006885234060036965\n",
            "loss 0.9495911598205566 average time 0.006887119060029363\n",
            "loss 0.6796457767486572 average time 0.007110162189947005\n",
            "loss 0.11376038193702698 average time 0.007147847749893117\n",
            "loss 0.23856221139431 average time 0.007187688030098798\n",
            "loss 0.05641698092222214 average time 0.006920788030083713\n",
            "loss 0.3583333492279053 average time 0.006944644710083594\n",
            "loss 0.22025084495544434 average time 0.007074249049956052\n",
            "loss 0.4409610331058502 average time 0.0070271845799925355\n",
            "loss 0.5643483996391296 average time 0.006879944689953846\n",
            "loss 1.0928951501846313 average time 0.006981617760020526\n",
            "loss 0.03990147262811661 average time 0.006958917720021418\n",
            "loss 0.41124391555786133 average time 0.006947314770031881\n",
            "loss 0.06322315335273743 average time 0.006964274000019941\n",
            "loss 0.8981786966323853 average time 0.006922800900038055\n",
            "loss 0.06427717208862305 average time 0.006881301319999693\n",
            "loss 0.08264487981796265 average time 0.006972554530038906\n",
            "loss 0.6294649839401245 average time 0.006989209870034756\n",
            "loss 1.7464157342910767 average time 0.006958827370026483\n",
            "loss 0.6929956674575806 average time 0.006928828990021429\n",
            "loss 0.2766560912132263 average time 0.0069073544599996236\n",
            "loss 0.013936097733676434 average time 0.006930666160005785\n",
            "loss 0.6038906574249268 average time 0.007013639619990499\n",
            "loss 0.29068899154663086 average time 0.006933560680026858\n",
            "loss 0.2541363537311554 average time 0.007090844820022539\n",
            "loss 0.12174402177333832 average time 0.0068336141100189705\n",
            "loss 1.0781289339065552 average time 0.006977577259995087\n",
            "loss 0.01482979767024517 average time 0.0068866867200085835\n",
            "loss 0.38957107067108154 average time 0.006934358769958635\n",
            "loss 0.0070212772116065025 average time 0.006969421500025419\n",
            "loss 0.8545411825180054 average time 0.00702725751001708\n",
            "loss 0.5031359791755676 average time 0.007162836060042537\n",
            "loss 0.19302304089069366 average time 0.006839206259983257\n",
            "loss 0.01566147431731224 average time 0.006997360759969524\n",
            "loss 0.4119816720485687 average time 0.006844094190028045\n",
            "loss 0.03378455340862274 average time 0.00685207539003386\n",
            "loss 0.9558881521224976 average time 0.006863440120032465\n",
            "loss 0.11538399755954742 average time 0.006989800029932667\n",
            "loss 0.70273756980896 average time 0.006815342740010237\n",
            "loss 0.3940265476703644 average time 0.0069157373499820094\n",
            "loss 0.09143726527690887 average time 0.006814101610025318\n",
            "loss 0.6642348766326904 average time 0.006897218799995244\n",
            "loss 0.4630804657936096 average time 0.00696708051999849\n",
            "loss 0.1701267659664154 average time 0.006870224660033273\n",
            "loss 0.6205863952636719 average time 0.006895352660039862\n",
            "loss 0.4468580484390259 average time 0.006954269040006693\n",
            "loss 0.38896414637565613 average time 0.006872723639962715\n",
            "loss 0.0073073860257864 average time 0.006837003019954864\n",
            "loss 0.15174360573291779 average time 0.006913633949989162\n",
            "loss 0.28668224811553955 average time 0.006798155190062971\n",
            "loss 0.21586604416370392 average time 0.00687881392995223\n",
            "loss 0.016308315098285675 average time 0.006900432430093133\n",
            "loss 1.1025193929672241 average time 0.006892102109923144\n",
            "loss 0.16545596718788147 average time 0.0069539777100362694\n",
            "loss 0.06642372161149979 average time 0.00689506105001783\n",
            "loss 0.18030036985874176 average time 0.0068933172299421135\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "loss 0.8516587018966675 average time 0.006936615999966307\n",
            "loss 0.021147821098566055 average time 0.006885198000009041\n",
            "loss 0.3110811412334442 average time 0.006806951859989568\n",
            "loss 0.5199915170669556 average time 0.006963377840038447\n",
            "loss 0.06705565750598907 average time 0.006860705099988991\n",
            "loss 0.08089383691549301 average time 0.007697780020007486\n",
            "loss 1.174700140953064 average time 0.0069802407300539925\n",
            "loss 0.35747578740119934 average time 0.006845400139936828\n",
            "loss 1.259089708328247 average time 0.006901469710028323\n",
            "loss 0.0366222970187664 average time 0.006939099600012924\n",
            "loss 0.09634776413440704 average time 0.006863649349979823\n",
            "loss 0.15774047374725342 average time 0.006885431220025566\n",
            "loss 0.3038216829299927 average time 0.007026394250060548\n",
            "loss 0.15854805707931519 average time 0.006927479340029095\n",
            "loss 0.39294686913490295 average time 0.006871567549987958\n",
            "loss 0.08792171627283096 average time 0.006856774829984715\n",
            "loss 0.10992524027824402 average time 0.006943915019946872\n",
            "loss 0.18497779965400696 average time 0.0069125002999953725\n",
            "loss 1.0436755418777466 average time 0.007004582689942254\n",
            "loss 0.20255540311336517 average time 0.006887461239948607\n",
            "loss 0.17178311944007874 average time 0.006912331119992814\n",
            "loss 0.09870733320713043 average time 0.006931505359971197\n",
            "loss 0.3063760995864868 average time 0.006885058929974548\n",
            "loss 0.29392704367637634 average time 0.006997420450015852\n",
            "loss 0.4109732508659363 average time 0.006968510570040962\n",
            "loss 0.0011612219968810678 average time 0.006897611210015384\n",
            "loss 0.041161902248859406 average time 0.00692741029998615\n",
            "loss 0.39814916253089905 average time 0.00695937467998192\n",
            "loss 0.08065679669380188 average time 0.006911602960008168\n",
            "loss 0.6833034753799438 average time 0.0069477756499691165\n",
            "loss 0.46198898553848267 average time 0.006932644939988677\n",
            "loss 0.022628046572208405 average time 0.006900622380007917\n",
            "loss 0.06459306925535202 average time 0.007114399920010328\n",
            "loss 0.0883660614490509 average time 0.007035496969947417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 10000\n",
              "\tepoch: 100\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 100\n",
              "\toutput: 0.0883660614490509\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPcuUUYwnwjK"
      },
      "source": [
        "It improves to compute each mini-batch in $8ms$. As we reduce the model weights to half precision for better performance, the loss need to be scaled to make sure the half precision dynamic range aligns with the computation. It is guessing what is the correct loss scaling factor and adjust it automatically if the gradient overflows. In the end, we will get the best hardware acceleration while maintaining the accuracy of model prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cPD5r6bnwjK"
      },
      "source": [
        "### Multiple GPU training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzLskEcSnwjK"
      },
      "source": [
        "Apex makes multiple GPU training easy. Working on the same training script, we need to take care of a few extra steps:\n",
        "\n",
        "1. Add the argument `--local_rank` which will be automatically set by the distributed launcher\n",
        "2. Initialize the process group\n",
        "2. Generate independent batched data based on process id in the dataset.\n",
        "3. Wrap the model and optimizer to handle distributed computation. \n",
        "4. Scale the loss and optimizer\n",
        "\n",
        "To launch distributed training, we need to put everything into a python file. Following is an example:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gH23WKAiFUf",
        "outputId": "96a37fd6-4270-4a03-95cb-25f355691a69"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhe-NMOknwjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d2c756-b4e1-4a1b-f48f-ba5394de7eca"
      },
      "source": [
        "%%writefile distributed_train.py \n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from apex import amp\n",
        "from ignite.engine import Engine, Events\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from apex.parallel import DistributedDataParallel \n",
        "import argparse\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser = argparse.ArgumentParser()\n",
        "# this local_rank arg is automaticall set by distributed launch\n",
        "parser.add_argument(\"--local_rank\", default=0, type=int)\n",
        "args = parser.parse_args()\n",
        "\n",
        "args.distributed = False\n",
        "if 'WORLD_SIZE' in os.environ:\n",
        "    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n",
        "\n",
        "if args.distributed:\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl',\n",
        "                                         init_method='env://')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "opt_level = 'O1'\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
        "if args.distributed:\n",
        "    model = DistributedDataParallel(model)\n",
        "#dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=10240, seed=args.local_rank)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2, stocks=3, seed=args.local_rank)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing distributed_train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsWtauwknwjL"
      },
      "source": [
        "To launch multiple processes training, we need to run the following command:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Yzx7KPnwjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6571ea3d-bc5d-41a7-8641-a587c930cb7a"
      },
      "source": [
        "%reset -f\n",
        "\n",
        "!python -m torch.distributed.launch --nproc_per_node=4 distributed_train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:164: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
            "  \"The module torch.distributed.launch is deprecated \"\n",
            "The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run\n",
            "*****************************************\n",
            "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "*****************************************\n",
            "WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.\n",
            " Please read local_rank from `os.environ('LOCAL_RANK')` instead.\n",
            "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\n",
            "  entrypoint       : distributed_train.py\n",
            "  min_nodes        : 1\n",
            "  max_nodes        : 1\n",
            "  nproc_per_node   : 4\n",
            "  run_id           : none\n",
            "  rdzv_backend     : static\n",
            "  rdzv_endpoint    : 127.0.0.1:29500\n",
            "  rdzv_configs     : {'rank': 0, 'timeout': 900}\n",
            "  max_restarts     : 3\n",
            "  monitor_interval : 5\n",
            "  log_dir          : None\n",
            "  metrics_cfg      : {}\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/utils/store.py:53: FutureWarning: This is an experimental API and will be changed in future.\n",
            "  \"This is an experimental API and will be changed in future.\", FutureWarning\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=0\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3]\n",
            "  role_ranks=[0, 1, 2, 3]\n",
            "  global_ranks=[0, 1, 2, 3]\n",
            "  role_world_sizes=[4, 4, 4, 4]\n",
            "  global_world_sizes=[4, 4, 4, 4]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_0/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_0/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_0/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_0/3/error.json\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1868) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=1\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3]\n",
            "  role_ranks=[0, 1, 2, 3]\n",
            "  global_ranks=[0, 1, 2, 3]\n",
            "  role_world_sizes=[4, 4, 4, 4]\n",
            "  global_world_sizes=[4, 4, 4, 4]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_1/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_1/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_1/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_1/3/error.json\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1882) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=2\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3]\n",
            "  role_ranks=[0, 1, 2, 3]\n",
            "  global_ranks=[0, 1, 2, 3]\n",
            "  role_world_sizes=[4, 4, 4, 4]\n",
            "  global_world_sizes=[4, 4, 4, 4]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_2/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_2/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_2/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_2/3/error.json\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1894) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=3\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3]\n",
            "  role_ranks=[0, 1, 2, 3]\n",
            "  global_ranks=[0, 1, 2, 3]\n",
            "  role_world_sizes=[4, 4, 4, 4]\n",
            "  global_world_sizes=[4, 4, 4, 4]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_3/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_3/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_3/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_u4iy59gn/none_61z6_o9s/attempt_3/3/error.json\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1908) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/utils/store.py:71: FutureWarning: This is an experimental API and will be changed in future.\n",
            "  \"This is an experimental API and will be changed in future.\", FutureWarning\n",
            "INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003161430358886719 seconds\n",
            "{\"name\": \"torchelastic.worker.status.FAILED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 0, \"group_rank\": 0, \"worker_id\": \"1908\", \"role\": \"default\", \"hostname\": \"9852bd71793c\", \"state\": \"FAILED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": \"{\\\"message\\\": \\\"<NONE>\\\"}\", \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [0], \\\"role_rank\\\": [0], \\\"role_world_size\\\": [4]}\", \"agent_restarts\": 3}}\n",
            "{\"name\": \"torchelastic.worker.status.FAILED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 1, \"group_rank\": 0, \"worker_id\": \"1909\", \"role\": \"default\", \"hostname\": \"9852bd71793c\", \"state\": \"FAILED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": \"{\\\"message\\\": \\\"<NONE>\\\"}\", \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [1], \\\"role_rank\\\": [1], \\\"role_world_size\\\": [4]}\", \"agent_restarts\": 3}}\n",
            "{\"name\": \"torchelastic.worker.status.FAILED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 2, \"group_rank\": 0, \"worker_id\": \"1910\", \"role\": \"default\", \"hostname\": \"9852bd71793c\", \"state\": \"FAILED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": \"{\\\"message\\\": \\\"<NONE>\\\"}\", \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [2], \\\"role_rank\\\": [2], \\\"role_world_size\\\": [4]}\", \"agent_restarts\": 3}}\n",
            "{\"name\": \"torchelastic.worker.status.FAILED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 3, \"group_rank\": 0, \"worker_id\": \"1911\", \"role\": \"default\", \"hostname\": \"9852bd71793c\", \"state\": \"FAILED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": \"{\\\"message\\\": \\\"<NONE>\\\"}\", \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [3], \\\"role_rank\\\": [3], \\\"role_world_size\\\": [4]}\", \"agent_restarts\": 3}}\n",
            "{\"name\": \"torchelastic.worker.status.SUCCEEDED\", \"source\": \"AGENT\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": null, \"group_rank\": 0, \"worker_id\": null, \"role\": \"default\", \"hostname\": \"9852bd71793c\", \"state\": \"SUCCEEDED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": null, \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\"}\", \"agent_restarts\": 3}}\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: \n",
            "\n",
            "**********************************************************************\n",
            "               CHILD PROCESS FAILED WITH NO ERROR_FILE                \n",
            "**********************************************************************\n",
            "CHILD PROCESS FAILED WITH NO ERROR_FILE\n",
            "Child process 1908 (local_rank 0) FAILED (exitcode 1)\n",
            "Error msg: Process failed with exitcode 1\n",
            "Without writing an error file to <N/A>.\n",
            "While this DOES NOT affect the correctness of your application,\n",
            "no trace information about the error will be available for inspection.\n",
            "Consider decorating your top level entrypoint function with\n",
            "torch.distributed.elastic.multiprocessing.errors.record. Example:\n",
            "\n",
            "  from torch.distributed.elastic.multiprocessing.errors import record\n",
            "\n",
            "  @record\n",
            "  def trainer_main(args):\n",
            "     # do train\n",
            "**********************************************************************\n",
            "  warnings.warn(_no_error_file_warning_msg(rank, failure))\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 173, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 169, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py\", line 624, in run\n",
            "    )(*cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 116, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 247, in launch_agent\n",
            "    failures=result.failures,\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "***************************************\n",
            "      distributed_train.py FAILED      \n",
            "=======================================\n",
            "Root Cause:\n",
            "[0]:\n",
            "  time: 2021-06-22_02:25:04\n",
            "  rank: 0 (local_rank: 0)\n",
            "  exitcode: 1 (pid: 1908)\n",
            "  error_file: <N/A>\n",
            "  msg: \"Process failed with exitcode 1\"\n",
            "=======================================\n",
            "Other Failures:\n",
            "[1]:\n",
            "  time: 2021-06-22_02:25:04\n",
            "  rank: 1 (local_rank: 1)\n",
            "  exitcode: 1 (pid: 1909)\n",
            "  error_file: <N/A>\n",
            "  msg: \"Process failed with exitcode 1\"\n",
            "[2]:\n",
            "  time: 2021-06-22_02:25:04\n",
            "  rank: 2 (local_rank: 2)\n",
            "  exitcode: 1 (pid: 1910)\n",
            "  error_file: <N/A>\n",
            "  msg: \"Process failed with exitcode 1\"\n",
            "[3]:\n",
            "  time: 2021-06-22_02:25:04\n",
            "  rank: 3 (local_rank: 3)\n",
            "  exitcode: 1 (pid: 1911)\n",
            "  error_file: <N/A>\n",
            "  msg: \"Process failed with exitcode 1\"\n",
            "***************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUAIQJKAnwjM"
      },
      "source": [
        "It works and all the GPUs are busy to train this network. However, it has a few problems:-\n",
        "   \n",
        "    1. There is no model serialization so the trained model is not saved\n",
        "    2. There is no validation dataset to check the training progress\n",
        "    3. Most of the time is spent in Monte Carlo simulation hence the training is slow\n",
        "    4. We use a few paths(1024) for each option parameter set which is noise and the model cannot converge to a low cost value.\n",
        "We will address these problems in the next notebook"
      ]
    }
  ]
}