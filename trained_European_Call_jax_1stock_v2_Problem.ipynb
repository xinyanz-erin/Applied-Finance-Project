{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Judy/trained_European_Call_jax_1stock_v2_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "33d62d7a-b6ae-447e-f355-9927f71ac72f"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0807]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.2597*0.2597\n",
        "initial_stocks = jnp.array([0.7178]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.2106\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.52334607\n",
            "[0.9997467]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de7cd44-a000-426c-b4d5-8c0ef183c170"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "# version 1, 2, 6\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 1.0)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          K = np.random.random(1) * 1.0\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32) # remember to change this!\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = 15, stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "d92edaa5-6626-4552-bb6b-007464327521"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 1.0, 1.0, 0.3, 0.1, 0.1]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "5d2416eb-3c66-4f07-a108-188e42a95cd2"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 36.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "087e5e27-01ff-44be-afa7-03aa70bc1d09"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.013804377056658268 average time 0.20097316244996363 iter num 20\n",
            "loss 0.013547497801482677 average time 0.10784878504999824 iter num 40\n",
            "loss 0.015108984895050526 average time 0.0768151311666467 iter num 60\n",
            "loss 0.008190256543457508 average time 0.06126157638748282 iter num 80\n",
            "loss 0.01769290119409561 average time 0.05192215447998933 iter num 100\n",
            "loss 0.02364984340965748 average time 0.03646207305002917 iter num 20\n",
            "loss 0.01126737892627716 average time 0.025683993675067997 iter num 40\n",
            "loss 0.015724733471870422 average time 0.02225810866671812 iter num 60\n",
            "loss 0.005341275129467249 average time 0.020356744412532636 iter num 80\n",
            "loss 0.0055426666513085365 average time 0.01933542493002278 iter num 100\n",
            "loss 0.016154490411281586 average time 0.0367463811999869 iter num 20\n",
            "loss 0.002646077424287796 average time 0.025774725599967498 iter num 40\n",
            "loss 0.004602628760039806 average time 0.022021366599968437 iter num 60\n",
            "loss 0.004555930383503437 average time 0.02020826411246617 iter num 80\n",
            "loss 0.0032389662228524685 average time 0.019163010759971257 iter num 100\n",
            "loss 0.001039105118252337 average time 0.03711476189998848 iter num 20\n",
            "loss 0.0009425661992281675 average time 0.026068012800021734 iter num 40\n",
            "loss 0.0005286865634843707 average time 0.022773172933360303 iter num 60\n",
            "loss 7.315122638829052e-05 average time 0.020799667262525644 iter num 80\n",
            "loss 0.0002579714637249708 average time 0.019592502380023687 iter num 100\n",
            "loss 0.0006481822929345071 average time 0.03639458315005868 iter num 20\n",
            "loss 0.0006201652577146888 average time 0.025531160675052435 iter num 40\n",
            "loss 0.0005881967954337597 average time 0.022311794250049387 iter num 60\n",
            "loss 0.0002345465763937682 average time 0.02053366223753983 iter num 80\n",
            "loss 0.00014648449723608792 average time 0.01944031873002132 iter num 100\n",
            "loss 0.0002345891116419807 average time 0.03680906565004989 iter num 20\n",
            "loss 0.0009123375057242811 average time 0.025758635100044103 iter num 40\n",
            "loss 0.0003416985855437815 average time 0.022205919516689696 iter num 60\n",
            "loss 0.0003016524133272469 average time 0.02031940752499963 iter num 80\n",
            "loss 0.00016347301425412297 average time 0.019447513410004832 iter num 100\n",
            "loss 0.0009443722665309906 average time 0.03664548025001295 iter num 20\n",
            "loss 0.0006564826471731067 average time 0.025744909800016556 iter num 40\n",
            "loss 0.00019740447169169784 average time 0.022186796216654633 iter num 60\n",
            "loss 0.0004023844376206398 average time 0.02048888596249867 iter num 80\n",
            "loss 0.0003811195492744446 average time 0.019396286959986356 iter num 100\n",
            "loss 0.0007367903599515557 average time 0.03735712599996077 iter num 20\n",
            "loss 0.0002797560300678015 average time 0.026394137849968045 iter num 40\n",
            "loss 0.0003209576243534684 average time 0.02264383701665338 iter num 60\n",
            "loss 0.0002202265168307349 average time 0.02086502906248029 iter num 80\n",
            "loss 0.00018789657042361796 average time 0.019845717859975593 iter num 100\n",
            "loss 0.0003562472411431372 average time 0.03753237294988594 iter num 20\n",
            "loss 0.0005352333537302911 average time 0.026358530224990774 iter num 40\n",
            "loss 0.00048499787226319313 average time 0.02247647933332549 iter num 60\n",
            "loss 0.00037289492320269346 average time 0.02054927669999529 iter num 80\n",
            "loss 0.00014382934023160487 average time 0.019372877270006937 iter num 100\n",
            "loss 0.0005472198245115578 average time 0.036875363799958906 iter num 20\n",
            "loss 0.0002389481960562989 average time 0.025822086149969437 iter num 40\n",
            "loss 0.0003529091482050717 average time 0.022313207249991744 iter num 60\n",
            "loss 0.0004383274063002318 average time 0.020440587537490274 iter num 80\n",
            "loss 0.000170099112438038 average time 0.019401310359985473 iter num 100\n",
            "loss 0.0005890073953196406 average time 0.0376821795999831 iter num 20\n",
            "loss 0.00043226132402196527 average time 0.026232533500024146 iter num 40\n",
            "loss 0.00011922120756935328 average time 0.022537448950023038 iter num 60\n",
            "loss 0.0006656981422565877 average time 0.020706015362526385 iter num 80\n",
            "loss 0.0002974683011416346 average time 0.0195656357200005 iter num 100\n",
            "loss 0.0005226711509749293 average time 0.03742283720002888 iter num 20\n",
            "loss 0.00021838568500243127 average time 0.026488622950023456 iter num 40\n",
            "loss 0.00021080096485093236 average time 0.022764231683337736 iter num 60\n",
            "loss 0.00014624130562879145 average time 0.021018697412506525 iter num 80\n",
            "loss 9.150971891358495e-05 average time 0.019777935490001255 iter num 100\n",
            "loss 0.00031937300809659064 average time 0.03726123549995464 iter num 20\n",
            "loss 0.0003655211185105145 average time 0.02621261107497048 iter num 40\n",
            "loss 0.00013804396439809352 average time 0.022523841299963956 iter num 60\n",
            "loss 6.570070399902761e-05 average time 0.02057314646247619 iter num 80\n",
            "loss 0.00046255410416051745 average time 0.019416349399984937 iter num 100\n",
            "loss 0.0006494401022791862 average time 0.036565209400009735 iter num 20\n",
            "loss 0.0006408192566595972 average time 0.025707575899969016 iter num 40\n",
            "loss 0.0008337196195498109 average time 0.021994149366643494 iter num 60\n",
            "loss 0.00013848673552274704 average time 0.020149753424971096 iter num 80\n",
            "loss 0.00037037715082988143 average time 0.019193115349976322 iter num 100\n",
            "loss 0.00015444001473952085 average time 0.03728206825003326 iter num 20\n",
            "loss 0.0012687405105680227 average time 0.026302495725042264 iter num 40\n",
            "loss 0.0015859021805226803 average time 0.02253164286670047 iter num 60\n",
            "loss 0.00024374053464271128 average time 0.020592667575044743 iter num 80\n",
            "loss 0.0001948061544680968 average time 0.01953446540002915 iter num 100\n",
            "loss 0.00025476637529209256 average time 0.036182199299992134 iter num 20\n",
            "loss 0.00030473709921352565 average time 0.025598160175013616 iter num 40\n",
            "loss 0.0009486984345130622 average time 0.021985263750002558 iter num 60\n",
            "loss 0.0001649654150241986 average time 0.020185160874990515 iter num 80\n",
            "loss 0.000185439464985393 average time 0.019258297909991597 iter num 100\n",
            "loss 0.00029752060072496533 average time 0.03696898035000231 iter num 20\n",
            "loss 0.00039490338531322777 average time 0.026014950124977076 iter num 40\n",
            "loss 0.00037688505835831165 average time 0.02225451506664437 iter num 60\n",
            "loss 0.0002662859915290028 average time 0.020528881762464833 iter num 80\n",
            "loss 0.00041160083492286503 average time 0.01943209127996397 iter num 100\n",
            "loss 0.0017471604514867067 average time 0.037157740250017925 iter num 20\n",
            "loss 0.0010123888496309519 average time 0.025915273774978688 iter num 40\n",
            "loss 0.0005347809637896717 average time 0.022203482066637057 iter num 60\n",
            "loss 0.00036026304587721825 average time 0.020337772399966526 iter num 80\n",
            "loss 3.9804261177778244e-05 average time 0.019358145049977793 iter num 100\n",
            "loss 0.0002069300098810345 average time 0.037408060150050916 iter num 20\n",
            "loss 0.00014218840806279331 average time 0.026567267175028064 iter num 40\n",
            "loss 0.00013744289753958583 average time 0.02274653471668747 iter num 60\n",
            "loss 0.00011333554721204564 average time 0.020807878400000847 iter num 80\n",
            "loss 0.00038921667146496475 average time 0.019655523460010045 iter num 100\n",
            "loss 0.0010756086558103561 average time 0.03699818680004228 iter num 20\n",
            "loss 0.0016886130906641483 average time 0.02629493022503766 iter num 40\n",
            "loss 0.0006390145281329751 average time 0.022448840183339296 iter num 60\n",
            "loss 0.0003634376625996083 average time 0.020600039375011647 iter num 80\n",
            "loss 0.00012265463010407984 average time 0.0194867719100057 iter num 100\n",
            "loss 0.0006674084579572082 average time 0.036280234049991124 iter num 20\n",
            "loss 0.0003298697411082685 average time 0.025558873624970602 iter num 40\n",
            "loss 0.0003791491035372019 average time 0.022284007166619327 iter num 60\n",
            "loss 0.0001673164078965783 average time 0.020452024237454226 iter num 80\n",
            "loss 7.121720409486443e-05 average time 0.01931292002996088 iter num 100\n",
            "loss 7.900062337284908e-05 average time 0.03679771040001469 iter num 20\n",
            "loss 0.0004677753313444555 average time 0.026200717775043358 iter num 40\n",
            "loss 0.0005341048818081617 average time 0.022491329316691615 iter num 60\n",
            "loss 0.00019132794113829732 average time 0.02062117128753016 iter num 80\n",
            "loss 0.0001909018901642412 average time 0.01952088304000881 iter num 100\n",
            "loss 0.0003724343841895461 average time 0.03769048295005177 iter num 20\n",
            "loss 0.00019842945039272308 average time 0.026577565225022682 iter num 40\n",
            "loss 0.0001947805576492101 average time 0.022604676033332303 iter num 60\n",
            "loss 5.474652425618842e-05 average time 0.020687939499987353 iter num 80\n",
            "loss 0.00012444399180822074 average time 0.019507141169983696 iter num 100\n",
            "loss 0.0006541108014062047 average time 0.03684251399995446 iter num 20\n",
            "loss 0.0003203862579539418 average time 0.025837850224979773 iter num 40\n",
            "loss 0.00046293274499475956 average time 0.022411141016626366 iter num 60\n",
            "loss 9.456886618863791e-05 average time 0.02049268418745669 iter num 80\n",
            "loss 0.00011533511860761791 average time 0.019290834849985002 iter num 100\n",
            "loss 0.0001601705007487908 average time 0.03656130255001244 iter num 20\n",
            "loss 0.00010008456592913717 average time 0.026108867799985093 iter num 40\n",
            "loss 5.881971446797252e-05 average time 0.02264033558332509 iter num 60\n",
            "loss 0.00019878448802046478 average time 0.020928837774994234 iter num 80\n",
            "loss 9.778267849469557e-05 average time 0.01981738479998967 iter num 100\n",
            "loss 0.0007382757030427456 average time 0.036843515950045 iter num 20\n",
            "loss 0.000834595353808254 average time 0.02616123295000534 iter num 40\n",
            "loss 0.00023304502246901393 average time 0.022709372116613242 iter num 60\n",
            "loss 3.849132190225646e-05 average time 0.02080176529998994 iter num 80\n",
            "loss 0.00010996127821272239 average time 0.01972776770002838 iter num 100\n",
            "loss 9.69398461165838e-05 average time 0.037761520199956065 iter num 20\n",
            "loss 6.358577229548246e-05 average time 0.026670643675038262 iter num 40\n",
            "loss 0.00024199552717618644 average time 0.022684627016648543 iter num 60\n",
            "loss 0.00016547154518775642 average time 0.020711756999958197 iter num 80\n",
            "loss 0.00012413313379511237 average time 0.01953316778995031 iter num 100\n",
            "loss 0.0002297821338288486 average time 0.03645429889998013 iter num 20\n",
            "loss 0.0003031915402971208 average time 0.02571612949993778 iter num 40\n",
            "loss 9.759904787642881e-05 average time 0.022197170833275475 iter num 60\n",
            "loss 0.00020791520364582539 average time 0.020423416062430987 iter num 80\n",
            "loss 0.0002024216955760494 average time 0.019267570119964147 iter num 100\n",
            "loss 0.0006171127315610647 average time 0.037815132099967744 iter num 20\n",
            "loss 0.00011065872968174517 average time 0.026234686425027576 iter num 40\n",
            "loss 0.00013794764527119696 average time 0.02243886440002522 iter num 60\n",
            "loss 0.00010479705088073388 average time 0.020512180462480954 iter num 80\n",
            "loss 0.00018004373123403639 average time 0.019319896699980747 iter num 100\n",
            "loss 0.0006919465376995504 average time 0.03709600160009359 iter num 20\n",
            "loss 0.00023177840921562165 average time 0.025861973000019134 iter num 40\n",
            "loss 3.81754944100976e-05 average time 0.022250453116703282 iter num 60\n",
            "loss 8.568789053242654e-05 average time 0.020557443499990315 iter num 80\n",
            "loss 0.0001514241739641875 average time 0.01949926832998244 iter num 100\n",
            "loss 0.00016660340770613402 average time 0.03705395279980621 iter num 20\n",
            "loss 8.166950283339247e-05 average time 0.026219558174852865 iter num 40\n",
            "loss 0.0001294993853662163 average time 0.022671831433232606 iter num 60\n",
            "loss 0.00011138750414829701 average time 0.020806939624912958 iter num 80\n",
            "loss 9.65311992331408e-05 average time 0.0197422335999363 iter num 100\n",
            "loss 0.0002694461145438254 average time 0.03681287870003871 iter num 20\n",
            "loss 0.0004021896456833929 average time 0.02607748907507812 iter num 40\n",
            "loss 0.0007854708237573504 average time 0.022322324483351016 iter num 60\n",
            "loss 0.0001014575973385945 average time 0.020442738725034815 iter num 80\n",
            "loss 0.0001414081925759092 average time 0.01933402579003996 iter num 100\n",
            "loss 0.0009126925142481923 average time 0.037237479100031126 iter num 20\n",
            "loss 0.002038855804130435 average time 0.026283391624974683 iter num 40\n",
            "loss 0.0002580590662546456 average time 0.022554890666651773 iter num 60\n",
            "loss 0.00016033365682233125 average time 0.020657763874987724 iter num 80\n",
            "loss 0.0001622053823666647 average time 0.019517346669981634 iter num 100\n",
            "loss 0.00018241829820908606 average time 0.03643956320001962 iter num 20\n",
            "loss 9.45087886066176e-05 average time 0.025608587375018033 iter num 40\n",
            "loss 6.857729749754071e-05 average time 0.021968138933364873 iter num 60\n",
            "loss 0.00012891645019408315 average time 0.020205648437513445 iter num 80\n",
            "loss 2.722083081607707e-05 average time 0.01911267598999984 iter num 100\n",
            "loss 8.668929513078183e-05 average time 0.03738880859996243 iter num 20\n",
            "loss 0.00010667841706890613 average time 0.026646713224999986 iter num 40\n",
            "loss 0.0001675804378464818 average time 0.022814576666708792 iter num 60\n",
            "loss 9.119629248743877e-05 average time 0.020791901562529348 iter num 80\n",
            "loss 1.1306765372864902e-05 average time 0.019564913990025162 iter num 100\n",
            "loss 0.0010310247307643294 average time 0.036771043599947005 iter num 20\n",
            "loss 0.00046746496809646487 average time 0.02580087729998013 iter num 40\n",
            "loss 0.0016706434544175863 average time 0.022331575500008208 iter num 60\n",
            "loss 9.521660103928298e-05 average time 0.02040325686247115 iter num 80\n",
            "loss 2.5780096621019766e-05 average time 0.019279225979962577 iter num 100\n",
            "loss 0.00010390088573331013 average time 0.03654139949999262 iter num 20\n",
            "loss 0.00019388506188988686 average time 0.025769725824966373 iter num 40\n",
            "loss 6.275906343944371e-05 average time 0.02231517391669513 iter num 60\n",
            "loss 7.075766916386783e-05 average time 0.02053676947502936 iter num 80\n",
            "loss 3.729658419615589e-05 average time 0.01952037921001647 iter num 100\n",
            "loss 0.0002483996213413775 average time 0.036362519749945935 iter num 20\n",
            "loss 0.00018051083316095173 average time 0.02571983735001595 iter num 40\n",
            "loss 4.395210271468386e-05 average time 0.02212656669998978 iter num 60\n",
            "loss 3.9934577216627076e-05 average time 0.02023685954999337 iter num 80\n",
            "loss 3.9173253753688186e-05 average time 0.019248979379981393 iter num 100\n",
            "loss 0.00010241415293421596 average time 0.037012165749956694 iter num 20\n",
            "loss 9.667188714956865e-05 average time 0.025983702050007197 iter num 40\n",
            "loss 0.0001580561074661091 average time 0.02222918835001716 iter num 60\n",
            "loss 3.4768447221722454e-05 average time 0.020384611087501982 iter num 80\n",
            "loss 0.00021341224783100188 average time 0.019259145970017925 iter num 100\n",
            "loss 6.0061560361646116e-05 average time 0.037151648200097045 iter num 20\n",
            "loss 0.0003166651586070657 average time 0.026099608300023648 iter num 40\n",
            "loss 0.00024158087035175413 average time 0.022237923116729993 iter num 60\n",
            "loss 1.5674955648137257e-05 average time 0.020569485837563663 iter num 80\n",
            "loss 3.6347195418784395e-05 average time 0.019400400340018678 iter num 100\n",
            "loss 4.251822974765673e-05 average time 0.037018610350060044 iter num 20\n",
            "loss 2.5191615350195207e-05 average time 0.026077228650024153 iter num 40\n",
            "loss 5.700120163965039e-05 average time 0.022270791499992507 iter num 60\n",
            "loss 6.605700764339417e-05 average time 0.020420908937455805 iter num 80\n",
            "loss 7.280157296918333e-05 average time 0.019406630709981984 iter num 100\n",
            "loss 0.00030182432965375483 average time 0.03691018520003127 iter num 20\n",
            "loss 0.0014778405893594027 average time 0.02613084680006068 iter num 40\n",
            "loss 0.0001545224222354591 average time 0.022409842066690545 iter num 60\n",
            "loss 0.0001151021133409813 average time 0.020569844312501574 iter num 80\n",
            "loss 5.7257366279372945e-05 average time 0.01939645768004084 iter num 100\n",
            "loss 0.00074429449159652 average time 0.037456296450091034 iter num 20\n",
            "loss 0.00011084479046985507 average time 0.02603046697499849 iter num 40\n",
            "loss 2.9688526410609484e-05 average time 0.022362459700025282 iter num 60\n",
            "loss 5.947516910964623e-05 average time 0.02047313885003632 iter num 80\n",
            "loss 3.65039668395184e-05 average time 0.019295571380016554 iter num 100\n",
            "loss 0.00017687960644252598 average time 0.03672321635017397 iter num 20\n",
            "loss 0.00023627090558875352 average time 0.025857075225098926 iter num 40\n",
            "loss 3.4168439015047625e-05 average time 0.022135093300100075 iter num 60\n",
            "loss 3.3698299375828356e-05 average time 0.020337349175053986 iter num 80\n",
            "loss 9.447001502849162e-05 average time 0.019224012020049485 iter num 100\n",
            "loss 0.00011940058175241575 average time 0.03619463855002323 iter num 20\n",
            "loss 0.00011536318925209343 average time 0.025436620974960533 iter num 40\n",
            "loss 0.00011682467447826639 average time 0.022005629266626178 iter num 60\n",
            "loss 8.286282536573708e-05 average time 0.020321218999924895 iter num 80\n",
            "loss 0.00012480231816880405 average time 0.019297906179945132 iter num 100\n",
            "loss 6.650124123552814e-05 average time 0.036916715749885046 iter num 20\n",
            "loss 0.00021255557658150792 average time 0.025802660999966066 iter num 40\n",
            "loss 5.2206480177119374e-05 average time 0.022176955566601465 iter num 60\n",
            "loss 1.4567778634955175e-05 average time 0.020390491774958264 iter num 80\n",
            "loss 2.2728789190296084e-05 average time 0.019232697419929536 iter num 100\n",
            "loss 0.00019738060655072331 average time 0.03728163509990736 iter num 20\n",
            "loss 0.00012565172801259905 average time 0.025918882900009522 iter num 40\n",
            "loss 4.7995170461945236e-05 average time 0.02214944714990755 iter num 60\n",
            "loss 1.3255306839710101e-05 average time 0.020633418037402863 iter num 80\n",
            "loss 9.847366527537815e-06 average time 0.01949840384992058 iter num 100\n",
            "loss 7.097351044649258e-05 average time 0.037950119750030355 iter num 20\n",
            "loss 0.00015619397163391113 average time 0.026578922550038443 iter num 40\n",
            "loss 0.00016200265963561833 average time 0.02267347491667048 iter num 60\n",
            "loss 4.433610229170881e-05 average time 0.02066954588749468 iter num 80\n",
            "loss 3.985613875556737e-05 average time 0.01952387707998241 iter num 100\n",
            "loss 0.0001552706235088408 average time 0.037301491149901264 iter num 20\n",
            "loss 2.3764896468492225e-05 average time 0.02607934389991442 iter num 40\n",
            "loss 9.742409019963816e-05 average time 0.022342284016561582 iter num 60\n",
            "loss 3.3225409424630925e-05 average time 0.02047664741241988 iter num 80\n",
            "loss 2.2174775949679315e-05 average time 0.019307095689919153 iter num 100\n",
            "loss 0.00016350505757145584 average time 0.03794644139989032 iter num 20\n",
            "loss 0.00010573364852461964 average time 0.02666188232485638 iter num 40\n",
            "loss 7.23019620636478e-05 average time 0.023000044099959874 iter num 60\n",
            "loss 1.271906694455538e-05 average time 0.021075458874963714 iter num 80\n",
            "loss 1.171179610537365e-05 average time 0.019944429419983863 iter num 100\n",
            "loss 0.0002633089607115835 average time 0.03747656729992741 iter num 20\n",
            "loss 0.0004048535192850977 average time 0.026242886349996296 iter num 40\n",
            "loss 0.00015320998500101268 average time 0.02260628738337497 iter num 60\n",
            "loss 3.708955046022311e-05 average time 0.020637376562513056 iter num 80\n",
            "loss 3.9288450352614745e-05 average time 0.019540429999979098 iter num 100\n",
            "loss 2.478607348166406e-05 average time 0.036890778550014144 iter num 20\n",
            "loss 1.5055114999995567e-05 average time 0.025748969099981877 iter num 40\n",
            "loss 5.555291136261076e-05 average time 0.02207846658332831 iter num 60\n",
            "loss 4.09986860177014e-05 average time 0.020405232849986986 iter num 80\n",
            "loss 5.759384293924086e-05 average time 0.019349927249995746 iter num 100\n",
            "loss 6.996878073550761e-05 average time 0.037468851299945524 iter num 20\n",
            "loss 6.450803630286828e-05 average time 0.02612519967494791 iter num 40\n",
            "loss 0.00017733112326823175 average time 0.022331229200002176 iter num 60\n",
            "loss 2.4214081349782646e-05 average time 0.020434483437509244 iter num 80\n",
            "loss 0.00010762643069028854 average time 0.019311268470028155 iter num 100\n",
            "loss 4.649635229725391e-05 average time 0.03669456385005106 iter num 20\n",
            "loss 5.4531617934117094e-05 average time 0.026032494675018825 iter num 40\n",
            "loss 2.258578570035752e-05 average time 0.02231030133331539 iter num 60\n",
            "loss 8.33899830467999e-05 average time 0.02042204647498238 iter num 80\n",
            "loss 1.538661672384478e-05 average time 0.01932622423000794 iter num 100\n",
            "loss 0.0001156332582468167 average time 0.037418884200042156 iter num 20\n",
            "loss 0.00023319781757891178 average time 0.02622306085004311 iter num 40\n",
            "loss 6.96125061949715e-05 average time 0.02235978573336676 iter num 60\n",
            "loss 4.345014167483896e-05 average time 0.020593282700042438 iter num 80\n",
            "loss 4.589666787069291e-05 average time 0.01953683084005206 iter num 100\n",
            "loss 0.0001620662515051663 average time 0.03693005604991413 iter num 20\n",
            "loss 6.510037201223895e-05 average time 0.025822731599942018 iter num 40\n",
            "loss 4.271707803127356e-05 average time 0.02239158336657662 iter num 60\n",
            "loss 1.3881245649827179e-05 average time 0.02057017976244424 iter num 80\n",
            "loss 2.855853563232813e-05 average time 0.019393987779931194 iter num 100\n",
            "loss 6.699524965370074e-05 average time 0.036619836150066476 iter num 20\n",
            "loss 4.895870733889751e-05 average time 0.025892376350043378 iter num 40\n",
            "loss 4.343823457020335e-05 average time 0.0221604312333208 iter num 60\n",
            "loss 3.872231172863394e-05 average time 0.02034788761246773 iter num 80\n",
            "loss 3.1090221455087885e-05 average time 0.01924100676998023 iter num 100\n",
            "loss 0.003874773159623146 average time 0.038586175949967584 iter num 20\n",
            "loss 0.0001052227962645702 average time 0.026819871125053397 iter num 40\n",
            "loss 3.437589111854322e-05 average time 0.022984491750063778 iter num 60\n",
            "loss 7.73496285546571e-05 average time 0.020995313812534278 iter num 80\n",
            "loss 2.226072501798626e-05 average time 0.019784512590022132 iter num 100\n",
            "loss 0.00016628409503027797 average time 0.036899388150095545 iter num 20\n",
            "loss 0.000307997950585559 average time 0.025859387625041565 iter num 40\n",
            "loss 4.600991451297887e-05 average time 0.02203774538338621 iter num 60\n",
            "loss 5.603309546131641e-05 average time 0.020298627662521086 iter num 80\n",
            "loss 8.242232433985919e-05 average time 0.019292458560012164 iter num 100\n",
            "loss 4.974117109668441e-05 average time 0.03601835105005193 iter num 20\n",
            "loss 0.00014259085583034903 average time 0.025336982825069753 iter num 40\n",
            "loss 3.5704731999430805e-05 average time 0.02195546928337535 iter num 60\n",
            "loss 2.874538768082857e-05 average time 0.020099293737541758 iter num 80\n",
            "loss 5.035172307543689e-06 average time 0.019005985900003 iter num 100\n",
            "loss 0.0002456728252582252 average time 0.037813895299996146 iter num 20\n",
            "loss 4.376091601443477e-05 average time 0.026601169475065946 iter num 40\n",
            "loss 2.6463727408554405e-05 average time 0.022772288500088205 iter num 60\n",
            "loss 1.548834734421689e-05 average time 0.02074661657507022 iter num 80\n",
            "loss 3.690645098686218e-05 average time 0.01954110492003565 iter num 100\n",
            "loss 5.528596375370398e-05 average time 0.03621986280004421 iter num 20\n",
            "loss 0.000108565844129771 average time 0.025746116000095753 iter num 40\n",
            "loss 2.828735887305811e-05 average time 0.02237193205000949 iter num 60\n",
            "loss 2.837627471308224e-05 average time 0.02054263941249701 iter num 80\n",
            "loss 9.05169054021826e-06 average time 0.019435017370014976 iter num 100\n",
            "loss 0.00022002262994647026 average time 0.036560103449892266 iter num 20\n",
            "loss 6.960428436286747e-05 average time 0.02568722674991477 iter num 40\n",
            "loss 5.5922257161000744e-05 average time 0.022194848899926 iter num 60\n",
            "loss 1.6061640053521842e-05 average time 0.020372520287446606 iter num 80\n",
            "loss 5.879567834199406e-05 average time 0.019402865609945367 iter num 100\n",
            "loss 4.0187635022448376e-05 average time 0.036310092700023236 iter num 20\n",
            "loss 5.3460920753423125e-05 average time 0.02582659307506674 iter num 40\n",
            "loss 0.00013777471031062305 average time 0.022130353850070606 iter num 60\n",
            "loss 1.05120916487067e-05 average time 0.020271063950053757 iter num 80\n",
            "loss 4.6967368689365685e-05 average time 0.019301784120034426 iter num 100\n",
            "loss 3.9539132558275014e-05 average time 0.036745580100068766 iter num 20\n",
            "loss 6.042989116394892e-05 average time 0.025851112124951215 iter num 40\n",
            "loss 1.932697523443494e-05 average time 0.022303212766640476 iter num 60\n",
            "loss 8.881850590114482e-06 average time 0.02041095351248714 iter num 80\n",
            "loss 5.6774581025820225e-05 average time 0.019239062629994804 iter num 100\n",
            "loss 7.462123176082969e-05 average time 0.037144527550071874 iter num 20\n",
            "loss 2.6648551283869892e-05 average time 0.026152032925006098 iter num 40\n",
            "loss 4.0633331082062796e-06 average time 0.022320754333334965 iter num 60\n",
            "loss 7.823907071724534e-05 average time 0.02064844834998212 iter num 80\n",
            "loss 2.4144015696947463e-05 average time 0.01956494642996404 iter num 100\n",
            "loss 7.544174150098115e-05 average time 0.037123615550081014 iter num 20\n",
            "loss 6.698002107441425e-05 average time 0.026112256150054237 iter num 40\n",
            "loss 3.49227266269736e-05 average time 0.022337473783409223 iter num 60\n",
            "loss 7.462983921868727e-05 average time 0.02046953311253219 iter num 80\n",
            "loss 4.86544968225644e-06 average time 0.01926741760000368 iter num 100\n",
            "loss 3.477206701063551e-05 average time 0.03606759700001021 iter num 20\n",
            "loss 5.3469262638827786e-05 average time 0.02540734522499406 iter num 40\n",
            "loss 4.0689559682505205e-05 average time 0.021863748766706218 iter num 60\n",
            "loss 7.44920689612627e-05 average time 0.02015149101252973 iter num 80\n",
            "loss 4.826865915674716e-05 average time 0.019059403820037915 iter num 100\n",
            "loss 0.00021959545847494155 average time 0.036654653499999766 iter num 20\n",
            "loss 0.00010018973989645019 average time 0.02555833690005329 iter num 40\n",
            "loss 8.004057599464431e-05 average time 0.021890484550067413 iter num 60\n",
            "loss 1.877850081655197e-05 average time 0.020209592962589795 iter num 80\n",
            "loss 1.311441337747965e-05 average time 0.019246262690076038 iter num 100\n",
            "loss 8.594070823164657e-05 average time 0.03651907144999313 iter num 20\n",
            "loss 0.00011629282380454242 average time 0.02562244277503396 iter num 40\n",
            "loss 6.908139766892418e-05 average time 0.021946226966671627 iter num 60\n",
            "loss 1.3210556062404066e-05 average time 0.020101761975001863 iter num 80\n",
            "loss 1.4959075997467153e-05 average time 0.019157392619990787 iter num 100\n",
            "loss 5.48011958017014e-05 average time 0.0359192197000084 iter num 20\n",
            "loss 4.876687307842076e-05 average time 0.025186368424942884 iter num 40\n",
            "loss 5.103109288029373e-05 average time 0.02167602018330399 iter num 60\n",
            "loss 3.206573455827311e-05 average time 0.019928373812490463 iter num 80\n",
            "loss 1.7745946024660952e-05 average time 0.01889319573998364 iter num 100\n",
            "loss 2.579549436632078e-05 average time 0.037136008300058164 iter num 20\n",
            "loss 4.442859426490031e-05 average time 0.025970537450075427 iter num 40\n",
            "loss 2.787805351545103e-05 average time 0.02219195195004128 iter num 60\n",
            "loss 2.555267565185204e-05 average time 0.020327220800015765 iter num 80\n",
            "loss 3.8276884879451245e-06 average time 0.019224234540024553 iter num 100\n",
            "loss 1.1209648619114887e-05 average time 0.03656672785009505 iter num 20\n",
            "loss 1.3960394426248968e-05 average time 0.025532065625066026 iter num 40\n",
            "loss 5.785669418401085e-05 average time 0.022038983716735554 iter num 60\n",
            "loss 3.56224445567932e-05 average time 0.020215548500038948 iter num 80\n",
            "loss 2.7267326004221104e-05 average time 0.019151241600038702 iter num 100\n",
            "loss 3.20487524732016e-05 average time 0.036739311449946396 iter num 20\n",
            "loss 4.715660907095298e-05 average time 0.02580506959993727 iter num 40\n",
            "loss 0.0001783746265573427 average time 0.022056689016638605 iter num 60\n",
            "loss 1.796176002244465e-05 average time 0.020298389662491444 iter num 80\n",
            "loss 7.462358098564437e-06 average time 0.019326922979989832 iter num 100\n",
            "loss 3.903394826920703e-05 average time 0.03814918295015559 iter num 20\n",
            "loss 6.0389236750779673e-05 average time 0.02628950517505473 iter num 40\n",
            "loss 5.477605009218678e-05 average time 0.02249532818335259 iter num 60\n",
            "loss 4.281655128579587e-05 average time 0.02050500592501976 iter num 80\n",
            "loss 3.8647776818834245e-06 average time 0.019345807910021905 iter num 100\n",
            "loss 1.825637446017936e-05 average time 0.03648112305008908 iter num 20\n",
            "loss 0.0002577352279331535 average time 0.02567880500018873 iter num 40\n",
            "loss 5.695839354302734e-05 average time 0.022079170383464467 iter num 60\n",
            "loss 1.343468375125667e-05 average time 0.020333676250152165 iter num 80\n",
            "loss 3.374132575117983e-05 average time 0.01918126432011377 iter num 100\n",
            "loss 0.000346692802850157 average time 0.03664546720006001 iter num 20\n",
            "loss 0.0001508015557192266 average time 0.025513354775034712 iter num 40\n",
            "loss 5.068521568318829e-05 average time 0.021927804333396732 iter num 60\n",
            "loss 8.12063371995464e-05 average time 0.020056721412561273 iter num 80\n",
            "loss 2.293006400577724e-05 average time 0.01909338749004746 iter num 100\n",
            "loss 0.0005018403753638268 average time 0.03683391244994709 iter num 20\n",
            "loss 2.734072768362239e-05 average time 0.02575114209989806 iter num 40\n",
            "loss 1.0251647836412303e-05 average time 0.022017921316592037 iter num 60\n",
            "loss 2.9702743631787598e-05 average time 0.02035201528746029 iter num 80\n",
            "loss 0.00010060430213343352 average time 0.019297549669954606 iter num 100\n",
            "loss 0.00019210370373912156 average time 0.03555687270004455 iter num 20\n",
            "loss 0.00019733929366338998 average time 0.024991392550077762 iter num 40\n",
            "loss 6.16261240793392e-05 average time 0.021738457466729717 iter num 60\n",
            "loss 1.6200861864490435e-05 average time 0.019975725262554533 iter num 80\n",
            "loss 6.1874616221757606e-06 average time 0.018907130720026543 iter num 100\n",
            "loss 3.761858170037158e-05 average time 0.03724309289996199 iter num 20\n",
            "loss 4.708680717158131e-05 average time 0.025886041074977585 iter num 40\n",
            "loss 5.220852472120896e-06 average time 0.022188249599958 iter num 60\n",
            "loss 6.550159923790488e-06 average time 0.020447785512487826 iter num 80\n",
            "loss 5.63571984457667e-06 average time 0.019367905639974196 iter num 100\n",
            "loss 2.8574471798492596e-05 average time 0.03669682584991278 iter num 20\n",
            "loss 2.8301685233600438e-05 average time 0.025790128474977792 iter num 40\n",
            "loss 2.057719939330127e-05 average time 0.02208314144995711 iter num 60\n",
            "loss 2.8865957574453205e-05 average time 0.02030441277495356 iter num 80\n",
            "loss 1.5700969015597366e-05 average time 0.019215125909950073 iter num 100\n",
            "loss 3.4344015148235485e-05 average time 0.03692890030001763 iter num 20\n",
            "loss 9.003550803754479e-05 average time 0.025942737449963716 iter num 40\n",
            "loss 5.285109727992676e-05 average time 0.022403002283272144 iter num 60\n",
            "loss 1.2901723493996542e-05 average time 0.020487344062462397 iter num 80\n",
            "loss 2.0283719095459674e-06 average time 0.01932472563996271 iter num 100\n",
            "loss 3.5380275221541524e-05 average time 0.03651024615001006 iter num 20\n",
            "loss 6.06044995947741e-05 average time 0.025716647950025616 iter num 40\n",
            "loss 1.5488560165977105e-05 average time 0.022082686683355255 iter num 60\n",
            "loss 9.660627256380394e-06 average time 0.02023722577504259 iter num 80\n",
            "loss 8.679147867951542e-06 average time 0.019291414070057727 iter num 100\n",
            "loss 3.8034635508665815e-05 average time 0.037234356600038154 iter num 20\n",
            "loss 2.618991675262805e-05 average time 0.02610861722498612 iter num 40\n",
            "loss 1.0477122486918233e-05 average time 0.022491795883297527 iter num 60\n",
            "loss 2.306523674633354e-05 average time 0.02060636439997552 iter num 80\n",
            "loss 4.210478891764069e-06 average time 0.01958376139999018 iter num 100\n",
            "loss 8.515227818861604e-05 average time 0.03747820599996885 iter num 20\n",
            "loss 2.019675957853906e-05 average time 0.02610699237498011 iter num 40\n",
            "loss 2.579254942247644e-05 average time 0.022410410166685324 iter num 60\n",
            "loss 1.6839821910252795e-05 average time 0.020506066849998207 iter num 80\n",
            "loss 7.982166607689578e-06 average time 0.01962590263000493 iter num 100\n",
            "loss 8.856772183207795e-05 average time 0.037573966499985546 iter num 20\n",
            "loss 0.0006460681324824691 average time 0.026121180224981798 iter num 40\n",
            "loss 0.00043729832395911217 average time 0.022348069116666616 iter num 60\n",
            "loss 6.158900941954926e-05 average time 0.02043055607500719 iter num 80\n",
            "loss 3.402133734198287e-05 average time 0.019286407060017153 iter num 100\n",
            "loss 0.0002129117929143831 average time 0.0368108517499877 iter num 20\n",
            "loss 8.223616168834269e-05 average time 0.025762205424939566 iter num 40\n",
            "loss 4.6937409933889285e-05 average time 0.022184155766687277 iter num 60\n",
            "loss 1.7419244613847695e-05 average time 0.020328825100000357 iter num 80\n",
            "loss 1.0288469638908282e-05 average time 0.019147130009996544 iter num 100\n",
            "loss 0.00010315116378478706 average time 0.037413352599969585 iter num 20\n",
            "loss 3.3247237297473475e-05 average time 0.02627827637502378 iter num 40\n",
            "loss 2.5602459572837688e-05 average time 0.02233007918336322 iter num 60\n",
            "loss 9.996609151130542e-06 average time 0.020461182237499997 iter num 80\n",
            "loss 1.1828160495497286e-05 average time 0.019348684739979943 iter num 100\n",
            "loss 0.00010296269465470687 average time 0.03720349739996891 iter num 20\n",
            "loss 1.758385587891098e-05 average time 0.026009663899958468 iter num 40\n",
            "loss 8.479252755932976e-06 average time 0.022519071449930077 iter num 60\n",
            "loss 6.981166279729223e-06 average time 0.020518334612438592 iter num 80\n",
            "loss 6.460261829488445e-06 average time 0.019372528849935407 iter num 100\n",
            "loss 9.62380290729925e-05 average time 0.036414886799911984 iter num 20\n",
            "loss 7.308203203137964e-05 average time 0.025850699174998226 iter num 40\n",
            "loss 7.258557161549106e-05 average time 0.022383842966655722 iter num 60\n",
            "loss 2.9018601708230563e-05 average time 0.02056231387499565 iter num 80\n",
            "loss 8.947469723352697e-06 average time 0.019359543599966854 iter num 100\n",
            "loss 2.164129909942858e-05 average time 0.03618949369993061 iter num 20\n",
            "loss 7.906822429504246e-05 average time 0.02536484857494088 iter num 40\n",
            "loss 0.0003700831439346075 average time 0.021940427699958794 iter num 60\n",
            "loss 3.910624946001917e-05 average time 0.020174625987499438 iter num 80\n",
            "loss 1.2144249012635555e-05 average time 0.0191266518300381 iter num 100\n",
            "loss 2.836155726981815e-05 average time 0.03664934869998433 iter num 20\n",
            "loss 0.0002185660705436021 average time 0.025817108100045515 iter num 40\n",
            "loss 3.796639430220239e-05 average time 0.02219749461670896 iter num 60\n",
            "loss 1.911335129989311e-05 average time 0.02041503082506324 iter num 80\n",
            "loss 7.752680176054128e-06 average time 0.019320615500037094 iter num 100\n",
            "loss 0.00021114877017680556 average time 0.036653455650002796 iter num 20\n",
            "loss 1.3297841178427916e-05 average time 0.026182633900020846 iter num 40\n",
            "loss 9.320231583842542e-06 average time 0.022452272899984867 iter num 60\n",
            "loss 4.0119894038070925e-06 average time 0.020534574087514558 iter num 80\n",
            "loss 5.155031431058887e-06 average time 0.0193276981100189 iter num 100\n",
            "loss 0.00029934619669802487 average time 0.03698620079999273 iter num 20\n",
            "loss 0.0001830945984693244 average time 0.026335516799963442 iter num 40\n",
            "loss 4.131465539103374e-05 average time 0.02255567754993232 iter num 60\n",
            "loss 3.435262988205068e-05 average time 0.02059130794992825 iter num 80\n",
            "loss 5.134889579494484e-05 average time 0.019542896389948508 iter num 100\n",
            "loss 0.0004015513404738158 average time 0.03768393215009382 iter num 20\n",
            "loss 0.0015554383862763643 average time 0.026233292699976117 iter num 40\n",
            "loss 6.908322393428534e-05 average time 0.022828923816647754 iter num 60\n",
            "loss 7.03934274497442e-05 average time 0.021004711625005258 iter num 80\n",
            "loss 5.0571103201946244e-05 average time 0.01977278975000445 iter num 100\n",
            "loss 0.00047745995107106864 average time 0.03741144700011319 iter num 20\n",
            "loss 0.00010412612755317241 average time 0.026115221800046128 iter num 40\n",
            "loss 8.936744416132569e-05 average time 0.022233984033330975 iter num 60\n",
            "loss 5.468065137392841e-05 average time 0.02047098345000222 iter num 80\n",
            "loss 0.00013018568279221654 average time 0.01935548225000275 iter num 100\n",
            "loss 6.12316798651591e-05 average time 0.03690372630007914 iter num 20\n",
            "loss 4.8606172640575096e-05 average time 0.02594704262508003 iter num 40\n",
            "loss 6.0383117670426145e-06 average time 0.022264305266708105 iter num 60\n",
            "loss 2.009483432630077e-05 average time 0.020465089712570262 iter num 80\n",
            "loss 3.454795660218224e-05 average time 0.01926675389009688 iter num 100\n",
            "loss 9.810682240640745e-05 average time 0.036693635199981145 iter num 20\n",
            "loss 1.505676846136339e-05 average time 0.025715300724959887 iter num 40\n",
            "loss 3.569909677025862e-05 average time 0.022149792566642644 iter num 60\n",
            "loss 1.2863455594924744e-05 average time 0.02025688797497196 iter num 80\n",
            "loss 1.0103758540935814e-05 average time 0.019196867129994643 iter num 100\n",
            "loss 5.761676584370434e-05 average time 0.03627530455005399 iter num 20\n",
            "loss 3.211283910786733e-05 average time 0.025706132875006914 iter num 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-50f4c3cfa7be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0minitial_stocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m           \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# assume no correlation between stocks here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m           \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   3567\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected input type for array: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0;32m--> 461\u001b[0;31m                                        weak_type=new_weak_type)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbitcast_convert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    264\u001b[0m     top_trace = find_top_trace(\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36msafe_map\u001b[0;34m(f, *args)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'length mismatch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munzip2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2a5dd4-0096-4ed9-df42-17648796f992"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'new_european_1stock_v5.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26c8700-8d04-4321-9ae7-5e79f8aec089"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d857a07-45a6-48d7-8cc7-08a6ae1c8889"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'new_european_1stock_v5.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\"\n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2df3cb7-3da9-459c-a596-46325e7eb898"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "outputId": "1006bd9d-04d2-416a-e989-8dd0d38efb6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "#model_save_name = 'jax_european_1stock_test_3.pth'\n",
        "#path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "#torch.save(model.state_dict(), path)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.0001402621273882687 average time 0.02874945370003843 iter num 20\n",
            "loss 5.979851266602054e-05 average time 0.019476595300011466 iter num 40\n",
            "loss 3.312109402031638e-05 average time 0.016282705733298524 iter num 60\n",
            "loss 5.119659363117535e-06 average time 0.014755172399975435 iter num 80\n",
            "loss 3.220312464691233e-06 average time 0.013863244079966535 iter num 100\n",
            "loss 5.911111657042056e-06 average time 0.027290238099794806 iter num 20\n",
            "loss 8.125519343593623e-06 average time 0.01882265027484209 iter num 40\n",
            "loss 2.7664118533721194e-06 average time 0.016014115016575185 iter num 60\n",
            "loss 2.893381861213129e-06 average time 0.01470371402492674 iter num 80\n",
            "loss 8.098094212982687e-07 average time 0.013838503949937148 iter num 100\n",
            "loss 2.164614670618903e-05 average time 0.028089943949998995 iter num 20\n",
            "loss 1.9729999621631578e-05 average time 0.01910989425009575 iter num 40\n",
            "loss 5.231552222539904e-06 average time 0.01617689423343715 iter num 60\n",
            "loss 1.7772172213881277e-05 average time 0.01474088380007288 iter num 80\n",
            "loss 1.444145709683653e-05 average time 0.013840886990064973 iter num 100\n",
            "loss 2.557443622208666e-05 average time 0.02720679825001753 iter num 20\n",
            "loss 7.600889603054384e-06 average time 0.018664423949962837 iter num 40\n",
            "loss 7.610446118633263e-06 average time 0.015851948216686652 iter num 60\n",
            "loss 6.3013171711645555e-06 average time 0.014609440775006987 iter num 80\n",
            "loss 3.249850578868063e-06 average time 0.013828573660002803 iter num 100\n",
            "loss 9.461161425861064e-06 average time 0.028103328899851475 iter num 20\n",
            "loss 1.622096351638902e-05 average time 0.019120094949880696 iter num 40\n",
            "loss 9.234455319528934e-06 average time 0.01626999831660214 iter num 60\n",
            "loss 9.724956726131495e-06 average time 0.014776882174976436 iter num 80\n",
            "loss 9.027539817907382e-06 average time 0.013861783189959169 iter num 100\n",
            "loss 8.814368629828095e-05 average time 0.027164254299850654 iter num 20\n",
            "loss 7.00097662047483e-05 average time 0.01885142682492642 iter num 40\n",
            "loss 6.978810688451631e-06 average time 0.015999005016601585 iter num 60\n",
            "loss 6.818969268351793e-06 average time 0.014581954099924133 iter num 80\n",
            "loss 3.8298076106002554e-06 average time 0.013701033229945097 iter num 100\n",
            "loss 6.793411012040451e-05 average time 0.028287846300099773 iter num 20\n",
            "loss 3.236466000089422e-06 average time 0.019179011125038413 iter num 40\n",
            "loss 4.255253770679701e-06 average time 0.016160542233319575 iter num 60\n",
            "loss 8.233131666202098e-06 average time 0.014645951374996002 iter num 80\n",
            "loss 4.883933343080571e-06 average time 0.013796935829959693 iter num 100\n",
            "loss 6.804917939007282e-05 average time 0.027216159299814534 iter num 20\n",
            "loss 6.510093953693286e-06 average time 0.018782329549890165 iter num 40\n",
            "loss 2.501809831301216e-05 average time 0.01607121173324231 iter num 60\n",
            "loss 1.6487914763274603e-05 average time 0.014576453574909465 iter num 80\n",
            "loss 3.599802539611119e-06 average time 0.01373756586995114 iter num 100\n",
            "loss 2.317498183401767e-05 average time 0.028384962299924155 iter num 20\n",
            "loss 4.8128968046512455e-05 average time 0.019441213049935867 iter num 40\n",
            "loss 7.998018190846778e-06 average time 0.01630010074997396 iter num 60\n",
            "loss 5.09016535943374e-06 average time 0.014749476912481897 iter num 80\n",
            "loss 3.4960112316184677e-06 average time 0.01394367382998098 iter num 100\n",
            "loss 1.6681442502886057e-05 average time 0.028793271899985484 iter num 20\n",
            "loss 6.819374902988784e-06 average time 0.019423687999983487 iter num 40\n",
            "loss 4.926371912006289e-05 average time 0.01642121718329387 iter num 60\n",
            "loss 4.400073976285057e-06 average time 0.014816673199948126 iter num 80\n",
            "loss 6.031998054822907e-06 average time 0.013926304459937455 iter num 100\n",
            "loss 2.708461761358194e-05 average time 0.027598658550004984 iter num 20\n",
            "loss 3.035079498658888e-05 average time 0.019020305099979852 iter num 40\n",
            "loss 2.74752392215305e-06 average time 0.016047031483337075 iter num 60\n",
            "loss 4.089255071448861e-06 average time 0.01460783175001552 iter num 80\n",
            "loss 4.315352725825505e-06 average time 0.013715424040010476 iter num 100\n",
            "loss 0.000122213750728406 average time 0.027322931450135 iter num 20\n",
            "loss 3.890164225595072e-05 average time 0.01896222250009032 iter num 40\n",
            "loss 4.299523425288498e-06 average time 0.016015673933361542 iter num 60\n",
            "loss 1.4294797438196838e-05 average time 0.014641982700061363 iter num 80\n",
            "loss 4.86902536067646e-06 average time 0.013737861850040644 iter num 100\n",
            "loss 3.5942134672950488e-06 average time 0.027889813450110525 iter num 20\n",
            "loss 2.5954999728128314e-05 average time 0.019096253625070857 iter num 40\n",
            "loss 4.81677234347444e-05 average time 0.01621122143339259 iter num 60\n",
            "loss 3.991554422100307e-06 average time 0.01471188362503426 iter num 80\n",
            "loss 1.1665151760098524e-05 average time 0.013852060360050019 iter num 100\n",
            "loss 8.291701306006871e-06 average time 0.027798561199961113 iter num 20\n",
            "loss 9.805247827898711e-06 average time 0.01905919297496439 iter num 40\n",
            "loss 7.235389034576656e-07 average time 0.016131603983315774 iter num 60\n",
            "loss 2.4063460841716733e-06 average time 0.014690145237466368 iter num 80\n",
            "loss 4.782843916473212e-06 average time 0.013806863379950301 iter num 100\n",
            "loss 5.960018825135194e-07 average time 0.02875682760000018 iter num 20\n",
            "loss 1.0770621884148568e-05 average time 0.019617297624995443 iter num 40\n",
            "loss 1.436372599528113e-06 average time 0.01651095641664142 iter num 60\n",
            "loss 2.416854476905428e-05 average time 0.015139672412465189 iter num 80\n",
            "loss 4.77136245535803e-06 average time 0.014263251459960885 iter num 100\n",
            "loss 1.9077679098700173e-05 average time 0.028159036200031552 iter num 20\n",
            "loss 3.6518740671454e-05 average time 0.019334635475024696 iter num 40\n",
            "loss 3.113878392468905e-06 average time 0.016382671316690297 iter num 60\n",
            "loss 5.244540261628572e-06 average time 0.014942013787515408 iter num 80\n",
            "loss 2.7964254059043014e-06 average time 0.014035262180022982 iter num 100\n",
            "loss 1.807277658372186e-05 average time 0.027463781600044968 iter num 20\n",
            "loss 6.984999345149845e-06 average time 0.018861956050022853 iter num 40\n",
            "loss 6.286024017754244e-06 average time 0.016125918816669582 iter num 60\n",
            "loss 4.1342877921124455e-06 average time 0.014657061150012396 iter num 80\n",
            "loss 2.464946874169982e-06 average time 0.013779924000027676 iter num 100\n",
            "loss 8.124774467432871e-05 average time 0.02776290770007108 iter num 20\n",
            "loss 7.181679393397644e-05 average time 0.018921322500000316 iter num 40\n",
            "loss 1.9327289919601753e-06 average time 0.01613928084999922 iter num 60\n",
            "loss 1.5240527318383101e-05 average time 0.014793408724983692 iter num 80\n",
            "loss 4.907944003207376e-06 average time 0.01394935360999625 iter num 100\n",
            "loss 3.674848358059535e-06 average time 0.02843824884998867 iter num 20\n",
            "loss 0.00017105021106544882 average time 0.019512828725009968 iter num 40\n",
            "loss 7.253199146362022e-05 average time 0.016571309333388247 iter num 60\n",
            "loss 2.549662531237118e-05 average time 0.015085267050028505 iter num 80\n",
            "loss 4.887257546215551e-06 average time 0.014158840460022476 iter num 100\n",
            "loss 0.00037167646223679185 average time 0.02813510905007206 iter num 20\n",
            "loss 0.0001108733777073212 average time 0.019200812175040483 iter num 40\n",
            "loss 1.0395451681688428e-05 average time 0.0162432736833883 iter num 60\n",
            "loss 3.217742005290347e-06 average time 0.014796651775100145 iter num 80\n",
            "loss 3.959004061471205e-06 average time 0.013911898890091834 iter num 100\n",
            "loss 3.5789995308732614e-05 average time 0.027440893499988305 iter num 20\n",
            "loss 9.904565558827017e-06 average time 0.019361165600025744 iter num 40\n",
            "loss 4.685986368713202e-06 average time 0.01638439308330817 iter num 60\n",
            "loss 8.906791890694876e-07 average time 0.014961941087460674 iter num 80\n",
            "loss 2.1783138436148874e-05 average time 0.014036138199962807 iter num 100\n",
            "loss 5.152401718078181e-05 average time 0.027544591200057766 iter num 20\n",
            "loss 3.441130320425145e-05 average time 0.01890120357502383 iter num 40\n",
            "loss 2.261551344417967e-06 average time 0.016055551850013217 iter num 60\n",
            "loss 1.0958369784930255e-05 average time 0.014677964775012242 iter num 80\n",
            "loss 1.8796577933244407e-05 average time 0.013832052240022676 iter num 100\n",
            "loss 0.0001051837534760125 average time 0.027574518599931253 iter num 20\n",
            "loss 4.208384780213237e-05 average time 0.01898807172499346 iter num 40\n",
            "loss 4.638234713638667e-06 average time 0.016068066916674676 iter num 60\n",
            "loss 1.7863323591882363e-05 average time 0.014655618612516718 iter num 80\n",
            "loss 3.3191142847499577e-06 average time 0.013815646440025375 iter num 100\n",
            "loss 6.5799149524536915e-06 average time 0.02856017694998627 iter num 20\n",
            "loss 4.987249667465221e-06 average time 0.01962784022498454 iter num 40\n",
            "loss 3.2324910534953233e-06 average time 0.01659945568327809 iter num 60\n",
            "loss 4.742104465549346e-06 average time 0.015196040687476398 iter num 80\n",
            "loss 1.9011374661204172e-06 average time 0.014188306329979241 iter num 100\n",
            "loss 1.1290083421044983e-05 average time 0.027916538399904312 iter num 20\n",
            "loss 1.2085860362276435e-05 average time 0.01927479967491763 iter num 40\n",
            "loss 1.9875504222000018e-05 average time 0.016271729949949076 iter num 60\n",
            "loss 3.0093564419075847e-05 average time 0.01481293482494266 iter num 80\n",
            "loss 2.3205143406812567e-06 average time 0.013921743119972234 iter num 100\n",
            "loss 0.0002461537951603532 average time 0.0286172295500819 iter num 20\n",
            "loss 2.234475323348306e-05 average time 0.019775618825019593 iter num 40\n",
            "loss 5.189398507354781e-05 average time 0.016642323166721932 iter num 60\n",
            "loss 2.7322564619680634e-06 average time 0.015064194662579666 iter num 80\n",
            "loss 5.126726591697661e-06 average time 0.014253650470091089 iter num 100\n",
            "loss 3.21326501762087e-06 average time 0.029300743199883073 iter num 20\n",
            "loss 1.5213733604468871e-05 average time 0.01976975557499827 iter num 40\n",
            "loss 1.7771639249986038e-05 average time 0.016629668599974442 iter num 60\n",
            "loss 6.912871867825743e-06 average time 0.015155097762476543 iter num 80\n",
            "loss 2.7668485927279107e-05 average time 0.014183555879972118 iter num 100\n",
            "loss 2.1723628378822468e-05 average time 0.029017878449803903 iter num 20\n",
            "loss 3.767046564462362e-06 average time 0.019820976499863717 iter num 40\n",
            "loss 1.98765201275819e-06 average time 0.01664762723324505 iter num 60\n",
            "loss 2.1339935756259365e-06 average time 0.0150689615124179 iter num 80\n",
            "loss 2.8053782443748787e-05 average time 0.014175271749945751 iter num 100\n",
            "loss 2.0311558728280943e-06 average time 0.029139503849864922 iter num 20\n",
            "loss 5.55269753022003e-06 average time 0.019774996649925925 iter num 40\n",
            "loss 2.028618382610148e-06 average time 0.016732110649930593 iter num 60\n",
            "loss 4.292448011256056e-06 average time 0.015219462049958565 iter num 80\n",
            "loss 4.113869181310292e-06 average time 0.014325086299950272 iter num 100\n",
            "loss 5.116445208841469e-06 average time 0.028042054800062034 iter num 20\n",
            "loss 2.8560682039824314e-05 average time 0.01921032517502681 iter num 40\n",
            "loss 1.03440361272078e-05 average time 0.01655342065006759 iter num 60\n",
            "loss 4.312753844715189e-06 average time 0.015021269575072438 iter num 80\n",
            "loss 1.8514599560148781e-06 average time 0.014086074980077683 iter num 100\n",
            "loss 9.335627510154154e-06 average time 0.02842271219997201 iter num 20\n",
            "loss 4.762273874803213e-06 average time 0.019430844024964245 iter num 40\n",
            "loss 4.6877303248038515e-06 average time 0.016412761233292866 iter num 60\n",
            "loss 2.679717908904422e-06 average time 0.014931577049981114 iter num 80\n",
            "loss 3.3230630833713803e-06 average time 0.014053392990026623 iter num 100\n",
            "loss 9.127637895289809e-06 average time 0.028897423050102587 iter num 20\n",
            "loss 1.344113661616575e-05 average time 0.019550103224969462 iter num 40\n",
            "loss 7.5399416346044745e-06 average time 0.01642929138332268 iter num 60\n",
            "loss 2.3678505385760218e-05 average time 0.015037875950019952 iter num 80\n",
            "loss 6.183518053148873e-07 average time 0.014138218600028268 iter num 100\n",
            "loss 1.2790319487976376e-05 average time 0.02877449185007208 iter num 20\n",
            "loss 1.3018034223932773e-05 average time 0.019448144925036104 iter num 40\n",
            "loss 1.1665493730106391e-05 average time 0.016565359516668345 iter num 60\n",
            "loss 6.568169737874996e-06 average time 0.014982888399993044 iter num 80\n",
            "loss 1.7820479115471244e-05 average time 0.014062746119952863 iter num 100\n",
            "loss 8.196539056370966e-06 average time 0.027682207450106945 iter num 20\n",
            "loss 7.3655819505802356e-06 average time 0.019045049575083794 iter num 40\n",
            "loss 6.0493108321679756e-05 average time 0.016158923900062898 iter num 60\n",
            "loss 2.1285251932567917e-05 average time 0.01465647391253242 iter num 80\n",
            "loss 3.146860990455025e-06 average time 0.013821784870033297 iter num 100\n",
            "loss 7.96534186520148e-06 average time 0.027930046149867847 iter num 20\n",
            "loss 2.1753503460786305e-06 average time 0.019198167174954504 iter num 40\n",
            "loss 5.873720965610119e-06 average time 0.01621910461661476 iter num 60\n",
            "loss 3.35961090058845e-06 average time 0.014834379524972973 iter num 80\n",
            "loss 4.303138211980695e-06 average time 0.01401521259997935 iter num 100\n",
            "loss 3.967279553762637e-05 average time 0.027828874999977417 iter num 20\n",
            "loss 1.1336949683027342e-05 average time 0.01898895072497453 iter num 40\n",
            "loss 4.609831648849649e-06 average time 0.016252603116708998 iter num 60\n",
            "loss 9.236756341124419e-06 average time 0.014850359525041767 iter num 80\n",
            "loss 2.1163041310501285e-06 average time 0.014019931390039346 iter num 100\n",
            "loss 0.00027800106909126043 average time 0.02902962169996499 iter num 20\n",
            "loss 3.598277908167802e-05 average time 0.019778025675009303 iter num 40\n",
            "loss 2.1532690880121663e-05 average time 0.016819665066683833 iter num 60\n",
            "loss 1.4244669728213921e-06 average time 0.015150462462497672 iter num 80\n",
            "loss 1.4422787444345886e-06 average time 0.014185593510010221 iter num 100\n",
            "loss 1.871566564659588e-05 average time 0.02762266875010937 iter num 20\n",
            "loss 4.398343662614934e-06 average time 0.019000239225078985 iter num 40\n",
            "loss 2.7284218958811834e-05 average time 0.016109157083428728 iter num 60\n",
            "loss 4.441237251739949e-06 average time 0.014696062662596886 iter num 80\n",
            "loss 2.0971137928427197e-06 average time 0.013797769700086065 iter num 100\n",
            "loss 1.5247692317643669e-05 average time 0.027714321299981748 iter num 20\n",
            "loss 1.3134490473021287e-05 average time 0.018958730724966698 iter num 40\n",
            "loss 1.912100924528204e-05 average time 0.01612132686662638 iter num 60\n",
            "loss 1.6865045836311765e-05 average time 0.014696397662476102 iter num 80\n",
            "loss 1.2274539358259062e-06 average time 0.013807519179999872 iter num 100\n",
            "loss 2.3532260456704535e-05 average time 0.027909544500016637 iter num 20\n",
            "loss 2.0995479644625448e-05 average time 0.019128496750067826 iter num 40\n",
            "loss 8.847979188431054e-06 average time 0.016142866316674068 iter num 60\n",
            "loss 5.079956281406339e-06 average time 0.014737065150006856 iter num 80\n",
            "loss 4.939285190630471e-06 average time 0.013862517760007903 iter num 100\n",
            "loss 9.76299725152785e-06 average time 0.028411681949819467 iter num 20\n",
            "loss 9.375775334774517e-06 average time 0.019444912949893478 iter num 40\n",
            "loss 3.1392514756589662e-06 average time 0.016302445966539382 iter num 60\n",
            "loss 3.150370503135491e-06 average time 0.014788018799890779 iter num 80\n",
            "loss 5.287668500386644e-06 average time 0.013881833299892605 iter num 100\n",
            "loss 4.4824239012086764e-05 average time 0.028244629999926475 iter num 20\n",
            "loss 1.373532086290652e-05 average time 0.01915298209996763 iter num 40\n",
            "loss 5.1339455239940435e-05 average time 0.016235379483350697 iter num 60\n",
            "loss 3.185409696015995e-06 average time 0.014747272437512038 iter num 80\n",
            "loss 1.538455398986116e-05 average time 0.01393470218001312 iter num 100\n",
            "loss 1.3194482562539633e-05 average time 0.027708499699974708 iter num 20\n",
            "loss 7.615172216901556e-05 average time 0.01909042507504637 iter num 40\n",
            "loss 2.5499279217910953e-05 average time 0.016374585350073783 iter num 60\n",
            "loss 1.4254619600251317e-05 average time 0.014913034050050556 iter num 80\n",
            "loss 1.4945745533623267e-05 average time 0.01407346525003959 iter num 100\n",
            "loss 0.00016647226584609598 average time 0.027792710950097898 iter num 20\n",
            "loss 0.00010923092486336827 average time 0.01918287702503676 iter num 40\n",
            "loss 2.4738455977058038e-05 average time 0.016293461550033802 iter num 60\n",
            "loss 3.8229212805163115e-05 average time 0.014927145962531085 iter num 80\n",
            "loss 5.416234216681914e-06 average time 0.014058668560037403 iter num 100\n",
            "loss 0.0014529479667544365 average time 0.028807963549888883 iter num 20\n",
            "loss 0.0004785935452673584 average time 0.019835427149928364 iter num 40\n",
            "loss 4.4946827983949333e-05 average time 0.016710977583291726 iter num 60\n",
            "loss 1.9767183403018862e-05 average time 0.015149679399939942 iter num 80\n",
            "loss 6.192996806930751e-06 average time 0.014151949489960316 iter num 100\n",
            "loss 2.661827784322668e-05 average time 0.028612538450079227 iter num 20\n",
            "loss 4.862435162067413e-06 average time 0.01960372167516198 iter num 40\n",
            "loss 1.4758022189198527e-05 average time 0.016459997500138948 iter num 60\n",
            "loss 4.285037903173361e-06 average time 0.01497278350010447 iter num 80\n",
            "loss 2.1194564396864735e-05 average time 0.01411106687008214 iter num 100\n",
            "loss 1.3148874131729826e-05 average time 0.029119918400010648 iter num 20\n",
            "loss 3.029817207789165e-06 average time 0.019959238349929364 iter num 40\n",
            "loss 6.1033101701468695e-06 average time 0.016763393616641527 iter num 60\n",
            "loss 1.0706800821935758e-05 average time 0.01512461087495467 iter num 80\n",
            "loss 5.900810720049776e-06 average time 0.014270567629973812 iter num 100\n",
            "loss 4.892102879239246e-05 average time 0.02855727935016148 iter num 20\n",
            "loss 1.3879485777579248e-05 average time 0.01948438970011921 iter num 40\n",
            "loss 5.220198545430321e-06 average time 0.016421896366758424 iter num 60\n",
            "loss 7.049673058645567e-06 average time 0.014856005850106158 iter num 80\n",
            "loss 2.1447572180477437e-06 average time 0.013930276670080274 iter num 100\n",
            "loss 3.976892912760377e-05 average time 0.029544073150054827 iter num 20\n",
            "loss 3.8234080420807004e-05 average time 0.01987731690005603 iter num 40\n",
            "loss 1.198798963741865e-05 average time 0.01670214635003807 iter num 60\n",
            "loss 2.0845129711233312e-06 average time 0.015173025887509083 iter num 80\n",
            "loss 6.291055797191802e-06 average time 0.014249465800039616 iter num 100\n",
            "loss 1.1136081411677878e-05 average time 0.027262969200091903 iter num 20\n",
            "loss 4.723181973531609e-06 average time 0.018715413250015446 iter num 40\n",
            "loss 9.827571375353727e-06 average time 0.015977759033315428 iter num 60\n",
            "loss 6.190419298945926e-06 average time 0.014731286999983694 iter num 80\n",
            "loss 1.4345486079037073e-06 average time 0.013819569439974658 iter num 100\n",
            "loss 3.9495344026363455e-06 average time 0.027981541700137315 iter num 20\n",
            "loss 9.087139915209264e-06 average time 0.019284659824984375 iter num 40\n",
            "loss 3.261837991885841e-05 average time 0.016305919266657535 iter num 60\n",
            "loss 1.2623095244634897e-06 average time 0.01490391791244292 iter num 80\n",
            "loss 4.416947376739699e-06 average time 0.014020314659974246 iter num 100\n",
            "loss 5.756067821494071e-06 average time 0.027678616850016623 iter num 20\n",
            "loss 2.6130295736948028e-05 average time 0.019168755225018687 iter num 40\n",
            "loss 4.64702388853766e-05 average time 0.016329356466712852 iter num 60\n",
            "loss 1.047533532982925e-05 average time 0.014984224237525722 iter num 80\n",
            "loss 1.3516641956812236e-05 average time 0.014059800360018925 iter num 100\n",
            "loss 5.937499736319296e-06 average time 0.027628515649894325 iter num 20\n",
            "loss 4.831624664802803e-06 average time 0.01896039134990133 iter num 40\n",
            "loss 1.69738686963683e-05 average time 0.01609899249994366 iter num 60\n",
            "loss 7.706797077844385e-06 average time 0.014678322799909438 iter num 80\n",
            "loss 3.5917551031161565e-06 average time 0.013957182299918713 iter num 100\n",
            "loss 6.642904918408021e-05 average time 0.029040206149829828 iter num 20\n",
            "loss 2.5139868739643134e-05 average time 0.019610290999889913 iter num 40\n",
            "loss 7.498754712287337e-06 average time 0.016484649683267585 iter num 60\n",
            "loss 4.1969260564656e-06 average time 0.01490647231244111 iter num 80\n",
            "loss 2.0122163277846994e-06 average time 0.013966602939963196 iter num 100\n",
            "loss 1.891215651994571e-05 average time 0.027755801900229925 iter num 20\n",
            "loss 3.51736298398464e-06 average time 0.01925913647512516 iter num 40\n",
            "loss 6.44745477984543e-06 average time 0.01631507363346524 iter num 60\n",
            "loss 6.3938678067643195e-06 average time 0.014842881275103536 iter num 80\n",
            "loss 3.6287813145463588e-06 average time 0.01394498959007251 iter num 100\n",
            "loss 3.60942940460518e-05 average time 0.028495883599907755 iter num 20\n",
            "loss 9.56305484578479e-06 average time 0.019334819374921607 iter num 40\n",
            "loss 6.125716936367098e-06 average time 0.016329480799913653 iter num 60\n",
            "loss 6.040773769200314e-06 average time 0.01482445487492896 iter num 80\n",
            "loss 3.551298959791893e-06 average time 0.013892134089883258 iter num 100\n",
            "loss 1.1486505172797479e-05 average time 0.028305588750072275 iter num 20\n",
            "loss 4.041729698656127e-05 average time 0.01962362777508133 iter num 40\n",
            "loss 1.9027582311537117e-05 average time 0.016579590966694015 iter num 60\n",
            "loss 2.9259745133458637e-06 average time 0.014948812287548208 iter num 80\n",
            "loss 4.929246642859653e-06 average time 0.01409724607003227 iter num 100\n",
            "loss 1.6020121620385908e-05 average time 0.028183788149954127 iter num 20\n",
            "loss 4.937005087413127e-06 average time 0.019259202549983458 iter num 40\n",
            "loss 7.40964514989173e-06 average time 0.016230329216674968 iter num 60\n",
            "loss 2.193514092141413e-06 average time 0.014798074887471558 iter num 80\n",
            "loss 3.912471129297046e-06 average time 0.013873344809962874 iter num 100\n",
            "loss 1.8154642020817846e-05 average time 0.02792612999987796 iter num 20\n",
            "loss 1.6664265785948373e-05 average time 0.019029755024916994 iter num 40\n",
            "loss 2.85208670902648e-06 average time 0.016133773366588382 iter num 60\n",
            "loss 3.2288539841829333e-06 average time 0.014654668037485408 iter num 80\n",
            "loss 1.651365778343461e-06 average time 0.013803779829986524 iter num 100\n",
            "loss 6.4337127696489915e-06 average time 0.02811810185007744 iter num 20\n",
            "loss 5.583642632700503e-06 average time 0.01927708987493588 iter num 40\n",
            "loss 7.590331279061502e-07 average time 0.016324944833271123 iter num 60\n",
            "loss 5.97544749325607e-06 average time 0.01479096103747679 iter num 80\n",
            "loss 1.8422113043925492e-06 average time 0.013863156269981119 iter num 100\n",
            "loss 1.7596569250599714e-06 average time 0.028483543349921092 iter num 20\n",
            "loss 1.2563838026835583e-05 average time 0.01944523062495591 iter num 40\n",
            "loss 6.2116640037857e-05 average time 0.016408203200004814 iter num 60\n",
            "loss 2.3744714781059884e-06 average time 0.014904004587504006 iter num 80\n",
            "loss 6.0412876337068155e-06 average time 0.013998172170031466 iter num 100\n",
            "loss 1.5969719243003055e-05 average time 0.02848015590002433 iter num 20\n",
            "loss 6.702708105876809e-06 average time 0.01944691597493602 iter num 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2af3d09a12b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#model_save_name = 'jax_european_1stock_test_3.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0minitial_stocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m           \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# assume no correlation between stocks here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m           \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   3567\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected input type for array: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0;32m--> 461\u001b[0;31m                                        weak_type=new_weak_type)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbitcast_convert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    266\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0945d34b-3561-4ffb-88fd-b8e773ccb36a"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 0.1, 0.8, 0.25, 0.05, 0.05]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.098688, 0.627409)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.7073]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0040], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2AXrPt7bNj",
        "outputId": "208cc4ad-4e7c-4138-ebbe-f82b8109352b"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([0.1]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.8\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "[0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-S_49NF3LjU"
      },
      "source": [
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.05*0.05\n",
        "T = 1.0\n",
        "K = 0.8\n",
        "\n",
        "Price_ChangeS = np.array([])\n",
        "Delta_ChangeS = np.array([])\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "for i in range(0,100,1):\n",
        "  ii = i/100\n",
        "  inputs = torch.tensor([[1, ii, 0.8, 0.25, 0.05, 0.05]]).cuda()\n",
        "  Price_ChangeS = np.append(Price_ChangeS,model(inputs.float()).item())\n",
        "  inputs.requires_grad = True\n",
        "  x = model(inputs.float())\n",
        "  x.backward()\n",
        "  first_order_gradient = inputs.grad\n",
        "  Delta_ChangeS = np.append(Delta_ChangeS,first_order_gradient[0][[2]].item())"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HJS6FYT6oB_"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([0.8]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.8\n",
        "\n",
        "Price_ChangeS_test = np.array([])\n",
        "Delta_ChangeS_test = np.array([])\n",
        "for i in range(0,100,1):\n",
        "  ii = i/100\n",
        "  initial_stocks = jnp.array([ii]*numstocks)\n",
        "  Price_ChangeS_test = np.append(Price_ChangeS_test,optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "  Delta_ChangeS_test = np.append(Delta_ChangeS_test,goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9yjYzRl7TfH",
        "outputId": "918ec290-613c-4dba-e725-c6e7893d81dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.array(range(0, 100, 1))/100,(Price_ChangeS- Price_ChangeS_test),'red')\n",
        "plt.title('Difference between Model Price and Test Price When S0 Is Changing(K = 0.8) ')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Difference between Model Price and Test Price When S0 Is Changing(K = 0.8) ')"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "lwApH0GT9bBK",
        "outputId": "0717d43f-bd46-43cf-d1a4-dedb48b11538"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.5, S, 0.25, 0.05, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(0, 1, 0.01)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcab9265650>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPT31tpTOibD"
      },
      "source": [
        "# import pylab\n",
        "# import numpy as np\n",
        "# prices = np.arange(0, 200, 1)\n",
        "# deltas = []\n",
        "# for p in prices:\n",
        "#     inputs = torch.tensor([[1, 110.0, p, 0.25, 0.05, 0.05]]).cuda() # T, K, S, sigma, mu, r\n",
        "#     deltas.append(model(inputs.float())[0][1])\n",
        "# fig = pylab.plot(prices, deltas)\n",
        "# pylab.xlabel('prices')\n",
        "# pylab.ylabel('Delta')\n",
        "# fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UaG5KUBaaRE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}