{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid Test_Knock Out Call 1stock SGD",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/Grid_Test_Knock_Out_Call_1stock_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYigDkiy0HU9",
        "outputId": "e0c3b860-737f-466a-875b-80441b966805"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 8)\n",
        "K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "B_range = jnp.linspace(0.5, 1.0, 5)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "\n",
        "print(S_range)\n",
        "print(K_range)\n",
        "print(B_range)\n",
        "print(sigma_range)\n",
        "print(r_range)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75       0.82142854 0.89285713 0.9642857  1.0357143  1.1071429\n",
            " 1.1785713  1.25      ]\n",
            "[0.75  0.875 1.    1.125 1.25 ]\n",
            "[0.5   0.625 0.75  0.875 1.   ]\n",
            "[0.15       0.25       0.35000002 0.45      ]\n",
            "[0.01  0.025 0.04 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQxpJqK6OZr"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "\n",
        "goptionvalueavg = jax.grad(optionvalueavg, argnums=1)\n",
        "\n",
        "#################################################################### Adjust all parameters here (not inside class)\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 8)\n",
        "K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "B_range = jnp.linspace(0.5, 1.0, 5)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "T = 1.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "####################################################################\n",
        "\n",
        "call = []\n",
        "count = 0\n",
        "\n",
        "for S in S_range:\n",
        "  for K in K_range:\n",
        "    for B in B_range:\n",
        "      for r in r_range:\n",
        "        for sigma in sigma_range:    \n",
        "\n",
        "          initial_stocks = jnp.array([S]*numstocks) # must be float\n",
        "          r_tmp = jnp.array([r]*numstocks)\n",
        "          drift = r_tmp\n",
        "          cov = jnp.identity(numstocks)*sigma*sigma\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "          Deltas = goptionvalueavg(keys, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "          call.append([T, K, B, S, sigma, r, r, European_Call_price] + list(Deltas)) #T, K, B, S, sigma, mu, r, price, delta\n",
        "          \n",
        "          count += 1\n",
        "          print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "e_OUtP8GUwj5",
        "outputId": "9c73750d-5f32-428c-9697-23ecc7102976"
      },
      "source": [
        "Thedataset = pd.DataFrame(call)\n",
        "Thedataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.04829822</td>\n",
              "      <td>0.555157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.07776406</td>\n",
              "      <td>0.563845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.10653644</td>\n",
              "      <td>0.572229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.13306102</td>\n",
              "      <td>0.569618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.053997554</td>\n",
              "      <td>0.594301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2395</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.19972013</td>\n",
              "      <td>0.471407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.10002679</td>\n",
              "      <td>0.631672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2397</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.1442415</td>\n",
              "      <td>0.583131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2398</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.17972831</td>\n",
              "      <td>0.529840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2399</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.20697877</td>\n",
              "      <td>0.482881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2400 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6            7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010   0.04829822  0.555157\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010   0.07776406  0.563845\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010   0.10653644  0.572229\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010   0.13306102  0.569618\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.053997554  0.594301\n",
              "...   ...   ...  ...   ...   ...    ...    ...          ...       ...\n",
              "2395  1.0  1.25  1.0  1.25  0.45  0.025  0.025   0.19972013  0.471407\n",
              "2396  1.0  1.25  1.0  1.25  0.15  0.040  0.040   0.10002679  0.631672\n",
              "2397  1.0  1.25  1.0  1.25  0.25  0.040  0.040    0.1442415  0.583131\n",
              "2398  1.0  1.25  1.0  1.25  0.35  0.040  0.040   0.17972831  0.529840\n",
              "2399  1.0  1.25  1.0  1.25  0.45  0.040  0.040   0.20697877  0.482881\n",
              "\n",
              "[2400 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQKnflf6peX"
      },
      "source": [
        "# save to csv\n",
        "Thedataset.to_csv('Knock_Out_Call_1stock_MC_Datset_v2.csv', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "skGWSSsG8TGG",
        "outputId": "6c6ed0f7-64ef-4372-a611-e5e078cf11fd"
      },
      "source": [
        "# read csv\n",
        "import pandas as pd\n",
        "\n",
        "Thedataset = pd.read_csv('Knock_Out_Call_1stock_MC_Datset_v2.csv', header=None)\n",
        "Thedataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.048298</td>\n",
              "      <td>0.555157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.077764</td>\n",
              "      <td>0.563845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.106536</td>\n",
              "      <td>0.572229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.133061</td>\n",
              "      <td>0.569618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.053998</td>\n",
              "      <td>0.594301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2395</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.199720</td>\n",
              "      <td>0.471407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.100027</td>\n",
              "      <td>0.631672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2397</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.144241</td>\n",
              "      <td>0.583131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2398</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.179728</td>\n",
              "      <td>0.529840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2399</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.206979</td>\n",
              "      <td>0.482881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2400 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6         7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010  0.048298  0.555157\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010  0.077764  0.563845\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010  0.106536  0.572229\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010  0.133061  0.569618\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.053998  0.594301\n",
              "...   ...   ...  ...   ...   ...    ...    ...       ...       ...\n",
              "2395  1.0  1.25  1.0  1.25  0.45  0.025  0.025  0.199720  0.471407\n",
              "2396  1.0  1.25  1.0  1.25  0.15  0.040  0.040  0.100027  0.631672\n",
              "2397  1.0  1.25  1.0  1.25  0.25  0.040  0.040  0.144241  0.583131\n",
              "2398  1.0  1.25  1.0  1.25  0.35  0.040  0.040  0.179728  0.529840\n",
              "2399  1.0  1.25  1.0  1.25  0.45  0.040  0.040  0.206979  0.482881\n",
              "\n",
              "[2400 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4HV-G44th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3e58b1-b067-4328-c08e-31f75aa92c82"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch\n",
        "torch.set_printoptions(precision=6)\n",
        "\n",
        "Thedataset_X = Thedataset.iloc[:,:7]\n",
        "Thedataset_Y = Thedataset.iloc[:,7:]\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.X = cupy.array(Thedataset_X)\n",
        "        self.Y = cupy.array(Thedataset_Y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(self.X.toDlpack()), from_dlpack(self.Y.toDlpack()))\n",
        "\n",
        "# print\n",
        "ds = OptionDataSet(max_len = 1)\n",
        "for i in ds:\n",
        "    print(i[0])\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    print(i[1].shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.000000, 0.750000, 0.500000,  ..., 0.150000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.500000,  ..., 0.250000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.500000,  ..., 0.350000, 0.010000, 0.010000],\n",
            "        ...,\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.250000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.350000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.450000, 0.040000, 0.040000]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([2400, 7])\n",
            "tensor([[0.048298, 0.555157],\n",
            "        [0.077764, 0.563845],\n",
            "        [0.106536, 0.572229],\n",
            "        ...,\n",
            "        [0.144241, 0.583131],\n",
            "        [0.179728, 0.529840],\n",
            "        [0.206979, 0.482881]], device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([2400, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "ea738fa3-d52b-4d2c-9217-e47071f1efe2"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(7*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.5, 0.3, 0.03, 0.03]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, B, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.5, 0.75, 0.15, 0.01, 0.01]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "0082a6b9-4962-4d03-c21d-a760331962cf"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeLVZiiaDS4y",
        "outputId": "3c5ea6ba-8807-4d4f-c04e-44385915e519"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CyULkENYKb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fafe228e-55cb-452f-ba34-4981ab2295ac"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss# \n",
        "from torch.optim import Adam\n",
        "from torch.optim import SGD\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-4, amsgrad=True) # try using higher epsilon and amsgrad\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]  # Now index 3 is stock price, not 2\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-3, 1e-5, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_SGD_1e-2_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.17419900978239464 average time 3.584076278850006 iter num 20\n",
            "loss 0.1666985554389366 average time 3.552174953975009 iter num 40\n",
            "loss 0.16359440106188006 average time 3.528785781816674 iter num 60\n",
            "loss 0.1625857111565841 average time 3.5346153084125063 iter num 80\n",
            "loss 0.16240348043166072 average time 3.532051259270005 iter num 100\n",
            "loss 0.15969248797344204 average time 3.567377091300011 iter num 20\n",
            "loss 0.15822661317077236 average time 3.543070307525011 iter num 40\n",
            "loss 0.15758187558438308 average time 3.52497428378335 iter num 60\n",
            "loss 0.157347314424101 average time 3.509990758575003 iter num 80\n",
            "loss 0.15730239935274545 average time 3.502047694989992 iter num 100\n",
            "loss 0.1565510988551274 average time 3.4565863827999808 iter num 20\n",
            "loss 0.15601899513581538 average time 3.456526519549976 iter num 40\n",
            "loss 0.155721126643848 average time 3.482646350049974 iter num 60\n",
            "loss 0.15559680106206714 average time 3.4792829837999735 iter num 80\n",
            "loss 0.1555716461502202 average time 3.477596175789977 iter num 100\n",
            "loss 0.15511662925336736 average time 3.432242883150013 iter num 20\n",
            "loss 0.15474273718166204 average time 3.4395394738750156 iter num 40\n",
            "loss 0.1545098617052706 average time 3.441957455983345 iter num 60\n",
            "loss 0.15440742147612882 average time 3.44852839375001 iter num 80\n",
            "loss 0.15438627207842484 average time 3.46548823259001 iter num 100\n",
            "loss 0.15399494492638738 average time 3.52637736585001 iter num 20\n",
            "loss 0.15365927314365296 average time 3.5101952577000075 iter num 40\n",
            "loss 0.15344365973807536 average time 3.4985753492333553 iter num 60\n",
            "loss 0.15334736277345926 average time 3.4955012372750303 iter num 80\n",
            "loss 0.15332735991741922 average time 3.486872883160024 iter num 100\n",
            "loss 0.15295512079788187 average time 3.4840780981000306 iter num 20\n",
            "loss 0.15263198047243484 average time 3.5039958239250155 iter num 40\n",
            "loss 0.1524225233111689 average time 3.5052155555666635 iter num 60\n",
            "loss 0.1523285403590713 average time 3.5003768772124886 iter num 80\n",
            "loss 0.1523089884300179 average time 3.4988067946399815 iter num 100\n",
            "loss 0.15194437765913904 average time 3.4233324919499637 iter num 20\n",
            "loss 0.1516266524405709 average time 3.4314837712249413 iter num 40\n",
            "loss 0.151420080850212 average time 3.454421281533261 iter num 60\n",
            "loss 0.15132725336140787 average time 3.4565418344749332 iter num 80\n",
            "loss 0.15130792177406066 average time 3.45048170259995 iter num 100\n",
            "loss 0.15094729306169521 average time 3.5024568689000035 iter num 20\n",
            "loss 0.15063258302536178 average time 3.480213079374971 iter num 40\n",
            "loss 0.15042773130246367 average time 3.4715985002666607 iter num 60\n",
            "loss 0.15033561359324907 average time 3.458888040424989 iter num 80\n",
            "loss 0.15031642768411202 average time 3.4586761317799937 iter num 100\n",
            "loss 0.1499583498160026 average time 3.4027647032000687 iter num 20\n",
            "loss 0.14964563299847627 average time 3.4069507889500414 iter num 40\n",
            "loss 0.14944195076067612 average time 3.4250291623000293 iter num 60\n",
            "loss 0.14935032274857957 average time 3.425874793487503 iter num 80\n",
            "loss 0.14933123614399516 average time 3.427838925400006 iter num 100\n",
            "loss 0.1489749309956002 average time 3.422388638649932 iter num 20\n",
            "loss 0.14866360665076583 average time 3.407938182549992 iter num 40\n",
            "loss 0.14846074850139748 average time 3.383651521666669 iter num 60\n",
            "loss 0.14836946522132088 average time 3.366566630012494 iter num 80\n",
            "loss 0.14835044566381803 average time 3.36198202226999 iter num 100\n",
            "loss 0.14799537719312175 average time 3.4356542147000253 iter num 20\n",
            "loss 0.14768500002407303 average time 3.4249927968999145 iter num 40\n",
            "loss 0.1474826838906917 average time 3.443678239383204 iter num 60\n",
            "loss 0.14739162667900516 average time 3.452589445362423 iter num 80\n",
            "loss 0.1473726533594939 average time 3.46202472503991 iter num 100\n",
            "loss 0.14701834139322983 average time 3.516081530200154 iter num 20\n",
            "loss 0.14670850669653662 average time 3.5194865770001345 iter num 40\n",
            "loss 0.1465064697307879 average time 3.518576476450092 iter num 60\n",
            "loss 0.1464155185776752 average time 3.518643781962544 iter num 80\n",
            "loss 0.14639656630938414 average time 3.5199179779100542 iter num 100\n",
            "loss 0.14604257884432936 average time 3.5279866182000204 iter num 20\n",
            "loss 0.1457329096347656 average time 3.5312637084000245 iter num 40\n",
            "loss 0.14553091537781437 average time 3.533737305883369 iter num 60\n",
            "loss 0.14543996694205694 average time 3.5375879737750098 iter num 80\n",
            "loss 0.14542101689621398 average time 3.536059928169998 iter num 100\n",
            "loss 0.14506694043641755 average time 3.5231757034998736 iter num 20\n",
            "loss 0.14475708296904513 average time 3.530070470999931 iter num 40\n",
            "loss 0.1445549047896548 average time 3.539070131383293 iter num 60\n",
            "loss 0.1444638550807224 average time 3.5354242233124635 iter num 80\n",
            "loss 0.14444488283767337 average time 3.5408654845099683 iter num 100\n",
            "loss 0.14409031637900066 average time 3.536095937150003 iter num 20\n",
            "loss 0.14377992381859703 average time 3.5324360587250565 iter num 40\n",
            "loss 0.1435773347479818 average time 3.5379649159167155 iter num 60\n",
            "loss 0.14348608333546678 average time 3.5286534578625264 iter num 80\n",
            "loss 0.14346706546334617 average time 3.5270825534900268 iter num 100\n",
            "loss 0.14311162469851435 average time 3.497391642750017 iter num 20\n",
            "loss 0.14280035947279832 average time 3.5124304047999884 iter num 40\n",
            "loss 0.14259714156447573 average time 3.5159967138166244 iter num 60\n",
            "loss 0.14250558836106342 average time 3.5224826868749575 iter num 80\n",
            "loss 0.1424865060074206 average time 3.5226304735599934 iter num 100\n",
            "loss 0.14212980934761163 average time 3.5350422954499665 iter num 20\n",
            "loss 0.14181734579724312 average time 3.532370643199988 iter num 40\n",
            "loss 0.1416132880791455 average time 3.529987518683341 iter num 60\n",
            "loss 0.1415213408265025 average time 3.528124025625016 iter num 80\n",
            "loss 0.14150217396776119 average time 3.5304068702300446 iter num 100\n",
            "loss 0.14114386065027337 average time 3.536806765649999 iter num 20\n",
            "loss 0.14082988053173284 average time 3.5440946278250296 iter num 40\n",
            "loss 0.14062477573899435 average time 3.5401564081666947 iter num 60\n",
            "loss 0.14053233975019014 average time 3.549246077187513 iter num 80\n",
            "loss 0.14051307317431375 average time 3.5450922241900207 iter num 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-18267e339e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_SGD_1e-2_1.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-18267e339e67>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-18267e339e67>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-18267e339e67>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_fEzULvKwR-"
      },
      "source": [
        "# 20:20\n",
        "# 22:40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3be0cc-7ec7-4b6f-f8b8-dc8109dcbf5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_SGD_1e-2_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7035cec5-0587-4292-cccc-1657dfd31fb2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47230422-6ed8-4e76-da1a-2ffffb32bc53"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_SGD_1e-2_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52912b5-1da1-46f6-8b87-a5a7a1c3e909"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=7, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a36357f-d341-4ad1-affd-20c15da14c96"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from torch.optim import SGD\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-4, amsgrad=True) # try using higher epsilon and amsgrad\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-3, 1e-5, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 500)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_SGD_1e-2_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.060278516932293864 average time 3.30628243914999 iter num 20\n",
            "loss 0.06027141543008477 average time 3.306097114274992 iter num 40\n",
            "loss 0.06026653683391036 average time 3.3091642636166614 iter num 60\n",
            "loss 0.06026432366571164 average time 3.307167822274998 iter num 80\n",
            "loss 0.060263881718924474 average time 3.304448888759999 iter num 100\n",
            "loss 0.0602552692010551 average time 3.311431064049998 iter num 20\n",
            "loss 0.060247742448841526 average time 3.3015198560249983 iter num 40\n",
            "loss 0.06024283423820358 average time 3.315256403783337 iter num 60\n",
            "loss 0.060240618264509 average time 3.3071586271124986 iter num 80\n",
            "loss 0.06024017725604241 average time 3.303182865939998 iter num 100\n",
            "loss 0.060231574605887704 average time 3.2840323078499862 iter num 20\n",
            "loss 0.060224048840815766 average time 3.2777483475000024 iter num 40\n",
            "loss 0.060219141479936684 average time 3.28071526155 iter num 60\n",
            "loss 0.060216926249580686 average time 3.2797437770125 iter num 80\n",
            "loss 0.060216483057684254 average time 3.278006304769996 iter num 100\n",
            "loss 0.060207899696938114 average time 3.284711259000028 iter num 20\n",
            "loss 0.06020038743006868 average time 3.2831321027250113 iter num 40\n",
            "loss 0.060195481530313795 average time 3.2856282825000034 iter num 60\n",
            "loss 0.060193263703469814 average time 3.284218471137501 iter num 80\n",
            "loss 0.060192819866112746 average time 3.281199815570003 iter num 100\n",
            "loss 0.060184262047243746 average time 3.2812781698999856 iter num 20\n",
            "loss 0.06017675757279486 average time 3.2884666693750093 iter num 40\n",
            "loss 0.06017186349379314 average time 3.2847574075333417 iter num 60\n",
            "loss 0.06016965305866469 average time 3.2838444193375067 iter num 80\n",
            "loss 0.060169206587716566 average time 3.28436575617 iter num 100\n",
            "loss 0.06016066063244019 average time 3.287637188649967 iter num 20\n",
            "loss 0.06015316055782694 average time 3.2908897191249764 iter num 40\n",
            "loss 0.06014827285492305 average time 3.2884496082999664 iter num 60\n",
            "loss 0.06014606600853008 average time 3.284437561074955 iter num 80\n",
            "loss 0.06014561997114625 average time 3.285523334549964 iter num 100\n",
            "loss 0.060137064881456086 average time 3.2751470189999736 iter num 20\n",
            "loss 0.060129579486807486 average time 3.286224080049999 iter num 40\n",
            "loss 0.060124699261799176 average time 3.2877834024833494 iter num 60\n",
            "loss 0.06012249182804759 average time 3.288548616987498 iter num 80\n",
            "loss 0.060122044919150144 average time 3.287540236499999 iter num 100\n",
            "loss 0.06011351171970442 average time 3.296191837499987 iter num 20\n",
            "loss 0.06010603808097476 average time 3.296089507949989 iter num 40\n",
            "loss 0.0601011674610647 average time 3.2907530728333465 iter num 60\n",
            "loss 0.06009896751628788 average time 3.2873868101750077 iter num 80\n",
            "loss 0.06009852049093176 average time 3.2945899030000008 iter num 100\n",
            "loss 0.06008998972834994 average time 3.279910936100009 iter num 20\n",
            "loss 0.06008252849217359 average time 3.270099838825024 iter num 40\n",
            "loss 0.06007766817863667 average time 3.2728425026166934 iter num 60\n",
            "loss 0.06007546664076455 average time 3.274469398062519 iter num 80\n",
            "loss 0.060075024877698245 average time 3.276358660640012 iter num 100\n",
            "loss 0.060066519477468834 average time 3.2919607122499657 iter num 20\n",
            "loss 0.0600590681599899 average time 3.285507519775001 iter num 40\n",
            "loss 0.06005421431332339 average time 3.2827747114500045 iter num 60\n",
            "loss 0.06005202848771738 average time 3.2864693155499993 iter num 80\n",
            "loss 0.060051588994885705 average time 3.2858491680399995 iter num 100\n",
            "loss 0.06004308971046139 average time 3.2922141941999827 iter num 20\n",
            "loss 0.06003565840750141 average time 3.2829330247999677 iter num 40\n",
            "loss 0.0600308187177827 average time 3.2771766409499756 iter num 60\n",
            "loss 0.060028630626769705 average time 3.279217017649978 iter num 80\n",
            "loss 0.06002819003691566 average time 3.2788660581199793 iter num 100\n",
            "loss 0.06001971374323983 average time 3.271794647500019 iter num 20\n",
            "loss 0.060012293895164256 average time 3.2744669762000287 iter num 40\n",
            "loss 0.06000745816566357 average time 3.274313051600037 iter num 60\n",
            "loss 0.06000527368690681 average time 3.274691764287536 iter num 80\n",
            "loss 0.0600048356570464 average time 3.278871844850028 iter num 100\n",
            "loss 0.05999638353346186 average time 3.270353562250102 iter num 20\n",
            "loss 0.05998898813916465 average time 3.272159839525057 iter num 40\n",
            "loss 0.05998416999952072 average time 3.2734906823333403 iter num 60\n",
            "loss 0.05998199106352872 average time 3.2863906791875026 iter num 80\n",
            "loss 0.05998154894170576 average time 3.2945715304199985 iter num 100\n",
            "loss 0.05997312473019313 average time 3.3105716800499065 iter num 20\n",
            "loss 0.05996575818902073 average time 3.3147136883249004 iter num 40\n",
            "loss 0.05996095173824588 average time 3.3046245901498876 iter num 60\n",
            "loss 0.05995877901072711 average time 3.306110547912385 iter num 80\n",
            "loss 0.059958339389829335 average time 3.3036109450699316 iter num 100\n",
            "loss 0.05994994170774593 average time 3.288226941100129 iter num 20\n",
            "loss 0.05994259159956038 average time 3.2917501433001233 iter num 40\n",
            "loss 0.05993779878616231 average time 3.2929701200333965 iter num 60\n",
            "loss 0.05993563374631411 average time 3.2972956385500085 iter num 80\n",
            "loss 0.059935196520934014 average time 3.297377859559983 iter num 100\n",
            "loss 0.059926813444061824 average time 3.28840174765005 iter num 20\n",
            "loss 0.059919480021111096 average time 3.2930153203749795 iter num 40\n",
            "loss 0.05991469352684024 average time 3.2934856813666555 iter num 60\n",
            "loss 0.059912532624413435 average time 3.2971174092999718 iter num 80\n",
            "loss 0.059912097060967395 average time 3.29679342346999 iter num 100\n",
            "loss 0.059903720892952095 average time 3.2933642406001127 iter num 20\n",
            "loss 0.05989639842186008 average time 3.2872855872750053 iter num 40\n",
            "loss 0.059891630382590086 average time 3.2861717840833533 iter num 60\n",
            "loss 0.05988947285317808 average time 3.292822774175022 iter num 80\n",
            "loss 0.05988903905423273 average time 3.2947739260000346 iter num 100\n",
            "loss 0.059880676172525286 average time 3.309412615700103 iter num 20\n",
            "loss 0.05987336952671302 average time 3.3119790574750825 iter num 40\n",
            "loss 0.0598686129666693 average time 3.310973241716723 iter num 60\n",
            "loss 0.059866463653403784 average time 3.3116479489875927 iter num 80\n",
            "loss 0.059866030305451276 average time 3.309004673020072 iter num 100\n",
            "loss 0.05985768222069749 average time 3.3329408305998185 iter num 20\n",
            "loss 0.05985038553897775 average time 3.3226129370749278 iter num 40\n",
            "loss 0.059845643961337076 average time 3.3337605720333006 iter num 60\n",
            "loss 0.0598434996630903 average time 3.322346061637461 iter num 80\n",
            "loss 0.05984306988805572 average time 3.31605705068001 iter num 100\n",
            "loss 0.05983473955153619 average time 3.2972269317001066 iter num 20\n",
            "loss 0.05982746137797828 average time 3.294586540525006 iter num 40\n",
            "loss 0.05982273802227745 average time 3.30258883856665 iter num 60\n",
            "loss 0.059820598645229886 average time 3.301035570824979 iter num 80\n",
            "loss 0.05982017225827368 average time 3.3002135303699744 iter num 100\n",
            "loss 0.059811866486753434 average time 3.300456606850048 iter num 20\n",
            "loss 0.059804617444111 average time 3.3080623710500277 iter num 40\n",
            "loss 0.05979988766270249 average time 3.3057267185833856 iter num 60\n",
            "loss 0.059797755687898635 average time 3.3058665735500314 iter num 80\n",
            "loss 0.05979733193786063 average time 3.305725262110036 iter num 100\n",
            "loss 0.059789042989886104 average time 3.3103535466500036 iter num 20\n",
            "loss 0.05978181183598183 average time 3.3174549850750052 iter num 40\n",
            "loss 0.059777102818835935 average time 3.312152149283338 iter num 60\n",
            "loss 0.059774977965662716 average time 3.31112882893749 iter num 80\n",
            "loss 0.0597745556728045 average time 3.3111406761299893 iter num 100\n",
            "loss 0.05976628758332971 average time 3.3045133402999 iter num 20\n",
            "loss 0.059759089905291156 average time 3.30757435997491 iter num 40\n",
            "loss 0.05975439464408294 average time 3.306918976549893 iter num 60\n",
            "loss 0.05975227514665662 average time 3.302945716099907 iter num 80\n",
            "loss 0.05975185103198512 average time 3.3031527925399313 iter num 100\n",
            "loss 0.059743621168170565 average time 3.3163000633000137 iter num 20\n",
            "loss 0.059736435457417525 average time 3.326058506299978 iter num 40\n",
            "loss 0.059731755037165865 average time 3.3209633990666285 iter num 60\n",
            "loss 0.05972964012340891 average time 3.3250170346999655 iter num 80\n",
            "loss 0.05972921797791671 average time 3.3247621467599675 iter num 100\n",
            "loss 0.05972100760299117 average time 3.3051272373499616 iter num 20\n",
            "loss 0.059713846244898276 average time 3.3034308408499555 iter num 40\n",
            "loss 0.05970917885096259 average time 3.302710422716594 iter num 60\n",
            "loss 0.059707072120612555 average time 3.3016580178625077 iter num 80\n",
            "loss 0.059706651661005875 average time 3.3021740799500092 iter num 100\n",
            "loss 0.059698477127305125 average time 3.304301243200007 iter num 20\n",
            "loss 0.05969134784854509 average time 3.3040981682000163 iter num 40\n",
            "loss 0.05968668621093077 average time 3.3021730364333357 iter num 60\n",
            "loss 0.05968458684990888 average time 3.3024031556625686 iter num 80\n",
            "loss 0.05968416774298249 average time 3.3046872609600357 iter num 100\n",
            "loss 0.0596760259614076 average time 3.309923864799566 iter num 20\n",
            "loss 0.059668911294573905 average time 3.305295969149893 iter num 40\n",
            "loss 0.05966425581959909 average time 3.3023023117165695 iter num 60\n",
            "loss 0.059662170252760195 average time 3.3007465734000108 iter num 80\n",
            "loss 0.059661745852956964 average time 3.3019978123199873 iter num 100\n",
            "loss 0.05965363480072493 average time 3.304517282049983 iter num 20\n",
            "loss 0.05964654459212796 average time 3.303465823200031 iter num 40\n",
            "loss 0.05964191052853459 average time 3.3038923749667446 iter num 60\n",
            "loss 0.05963982584065394 average time 3.3012934701375345 iter num 80\n",
            "loss 0.05963941159004896 average time 3.30551712879007 iter num 100\n",
            "loss 0.05963132206068532 average time 3.2955526777995146 iter num 20\n",
            "loss 0.05962425286570308 average time 3.29244348779971 iter num 40\n",
            "loss 0.05961963897509801 average time 3.299055584483146 iter num 60\n",
            "loss 0.05961756054316839 average time 3.307674449974866 iter num 80\n",
            "loss 0.05961714930160843 average time 3.314055685309886 iter num 100\n",
            "loss 0.05960908606964713 average time 3.318113295999956 iter num 20\n",
            "loss 0.059602038266373863 average time 3.303792590749981 iter num 40\n",
            "loss 0.059597431551772335 average time 3.297525358749984 iter num 60\n",
            "loss 0.05959536030782887 average time 3.302319812399946 iter num 80\n",
            "loss 0.05959494930800721 average time 3.299800206359923 iter num 100\n",
            "loss 0.05958690804046555 average time 3.296179023050172 iter num 20\n",
            "loss 0.05957988224339106 average time 3.294169813875078 iter num 40\n",
            "loss 0.059575290665282825 average time 3.294654154550123 iter num 60\n",
            "loss 0.05957323452379944 average time 3.300587564112493 iter num 80\n",
            "loss 0.059572815605664825 average time 3.299481385010058 iter num 100\n",
            "loss 0.059564801538102126 average time 3.2992099735998637 iter num 20\n",
            "loss 0.05955779637881947 average time 3.299953653299872 iter num 40\n",
            "loss 0.059553215593770865 average time 3.2998736119998893 iter num 60\n",
            "loss 0.059551161595493944 average time 3.3057935405249284 iter num 80\n",
            "loss 0.05955074151267751 average time 3.3024013818599633 iter num 100\n",
            "loss 0.059542751381683925 average time 3.304458889149919 iter num 20\n",
            "loss 0.05953576403410624 average time 3.302863524550048 iter num 40\n",
            "loss 0.05953120269298436 average time 3.3058130053334933 iter num 60\n",
            "loss 0.059529155135293714 average time 3.305486498650134 iter num 80\n",
            "loss 0.05952874572487427 average time 3.305646092440038 iter num 100\n",
            "loss 0.05952077520498483 average time 3.305288161449698 iter num 20\n",
            "loss 0.05951380654302375 average time 3.307080687799953 iter num 40\n",
            "loss 0.05950925861893497 average time 3.30981958721668 iter num 60\n",
            "loss 0.0595072197732361 average time 3.309919123475015 iter num 80\n",
            "loss 0.05950680545250603 average time 3.3179468895999706 iter num 100\n",
            "loss 0.059498852121987564 average time 3.3214506150999115 iter num 20\n",
            "loss 0.059491910637269364 average time 3.3371368688498477 iter num 40\n",
            "loss 0.059487378572970616 average time 3.328543170183184 iter num 60\n",
            "loss 0.05948533690706437 average time 3.3213761870498275 iter num 80\n",
            "loss 0.059484925528318175 average time 3.3170537202898958 iter num 100\n",
            "loss 0.05947700758420135 average time 3.2930407096000636 iter num 20\n",
            "loss 0.05947008565474156 average time 3.2983678431750834 iter num 40\n",
            "loss 0.05946557306797359 average time 3.2973116826501 iter num 60\n",
            "loss 0.05946353807479927 average time 3.298532568050109 iter num 80\n",
            "loss 0.05946312517424348 average time 3.296754164190079 iter num 100\n",
            "loss 0.05945522435507618 average time 3.288817316049972 iter num 20\n",
            "loss 0.05944832203192297 average time 3.3058350505999443 iter num 40\n",
            "loss 0.059443822497027766 average time 3.299476418066691 iter num 60\n",
            "loss 0.05944179081418016 average time 3.2993970530624663 iter num 80\n",
            "loss 0.05944138233994728 average time 3.297212955080031 iter num 100\n",
            "loss 0.0594335058066212 average time 3.2904155715502386 iter num 20\n",
            "loss 0.05942661611270602 average time 3.299717756275231 iter num 40\n",
            "loss 0.05942212833800296 average time 3.2977749826001856 iter num 60\n",
            "loss 0.0594201054582671 average time 3.2978651533877157 iter num 80\n",
            "loss 0.0594196972067891 average time 3.297505733720154 iter num 100\n",
            "loss 0.05941183389837312 average time 3.2986814388999846 iter num 20\n",
            "loss 0.05940497292627877 average time 3.2982273232249555 iter num 40\n",
            "loss 0.05940049911940901 average time 3.301526800016624 iter num 60\n",
            "loss 0.05939848350825117 average time 3.3010967314374966 iter num 80\n",
            "loss 0.05939807814765281 average time 3.300031938619977 iter num 100\n",
            "loss 0.05939025657804991 average time 3.343082799049989 iter num 20\n",
            "loss 0.059383407298688635 average time 3.3338581965499996 iter num 40\n",
            "loss 0.059378950659677104 average time 3.334695244449934 iter num 60\n",
            "loss 0.05937694003501856 average time 3.3296041322374323 iter num 80\n",
            "loss 0.059376538990655696 average time 3.3234218735899232 iter num 100\n",
            "loss 0.05936873546774082 average time 3.310055883449786 iter num 20\n",
            "loss 0.05936190506835375 average time 3.30570751199989 iter num 40\n",
            "loss 0.05935745769737903 average time 3.302744983750017 iter num 60\n",
            "loss 0.05935546086717931 average time 3.3041956251750206 iter num 80\n",
            "loss 0.059355058102056386 average time 3.301762201149977 iter num 100\n",
            "loss 0.05934728380740266 average time 3.3134621975501433 iter num 20\n",
            "loss 0.05934047409523091 average time 3.311232287699977 iter num 40\n",
            "loss 0.05933604442017111 average time 3.307476380666594 iter num 60\n",
            "loss 0.0593340469661342 average time 3.3040206096874725 iter num 80\n",
            "loss 0.05933364486140823 average time 3.30694252051002 iter num 100\n",
            "loss 0.05932588896135181 average time 3.296538065650293 iter num 20\n",
            "loss 0.0593190942265855 average time 3.2963673933250446 iter num 40\n",
            "loss 0.05931467881994021 average time 3.295853592750033 iter num 60\n",
            "loss 0.05931268436324029 average time 3.296894892737555 iter num 80\n",
            "loss 0.059312287434716705 average time 3.305041145550076 iter num 100\n",
            "loss 0.059304548772054946 average time 3.301519874050155 iter num 20\n",
            "loss 0.05929776992256887 average time 3.2995311079749627 iter num 40\n",
            "loss 0.05929336207020133 average time 3.298595086966725 iter num 60\n",
            "loss 0.059291370521601036 average time 3.2969040411250488 iter num 80\n",
            "loss 0.059290973202280016 average time 3.3010954127900005 iter num 100\n",
            "loss 0.05928325973756783 average time 3.305567848300052 iter num 20\n",
            "loss 0.05927650193163958 average time 3.314068621800061 iter num 40\n",
            "loss 0.05927210395217862 average time 3.316668517566692 iter num 60\n",
            "loss 0.05927012321628586 average time 3.3181054421625733 iter num 80\n",
            "loss 0.05926972391966477 average time 3.3223866290400657 iter num 100\n",
            "loss 0.05926202554551176 average time 3.2827828399498684 iter num 20\n",
            "loss 0.05925529211269843 average time 3.288746799674982 iter num 40\n",
            "loss 0.05925090225815105 average time 3.291503435083359 iter num 60\n",
            "loss 0.059248926149902535 average time 3.296441034737586 iter num 80\n",
            "loss 0.05924852867277157 average time 3.2989132631301072 iter num 100\n",
            "loss 0.05924084557356928 average time 3.298836200599999 iter num 20\n",
            "loss 0.05923412973169987 average time 3.2983815445250912 iter num 40\n",
            "loss 0.059229753553408634 average time 3.2999323207999924 iter num 60\n",
            "loss 0.05922778703865791 average time 3.304608062737498 iter num 80\n",
            "loss 0.0592273861911861 average time 3.3050804108400187 iter num 100\n",
            "loss 0.05921972146068254 average time 3.2965449760999945 iter num 20\n",
            "loss 0.05921302626209458 average time 3.2920578036749704 iter num 40\n",
            "loss 0.059208661454039364 average time 3.2977217762999924 iter num 60\n",
            "loss 0.059206694148498454 average time 3.3010767474500198 iter num 80\n",
            "loss 0.05920630076915911 average time 3.299787873590012 iter num 100\n",
            "loss 0.059198662337161784 average time 3.30484891465012 iter num 20\n",
            "loss 0.059191993404060096 average time 3.294266907675092 iter num 40\n",
            "loss 0.05918764551781325 average time 3.2941177172999536 iter num 60\n",
            "loss 0.05918569009145211 average time 3.2946743750749645 iter num 80\n",
            "loss 0.05918529485605014 average time 3.296237954469907 iter num 100\n",
            "loss 0.059177685641964364 average time 3.3002935344502475 iter num 20\n",
            "loss 0.05917103161235816 average time 3.2945680620502573 iter num 40\n",
            "loss 0.05916669322637438 average time 3.2994908301834585 iter num 60\n",
            "loss 0.05916474800261065 average time 3.3105439026251133 iter num 80\n",
            "loss 0.05916435260054396 average time 3.3136976148801476 iter num 100\n",
            "loss 0.05915676750742232 average time 3.3522177084494613 iter num 20\n",
            "loss 0.05915013160186478 average time 3.3278951277250597 iter num 40\n",
            "loss 0.059145808411463156 average time 3.316178213383258 iter num 60\n",
            "loss 0.05914386159438628 average time 3.310773094150045 iter num 80\n",
            "loss 0.05914346875481111 average time 3.307731145459984 iter num 100\n",
            "loss 0.059135918495969414 average time 3.3055130790000478 iter num 20\n",
            "loss 0.059129300431430644 average time 3.3090179714751686 iter num 40\n",
            "loss 0.059124990671978024 average time 3.3043167056668 iter num 60\n",
            "loss 0.05912304795648351 average time 3.3055667520625773 iter num 80\n",
            "loss 0.059122655003729825 average time 3.3010988257901044 iter num 100\n",
            "loss 0.05911512668344119 average time 3.2974226447004185 iter num 20\n",
            "loss 0.05910852363450089 average time 3.3028590497503503 iter num 40\n",
            "loss 0.05910422367651589 average time 3.2966832531002486 iter num 60\n",
            "loss 0.05910228421479094 average time 3.292668112662659 iter num 80\n",
            "loss 0.05910189370103296 average time 3.2930205064401887 iter num 100\n",
            "loss 0.0590943833163369 average time 3.2933307918998254 iter num 20\n",
            "loss 0.05908779772011418 average time 3.2956120279247445 iter num 40\n",
            "loss 0.059083510546053986 average time 3.29089859293296 iter num 60\n",
            "loss 0.059081578619625055 average time 3.292188068712403 iter num 80\n",
            "loss 0.05908119424308293 average time 3.2916872839899587 iter num 100\n",
            "loss 0.059073702622968634 average time 3.279836922000686 iter num 20\n",
            "loss 0.05906713879892933 average time 3.290086941325535 iter num 40\n",
            "loss 0.05906286763313174 average time 3.289509531133626 iter num 60\n",
            "loss 0.059060939075487394 average time 3.287071627175146 iter num 80\n",
            "loss 0.05906055866619178 average time 3.296647735300212 iter num 100\n",
            "loss 0.05905308293307544 average time 3.318799389349988 iter num 20\n",
            "loss 0.05904653821974623 average time 3.320058005199735 iter num 40\n",
            "loss 0.059042276388594696 average time 3.3162316629831667 iter num 60\n",
            "loss 0.05904035596636707 average time 3.3101047436623956 iter num 80\n",
            "loss 0.05903997312025807 average time 3.308746658899945 iter num 100\n",
            "loss 0.059032513364047474 average time 3.3065316747999534 iter num 20\n",
            "loss 0.05902598442928384 average time 3.2981077191751864 iter num 40\n",
            "loss 0.05902173076128705 average time 3.2972541039000136 iter num 60\n",
            "loss 0.05901982070048736 average time 3.2980190569749994 iter num 80\n",
            "loss 0.059019437223761316 average time 3.302684491819964 iter num 100\n",
            "loss 0.05901199837063954 average time 3.302581761050533 iter num 20\n",
            "loss 0.05900549376636355 average time 3.306272106800134 iter num 40\n",
            "loss 0.059001252971165596 average time 3.30332858401689 iter num 60\n",
            "loss 0.05899934404691601 average time 3.300140424550091 iter num 80\n",
            "loss 0.058998957959932125 average time 3.3029843436000372 iter num 100\n",
            "loss 0.05899154790756799 average time 3.304787684499206 iter num 20\n",
            "loss 0.05898505905411657 average time 3.308023878024869 iter num 40\n",
            "loss 0.05898082298344004 average time 3.3105946134164697 iter num 60\n",
            "loss 0.05897892267190924 average time 3.311000312237502 iter num 80\n",
            "loss 0.05897853649711684 average time 3.312708120210009 iter num 100\n",
            "loss 0.05897114946252568 average time 3.3112014890497448 iter num 20\n",
            "loss 0.058964681463304525 average time 3.3050242937247276 iter num 40\n",
            "loss 0.05896045829886293 average time 3.3059913391664546 iter num 60\n",
            "loss 0.05895856151509378 average time 3.3065106534123516 iter num 80\n",
            "loss 0.05895818054625718 average time 3.30601499625991 iter num 100\n",
            "loss 0.058950807883210055 average time 3.313820965499508 iter num 20\n",
            "loss 0.058944360692882274 average time 3.3167078548247444 iter num 40\n",
            "loss 0.05894014956365491 average time 3.319048678166291 iter num 60\n",
            "loss 0.058938258681371196 average time 3.3220097381371487 iter num 80\n",
            "loss 0.05893787281712079 average time 3.315077625799786 iter num 100\n",
            "loss 0.058930509352949904 average time 3.299177697849518 iter num 20\n",
            "loss 0.05892408118113526 average time 3.2952917128498482 iter num 40\n",
            "loss 0.05891988596750718 average time 3.296691873449769 iter num 60\n",
            "loss 0.05891799601081104 average time 3.298983796087259 iter num 80\n",
            "loss 0.058917617783475665 average time 3.298014764519794 iter num 100\n",
            "loss 0.05891027666278701 average time 3.2944355285500935 iter num 20\n",
            "loss 0.058903863860622054 average time 3.300145570375298 iter num 40\n",
            "loss 0.05889967748734722 average time 3.3005648424334746 iter num 60\n",
            "loss 0.058897796061728354 average time 3.303257602837584 iter num 80\n",
            "loss 0.058897419276308066 average time 3.3020586548500432 iter num 100\n",
            "loss 0.05889009795686786 average time 3.2959917845000746 iter num 20\n",
            "loss 0.05888370940776679 average time 3.298975754949788 iter num 40\n",
            "loss 0.058879537240320544 average time 3.3040039176998106 iter num 60\n",
            "loss 0.05887767060522414 average time 3.30176695622481 iter num 80\n",
            "loss 0.05887729299430922 average time 3.299397106519973 iter num 100\n",
            "loss 0.05886999026708166 average time 3.3115428160999727 iter num 20\n",
            "loss 0.058863618093693504 average time 3.3069473146251767 iter num 40\n",
            "loss 0.05885945540947983 average time 3.309106039833508 iter num 60\n",
            "loss 0.058857584733949524 average time 3.3081910443500417 iter num 80\n",
            "loss 0.058857210186935106 average time 3.305606674710034 iter num 100\n",
            "loss 0.05884992455553766 average time 3.291946303549776 iter num 20\n",
            "loss 0.05884356428460886 average time 3.2961901152249995 iter num 40\n",
            "loss 0.05883941579367394 average time 3.312623309149785 iter num 60\n",
            "loss 0.058837547904511234 average time 3.316246317374862 iter num 80\n",
            "loss 0.05883717726826321 average time 3.318276610019857 iter num 100\n",
            "loss 0.05882991341324302 average time 3.2897818844998254 iter num 20\n",
            "loss 0.058823577217220376 average time 3.3008957612251835 iter num 40\n",
            "loss 0.05881943512321996 average time 3.3005283711502975 iter num 60\n",
            "loss 0.05881756899491443 average time 3.3019296832252167 iter num 80\n",
            "loss 0.058817201463820246 average time 3.302241318250126 iter num 100\n",
            "loss 0.05880995314896431 average time 3.300930900250205 iter num 20\n",
            "loss 0.05880362553779021 average time 3.3156936299500557 iter num 40\n",
            "loss 0.05879949830022274 average time 3.3121886116168753 iter num 60\n",
            "loss 0.058797642977894984 average time 3.3080777238627888 iter num 80\n",
            "loss 0.058797271516444385 average time 3.3088116264001655 iter num 100\n",
            "loss 0.05879005040500718 average time 3.300969399600035 iter num 20\n",
            "loss 0.058783732881854615 average time 3.3106459125501715 iter num 40\n",
            "loss 0.05877961257019803 average time 3.302115034000114 iter num 60\n",
            "loss 0.05877775789366149 average time 3.300691994237559 iter num 80\n",
            "loss 0.058777385098728464 average time 3.300656567720034 iter num 100\n",
            "loss 0.05877018864548236 average time 3.3159860929503338 iter num 20\n",
            "loss 0.05876388346951353 average time 3.307641929525471 iter num 40\n",
            "loss 0.058759782551831057 average time 3.305130296916953 iter num 60\n",
            "loss 0.05875793570654269 average time 3.30177795657537 iter num 80\n",
            "loss 0.05875756475486607 average time 3.302044026620206 iter num 100\n",
            "loss 0.058750382245851517 average time 3.3116523219505325 iter num 20\n",
            "loss 0.058744094451186425 average time 3.3026121103755033 iter num 40\n",
            "loss 0.058739993891271325 average time 3.3026214212835234 iter num 60\n",
            "loss 0.05873815314692752 average time 3.3090465301500442 iter num 80\n",
            "loss 0.05873777999406866 average time 3.3079565986300077 iter num 100\n",
            "loss 0.058730615659547195 average time 3.3452850465500887 iter num 20\n",
            "loss 0.05872434220103738 average time 3.3240345209750557 iter num 40\n",
            "loss 0.058720252488614746 average time 3.318754815449999 iter num 60\n",
            "loss 0.05871841794622284 average time 3.3146252742750675 iter num 80\n",
            "loss 0.05871805002568348 average time 3.3147182381100357 iter num 100\n",
            "loss 0.05871090435851 average time 3.298752816450178 iter num 20\n",
            "loss 0.05870463862095276 average time 3.2959663910003654 iter num 40\n",
            "loss 0.058700551863226084 average time 3.297574558833609 iter num 60\n",
            "loss 0.058698719335109784 average time 3.296226474250216 iter num 80\n",
            "loss 0.058698351171726365 average time 3.298353306550125 iter num 100\n",
            "loss 0.05869121877800601 average time 3.3017999641002462 iter num 20\n",
            "loss 0.05868497103598072 average time 3.3023746357753225 iter num 40\n",
            "loss 0.058680897756115694 average time 3.30147722981698 iter num 60\n",
            "loss 0.058679068090511544 average time 3.3009880186877125 iter num 80\n",
            "loss 0.05867870369619028 average time 3.3022539974001486 iter num 100\n",
            "loss 0.05867159208742011 average time 3.288024681650131 iter num 20\n",
            "loss 0.05866536078848948 average time 3.296719539274909 iter num 40\n",
            "loss 0.05866130833129604 average time 3.299458893583384 iter num 60\n",
            "loss 0.058659480637558686 average time 3.300042765737635 iter num 80\n",
            "loss 0.05865911991612057 average time 3.301244530960066 iter num 100\n",
            "loss 0.05865201234546377 average time 3.3029846892000934 iter num 20\n",
            "loss 0.05864580230943997 average time 3.295499114474933 iter num 40\n",
            "loss 0.05864175245791784 average time 3.2935096574333027 iter num 60\n",
            "loss 0.05863992684350086 average time 3.299496776712431 iter num 80\n",
            "loss 0.05863956748871817 average time 3.3046910429100897 iter num 100\n",
            "loss 0.05863247893110681 average time 3.3056085368501953 iter num 20\n",
            "loss 0.058626282435275595 average time 3.3158040845752113 iter num 40\n",
            "loss 0.05862224916186607 average time 3.3164350901166473 iter num 60\n",
            "loss 0.05862042426259113 average time 3.3150026244126365 iter num 80\n",
            "loss 0.05862006407262525 average time 3.3093643615501422 iter num 100\n",
            "loss 0.058612998362213765 average time 3.29256520270028 iter num 20\n",
            "loss 0.05860681366031615 average time 3.295371953775157 iter num 40\n",
            "loss 0.05860279123713992 average time 3.2975184008001936 iter num 60\n",
            "loss 0.0586009791719374 average time 3.302913671600072 iter num 80\n",
            "loss 0.05860062229945512 average time 3.303526637390169 iter num 100\n",
            "loss 0.05859356202087735 average time 3.291257828949347 iter num 20\n",
            "loss 0.05858739753990069 average time 3.300784297224709 iter num 40\n",
            "loss 0.05858338234135851 average time 3.3071135191832584 iter num 60\n",
            "loss 0.05858157056722454 average time 3.306294417049867 iter num 80\n",
            "loss 0.058581214707109815 average time 3.304790603059846 iter num 100\n",
            "loss 0.0585741813792375 average time 3.3004555178500596 iter num 20\n",
            "loss 0.058568023697143055 average time 3.3052666107750155 iter num 40\n",
            "loss 0.058564022638700405 average time 3.3067130703166185 iter num 60\n",
            "loss 0.0585622203464544 average time 3.3045934450125514 iter num 80\n",
            "loss 0.05856185952043491 average time 3.302234729269985 iter num 100\n",
            "loss 0.05855484238254895 average time 3.2872893384002966 iter num 20\n",
            "loss 0.05854870958536544 average time 3.290935749224809 iter num 40\n",
            "loss 0.05854471059258808 average time 3.297197696749572 iter num 60\n",
            "loss 0.058542914427846676 average time 3.297025251337209 iter num 80\n",
            "loss 0.05854255759307044 average time 3.2963927524999237 iter num 100\n",
            "loss 0.05853555670657866 average time 3.322960081450219 iter num 20\n",
            "loss 0.05852943439878982 average time 3.3299779800003306 iter num 40\n",
            "loss 0.058525448495787426 average time 3.325498718766903 iter num 60\n",
            "loss 0.05852365186254442 average time 3.328624637125176 iter num 80\n",
            "loss 0.05852329710860067 average time 3.321446953250197 iter num 100\n",
            "loss 0.05851631328909612 average time 3.298966370449489 iter num 20\n",
            "loss 0.058510212916316764 average time 3.3078605444997264 iter num 40\n",
            "loss 0.05850623529677663 average time 3.306570912183088 iter num 60\n",
            "loss 0.0585044455237437 average time 3.305628344837305 iter num 80\n",
            "loss 0.058504090420797766 average time 3.3053053409599307 iter num 100\n",
            "loss 0.05849712051010425 average time 3.3028334444503344 iter num 20\n",
            "loss 0.05849103248587189 average time 3.3099198939752568 iter num 40\n",
            "loss 0.05848706963896875 average time 3.309401379733572 iter num 60\n",
            "loss 0.05848527795047678 average time 3.3093004991626005 iter num 80\n",
            "loss 0.058484929595240154 average time 3.3100264051900377 iter num 100\n",
            "loss 0.05847797299545269 average time 3.3167447026000447 iter num 20\n",
            "loss 0.05847190377284571 average time 3.308767741375232 iter num 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bb7f328e351a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_SGD_1e-2_2.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bb7f328e351a>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bb7f328e351a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bb7f328e351a>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQs-OZHGEwac"
      },
      "source": [
        "# 22:45\n",
        "# 19:26"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 0.8, 1, 0.25, 0.02, 0.02]]).cuda() # T, K, B, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[3]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.10632345, 0.5543747)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqpasxVi0hx3",
        "outputId": "91788b74-ac25-43d2-d57a-9dedd3c42b76"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "        dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx2)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.02]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.]*numstocks) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "B = 0.8 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10632345\n",
            "[0.5543747]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovJwXo3-YEu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1a688e44-e63c-4519-f43f-226b45759239"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "correct_call_prices = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    correct_call_prices.append(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, correct_call_prices, label = \"correct_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(correct_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "77bf560d-5ade-49c7-938f-066562b0857a"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.0, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgD1NjP7DOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "4014d547-8d24-4a76-c5bd-3919eb48b2c9"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.775, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 0.775, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZdrH8e+TRkhICCShhpAAoXcCCKEICNIEGyoqUlxdX/uq66oLyiquoLgrIuqiFAWUFRFEqSJEQIqEBakBAgQIIOm9TDLzvH+cGAMGM8BkziS5P9c1V+bMOTNzn5Qfh2eeorTWCCGEqPzczC5ACCGEY0igCyFEFSGBLoQQVYQEuhBCVBES6EIIUUV4mPXGQUFBOiwszKy3F0KISmnPnj3JWuvgsvaZFuhhYWHExMSY9fZCCFEpKaVOX2mfNLkIIUQVIYEuhBBVhAS6EEJUEaa1oZelsLCQhIQE8vPzzS5FmMDb25uQkBA8PT3NLkWISsmlAj0hIQE/Pz/CwsJQSpldjnAirTUpKSkkJCQQHh5udjlCVEou1eSSn59PYGCghHk1pJQiMDBQ/ncmxHVwqUAHJMyrMfnZC3F9XKrJRQghKprWmkKrJs9iJbewiFyL1bhvsZJrKSK/0EZBkZX8Qiv5hbaSrxqNt6c73h5u1PB0x9vTDW8Pd7y93PGr4YGftye1vD3w8/bA18sDdzfnX6BIoAshXFaR1UZWfhGZ+YVk5BWSXVBEboGVHEsReRYrORYreZai4q9GIJcO6LxCI5hL7lus5BZasdoqfh0IXy/3S0K+Vg0P/L09qVXDg9u6NuaGZoEOf08J9Ar062jYoKCg6zrGXgsXLiQmJob33nuPqVOnUqtWLZ577rlynxcfH8/IkSM5ePCgXcfs27eP8+fPM3z48OuuWVQPWmvyCq2k5lhIzy0kNcdCWq6FtBwLqbmFxV+Lt3MsZOYVkplfRHZBkV2v7+muqOnpjo+XBz5e7nh7uuPj5Y6ftwf1/Grg4+VOzVKP1/R0p2bxsb9u+3h5ULP4vrenm3E1Xny/hoc7AJai4iv2IisFhTbyi4x/LLKLa83KLyQrv6jkll1QWPy1iMz8Is6l55GdX0SP8LoV8n2WQBdXbd++fcTExEigV3M2myYpu4Dz6XmcT88nMSuftMvCufS2pchW5usoBQE1Panj60VdHy9C6vgQ0NgTf29Patf0xL+mh/HV2xPfGsaVbk0vd3xruOPjadz38nDOx4E1i/9hcFUuG+j/+OYQh89nOvQ12zby55Vb2v3hMfHx8QwdOpQbbriB7du30717dyZOnMgrr7xCYmIiS5YsoUWLFkyaNImTJ0/i4+PD3Llz6dixIykpKYwdO5Zz587Rq1cvSi/vt3jxYt59910sFgs9e/bk/fffx929/F+MTz/9lJkzZ6KUomPHjixatIhvvvmGadOmYbFYCAwMZMmSJdSvX/+qvhd79uxh0qRJAAwZMqTkcavVygsvvEB0dDQFBQU89thj/PnPfy7Zb7FYePnll8nLy2Pbtm28+OKLhIeH89RTT5Gfn0/NmjVZsGABrVq14tChQ0ycOBGLxYLNZmP58uVERERcVZ3CXBm5hcSn5Bi35FxOp+SQkJbH+Yw8fsnIp+iypouScPbxoo6vF40DatK+kT91fY3tOj7Gvrq+XgQUf61d09OU9uaqyGUD3UxxcXEsW7aM+fPn0717dz777DO2bdvGqlWr+Oc//0mTJk3o0qULK1euZNOmTTzwwAPs27ePf/zjH/Tp04eXX36Z1atXM2/ePACOHDnCf//7X3788Uc8PT159NFHWbJkCQ888MAf1nHo0CGmTZvG9u3bCQoKIjU1FYA+ffqwc+dOlFJ8/PHHvPnmm7z99ttXdY4TJ07kvffeo1+/fvz1r38teXzevHnUrl2b3bt3U1BQQFRUFEOGDCnpgeLl5cWrr75a0rQDkJmZydatW/Hw8GDjxo289NJLLF++nA8//JCnnnqK++67D4vFgtVqvaoahXNk5BZyKiWH+ORfgzuH+JRc4lNySM8tvOTYRrW9CanjQ2TTOjQKqEnDgJo0qu1No4Ca1POrQYCPl4SziVw20Mu7kq5I4eHhdOjQAYB27doxaNAglFJ06NCB+Ph4Tp8+zfLlywEYOHAgKSkpZGZmsmXLFr766isARowYQZ06dQD4/vvv2bNnD927dwcgLy+PevXqlVvHpk2bGDNmTEn7et26RrtbQkICd999NxcuXMBisVz1QJz09HTS09Pp168fAOPGjWPt2rUAbNiwgf379/Pll18CkJGRwfHjx2nZsuUVXy8jI4Px48dz/PhxlFIUFhoh0KtXL15//XUSEhK4/fbb5ercREVWG2fT8jiRmM2JpF9vOZxMyiatVGgrBY1q1yQsyIcRHRoSFuhL00AfwoN8aVLXB29P121uEC4c6GaqUaNGyX03N7eSbTc3N4qKiq56aLrWmvHjx/PGG284pL4nnniCZ555hlGjRhEdHc3UqVMd8rpg1Dp79mxuvvnmSx6Pj4+/4nOmTJnCgAEDWLFiBfHx8dx4440A3HvvvfTs2ZPVq1czfPhw/vOf/zBw4ECH1SrKlpxdwJELmcW3LI5cyOREUjaF1t+aR4Jq1aB5sC9D2zckPMiHsEBfCe0qQAL9GvTt25clS5YwZcoUoqOjCQoKwt/fn379+vHZZ58xefJk1q5dS1paGgCDBg1i9OjR/OUvf6FevXqkpqaSlZVF06ZN//B9Bg4cyG233cYzzzxDYGAgqamp1K1bl4yMDBo3bgzAJ598ctX1BwQEEBAQwLZt2+jTpw9Lliwp2XfzzTfzwQcfMHDgQDw9PTl27FjJe/3Kz8+PrKysku3S9SxcuLDk8ZMnT9KsWTOefPJJzpw5w/79+yXQHUhrzS+Z+RxIyODAOeN26HwmSVkFJcfU969Bm4b+9G8VTIvgWjSvV4vmQbWo7SPz5VRFEujXYOrUqUyaNImOHTvi4+NTEqqvvPIKY8eOpV27dvTu3ZvQ0FAA2rZty7Rp0xgyZAg2mw1PT0/mzJlTbqC3a9eOv//97/Tv3x93d3e6dOnCwoULmTp1KmPGjKFOnToMHDiQU6dOXfU5LFiwgEmTJqGUuuRD0T/96U/Ex8fTtWtXtNYEBwezcuXKS547YMAApk+fTufOnXnxxRd5/vnnGT9+PNOmTWPEiBElx33xxRcsWrQIT09PGjRowEsvvXTVdYrfJGbls/9sBvvPZXAgIZ0D5zJJzjbC291NEVGvFv0igmnT0I+2Df1p3dD4MFJUH6p0TwxnioyM1JevWHTkyBHatGljSj3CNcjvgCHPYuV/Z9LYdzad/Qnp7E/I4EKGMc+Nm4KIen60b1ybjiG1ad+4Nm0b+rt0dzrhOEqpPVrryLL2yRW6EC7AUmRjf0I6P8alsP1EMnvPpGOxGv22wwJ96B5Wl44htenUJIB2jfzx8ZI/XfF78lvhAlJSUhg0aNDvHv/+++8JDLy+4cGPPfYYP/744yWPPfXUU0ycOPG6XldcH601J5NziD6axJZjSeyOTyXXYkUpaNfInwlRYfRqHkjXJnWkvVvYTQLdBQQGBrJv374Kee05c+ZUyOuKq5dTUMSOEylEH0vkh2NJnE3NA6BZkC93dA0hqkUgPcMDqSPt3uIaSaALUUG01hy7mM0PxQG++1QaFqsNHy93ejcP5OF+zbmxZTBN6vqYXaqoIiTQhXCgjNxCtsUl88OxRLYcS+aXTOODzFb1/Rjfuyk3tqpHZFidksmehHAkCXQhroPWmsMXMok+mkT00UT+dyYdq03j5+1B34gg+rcMpl/LYBrWrml2qaIakEAX4irlF1qJPprIpthEoo8mkVg8kKd9Y3/+r39zbmwVTOcmAXi4u9yCYKKKk0B3Eenp6Xz22Wc8+uijDnvN0nOiT5gwgZEjR3LnnXeW+7zo6GhmzpzJt99+a9cx0dHReHl50bt3b4fV7moKiqxsPZbMN/vPs/HwRXIsVvy9PejbMpgBrerRr2UQ9fy8zS5TVHMS6A5SVFSEh4fHFbfLk56ezvvvv+/QQHeW6OhoatWqVeUC3WrTbItL5pufz7P+0C9k5RcR4OPJqM6NGNmxET3D68pVuHAp5SaOUmo+MBJI1Fq3L2O/AmYBw4FcYILW+n/XXdnaF+CXA9f9Mpdo0AGGTS/3sMvnIH/ttdeYNGkSycnJBAcHs2DBAkJDQ5kwYQLe3t7s3buXqKgoUlNTL9l+7LHHeOyxx0hKSsLHx4ePPvqI1q1bc/HiRR555BFOnjwJwAcffMC7777LiRMn6Ny5M4MHD+att94qs7YZM2awePFi3NzcGDZsGNOnT+ejjz5i7ty5WCwWWrRowaJFi/DxubqeE+vWrePpp5/Gx8eHPn36lDyek5PDE088wcGDByksLGTq1KmMHj26ZH98fDwffvgh7u7uLF68mNmzZ5Oenl7mfO0//PADTz31FGAsCL1lyxb8/Pyuqk5nSEjL5YuYBJbFnOVCRj5+NTwY0q4Bt3RqSFSLIDwlxIWLsucSciHwHvDpFfYPAyKKbz2BD4q/VkplzUE+fvz4ktv8+fN58sknS+Y3SUhIYPv27bi7uzNhwoRLtgcNGsSHH35IREQEu3bt4tFHH2XTpk08+eST9O/fnxUrVmC1WsnOzmb69Okly7tdydq1a/n666/ZtWsXPj4+JfOj33777Tz00EMATJ48mXnz5vHEE0/Yfc75+fk89NBDbNq0iRYtWnD33XeX7Hv99dcZOHAg8+fPJz09nR49enDTTTeV7A8LC+ORRx65ZLm7tLS0MudrnzlzJnPmzCEqKors7Gy8vV2nicJSZGPjkYss3X2WrceTAOgXEcyUkW0Z1Kae9EoRlUK5ga613qKUCvuDQ0YDn2pjUpidSqkApVRDrfWF66rMjivpilDWHOQ7duwomed83LhxPP/88yXHjxkz5pKVh37dzs7OZvv27YwZM6ZkX0FBQcl7fPqp8e+ju7s7tWvXLpmZ8Y9s3LiRiRMnllx9/zo/+sGDB5k8eTLp6elkZ2f/burb8sTGxhIeHl4yX/n999/P3LlzAWN+9FWrVjFz5kzACP8zZ8784etdab72qKgonnnmGe677z5uv/12QkJCrqrOihCfnMPnP53hyz0JpORYaFTbmycHRjAmMoSQOtI/XFQujmhDbwycLbWdUPzY9QV6JeHr61vmts1mIyAgoMJGgJY2YcIEVq5cSadOnVi4cCHR0dEOe22tNcuXL6dVq1aXPH7x4sUrPudK87W/8MILjBgxgjVr1hAVFcX69etp3bq1w2q1l6XIxneHL/LZT6f5MS4FdzfFTW3qcU+PUPpFBMuKO6LScmpjoFLqYaVUjFIqJikpyZlvbbeBAweybNkyUlJSAEhNTaV3794sXboUgCVLltC3b99yX8ff35/w8HCWLVsGGMH4888/A8b86B988AFgrOGZkZHxuznGyzJ48GAWLFhAbm5uSW0AWVlZNGzYkMLCwkvmNrdX69atiY+P58SJEwB8/vnnJftuvvlmZs+eXbI+6t69e3/3/D+aH730fO0nTpygQ4cO/O1vf6N79+7ExsZeda3X42xqLm+ui6X39E089tn/iE/O5dnBLdn+wkD+My6SAa3qSZiLSs0RgX4OaFJqO6T4sd/RWs/VWkdqrSODg4Md8NaOV3oO8k6dOvHMM88we/ZsFixYULJI86xZs+x6rSVLljBv3jw6depEu3bt+PrrrwGYNWsWmzdvpkOHDnTr1o3Dhw8TGBhIVFQU7du3v2SNz9KGDh3KqFGjiIyMpHPnziXNIK+99ho9e/YkKirqmq54vb29mTt3LiNGjKBr166XLI83ZcoUCgsL6dixI+3atWPKlCm/e/4tt9zCihUr6Ny5M1u3bi2Zr71bt24lTVcA77zzDu3bt6djx454enoybNiwq671alltmk2xF5m44Cf6vbWZD384QecmASyY0J0tzw/giUER1Pd3nbZ8Ia6HXfOhF7ehf3uFXi4jgMcxern0BN7VWvco7zVlPnRRFkf9DiRnF/BFzFmW7DzDufQ8gv1qMLZ7E+7pEUqjABm1KSqv65oPXSn1OXAjEKSUSgBeATwBtNYfAmswwjwOo9uizMsqTHMqOYf3NsXxzc/nsVht3NCsLi8Nb8OQdvWlu6Go8uzp5TK2nP0aeMxhFQkOHDjAuHHjLnmsRo0a7Nq167pf+7bbbvvdknUzZsy46p4xriY+OYfZm+JYue8cnu6KsT2acP8NTYmo73r93IWoKC43UlRrjTFWqfrq0KFDhfWOWbFiRYW8riNcy3KIZ1JyeXfTcVbsPYeHm2JC7zD+3L+ZDMMX1ZJLBbq3tzcpKSkEBgZW+1CvbrTWpKSk2D3YKDEzn399d4xlexLwcFOM7xXGIzdKkAsXpjXkpkLWBahVH2o5vmOISwV6SEgICQkJuGqXRlGxvL29yx1slF9oZd62U7y/OQ6L1ca4G5ry6I3NqSc9VYSZrIVGUGecg8xzkHneuGVdKHX7BawW4/iR/4bISQ4vw6UC3dPTs2RUoRClaa1Zf+gir685zNnUPAa3rc/fh7chLMi3/CcLcb0KsiDtNGSchfSzkHGm+OtZyEiA7ETgsiZDr1rg1xD8GkBoL+OrX0Pj1rhrhZTpUoEuRFmOXMjk1W8Os+NkCi3r12Lxgz3pExFU/hOFsJfNZlxZp56EtPhLb+mnITfl0uPda0DtEAhoAhGDwT8E/BtB7cbg39i4713b6achgS5cVkp2AW9/d4ylP53Bv6Ynr41ux9geoTJlrbg2NhtknYfk45ASZ4R36klIPWUEt7Xgt2PdPCAgFAKaQptRUCeseDsUajcB32Bwc73fQwl04XIsRTY+3RHPrO+Pk2ux8kCvMJ6+KYIAHy+zSxOVQVGBEdhJsZB0tDjAj0PKCSjM/e04Tx+o2wyCW0Krocb9OuFQN9y4ynarfDNsSqALl7I5NpHXVh/mZFIO/VoGM2VEG+lLLspWZDGC+uJhSDpihHdSrHHVrW3GMcrNuKoOjICwvhDYAoIijG2/BlDFetNJoAuXEJeYzbTVh4k+mkSzIF/mTzAmy5LuqwKtjR4jvxyAiwch8bAR4inHwVZkHOPmYYR1/XbQ/g4IagnBrY3HPKtPDygJdGGq1BwLszYeY/GuM/h4uTN5RBse6BWGl4frtU8KJ7AWGVfZvxwoDvDir3ml1guoHQr120KrYUaA12trBLeHNMlJoAtT/NpO/u73x8kuKOLenqH85aaWBNaqYXZpwllsVqN9+/ze326/HICiPGO/h7cR1m1GGctHNuhgbHv7m1u3C5NAF06ltWbD4Yu8seYI8Sm59GsZzOQRbWgp7eRVX+Z5SIiBc3uM2/m9YMk29nn6QsNOxmCbRp2N+3Wbg7tE1NWQ75ZwmkPnM5j27RF2nEyhRb1aLJzYnRtb1Sv/iaLyKbLAhZ/h7E44uwsS9hhdBgHcPI2r7U5jjQE2jboYbd6VsFeJq5FAFxUuMTOfmRuOsmxPAgHSn7xqys+AM7uMAD+z07gCL8o39gU0haa9ISQSGkcaYV6NPqh0Jgl0UWHyC618vPUk70efoNBq4099wnl8YAS1a3qaXZq4XrmpcGYHxP8Ip7fBhf2ABuX+W9NJ6A3Q5Abwq292tdWGBLpwOK01q34+z4y1sZzPyGdouwa8MKy1zLtSmRVkGeF9MhritxndB9HGEPgmPaD/3367CveSn7NZJNCFQx1IyGDqN4fYczqNdo38+dfdnbmhWaDZZYmrVWSBczFw8gcjxM/FGH2+PbyhSU8Y8BI0jYLG3aT5xIVIoAuHSMoqYOb6o3yx5yyBvl68eUdH7uwWgpubDAyqNNLPwPHvIG4jnNpi9EBRbsaHlr2fhGY3GmEuAe6yJNDFdbEU2Vi4/RTvfh9HQZGVh/o24/GBLfD3lnZyl1dUAKd/hOMbIe47SD5mPF47FDqMgRaDIKwP1Kxjbp3CbhLo4ppordkUm8i01Uc4lZzDwNb1mDyiDc2Ca5ldmvgjOclwfAMcXQsnNhlX4e41ICwKuk2EFjcZc53IlAuVkgS6uGrHL2bx6reH2Xo8mWbBviyY0J0BraU/uctKPg6x38LRdZDwkzFxlV9D4yq81TBj0iovH7OrFA4ggS7slp5r4d/fGfOu+Hq5M2VkWx7o1RRP6U/uWrSGi4fgyCo4vMqYiRCM7oT9njemim3YWa7CqyAJdFGuQquNJTtP8++Nx8nKL+TenqE8M7gVdX1lMiSXoTVc2AeHVhpBnnrS+EAztDcMnQFtRhor7IgqTQJdXJHWms1HE3l99RFOJOUQ1SKQKSPb0rqBTI7kMhJj4eBy45Z6wphGNryf0Sul9QioJU1h1YkEuihT7C+ZvL76CFuPJxMe5Mvccd0Y3La+zE/uCtLijQA/sBwSDxlX4mF9oc/T0Hok+NQ1u0JhEgl0cYmkrAL+9d0x/rv7DH7enrw8si3339BU5ic3W34GHP4a9n0OZ7YbjzXpCcPehLa3yvB6AUigi2L5hVbmbTvFB9EnyC+0Mr53GE8NknU8TWWzwonN8PPnRi+Vonxj6bRBLxs9VAJCza5QuBi7Al0pNRSYBbgDH2utp1+2PxT4BAgoPuYFrfUaB9cqKoDNplmx9xxvbzjK+Yx8bmpTnxeHt6a59Cc3T+pJ2LsY9n0GWRfAOwC63A+d7jWmm5VmL3EF5Qa6UsodmAMMBhKA3UqpVVrrw6UOmwx8obX+QCnVFlgDhFVAvcKBtscl8/qaIxw6n0nHkNoy74qZigrgyDfwv0/h1A9Gu3jEEKNJpeXN4CErOYny2XOF3gOI01qfBFBKLQVGA6UDXQO/dn2oDZx3ZJHCsY5fzOKNtbFsik2kcUBNZt3TmVs6NpJ5V8yQfBxi5hvNKnlpRjPKgMnQ5T7wb2R2daKSsSfQGwNnS20nAD0vO2YqsEEp9QTgC9xU1gsppR4GHgYIDZX2P2e7mJnPv787xhcxZ/Gt4cGLw1ozvncY3p6yUoxT2axwbD3s/sgYfu/maXQx7PoANBsAbvIBtLg2jvpQdCywUGv9tlKqF7BIKdVea20rfZDWei4wFyAyMlI76L1FObLyC5m75SQfbT2J1aaZ0Ducxwe2kIFBzpabajSp7J4HGWfAr5FxNd5tvPQXFw5hT6CfA5qU2g4pfqy0B4GhAFrrHUopbyAISHREkeLaFFptfP7TGWZtPE5KjoVRnRrx3JBWhAbKvB1OlRgLO96D/V+AtcDoM37zNGg1HNxlVkrhOPYE+m4gQikVjhHk9wD3XnbMGWAQsFAp1QbwBpIcWaiwn82mWX3gAm9vOEp8Si43NKvLguFt6BgSYHZp1YfWxsIQO+YYU9N61ITO90KPh6F+W7OrE1VUuYGutS5SSj0OrMfokjhfa31IKfUqEKO1XgU8C3yklPoLxgekE7TW0qTiZFprth5P5s31sRw8l0nrBn7MnxDJgFb1ZISnsxRZjFGcO+bAxQPgW89oVomcBL7Sg0hULGVW7kZGRuqYmBhT3rsq2nc2nRlrY9lxMoWQOjV5dkhLRnVqjLv0XHEOSw7s+QS2z4as8xDcBno9ZgwAkhV+hAMppfZorSPL2icjRSu5uMRsZq4/yrpDvxDo68XUW9oytmcoNTyk54pT5KXBTx/Bzg8gLxWa9oFR7xoLRcj/ioSTSaBXUufT83hn4zG+3JNATU93nr4pgj/1bUatGvIjdYqsi7BzjtFjxZINLYdCn2cg9PIevUI4j/z1VzJpORbmbI7j052nQcPEqHAevbE5gbVkJKFTZF2EH98xBgNZLdDuNujzF2jQwezKhJBAryxyLUXM23qKuVtOkmMp4o6uITx1UwQhdaQLolNkJ8K2dyBmHlgLodM90PdZCGxudmVClJBAd3E2m2blvnPMWBfLxcwCbm5Xn+eGtCKivp/ZpVUP2UnGFfnueUYf8o53Q7+/SpALlySB7sL2nE7j1W8P8/PZdDqF1GbOvV2JDJPFC5wiPxO2vws73oeiPKO3Sr/nIaiF2ZUJcUUS6C7ofHoe09fGsurn89Tzq8HbYzpxW5fGMnmWMxQVGO3jW96C3BSjjXzA3yEowuzKhCiXBLoLySko4j8/nGDu1pNoDU8MbMEj/ZvjKz1XKp7NBgeWweZpkH4GwvvD4H9Aoy5mVyaE3SQpXIDVplm+J4G3NhwlKauAkR0b8sKw1vKBp7Oc2AwbphgjOxt0hHGzoPlAs6sS4qpJoJtse1wyr60+wpELmXQJDeDD+7vRrWkds8uqHpLjYMNkOLYWAprCHfOg3e0yfa2otCTQTXIiKZs31hxh4xFjkYnZY7swsmNDmXPFGfLS4Ic34ae5xqRZg1+Fno/IqkCi0pNAd7LUHAuzNh5jya4zeHu687ehrZkYJYtMOIW1CPYsgM2vQ36GsaDEgL/LXOSiypBAd5L8QiufbI/nvc1x5FqsjO3RhKdvakmQjPB0jvhtsPo5SDpifOB58z+hQXuzqxLCoSTQK5jWmm/2X+DNdbEkpOUxsHU9XhzWWgYGOUvmBfhuitGDJSAU7vnMWFhCmrZEFSSBXoEOJGTw8qqD7D2TTpuG/iz5U0eiWgSZXVb1YC2EXR9C9HTjfv+/GXOueNY0uzIhKowEegVIy7Hw1oajfP7TGQJ9a/DmnR25o2uIzE3uLKe2wprnICkWIm6GYdOhbjOzqxKiwkmgO5DNpvlvzFneXBdLZn4RE3uH8/TgCPy9Zd1Ip8hNNZpX9i42mlfGLoVWw8yuSginkUB3kJ/PpvPy1wf5OSGDHmF1efXWdrRu4G92WdWD1nDoK1j7NyPUo542mli8ZGCWqF4k0K9T6eaVoFo1eOfuzozu3Ej6kztL+llY/SwcX28M07//K2jY0eyqhDCFBPo1stk0y/acZfra35pX/jI4Aj9pXnEOmw12fwTfvwraZnRD7PFncJdfaVF9yW//NTh0PoMpKw/yvzPpdA+rw6uj29OmoTSvOE3qSfj6cTj9o7F254h/QZ2mZlclhOkk0K9CRl4h//7uGJ/uiKeOjxczx3Tijq6NpXnFWWw2Y8Wg714GN0+49QPoNFb6lAtRTALdDjab5mF6RA4AABL8SURBVMs9CcxYF0tqroX7ezbluSGtqO0jzStOk3YaVj0Op7ZA80EwajbUbmx2VUK4FAn0cuw7m84rqw7x89l0ujWtwyejetC+cW2zy6o+tIY9C41ZEVFwy7vGHCxyVS7E70igX0FydgFvrovli5gEgv1q8K+7jFWDpHnFibITjbby4+uN+VdGv2f0LxdClEkC/TKFVhuf7jjNOxuPkWex8nC/ZjwxsIX0XnG2o+vg68egIAuGvQndH5J5yoUoh12BrpQaCswC3IGPtdbTyzjmLmAqoIGftdb3OrBOp9gcm8hrqw9zMimHvhFBvHJLO1rUq2V2WdWLJRc2/N1Y17N+B5jwLdRrY3ZVQlQK5Qa6UsodmAMMBhKA3UqpVVrrw6WOiQBeBKK01mlKqUo1wXRcYhavfXuEH44lER7ky7zxkQxsXU+aV5zt/F5Y/hCkHIfeT8DAKbLohBBXwZ4r9B5AnNb6JIBSaikwGjhc6piHgDla6zQArXWiowutCBm5hbzz/TEW7ThNTS93Jo9owwO9wvDykP/aO5XWsOM92DgVfOvBA6ugWX+zqxKi0rEn0BsDZ0ttJwA9LzumJYBS6keMZpmpWut1l7+QUuph4GGA0FDzPtwqtNpYvPM0s74/TmZeIff0COXZwS0JlMUmnC8vDVY+CkfXQJtbjF4sPnXNrkqISslRH4p6ABHAjUAIsEUp1UFrnV76IK31XGAuQGRkpHbQe9tNa813hy/yxtpYTiXn0KdFEC8Nb0PbRjLK0xQJe2DZBMi6AENnQM8/S3dEIa6DPYF+DmhSajuk+LHSEoBdWutC4JRS6hhGwO92SJUOcPBcBq99e5hdp1JpUa8WCyZ058ZWwdJObgatYdd/jL7lfg1h0noI6WZ2VUJUevYE+m4gQikVjhHk9wCX92BZCYwFFiilgjCaYE46stBrdS49j7c3HGXF3nPU8fHitVvbM7Z7EzzcpZ3cFPkZRt/yI6ug5TC49X1pYhHCQcoNdK11kVLqcWA9Rvv4fK31IaXUq0CM1npV8b4hSqnDgBX4q9Y6pSILL09GbiHv/xDHgh/jAXi4XzMeG9BCFpswU2IsLL0X0uJhyDTo9bg0sQjhQEprpzdlA0YbekxMjMNfN7/QyqIdp3lvcxyZ+YXc1qUxzw5pReMAWUvSVIdXwcr/A08fuOtTaNrL7IqEqJSUUnu01pFl7asyI0VtNs2qn8/z1vqjnEvPo1/LYF4Y2lo+8DSbzQqb/wlbZ0LjSLh7Efg3MrsqIaqkSh/oWmu+P5LIzA1Hif0li3aN/JlxR0f6RASZXZrIS4evHoLjG4wJtYbPlIFCQlSgSh3oO06k8Nb6WP53Jp2wQB9m3dOZWzo2ws1N2mVNl3jEaC9PPwsj/w3dJkp7uRAVrFIG+v6EdN5af5Stx5Np4O/NG7d34M5uIXhKzxXXcGw9fDkJvHyNuVhCbzC7IiGqhUoX6P/54QRvrI2ljo8nk0e04f4bmuLt6W52WQKM/uU73zf6lzfoAGOXSnu5EE5U6QK9f6tg8gqtPNgnXKa0dSXWQlj9LPzvE2MI/23/Ma7QhRBOU+kCvXUDf1o3kJ4rLiU3FZaNN5aH6/ssDJgsc5cLYYJKF+jCxSTHwWd3QcZZ46q80z1mVyREtSWBLq5d/I9GTxY3d2PKWxksJISpJNDFtTnwpTHys04Y3LfM+CqEMJU0dIqrozVs+zcsfxBCuhszJUqYC+ES5Apd2M9aBGufh5h50P4OuPUDGfkphAuRQBf2seQYg4WOrYOop2HQK9KTRQgXI4EuypedCEvGwC/7YcS/oPuDZlckhCiDBLr4Y2mnYdGtkPUL3PM5tBpqdkVCiCuQQBdXlhhrhHlhLjzwNTTpYXZFQog/IIEuypYQA0vuBPcaMHEt1G9ndkVCiHLIp1ri905sgk9GgXdtmLROwlyISkICXVzq0EpYcpfRt3zSeqgbbnZFQgg7SaCL3/xvEXw5ERp3g4mrwa+B2RUJIa6CBLow7P4YVj0OzW6EcSugZh2zKxJCXCUJdAE7PzDmMm85zOia6OVjdkVCiGsggV7dbXsH1r0AbUbBXZ+Cp7fZFQkhrpF0W6zOfngTNr9uzMty21xwl18HISoz+QuujrQ2gnzLW9BpLIyeY8xpLoSo1CTQqxutYeNU+PEd6PoAjJwlk2wJUUXY9ZeslBqqlDqqlIpTSr3wB8fdoZTSSqlIx5UoHEZr+P5VI8wjH5QwF6KKKfevWSnlDswBhgFtgbFKqbZlHOcHPAXscnSRwkGi34Bt/4JuE2H4TAlzIaoYe/6iewBxWuuTWmsLsBQYXcZxrwEzgHwH1iccJXoG/DADuowzpsCVMBeiyrHnr7oxcLbUdkLxYyWUUl2BJlrr1X/0Qkqph5VSMUqpmKSkpKsuVlyjLTMh+p/Q+T645V0JcyGqqOv+y1ZKuQH/Ap4t71it9VytdaTWOjI4OPh631rYY9s7sOk16Hg3jJotYS5EFWbPX/c5oEmp7ZDix37lB7QHopVS8cANwCr5YNQF7JgDG18x+pmPfl+6JgpRxdkT6LuBCKVUuFLKC7gHWPXrTq11htY6SGsdprUOA3YCo7TWMRVSsbDPnoWw/iVoO1oGDQlRTZQb6FrrIuBxYD1wBPhCa31IKfWqUmpURRcorsHB5fDN09BiMNz+sYS5ENWEXX/pWus1wJrLHnv5CsfeeP1liWt2bAN89TCE9jLmZvHwMrsiIYSTyCdkVcnp7fDFOGOFoXuXyqyJQlQzEuhVxfl98NndEBAK939lLB8nhKhWJNCrgqRjsPh28A6AcSvBN8jsioQQJpBAr+wyEmDRraDc4YGVULtx+c8RQlRJ0v2hMstNhcV3QEEWTFwDgc3NrkgIYSIJ9MqqMA+W3gupJ4028wYdzK5ICGEyCfTKyGaF5X+CMzthzAII72t2RUIIFyCBXtloDWv+CrHfwtAZ0O42sysSQrgI+VC0stk6E2LmQdRTcMMjZlcjhHAhEuiVyd7FsGmaMXPioKlmVyOEcDES6JXF8Y2w6kloPhBGvSfT4AohfkdSoTK4eAiWTYD6bWV+FiHEFUmgu7qsX2DJXVCjFoz9L9TwM7siIYSLkl4ursySA5/fA3lpMGmtjAIVQvwhCXRXZbMZ0+Be+Bnu+QwadjK7IiGEi5NAd1UbXy7uaz4dWg0zuxohRCUgbeiuKGYBbJ8N3R+CntLXXAhhHwl0V3NiE6x+1lg+buh0UMrsioQQlYQEuitJPg5fTIDg1nDnfFkLVAhxVSTQXUVuqrHikLunsXyct7/ZFQkhKhm5BHQF1kJj4FDGWRj/jbGMnBBCXCUJdFew7gU49QPc+gGE3mB2NUKISkqaXMz200ew+2Po/SR0vtfsaoQQlZgEuplObIa1f4OWQ+GmqWZXI4So5CTQzZIcB8vGQ3AruONjcHM3uyIhRCUngW6GvHRjjhY3Dxi7VCbcEkI4hF2BrpQaqpQ6qpSKU0q9UMb+Z5RSh5VS+5VS3yulmjq+1CrCZoXlD0LaKbhrEdSRb5UQwjHKDXSllDswBxgGtAXGKqXaXnbYXiBSa90R+BJ409GFVhkbX4G4jTDibQiLMrsaIUQVYs8Veg8gTmt9UmttAZYCo0sfoLXerLXOLd7cCYQ4tswqYt/nxhwtPR6GbhPMrkYIUcXYE+iNgbOlthOKH7uSB4G111NUlZQQA988BWF94eZ/ml2NEKIKcujAIqXU/UAk0P8K+x8GHgYIDa1GoyEzz8PS+8CvgbGEnLun2RUJIaoge67QzwFNSm2HFD92CaXUTcDfgVFa64KyXkhrPVdrHam1jgwODr6WeiufwjwjzC3ZRo8Wn7pmVySEqKLsCfTdQIRSKlwp5QXcA6wqfYBSqgvwH4wwT3R8mZWU1rDqSTi/F27/yFjkWQghKki5ga61LgIeB9YDR4AvtNaHlFKvKqVGFR/2FlALWKaU2qeUWnWFl6tetsyEA1/AwMnQerjZ1Qghqji72tC11muANZc99nKp+zc5uK7K7+By2DwNOt4DfZ81uxohRDUgI0UrwtmfYMX/QWhvGPWurDokhHAKCXRHS4uHz8eCfyO4ezF41DC7IiFENSGB7kj5GcaqQ7ZCuG8Z+AaaXZEQohqRBS4cxVpkrDqUEgf3fwVBEWZXJISoZiTQHUFrWPs8nNgEo2ZDszLHVQkhRIWSJhdH2Po2xMwzVh3q+oDZ1QghqikJ9Ou1+2PY9Bp0uAtu+ofZ1QghqjEJ9Otx4EtY/ZyxhNyt74ObfDuFEOaRBLpWxzfCij9DaC8Ys1Am3BJCmE4C/Vqc2Qn/vR/qtYF7l4JnTbMrEkIICfSr9ssBWHKXMXDo/hXgXdvsioQQApBAvzrJcbDodvDyhQdWQq1qMgWwEKJSkEC314X9sGAoaKsR5gHVaIEOIUSlIIFuj9M7YOEIcK8BE9dBcCuzKxJCiN+RQC/PsQ2w6DaoVQ8mrYPglmZXJIQQZZJA/yMHvoSlY40Qn7gOApqU/xwhhDCJBPqV7P4Ylv8JmvSE8d/KB6BCCJcnk3NdzloE0f805mdpOdQYNCT9zIUQlYAEemnpZ4yr8rO7oMs4GPlvGQEqhKg0JNB/dWglfPMk2Gxw+0fQ8S6zKxJCiKsigW7JhfUvwp6F0Kgr3DkP6jYzuyohhLhq1TvQL+w3mliSj0LU0zDg7+DhZXZVQghxTapnoKeehOjpsP8L8A2GcSug+UCzqxJCiOtSvQI9/SxseQv2LgZ3L+j9OET9RRZzFkJUCdUj0LMuGt0Q9yww1v/s/iD0fRb8GphdmRBCOEzVDfS8NIhdA4dXwonNoG3Q5T7o91eZWEsIUSVVrUC/PMRthVC7CfT8M0ROgsDmZlcohBAVxq5AV0oNBWYB7sDHWuvpl+2vAXwKdANSgLu11vGOLfUyOSlw8YCx4MSvt6SjxvS2v4Z4u9uhcVdQqkJLEUIIV1BuoCul3IE5wGAgAditlFqltT5c6rAHgTStdQul1D3ADODuiiiYPZ/ADzMg89xvj/k1ggYdoPVIY7i+hLgQohqy5wq9BxCntT4JoJRaCowGSgf6aGBq8f0vgfeUUkprrR1Yq6FWfWgaZQT4rzffIIe/jRBCVDb2BHpj4Gyp7QSg55WO0VoXKaUygEAgufRBSqmHgYcBQkOv8YPJVkONmxBCiEs4dfpcrfVcrXWk1joyOFimoxVCCEeyJ9DPAaVXdggpfqzMY5RSHkBtjA9HhRBCOIk9gb4biFBKhSulvIB7gFWXHbMKGF98/05gU4W0nwshhLiictvQi9vEHwfWY3RbnK+1PqSUehWI0VqvAuYBi5RScUAqRugLIYRwIrv6oWut1wBrLnvs5VL384Exji1NCCHE1ZA1RYUQooqQQBdCiCpCAl0IIaoIZVZnFKVUEnDalDe/PkFcNmCqmqiu5w3V99zlvF1TU611mQN5TAv0ykopFaO1jjS7DmerrucN1ffc5bwrH2lyEUKIKkICXQghqggJ9Ks31+wCTFJdzxuq77nLeVcy0oYuhBBVhFyhCyFEFSGBLoQQVYQE+hUopYYqpY4qpeKUUi+UsT9UKbVZKbVXKbVfKTXcjDodzY7zbqqU+r74nKOVUiFm1OloSqn5SqlEpdTBK+xXSql3i78v+5VSXZ1dY0Ww47xbK6V2KKUKlFLPObu+imLHed9X/HM+oJTarpTq5Owar4UEehlKraM6DGgLjFVKtb3ssMnAF1rrLhizS77v3Codz87zngl8qrXuCLwKvOHcKivMQuCPlsIaBkQU3x4GPnBCTc6wkD8+71TgSYyfe1WykD8+71NAf611B+A1KskHpRLoZStZR1VrbQF+XUe1NA34F9+vDZx3Yn0VxZ7zbgtsKr6/uYz9lZLWegtGeF3JaIx/yLTWeicQoJRq6JzqKk555621TtRa7wYKnVdVxbPjvLdrrdOKN3diLOzj8iTQy1bWOqqNLztmKnC/UioBY2rhJ5xTWoWy57x/Bm4vvn8b4KeUCnRCbWaz53sjqqYHgbVmF2EPCfRrNxZYqLUOAYZjLPBRHb6fzwH9lVJ7gf4Yyw9azS1JiIqhlBqAEeh/M7sWe9i1wEU1ZM86qg9S3Aantd6hlPLGmNQn0SkVVoxyz1trfZ7iK3SlVC3gDq11utMqNI89vxOiClFKdQQ+BoZprSvFGsnV4YryWtizjuoZYBCAUqoN4A0kObVKxyv3vJVSQaX+J/IiMN/JNZplFfBAcW+XG4AMrfUFs4sSFUMpFQp8BYzTWh8zux57yRV6GexcR/VZ4COl1F8wPiCdUNkXxrbzvG8E3lBKaWAL8JhpBTuQUupzjHMLKv5c5BXAE0Br/SHG5yTDgTggF5hoTqWOVd55K6UaADEYHQBsSqmngbZa60yTSnYIO37eLwOBwPtKKYCiyjADowz9F0KIKkKaXIQQooqQQBdCiCpCAl0IIaoICXQhhKgiJNCFEKKKkEAXQogqQgJdCCGqiP8H5uLdlM0bUIEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7MfujMQ7oij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "db38961a-0b40-4ffa-ff90-3d8fb8671537"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.225, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.225, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}