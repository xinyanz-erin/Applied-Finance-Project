{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Combined_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/Combined_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "3df751f0-fb77-4f55-c35a-d63634ed2072"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0  20519      0 --:--:-- --:--:-- --:--:-- 20519\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9MB 51kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 34.5MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. \n",
        "\n",
        "Loading all the necessary libraries:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6no5JzH-B6"
      },
      "source": [
        "# !pip install cupy-cuda101"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbkx3hXWnwi8"
      },
      "source": [
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBN3YFOnwi-"
      },
      "source": [
        "The CuPy version of batched barrier option pricing simulation is as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhj4DtLnwi-"
      },
      "source": [
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRjmX5zcnwi_"
      },
      "source": [
        "Note, the parameters (K, B, S0, sigma, mu, r) are passed in as an array with length of batch size. The output array is a two dimensional array flatten to 1-D. The first dimension is for Batch and the second dimension is for Path. \n",
        "\n",
        "Testing it out by entering two sets of option parameters:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4PMo7Inwi_"
      },
      "source": [
        "# N_PATHS = 2048000\n",
        "# N_STEPS = 365\n",
        "# N_BATCH = 2\n",
        "# T = 1.0\n",
        "\n",
        "# K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
        "# B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
        "# S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
        "# sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
        "# mu = cupy.array([0.15, 0.1], dtype=cupy.float32)\n",
        "# r =cupy.array([0.05, 0.05], dtype=cupy.float32)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpWK3wcEnwjA"
      },
      "source": [
        "Put everything into a simple function to launch this GPU kernel. The option prices for each batch is the average of the corresponding path terminal values. This can be computed easily by Cupy function `mean(axis=1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAb34NTnwjA"
      },
      "source": [
        "# def batch_run():\n",
        "#     number_of_threads = 256\n",
        "#     number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
        "#     randoms_gpu = cupy.random.normal(0, 1, N_BATCH*N_PATHS * N_STEPS, dtype=cupy.float32)\n",
        "#     output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     s = time.time()\n",
        "#     cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
        "#                        (output, np.float32(T), K, B, S0, sigma, mu, r,\n",
        "#                         randoms_gpu, N_STEPS, N_PATHS, N_BATCH))\n",
        "#     v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     e = time.time()\n",
        "#     print('time', e-s, 'v',v)\n",
        "# batch_run()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRgQCelnwjC"
      },
      "source": [
        "This produces the option prices $21.22$ and $0.848$ for these two sets of option parameters in $66ms$.\n",
        "\n",
        "It works efficiently hence we will construct an `OptionDataSet` class to wrap the above code so we can use it in Pytorch. For every `next` element, it generates uniform distributed random option parameters in the specified range, launches the GPU kernel to compute the option prices, convert the CuPy array to Pytorch tensors with zero copy via the DLPack. Note how we implemented the iterable Dataset interface:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1KUra7ZnwjC"
      },
      "source": [
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo46Vf4XnwjD"
      },
      "source": [
        "Put everything related to Pytorch dataset into a file `cupy_dataset.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQUGMBlnwjE"
      },
      "source": [
        "# #%%writefile cupy_dataset.py \n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "# cupy.cuda.set_allocator(None)\n",
        "\n",
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')\n",
        "\n",
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPAsh7JnwjF"
      },
      "source": [
        "Here is a test code to sample 10 data points with batch size 16:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLKxMF05nwjF"
      },
      "source": [
        "# from cupy_dataset import OptionDataSet\n",
        "# ds = OptionDataSet(10, number_path=100000, batch=16, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlzRTD0nwjG"
      },
      "source": [
        "We can implement the same code by using Numba to accelerate the calculation in GPU:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IsfSwVwnwjG"
      },
      "source": [
        "# import numba\n",
        "# from numba import cuda\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average)\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]:\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                               X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
        "#         o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
        "#         Y = o.mean(axis = 1) \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=1, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "597c8262-48b0-49bc-d259-7fbe90bb20ec"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "\n",
        "          X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          stocks_randoms_cov = cupy.array([0.9] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i[0])\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBOv_RiBsCWa"
      },
      "source": [
        "### PUI TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BME87CgGsFrd"
      },
      "source": [
        "# %%writefile cupy_dataset.py\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n",
        "\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp2 = math.exp(-r*T)\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)    \n",
        "\n",
        "#     for i in range(ii, N_PATHS, stride): # for each path          \n",
        "#         running_average = 0.0\n",
        "\n",
        "#         for j in range(N_STOCKS): # initialize S0\n",
        "#             s_curr[j] = S0[j]\n",
        "\n",
        "#         for n in range(N_STEPS): # for each step\n",
        "#             s_curr_avg = 0.0\n",
        "\n",
        "#             for j in range(N_STOCKS): # for each stock\n",
        "#                 tmp1 = mu[j]*T/N_STEPS  \n",
        "#                 s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n",
        "#                 s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n",
        "\n",
        "#             # add stock average to running average\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n",
        "\n",
        "#             # compare to barrier\n",
        "#             if running_average <= B:\n",
        "#                 break\n",
        "\n",
        "#         payoff = running_average - K if running_average > K else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "    \n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, number_stocks = 3, batch=1, threads=512, seed=15, T=1):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_STOCKS = number_stocks\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(T)\n",
        "#         self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "\n",
        "#         ############ <new\n",
        "#         self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n",
        "#         self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#         cupy.fill_diagonal(self.Z_cov, 1)\n",
        "#         ############ new>\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "\n",
        "#         X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "\n",
        "#         for i in range(self.N_BATCH): # for each batch\n",
        "#           self.S0 = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 200\n",
        "#           self.K = 110.0\n",
        "#           self.B = 100.0\n",
        "#           self.sigma = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.mu = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.r = 0.05\n",
        "#           self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n",
        "\n",
        "#           ############ <new - add correlation between stocks\n",
        "#           all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "          \n",
        "#           single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n",
        "#                                                                                     self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n",
        "#           Y[i] = self.output.mean()\n",
        "\n",
        "#           ############ <new - combine to get X matrix\n",
        "#           X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n",
        "#                                 self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "        \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "# ds = NumbaOptionDataSet(max_len=10, number_path=100, batch=2, seed=15)\n",
        "# for i in ds:\n",
        "#   print(i)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQt8PqinwjI"
      },
      "source": [
        "# %%writefile model.py\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6, hidden)\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0,\n",
        "#                                            198.0,\n",
        "#                                            200.0,\n",
        "#                                            0.4,\n",
        "#                                            0.2,\n",
        "#                                            0.2,]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9404bbca-3100-409c-b296-102116c5ddef"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm', torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2] * 3))\n",
        "        # self.register_buffer('norm',\n",
        "        #                      torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37160741-79bc-4a06-a383-0b9a556d4a00"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/c3/f472843797b5ccbb2f0e806a6927f52c7c9522bfcea8e7e881d39258368b/pytorch_ignite-0.4.5-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 32.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 34.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 33.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 34.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 35.2MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 34.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102kB 36.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 36.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 36.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133kB 36.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 36.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 36.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163kB 36.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 36.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 36.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 36.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204kB 36.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 36.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786ea4e9-fb21-4286-82bf-2ac64a247761"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=8, stocks=3)\n",
        "dataset = NumbaOptionDataSet(max_len=200, number_path = 2000, batch=20, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 111.9479751586914 average time 0.007122041239917962\n",
            "loss 185.5666961669922 average time 0.005363116824996723\n",
            "loss 133.86209106445312 average time 0.0065257561700218505\n",
            "loss 77.8807144165039 average time 0.005080245040021509\n",
            "loss 129.7606964111328 average time 0.0066308695600037024\n",
            "loss 216.6125030517578 average time 0.005158800545009399\n",
            "loss 171.6206512451172 average time 0.006633321220078869\n",
            "loss 166.62646484375 average time 0.005130261245030851\n",
            "loss 152.9368438720703 average time 0.006550995750021684\n",
            "loss 233.41880798339844 average time 0.005097085205038638\n",
            "loss 144.59103393554688 average time 0.0065901839300022405\n",
            "loss 92.63045501708984 average time 0.005130648030003613\n",
            "loss 76.35770416259766 average time 0.0065527798999664814\n",
            "loss 57.9631462097168 average time 0.00509821204499076\n",
            "loss 110.210693359375 average time 0.006527379810013372\n",
            "loss 125.8466567993164 average time 0.005085515265013782\n",
            "loss 173.2860565185547 average time 0.006660139609984981\n",
            "loss 121.25146484375 average time 0.005137058189989148\n",
            "loss 86.01773834228516 average time 0.006563853650022793\n",
            "loss 83.68541717529297 average time 0.005089067244994112\n",
            "loss 93.87401580810547 average time 0.0065845643500051666\n",
            "loss 182.5594940185547 average time 0.005115089114988223\n",
            "loss 109.81816864013672 average time 0.006530394850005905\n",
            "loss 53.06232833862305 average time 0.005091534874982245\n",
            "loss 67.44657135009766 average time 0.006567539920079071\n",
            "loss 67.58792877197266 average time 0.0050818314050548\n",
            "loss 54.57828903198242 average time 0.0065791601199725845\n",
            "loss 85.07445526123047 average time 0.005091202804969726\n",
            "loss 144.48731994628906 average time 0.006541375419928954\n",
            "loss 36.86122512817383 average time 0.005046518744966306\n",
            "loss 18.037643432617188 average time 0.006543972500048767\n",
            "loss 17.76913833618164 average time 0.005059611730030156\n",
            "loss 12.250141143798828 average time 0.006618927840045217\n",
            "loss 15.385986328125 average time 0.005073903770007746\n",
            "loss 8.510908126831055 average time 0.006569391829943924\n",
            "loss 8.310324668884277 average time 0.005064109574968825\n",
            "loss 6.057577610015869 average time 0.006562219110037404\n",
            "loss 6.278327465057373 average time 0.00506108561502515\n",
            "loss 8.905303001403809 average time 0.006516209799992794\n",
            "loss 3.2268950939178467 average time 0.005056622305023666\n",
            "loss 14.36169147491455 average time 0.0064361518700661695\n",
            "loss 7.332719326019287 average time 0.004987006275005115\n",
            "loss 14.671229362487793 average time 0.006445115020014782\n",
            "loss 6.573141574859619 average time 0.005043587925019893\n",
            "loss 10.399848937988281 average time 0.006534992199976841\n",
            "loss 7.151106357574463 average time 0.0050307702299824085\n",
            "loss 8.059160232543945 average time 0.006501516830021501\n",
            "loss 2.2571518421173096 average time 0.005007458650015906\n",
            "loss 6.632087230682373 average time 0.006570963670010315\n",
            "loss 12.274073600769043 average time 0.005088812945004974\n",
            "loss 10.049020767211914 average time 0.006461525670047195\n",
            "loss 6.0967230796813965 average time 0.005026492475039958\n",
            "loss 4.666905403137207 average time 0.0065229922100752446\n",
            "loss 3.0868632793426514 average time 0.005009264290074498\n",
            "loss 6.59853458404541 average time 0.006389187799968567\n",
            "loss 6.685290813446045 average time 0.004970022154989238\n",
            "loss 4.506593227386475 average time 0.006574151200029518\n",
            "loss 3.9943687915802 average time 0.005048561620037617\n",
            "loss 6.560020446777344 average time 0.006581694550004613\n",
            "loss 6.509414196014404 average time 0.004989193769984013\n",
            "loss 8.871797561645508 average time 0.0063252942199778775\n",
            "loss 6.063195705413818 average time 0.0048946390800301744\n",
            "loss 8.117849349975586 average time 0.006347053370091089\n",
            "loss 3.8810746669769287 average time 0.004893022760038548\n",
            "loss 8.432990074157715 average time 0.006362863280019155\n",
            "loss 6.146895885467529 average time 0.004900846299988188\n",
            "loss 12.977869033813477 average time 0.006371683210008996\n",
            "loss 3.6419219970703125 average time 0.004861567110006035\n",
            "loss 7.335762977600098 average time 0.00638471944002049\n",
            "loss 2.907585382461548 average time 0.004936391369997181\n",
            "loss 3.552593231201172 average time 0.006729969329981032\n",
            "loss 4.706162929534912 average time 0.005227188980011305\n",
            "loss 5.866436004638672 average time 0.006676742859972364\n",
            "loss 6.877624034881592 average time 0.00516639647500142\n",
            "loss 7.601619720458984 average time 0.00672096044002501\n",
            "loss 7.471766948699951 average time 0.005181572069977847\n",
            "loss 2.0952329635620117 average time 0.006732422480054083\n",
            "loss 3.8685765266418457 average time 0.005158902455023053\n",
            "loss 4.133150100708008 average time 0.006534725219989923\n",
            "loss 5.961799621582031 average time 0.005038571665013478\n",
            "loss 3.831860303878784 average time 0.006473413799931223\n",
            "loss 6.4148149490356445 average time 0.005061252204959601\n",
            "loss 4.08107328414917 average time 0.006566325380008493\n",
            "loss 3.5393776893615723 average time 0.005090890495002895\n",
            "loss 3.259141683578491 average time 0.006562425709980744\n",
            "loss 3.1095452308654785 average time 0.005073950594983216\n",
            "loss 7.752475261688232 average time 0.006539623519956877\n",
            "loss 6.213657855987549 average time 0.00508638526498089\n",
            "loss 11.09672737121582 average time 0.0067224607900334375\n",
            "loss 4.850911617279053 average time 0.005236574385030508\n",
            "loss 6.770350933074951 average time 0.0067664023400175214\n",
            "loss 2.5340654850006104 average time 0.005285551040005884\n",
            "loss 3.776719808578491 average time 0.006692641349973201\n",
            "loss 3.2754642963409424 average time 0.005195955219996904\n",
            "loss 6.001115322113037 average time 0.006684490209981959\n",
            "loss 3.193734645843506 average time 0.005154447979962242\n",
            "loss 3.6410179138183594 average time 0.006675745189977533\n",
            "loss 5.113941192626953 average time 0.005207750194972505\n",
            "loss 8.115635871887207 average time 0.00677041049001673\n",
            "loss 7.423539638519287 average time 0.005272082995043092\n",
            "loss 4.254001140594482 average time 0.006778806150005039\n",
            "loss 5.087361812591553 average time 0.005232492634991104\n",
            "loss 4.359730243682861 average time 0.006660039839953242\n",
            "loss 2.0265777111053467 average time 0.005210196239963807\n",
            "loss 5.299363136291504 average time 0.006881453530058934\n",
            "loss 4.056176662445068 average time 0.00528071653000552\n",
            "loss 4.536438941955566 average time 0.0066596965600183465\n",
            "loss 4.514766216278076 average time 0.005145604975009519\n",
            "loss 3.3040528297424316 average time 0.006621551780035588\n",
            "loss 3.4254403114318848 average time 0.005138473774986778\n",
            "loss 3.6154091358184814 average time 0.006611762689990428\n",
            "loss 3.028106689453125 average time 0.005157210584989116\n",
            "loss 5.401843547821045 average time 0.0066267974100264835\n",
            "loss 1.6295915842056274 average time 0.0051770120700257395\n",
            "loss 6.048686981201172 average time 0.006594271660060258\n",
            "loss 2.121091604232788 average time 0.005089944000046671\n",
            "loss 3.543832778930664 average time 0.006374314240019885\n",
            "loss 2.531949281692505 average time 0.004927031365014045\n",
            "loss 1.870440125465393 average time 0.006453001440013395\n",
            "loss 2.420360565185547 average time 0.004945264505008709\n",
            "loss 2.454338788986206 average time 0.0063465414999791395\n",
            "loss 2.8196682929992676 average time 0.004922428994977963\n",
            "loss 1.6460460424423218 average time 0.0063567512300051025\n",
            "loss 1.8079944849014282 average time 0.004899426674996903\n",
            "loss 2.156616687774658 average time 0.006390104240017536\n",
            "loss 4.252687931060791 average time 0.004988920584974039\n",
            "loss 1.245919108390808 average time 0.006677543229925504\n",
            "loss 1.9120330810546875 average time 0.005234141739956613\n",
            "loss 1.6436948776245117 average time 0.006690483269976539\n",
            "loss 1.097408413887024 average time 0.005203695354994125\n",
            "loss 2.0676918029785156 average time 0.0066620568598955284\n",
            "loss 1.3627948760986328 average time 0.005157437729949379\n",
            "loss 1.1415977478027344 average time 0.00655523590003213\n",
            "loss 0.8590497374534607 average time 0.0050375453400283736\n",
            "loss 1.332993507385254 average time 0.006491765689861495\n",
            "loss 1.4330339431762695 average time 0.005036595589954232\n",
            "loss 1.108095407485962 average time 0.0065625843800626174\n",
            "loss 2.0098824501037598 average time 0.005052872860069328\n",
            "loss 2.4900548458099365 average time 0.006741669180100871\n",
            "loss 1.5567944049835205 average time 0.005169779760071833\n",
            "loss 2.079132080078125 average time 0.006556533329967351\n",
            "loss 0.4866602122783661 average time 0.005087984474976111\n",
            "loss 1.5166234970092773 average time 0.0066534544000933235\n",
            "loss 1.0722836256027222 average time 0.005127595175081296\n",
            "loss 0.5229282379150391 average time 0.006601154950039927\n",
            "loss 0.999603271484375 average time 0.005105960880000566\n",
            "loss 2.29219126701355 average time 0.006592688799992174\n",
            "loss 0.325942724943161 average time 0.0051278628699401455\n",
            "loss 0.7013857960700989 average time 0.006780936530049075\n",
            "loss 0.7578131556510925 average time 0.005191053514990927\n",
            "loss 1.0916296243667603 average time 0.00655300102002002\n",
            "loss 0.6011547446250916 average time 0.00508598940498814\n",
            "loss 0.8996939063072205 average time 0.006664173220106023\n",
            "loss 0.6211652159690857 average time 0.00515284832999896\n",
            "loss 0.7417933344841003 average time 0.006549281909883575\n",
            "loss 0.6389151811599731 average time 0.00509742145490236\n",
            "loss 0.4085789620876312 average time 0.006734664310024527\n",
            "loss 0.7910467982292175 average time 0.005156287559993871\n",
            "loss 1.3625229597091675 average time 0.006579685399847221\n",
            "loss 0.45113635063171387 average time 0.00509590416496394\n",
            "loss 0.7621205449104309 average time 0.00652693570993506\n",
            "loss 0.7632829546928406 average time 0.005059685629894375\n",
            "loss 1.318967580795288 average time 0.006537060229948111\n",
            "loss 0.6933819651603699 average time 0.005036718169985761\n",
            "loss 0.8355033993721008 average time 0.006619731750015489\n",
            "loss 0.34523412585258484 average time 0.005113083019996339\n",
            "loss 0.3554077446460724 average time 0.006492424930074776\n",
            "loss 0.5580236315727234 average time 0.00511749749001865\n",
            "loss 1.0855082273483276 average time 0.006915645559874974\n",
            "loss 0.6757031679153442 average time 0.005290246614904391\n",
            "loss 0.7534977197647095 average time 0.006624089520009875\n",
            "loss 0.7308645844459534 average time 0.005102298134979719\n",
            "loss 1.0700856447219849 average time 0.006599165449897555\n",
            "loss 0.25700822472572327 average time 0.005099762649888362\n",
            "loss 0.6047298312187195 average time 0.006595270999914646\n",
            "loss 0.8394257426261902 average time 0.005164209459944686\n",
            "loss 0.642698347568512 average time 0.006689248820002831\n",
            "loss 0.5206698179244995 average time 0.0051890844150057095\n",
            "loss 0.5210964679718018 average time 0.006696276070015301\n",
            "loss 0.5541099905967712 average time 0.005129028935007227\n",
            "loss 0.5384442210197449 average time 0.0064886795199890915\n",
            "loss 0.4108254909515381 average time 0.005034342069957347\n",
            "loss 0.972551167011261 average time 0.006741372090054938\n",
            "loss 0.4394550025463104 average time 0.005156235994945746\n",
            "loss 0.6662102937698364 average time 0.006557190799958335\n",
            "loss 0.3021993637084961 average time 0.005060879654974997\n",
            "loss 0.9491413235664368 average time 0.006771309099985956\n",
            "loss 0.5151211023330688 average time 0.0053200187950005785\n",
            "loss 1.3278553485870361 average time 0.006842045340090408\n",
            "loss 0.6021265387535095 average time 0.005328558635137597\n",
            "loss 0.8758893013000488 average time 0.00676380861994403\n",
            "loss 0.3654462695121765 average time 0.005267186849969221\n",
            "loss 0.6516082882881165 average time 0.006807056910038228\n",
            "loss 0.4123411178588867 average time 0.005192777994980133\n",
            "loss 1.0017894506454468 average time 0.006590934160158213\n",
            "loss 0.6627874970436096 average time 0.005124942265119899\n",
            "loss 0.738943874835968 average time 0.006509623930032831\n",
            "loss 0.4613367021083832 average time 0.0050530511099896105\n",
            "loss 0.9116935133934021 average time 0.006669753640089766\n",
            "loss 0.45274850726127625 average time 0.005083528565064626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 20000\n",
              "\tepoch: 100\n",
              "\tepoch_length: 200\n",
              "\tmax_epochs: 100\n",
              "\toutput: 0.45274850726127625\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226881a0-538a-40bc-c55f-3d2f36cb20ae"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14.3261]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd1cfb9-6d63-406e-82a0-5463e0bd9fc7"
      },
      "source": [
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.4900e-01, -1.7541e-02,  2.7534e-01,  5.7023e+00,  1.4398e+01,\n",
              "         -3.2524e+00, -2.8378e-01, -1.9489e-02,  3.0818e-01,  3.3059e+00,\n",
              "          1.4341e+01, -4.9879e+00, -1.5541e-01,  7.2368e-03,  1.7185e-01,\n",
              "          4.4022e+00,  5.7254e+00, -1.2559e+00]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "fd7b36b5-deb2-48c3-b17e-b4e62977c5e4"
      },
      "source": [
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, S, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 300, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7bd3ce78d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3819b89-51b3-42c6-9164-bde45ed21a8c"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-4.3884e-03, -2.8772e-04,  4.3601e-03, -9.1900e-02,  4.0620e-01,\n",
              "          -3.7367e-01,  1.1323e-05,  5.0322e-06,  5.4141e-05,  1.3438e-02,\n",
              "          -1.6149e-03, -3.8490e-03, -6.8389e-05,  1.6105e-04,  4.6162e-05,\n",
              "           1.0017e-02,  1.6978e-02,  3.7639e-03]], device='cuda:0'),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "6607481b-52b1-44d7-a39c-1c3a62d98643"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, S, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7bd3c24350>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 100.0, 120.0, sigma, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}