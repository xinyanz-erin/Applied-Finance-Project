{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/European_Call_jax_1stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "30c155e9-1fa6-42c9-f1de-f507238f3eea"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 100\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12339937\n",
            "[0.62721133]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927eaa84-67ff-41fe-e79e-c874fcdaca51"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 1.0)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "          drift = jnp.array(np.random.random(self.N_STOCKS) * 0.1)\n",
        "\n",
        "          T = self.T\n",
        "          K = np.random.random(1) * 1.0\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 10000, batch = 2, seed = 15, stocks=3) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "e7b45682-3964-4644-ba56-4b9c5df4fcd4"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 1.0, 1.0, 0.3, 0.1, 0.1]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "0e9a8279-fbaf-4408-f2f6-785a3acf15cd"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[K     |████████████████████████████████| 240 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "50e9be35-9729-4c74-ee47-6d522b6de7e1"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1024*50, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 182.80796706676483 average time 0.2035638655500179 iter num 20\n",
            "loss 138.63280415534973 average time 0.11732268437501717 iter num 40\n",
            "loss 123.34054335951805 average time 0.08901597043335036 iter num 60\n",
            "loss 71.24339696019888 average time 0.07454980100001193 iter num 80\n",
            "loss 102.99415327608585 average time 0.06598847809000745 iter num 100\n",
            "loss 159.0585708618164 average time 0.09165397274998668 iter num 20\n",
            "loss 77.96648889780045 average time 0.06157221129998334 iter num 40\n",
            "loss 66.63292646408081 average time 0.0514710339833376 iter num 60\n",
            "loss 48.30049350857735 average time 0.04627021851250675 iter num 80\n",
            "loss 35.865032114088535 average time 0.0433498066500033 iter num 100\n",
            "loss 6.761997938156128 average time 0.09718935995006177 iter num 20\n",
            "loss 3.9406068390235305 average time 0.0649657183750378 iter num 40\n",
            "loss 4.589779418893158 average time 0.053784999900032444 iter num 60\n",
            "loss 2.381227968726307 average time 0.0481094561000134 iter num 80\n",
            "loss 2.514897205401212 average time 0.044773373920011184 iter num 100\n",
            "loss 2.3901024542283267 average time 0.09202166689997285 iter num 20\n",
            "loss 4.307996132411063 average time 0.06167837362502269 iter num 40\n",
            "loss 3.0045717721804976 average time 0.051378380100004506 iter num 60\n",
            "loss 2.0476854115258902 average time 0.04625773846251491 iter num 80\n",
            "loss 3.8376779411919415 average time 0.04317660495001746 iter num 100\n",
            "loss 4.488782142288983 average time 0.09162008354994669 iter num 20\n",
            "loss 3.23716172715649 average time 0.06132916357498743 iter num 40\n",
            "loss 3.8639205740764737 average time 0.05116333214997818 iter num 60\n",
            "loss 1.9202270777896047 average time 0.046137401237479024 iter num 80\n",
            "loss 2.7355787460692227 average time 0.043081002049984815 iter num 100\n",
            "loss 2.689840621314943 average time 0.09241996299995207 iter num 20\n",
            "loss 6.089335074648261 average time 0.06165740249997498 iter num 40\n",
            "loss 4.551227611955255 average time 0.05146034558330636 iter num 60\n",
            "loss 2.4258694611489773 average time 0.0463932619874754 iter num 80\n",
            "loss 2.2509436530526727 average time 0.04329735470997548 iter num 100\n",
            "loss 3.3413353958167136 average time 0.09104280890005612 iter num 20\n",
            "loss 2.9705290216952562 average time 0.06126849852503256 iter num 40\n",
            "loss 9.634751477278769 average time 0.05128758433334042 iter num 60\n",
            "loss 2.8922752244398 average time 0.04620413022502703 iter num 80\n",
            "loss 1.459331251680851 average time 0.04319820765002533 iter num 100\n",
            "loss 3.319329407531768 average time 0.09230244565003431 iter num 20\n",
            "loss 2.445278805680573 average time 0.06154629660002229 iter num 40\n",
            "loss 2.8482801280915737 average time 0.051442615133320636 iter num 60\n",
            "loss 1.295719703193754 average time 0.04631480634998297 iter num 80\n",
            "loss 3.6190118407830596 average time 0.04330470628997773 iter num 100\n",
            "loss 4.800983879249543 average time 0.09229191950003042 iter num 20\n",
            "loss 1.6358734865207225 average time 0.06180719760004649 iter num 40\n",
            "loss 2.287178358528763 average time 0.051648239016700855 iter num 60\n",
            "loss 1.1050200555473566 average time 0.04646916038752806 iter num 80\n",
            "loss 1.6564081306569278 average time 0.043515866570014626 iter num 100\n",
            "loss 3.6086206091567874 average time 0.09249181324996698 iter num 20\n",
            "loss 2.4377118097618222 average time 0.06203333009997323 iter num 40\n",
            "loss 1.9231608894187957 average time 0.05190258508333348 iter num 60\n",
            "loss 1.3796774146612734 average time 0.04681223661252147 iter num 80\n",
            "loss 2.8305070009082556 average time 0.043848728700018 iter num 100\n",
            "loss 2.314455050509423 average time 0.09137816325012409 iter num 20\n",
            "loss 3.118215245194733 average time 0.06115084805006745 iter num 40\n",
            "loss 2.185784833272919 average time 0.05122710421670339 iter num 60\n",
            "loss 1.5843447181396186 average time 0.04622662811252667 iter num 80\n",
            "loss 2.2496383462566882 average time 0.04309551513002134 iter num 100\n",
            "loss 13.749131467193365 average time 0.09204634609998266 iter num 20\n",
            "loss 3.6677817115560174 average time 0.06135963747502728 iter num 40\n",
            "loss 2.7493451489135623 average time 0.051309326716682334 iter num 60\n",
            "loss 1.3462561764754355 average time 0.04635170352502769 iter num 80\n",
            "loss 2.1927626221440732 average time 0.04325703421002345 iter num 100\n",
            "loss 1.4704880595672876 average time 0.09203242665000744 iter num 20\n",
            "loss 1.3455472071655095 average time 0.06161169062499994 iter num 40\n",
            "loss 2.8950211708433926 average time 0.05145551308335143 iter num 60\n",
            "loss 2.23149181692861 average time 0.04648441647503319 iter num 80\n",
            "loss 1.3558656792156398 average time 0.043405011740014744 iter num 100\n",
            "loss 6.537779700011015 average time 0.0911598190999939 iter num 20\n",
            "loss 8.92218784429133 average time 0.061255490225050835 iter num 40\n",
            "loss 2.3721758043393493 average time 0.051129652283369374 iter num 60\n",
            "loss 1.1444867413956672 average time 0.04607884008752876 iter num 80\n",
            "loss 2.209834783570841 average time 0.042954713600042854 iter num 100\n",
            "loss 2.6124389842152596 average time 0.09175416049997694 iter num 20\n",
            "loss 0.9834342927206308 average time 0.06125484959998175 iter num 40\n",
            "loss 2.792476734612137 average time 0.05111390214998058 iter num 60\n",
            "loss 2.1115579875186086 average time 0.04614719472497768 iter num 80\n",
            "loss 0.9867292828857899 average time 0.043305409689983206 iter num 100\n",
            "loss 1.011332860798575 average time 0.09183288369999901 iter num 20\n",
            "loss 1.9219936802983284 average time 0.06137746772499213 iter num 40\n",
            "loss 1.3372776447795331 average time 0.05127697378331959 iter num 60\n",
            "loss 1.4320726040750742 average time 0.04620104566249097 iter num 80\n",
            "loss 2.2314903617370874 average time 0.043163462649995384 iter num 100\n",
            "loss 2.7614738792181015 average time 0.09116337130003557 iter num 20\n",
            "loss 2.3903543478809297 average time 0.06118273615004455 iter num 40\n",
            "loss 1.7073846538551152 average time 0.0511468918000143 iter num 60\n",
            "loss 1.330780505668372 average time 0.04609654092502069 iter num 80\n",
            "loss 1.2723520922008902 average time 0.0430602570600513 iter num 100\n",
            "loss 2.1852334612049162 average time 0.090888298749951 iter num 20\n",
            "loss 1.0937797924270853 average time 0.061268637849980226 iter num 40\n",
            "loss 1.4032333274371922 average time 0.05118225604996951 iter num 60\n",
            "loss 1.2712212628684938 average time 0.04606264954998096 iter num 80\n",
            "loss 1.2058595893904567 average time 0.04306862134998482 iter num 100\n",
            "loss 2.3137438984122127 average time 0.09138377679987571 iter num 20\n",
            "loss 2.568161580711603 average time 0.06102660112489957 iter num 40\n",
            "loss 1.1012916365871206 average time 0.05101106689990047 iter num 60\n",
            "loss 1.3204202696215361 average time 0.046025375787417035 iter num 80\n",
            "loss 1.534197072032839 average time 0.04303570837992993 iter num 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b709b78c9cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_BATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m           \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m           \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/random.py\u001b[0m in \u001b[0;36mPRNGKey\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    103\u001b[0m   \"\"\"\n\u001b[1;32m    104\u001b[0m   return _return_prng_keys(\n\u001b[0;32m--> 105\u001b[0;31m       True, prng.seed_with_impl(prng.threefry_prng_impl, seed))\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKeyArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mKeyArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/prng.py\u001b[0m in \u001b[0;36mseed_with_impl\u001b[0;34m(impl, seed)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseed_with_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPRNGImpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPRNGKeyArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mPRNGKeyArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/prng.py\u001b[0m in \u001b[0;36mthreefry_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0mk1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift_right_logical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m   \u001b[0mk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitwise_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0xFFFFFFFF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mshift_right_logical\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshift_right_logical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m   \u001b[0;34mr\"\"\"Elementwise logical right shift: :math:`x \\gg y`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshift_right_logical_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    266\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27b9362-f56b-4157-e6e1-ce8f893451a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_test_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b77202-8fd2-4d77-c903-4dbb966b8a5d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074777c5-742d-4798-bdcb-fd27ebd76b56"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_test_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26ccf0a-1471-475d-b8c5-75ea0637ded4"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47719093-ab2f-44c9-d784-dd3c48fb3722"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1000000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 30)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_test_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 9.12090647034347 average time 0.09411642375007431 iter num 20\n",
            "loss 0.8995448297355324 average time 0.05216499059993111 iter num 40\n",
            "loss 0.29933053156128153 average time 0.038401626166705685 iter num 60\n",
            "loss 0.10731004294939339 average time 0.03147281592500804 iter num 80\n",
            "loss 0.226803203986492 average time 0.027255156449991772 iter num 100\n",
            "loss 0.19930361304432154 average time 0.09348549954966075 iter num 20\n",
            "loss 0.4021249696961604 average time 0.051847731474936154 iter num 40\n",
            "loss 0.43462783651193604 average time 0.03798604956658285 iter num 60\n",
            "loss 0.22722966605215333 average time 0.03106180361241968 iter num 80\n",
            "loss 0.5652793697663583 average time 0.026907684599937056 iter num 100\n",
            "loss 0.4850328332395293 average time 0.09363120119987797 iter num 20\n",
            "loss 0.3098317392868921 average time 0.05211543697491834 iter num 40\n",
            "loss 0.29846811230527237 average time 0.038207844866641 iter num 60\n",
            "loss 0.3846608888125047 average time 0.03121005851241989 iter num 80\n",
            "loss 0.2695314833545126 average time 0.027026031049917947 iter num 100\n",
            "loss 0.3438072599237785 average time 0.09383216399992307 iter num 20\n",
            "loss 0.7860939513193443 average time 0.0520288292499572 iter num 40\n",
            "loss 0.37763376894872636 average time 0.03812121866661376 iter num 60\n",
            "loss 0.32938729418674484 average time 0.031179463124885842 iter num 80\n",
            "loss 0.5110312122269534 average time 0.02701080713992269 iter num 100\n",
            "loss 0.24904114980017766 average time 0.09382771480022711 iter num 20\n",
            "loss 1.2239730858709663 average time 0.052037630200175045 iter num 40\n",
            "loss 0.6125723302830011 average time 0.0381337351833281 iter num 60\n",
            "loss 0.1748250360833481 average time 0.03117218582503938 iter num 80\n",
            "loss 0.36633966374211013 average time 0.027057584960093663 iter num 100\n",
            "loss 0.4769454972120002 average time 0.09350251610021587 iter num 20\n",
            "loss 0.13670096450368874 average time 0.05183708780014058 iter num 40\n",
            "loss 0.3230808215448633 average time 0.03797491225004705 iter num 60\n",
            "loss 0.09403138392372057 average time 0.031053102300029424 iter num 80\n",
            "loss 0.15658422853448428 average time 0.026872474270039675 iter num 100\n",
            "loss 1.594636996742338 average time 0.09321502720003991 iter num 20\n",
            "loss 3.442135639488697 average time 0.05184089762506119 iter num 40\n",
            "loss 0.29440008802339435 average time 0.0379640511332885 iter num 60\n",
            "loss 0.31858689908403903 average time 0.031126457624918658 iter num 80\n",
            "loss 0.10520911928324495 average time 0.027003680529942356 iter num 100\n",
            "loss 0.7227698370115831 average time 0.09325356089984779 iter num 20\n",
            "loss 0.27393907657824457 average time 0.05172441109993997 iter num 40\n",
            "loss 0.791247803135775 average time 0.03795140824995542 iter num 60\n",
            "loss 0.31866318749962375 average time 0.031006942262456504 iter num 80\n",
            "loss 0.4705472383648157 average time 0.026837486940003145 iter num 100\n",
            "loss 0.1837948730099015 average time 0.09319557154994981 iter num 20\n",
            "loss 0.8284892828669399 average time 0.05174526214996149 iter num 40\n",
            "loss 0.15799418179085478 average time 0.03797309109998726 iter num 60\n",
            "loss 0.10902705980697647 average time 0.03111634522490476 iter num 80\n",
            "loss 0.05857436917722225 average time 0.0269897410998783 iter num 100\n",
            "loss 0.14638680113421287 average time 0.09365075364976291 iter num 20\n",
            "loss 0.8966093446360901 average time 0.05192845457486328 iter num 40\n",
            "loss 0.4956115299137309 average time 0.03804251751650251 iter num 60\n",
            "loss 0.28606147679965943 average time 0.031098120662363726 iter num 80\n",
            "loss 0.30531355150742456 average time 0.026949755169862328 iter num 100\n",
            "loss 4.759716393891722 average time 0.09336046835014714 iter num 20\n",
            "loss 0.8193186658900231 average time 0.05189584780009682 iter num 40\n",
            "loss 0.1869282641564496 average time 0.03811958700001317 iter num 60\n",
            "loss 0.22576849005417898 average time 0.031128875362514918 iter num 80\n",
            "loss 0.6794159708078951 average time 0.026995491229990877 iter num 100\n",
            "loss 0.222443813981954 average time 0.09367545714985681 iter num 20\n",
            "loss 0.37173980672378093 average time 0.05211468492484528 iter num 40\n",
            "loss 0.34732394851744175 average time 0.03815523643315828 iter num 60\n",
            "loss 0.8294135477626696 average time 0.031205156662372245 iter num 80\n",
            "loss 0.04380408427095972 average time 0.027012110369905713 iter num 100\n",
            "loss 0.34579898056108505 average time 0.0937038111498623 iter num 20\n",
            "loss 1.1678745795506984 average time 0.05198470035006721 iter num 40\n",
            "loss 0.1579134914209135 average time 0.03819864906681687 iter num 60\n",
            "loss 0.302143507724395 average time 0.031215810687581325 iter num 80\n",
            "loss 0.18369013560004532 average time 0.027055570800039275 iter num 100\n",
            "loss 0.1107568823499605 average time 0.09330464959966775 iter num 20\n",
            "loss 1.8776344950310886 average time 0.05188521179998133 iter num 40\n",
            "loss 0.5436689389171079 average time 0.03801946703330638 iter num 60\n",
            "loss 0.21754171029897407 average time 0.031092823774952195 iter num 80\n",
            "loss 0.17179494534502737 average time 0.026935436000003393 iter num 100\n",
            "loss 0.33102711313404143 average time 0.09336609424981361 iter num 20\n",
            "loss 1.7814431339502335 average time 0.051783959799968214 iter num 40\n",
            "loss 0.6055039193597622 average time 0.03794319451656823 iter num 60\n",
            "loss 0.10201459190284368 average time 0.03101242318750792 iter num 80\n",
            "loss 0.06931536518095527 average time 0.026843592959994566 iter num 100\n",
            "loss 0.8688322850503027 average time 0.09312345179996555 iter num 20\n",
            "loss 0.5516469536814839 average time 0.05184300022492607 iter num 40\n",
            "loss 1.3833311095368117 average time 0.03794631348328039 iter num 60\n",
            "loss 0.16012496416806243 average time 0.03113244933745136 iter num 80\n",
            "loss 0.03760476374736754 average time 0.026951200919920665 iter num 100\n",
            "loss 0.21649211703334004 average time 0.09353207909980484 iter num 20\n",
            "loss 8.125554886646569 average time 0.05184920242472799 iter num 40\n",
            "loss 4.4292089296504855 average time 0.03805065834988757 iter num 60\n",
            "loss 0.2940834383480251 average time 0.031086060124926006 iter num 80\n",
            "loss 0.07673432264709845 average time 0.026903783949946955 iter num 100\n",
            "loss 0.9773058263817802 average time 0.0932809225998426 iter num 20\n",
            "loss 0.12352695193840191 average time 0.05174916147507247 iter num 40\n",
            "loss 0.3639215719886124 average time 0.03802513658338284 iter num 60\n",
            "loss 0.24802822736091912 average time 0.03107753118756591 iter num 80\n",
            "loss 0.07109385933290469 average time 0.02690281168999718 iter num 100\n",
            "loss 0.41431780118728057 average time 0.09338834814998336 iter num 20\n",
            "loss 0.3351931445649825 average time 0.05208396322509543 iter num 40\n",
            "loss 0.31133138691075146 average time 0.03819932720007273 iter num 60\n",
            "loss 0.1842238452809397 average time 0.0312689859750435 iter num 80\n",
            "loss 0.051896367949666455 average time 0.02708696249002969 iter num 100\n",
            "loss 1.7480757378507406 average time 0.0937876909500119 iter num 20\n",
            "loss 0.219899884541519 average time 0.052444480924987144 iter num 40\n",
            "loss 0.10226502126897685 average time 0.03834657649995279 iter num 60\n",
            "loss 0.24009368644328788 average time 0.03132136049991914 iter num 80\n",
            "loss 0.24014621885726228 average time 0.027140851309995925 iter num 100\n",
            "loss 2.545270835980773 average time 0.09338738250025927 iter num 20\n",
            "loss 2.5039358297362924 average time 0.05179337295016921 iter num 40\n",
            "loss 5.538453697226942 average time 0.037938236500091684 iter num 60\n",
            "loss 1.1775679013226181 average time 0.031037469312605027 iter num 80\n",
            "loss 0.30175200663506985 average time 0.02694197877004626 iter num 100\n",
            "loss 0.49741604016162455 average time 0.09339666764972207 iter num 20\n",
            "loss 0.39959719288162887 average time 0.05179196309973122 iter num 40\n",
            "loss 0.08572991646360606 average time 0.03808713129974421 iter num 60\n",
            "loss 0.28230737370904535 average time 0.031197007737250714 iter num 80\n",
            "loss 0.35906930861528963 average time 0.02704659325978355 iter num 100\n",
            "loss 0.8610064105596393 average time 0.09326738879990444 iter num 20\n",
            "loss 0.3908637154381722 average time 0.0518325238750549 iter num 40\n",
            "loss 0.1821006298996508 average time 0.03805736903338281 iter num 60\n",
            "loss 0.12010298632958438 average time 0.03109170682507738 iter num 80\n",
            "loss 0.9169136319542304 average time 0.026973948829981963 iter num 100\n",
            "loss 0.22417189029511064 average time 0.09370042225009456 iter num 20\n",
            "loss 0.3441274384385906 average time 0.0520193876000576 iter num 40\n",
            "loss 0.08118657206068747 average time 0.038079945616694505 iter num 60\n",
            "loss 0.5027064617024735 average time 0.031143596575020638 iter num 80\n",
            "loss 0.2307109025423415 average time 0.0269978685800379 iter num 100\n",
            "loss 0.2969890738313552 average time 0.0932966155000031 iter num 20\n",
            "loss 0.2546803079894744 average time 0.05181367530003626 iter num 40\n",
            "loss 0.2981583929795306 average time 0.03800279826658273 iter num 60\n",
            "loss 0.1628561585675925 average time 0.031076371649942303 iter num 80\n",
            "loss 0.2127793777617626 average time 0.02693721696994544 iter num 100\n",
            "loss 0.5230181704973802 average time 0.09370158219999211 iter num 20\n",
            "loss 0.3896653652191162 average time 0.0519950540249738 iter num 40\n",
            "loss 0.4335261837695725 average time 0.038082335616612305 iter num 60\n",
            "loss 0.5099261761642992 average time 0.03111113769991789 iter num 80\n",
            "loss 0.1505427462689113 average time 0.026963973909896594 iter num 100\n",
            "loss 0.32748313969932497 average time 0.09330768825011546 iter num 20\n",
            "loss 0.0690694514560164 average time 0.051863113100034754 iter num 40\n",
            "loss 0.07976962479006033 average time 0.03801722950001931 iter num 60\n",
            "loss 0.16954145394265652 average time 0.03112396033754976 iter num 80\n",
            "loss 0.5020621028961614 average time 0.026944275690038922 iter num 100\n",
            "loss 0.6227754784049466 average time 0.09391860665000422 iter num 20\n",
            "loss 1.5859765699133277 average time 0.052204777625001954 iter num 40\n",
            "loss 4.0154808084480464 average time 0.03821143123332149 iter num 60\n",
            "loss 0.1483701908000512 average time 0.031217580087559326 iter num 80\n",
            "loss 0.07028805612208089 average time 0.02702295696004512 iter num 100\n",
            "loss 0.8502694254275411 average time 0.09342393544984588 iter num 20\n",
            "loss 4.270700737833977 average time 0.051874880624927754 iter num 40\n",
            "loss 1.492823357693851 average time 0.03800438581668762 iter num 60\n",
            "loss 0.2453289198456332 average time 0.031053179987566182 iter num 80\n",
            "loss 0.05410070571087999 average time 0.026922446450025745 iter num 100\n",
            "loss 0.22978645574767143 average time 0.09323427789995549 iter num 20\n",
            "loss 1.9836204592138529 average time 0.05176927742513726 iter num 40\n",
            "loss 0.3630317223723978 average time 0.037932000200089536 iter num 60\n",
            "loss 0.2736523310886696 average time 0.031039595887568792 iter num 80\n",
            "loss 0.4369873204268515 average time 0.026883898500091163 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0694b175-e51b-4585-e312-bc0681571ded"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 0.8, 0.8, 0.25, 0.05, 0.05]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.098688, 0.627409)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.0861]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5437], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2AXrPt7bNj",
        "outputId": "b1af42c3-0211-400e-b37a-6b56cceda831"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([0.8]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.8\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09872279\n",
            "[0.62781]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "lwApH0GT9bBK",
        "outputId": "472b1b3e-e457-4eaf-8650-35238ae8f72d"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.8, S, 0.25, 0.05, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(0, 1, 0.01)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb810bf69d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8dcnG0mAhCVhX4IYEGRRCaBVK+5bC52qFa21tvy01WpnrD87djrjtHacTlvb33Q6/mrdat3q2jrYYulMS1VUFBRRtkgMW4iBhCUrWe9n/rgXG2MgAXJzcu95Px+PPHLuud/cfL4k3HfO+Z7z/Zq7IyIi4ZUSdAEiIhIsBYGISMgpCEREQk5BICIScgoCEZGQSwu6gMOVl5fnBQUFQZchIpJQ3nzzzSp3z+/suYQLgoKCAlatWhV0GSIiCcXMth7sOZ0aEhEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkEu4+AhGRsIhEnG17GthYUUtxRS1nTxnGtNG5Pf59FAQiIn1AbWMLGytqWV9ew8aKGtZ/UMt7FbXsb2kDwAyGDMhQEIiIJDp3Z1dtE2t3VLO+vIZ15TWs/6CGbXsaPmwzKDudKSNyWDhnLFNG5DB5xEAKhw8gOyM+b9kKAhGROHF3KmubWFNWzbtl+3hnRzVrd9RQVdf0YZuCodlMG53D54rGMHVUDlNG5jAiJxMz67U6FQQiIkfB3dld38zmqno2V9ZTWlXP1t31bNvTwLY9DdQ2tgKQYlA4bCBnTMpn2ugcpo3OZcrIHAb0C/5tOPgKRET6sOqGFrbsrmdvQzP7GlqoqmuiorqRnbVNbNvTQGll3Ydv9gDpqcbYwdmMG5rNrPGDKRjanxljcpk6Kidup3aOVt+sSkSkD4hEnAt/+hLl1Y0f2Z+ZnsKInExGD87iMyeMZkJefybk92di3gBGDcokLTWxrsxXEIiIHMT6D2oor27ka2dO5KzjhpGblUH+gH7kZKX16jn8eFMQiIgcxPKSKgC+eEoBw3IyA64mfhLr+EVEpBe9vKmS40YMTOoQAAWBiEin9je3sXLzXk4vzAu6lLhTEIiIdOL1zbtpbotwemGny/wmFQWBiEgnXt5URUZaCnMmDAm6lLhTEIiIdOLlTZXMnTCEzPTUoEuJOwWBiEgHFdWNvLezjtOOTf7xAVAQiIh8zMubKgFCMT4ACgIRkY9ZXlJF3oB+HDdiYNCl9AoFgYhIO5GIs3xTFacX5pGSkjx3Dx+KgkBEpJ2NFbXsrm/m1JCMD4CCQETkI159PzqtRFgGikFBICLyEctLqjgmvz8jcpN7Won2FAQiIjHNrRHe2LyHUyeG52gAFAQiIh9aU7aPhuY2Tj12aNCl9Kq4BoGZXWBmxWZWYma3dfL8ODNbZmarzewdM7sonvWIiBzKKyVVmMHJxygIeoSZpQJ3AxcCU4ErzGxqh2b/CDzl7icCC4H/H696RES68mrJbqaPzmVQdkbQpfSqeB4RzAFK3L3U3ZuBJ4AFHdo4kBPbzgXK41iPiMhB1Te1snr7Xj4RsvEBiG8QjAa2t3tcFtvX3neAq8ysDFgC3NTZC5nZdWa2ysxWVVZWxqNWEQm5N7bsoaXNQzc+AMEPFl8BPOTuY4CLgEfM7GM1ufu97l7k7kX5+eGY+0NEeterJVVkpKZQND75p53uKJ5BsAMY2+7xmNi+9hYBTwG4+2tAJhC+4zIRCdwrJbs5afwgsjKSf9rpjuIZBCuBQjObYGYZRAeDF3dosw04G8DMphANAp37EZFetbuuifUf1ITu/oED4hYE7t4K3AgsBTYQvTponZndYWbzY81uAa41szXAr4Fr3N3jVZOISGdefC/69+e8ycMCriQYafF8cXdfQnQQuP2+29ttrwdOjWcNIiJdWVZcSd6Afhw/Kqfrxkko6MFiEZFAtbZFeOm9SuZNzg/NtNMdKQhEJNTe3r6P6v0tnHVcOE8LgYJAREJuWfEuUlOM0wrDOVAMCgIRCbllGyspGj+YnMz0oEsJjIJAREKrorqR9R/UcGaITwuBgkBEQuzF93YBcGZILxs9QEEgIqG1bGMlo3IzmTR8QNClBEpBICKh1NwaYXlJFfOOG4ZZOC8bPUBBICKhtKJ0N3VNrZwV8tNCoCAQkZBauq6C7IzUUF82eoCCQERCJxJx/rh+J/Mm55OZHr7ZRjtSEIhI6KzevpfK2ibOP35E0KX0CQoCEQmdpet2kp5qob9/4AAFgYiEiruzdF0Fn5iYF+q7idtTEIhIqGysqGXr7gadFmpHQSAiobJ0XQVmcO7U4UGX0mcoCEQkVJau28mscYPJH9gv6FL6DAWBiITGtt0NbPigRqeFOlAQiEhoPP9OOQAXTFMQtKcgEJHQWPx2ObPGD2bskOygS+lTFAQiEgrFFbUU76xl/sxRQZfS5ygIRCQUFq/ZQYrBRdNHBl1Kn6MgEJGk5+48v+YDTj02T1cLdUJBICJJ7+3t+9i2p4FP67RQpxQEIpL0Fq8pJyM1RZeNHoSCQESSWlvE+d07HzBvcj65WZpbqDMKAhFJaitKd1NZ28T8E3Ra6GAUBCKS1J5etZ2BmWmcM0VzCx2MgkBEklZNYwsvrK1gwQmjtBLZISgIRCRpPb+mnKbWCJfNGht0KX2agkBEktbTq8qYPHwgM8bkBl1Kn6YgEJGk9N7OWt7evo/LisZgZkGX06cpCEQkKT29ajtpKcZnThwddCl9noJARJJOS1uE367ewdlThpE3QFNKdCWuQWBmF5hZsZmVmNltB2nzOTNbb2brzOzxeNYjIuHw5427qKpr1iBxN6XF64XNLBW4GzgXKANWmtlid1/frk0h8C3gVHffa2bD4lWPiITHI69tZWRuJvMm5wddSkKI5xHBHKDE3UvdvRl4AljQoc21wN3uvhfA3XfFsR4RCYH3K+tYXlLFlXPGkZaqs9/dEc9/pdHA9naPy2L72psETDKzV8xshZld0NkLmdl1ZrbKzFZVVlbGqVwRSQaPvLaV9FRj4ZxxQZeSMIKOyzSgEJgHXAHcZ2aDOjZy93vdvcjdi/LzdagnIp2rb2rl2TfLuGj6SK07cBjiGQQ7gPYjNWNi+9orAxa7e4u7bwbeIxoMIiKH7bm3d1Db1MrVpxQEXUpCiWcQrAQKzWyCmWUAC4HFHdo8R/RoADPLI3qqqDSONYlIknJ3Hn51K8ePyuGkcR87sSCHELcgcPdW4EZgKbABeMrd15nZHWY2P9ZsKbDbzNYDy4Bb3X13vGoSkeT1xuY9FO+s5epTxutO4sMUt8tHAdx9CbCkw77b22078I3Yh4jIEbt/+WYGZaczf6buJD5cQQ8Wi4gctfcr6/ifDTu5+uTxZGVouunDpSAQkYR3/8ulZKSmcPUnCoIuJSEpCEQkoe2qbeTZt3ZwyawxmlfoCCkIRCSh/erVLbS0Rbj29GOCLiVhKQhEJGHVN7Xy6IptnDd1OBPy+gddTsJSEIhIwnpi5Xaq97dw3ScnBl1KQlMQiEhCamxp454X32fuhCHMGj846HISmoJARBLSY69vo7K2iZvPnRR0KQlPQSAiCWd/cxs//8v7nHLMUE4+ZmjQ5SQ8BYGIJJzHXt9KVV0Tf3eO5qjsCQoCEUko+5vbuOfFUj4xcShzdTTQIxQEIpJQHlmxhao6jQ30JAWBiCSM6oYW7l72PqcX5jG7YEjQ5SQNBYGIJIy7/1JCTWML/3DRlKBLSSoKAhFJCNv3NPDQK1u45KQxTBmZE3Q5SaVb6xGYWSHwfWAqkHlgv7trcg8R6RV3/bGYlBS45TyNDfS07h4R/BL4OdAKnAk8DDwar6JERNp7t6ya/3q7nEWnTWBkblbQ5SSd7gZBlrv/CTB33+ru3wEujl9ZIiJR7s73freeIf0z+OoZmlMoHrq7VGWTmaUAm8zsRmAHMCB+ZYmIRC1eU84bW/bwb5+dzsDM9KDLSUrdPSL4WyAb+DowC7gKuDpeRYmIANQ1tXLn7zcwc0wunysaG3Q5Sau7QVDg7nXuXubuX3L3S4Bx8SxMRORnf9rErtomvrtgGikpFnQ5Sau7QfCtbu4TEekRJbvqeGD5Zi4vGssJYwcFXU5SO+QYgZldCFwEjDaz/2j3VA7RK4hERHqcu3P7f60lOyOVb14wOehykl5Xg8XlwJvA/NjnA2qBm+NVlIiE2zNvlvHq+7v5l89MY6gWpI+7QwaBu68B1pjZo+6uIwARibuquibuXLKB2QWDuXKOhiJ7Q1enht4FPLb9sefdfUZ8yhKRsLrj+fU0NLXx/c9O1wBxL+nq1NCneqUKERFgWfEuFq8p5+/OKeTYYQODLic0ujo1tPXAtpmNBwrd/X/MLKurrxURORw1jS18+zfvcuywAVw/T3cQ96ZuXT5qZtcCzwC/iO0aAzwXr6JEJHy+u3g9O2ubuOuymfRLSw26nFDp7n0EXwNOBWoA3H0TMCxeRYlIuPxhbQXPvlXG1+ZN1D0DAehuEDS5e/OBB2aWRmwQWUTkaFTVNfHt377L8aNyuPEsLUYfhO6e53/RzP4ByDKzc4EbgOfjV5aIhIG7863fvEttUyu/vvwEMtK0VlYQuvuvfhtQCbwLfAVYAvxjvIoSkXB4ZMVW/nv9Tr55/mQmDddVQkHpVhC4e4To4PAN7n6pu9/n7l2eGjKzC8ys2MxKzOy2Q7S7xMzczIq6X7qIJLJ15dX8y+82cObkfL586oSgywm1QwaBRX3HzKqAYqDYzCrN7PauXtjMUoG7gQuJLnF5hZlN7aTdQKLTXL9+JB0QkcRT39TKTY+vZnD/dO66bKZuHAtYV0cENxO9Wmi2uw9x9yHAXOBUM+tqrqE5QIm7l8YGmp8AFnTS7nvAD4DGwytdRBKRu/NPz61ly+56/v3yEzWXUB/QVRB8AbjC3Tcf2OHupXRvYZrRwPZ2j8ti+z5kZicBY939992uWEQS2mOvb+M3q3dw01mFnDJxaNDlCF0HQbq7V3Xc6e6VwFGtGRdb+vInwC3daHudma0ys1WVlZVH821FJEBvbdvLd59fxxmT8vn62bpUtK/oKgiaj/A5iK5r3H5tuTGxfQcMBKYBfzGzLcDJwOLOBozd/V53L3L3ovz8/C6+rYj0RVV1Tdzw6FuMyM3kpwtPIFXjAn1GV/cRzDSzmk72G5DZxdeuBArNbALRAFgIXHngSXevBvI+fEGzvwD/191XdaNuEUkgLW0Rbnz8LfY2NPPs9Z9gUHZG0CVJO11NOnfEE364e6uZ3QgsBVKBB919nZndAaxy98VH+toikli++/w6VpTu4Sefm8m00blBlyMdxHUGUXdfQvTms/b7Or301N3nxbMWEQnGw69t4dEV2/jKGcfw2ZPGBF2OdEL3c4tI3Ly8qZLvPr+ec6YM45vnHxd0OXIQCgIRiYtNO2v52mNvUThsAP++8EQNDvdhCgIR6XG7ahq55pcryUhL5b6rixjQT+tY9WUKAhHpUfVNrXz5VyvZU9/ML6+Zzdgh2UGXJF1QEIhIj2mNXSa6vryGuz9/ItPH6AqhRKDjNRHpEZGIc+sz77CsuJI7/2YaZx03POiSpJt0RCAiR83duX3xWn67ege3nj+Zz88dH3RJchgUBCJy1H7wh2IeXbGNr54xkRvmTQy6HDlMOjUkIkfM3fl//7OJe158n6tOHsffXzAZM10mmmgUBCJyRNydH//xPf5zWQmXF43ljvnTFAIJSkEgIofN3fnh0mJ+/pf3uWLOWO78zHStMpbAFAQiclgiEeeO363noVe38Pm54/jegmkKgQSnIBCRbmtpi3Dr02t47u1yFp02gW9fNEUhkAQUBCLSLfub27jhsTdZVlzJredP5oZ5EzUmkCQUBCLSpZrGFhY9tJJVW/fy/c9O54o544IuSXqQgkBEDqmqrokvPvgG7+2s5T+vOImLZ4wMuiTpYQoCETmoiupGrrxvBeXV+7nv6iLmTR4WdEkSBwoCEelUJOL83ZOr2VnTyCOL5jK7YEjQJUmcaIoJEenU429sY0XpHv7pU1MVAklOQSAiH1O2t4HvL9nAacfmcfnssUGXI3GmIBCRj3B3vvWbd3Hg+5+drktEQ0BBICIf8fSqMl7eVMVtFx6n1cVCQkEgIh/avqeBO363nrkThnCV1hQIDQWBiADQFnFueXoNAD/+3ExNHREiCgIRAeCB5aW8sXkP//zpqYwZrFNCYaIgEBGKK2q5a+l7nDd1OJfOGhN0OdLLFAQiIdfY0sbfPrGanKw0XSUUUrqzWCTk7vz9BjZW1PLQl2YzdEC/oMuRAOiIQCTE/rC2gkdWbOXa0ydoHqEQUxCIhNSOffv5+2ffYfroXG49/7igy5EAKQhEQqi5NcLXf72a1rYIP7viRDLS9FYQZhojEAmhf12ygTe37uVnV5xIQV7/oMuRgOnPAJGQ+a+3d/DQq1v48qkT+PTMUUGXI32AgkAkRIorarnt2XeZXTCYb12kcQGJimsQmNkFZlZsZiVmdlsnz3/DzNab2Ttm9icz0+QmInGyt76Z6x5ZxYDMNO6+8iTSU/V3oETF7TfBzFKBu4ELganAFWY2tUOz1UCRu88AngF+GK96RMKsuTXCVx99kw+qG7nnqlkMy8kMuiTpQ+L5J8EcoMTdS929GXgCWNC+gbsvc/eG2MMVgO5tF+lh7s4/PbeW1zfv4YeXzGDW+MFBlyR9TDyDYDSwvd3jsti+g1kEvNDZE2Z2nZmtMrNVlZWVPViiSPK7/+XNPLlqOzeddSyfOfFQ/wUlrPrESUIzuwooAn7U2fPufq+7F7l7UX5+fu8WJ5LAFq8p519f2MDF00dy8zmTgi5H+qh43kewA2i/2OmY2L6PMLNzgG8DZ7h7UxzrEQmV5ZuquOWpt5ldMETrC8ghxfOIYCVQaGYTzCwDWAgsbt/AzE4EfgHMd/ddcaxFJFTW7qjmK4+sYmL+AO67uojM9NSgS5I+LG5B4O6twI3AUmAD8JS7rzOzO8xsfqzZj4ABwNNm9raZLT7Iy4lIN5XsquWaX77BoOwMfvXlOeRmpQddkvRxcZ1iwt2XAEs67Lu93fY58fz+ImGzuaqeK+97HTAeXjSH4bpMVLqhTwwWi8jR276ngSvvW0FrxHn82rlMzB8QdEmSIBQEIklg6+56Ft67gobmNh5dNJdJwwcGXZIkEM0+KpLgSnbV8vn7X6epNcKji+YydVRO0CVJglEQiCSwdeXVfOGBN0hNMZ687hQmj9CRgBw+nRoSSVArSnez8N4VZKal8NRXFAJy5BQEIgno+TXlXP3AGwzPyeTp6z/BBC0uI0dBp4ZEEoi7c//Lm7lzyQbmTBjCfV8oIjdb9wnI0VEQiCSIptY2bn9uHU+u2s7FM0by48tm6o5h6REKApEEUFnbxFcffZM3t+7lprOO5eZzJmnuIOkxCgKRPu6tbXv52mNvsa+hhbuvPImLZ4wMuiRJMgoCkT7K3Xlg+Wb+7YWNjMjN5JnrT+H4UblBlyVJSEEg0gdVN7TwzWfXsHTdTs6dOpy7Lp2pQWGJGwWBSB/zSkkVtzy1hqq6Jv7x4iksOm0CZhoPkPhREIj0EY0tbfzwD8U8+MpmJub3576rT2X6GJ0KkvhTEIj0Aa+X7ua237zL5qp6rj5lPN+6cApZGbo0VHqHgkAkQDWNLfzghY089vo2xg7J4tFFczmtMC/osiRkFAQiAXB3fvPWDr7/wkb21Dfxf06bwDfOm0R2hv5LSu/Tb51IL1u7o5rvLF7Hqq17OWHsIH55zWyNBUigFAQivWTHvv3ctbSY367ewdD+GfzwkhlcOmuM7hCWwCkIROJsd10T97z4Pr96bSsGXD9vItfPm0hOpu4LkL5BQSASJ/samrn3pVIeenULjS1t/M2JY7jlvEmMGpQVdGkiH6EgEOlhFdWN3P9yKY+/sY39LW18asYo/vbsQo4dpsXkpW9SEIj0kI0VNfxy+RZ+u3oHbe7MnzmKr54xUSuHSZ+nIBA5Cq1tEf60cRcPvbKF10p3k5WeyuWzx3LdJ49h7JDsoMsT6RYFgcgRKN+3nydWbuepldupqGlkZG4mt114HAtnj2VQdkbQ5YkcFgWBSDc1NLeydF0Fz765g1ferwLgk4X5fHfB8Zx93DDSUrUEuCQmBYHIITS1tvHye1U8/045/71+Jw3NbYwdksXXzyrk0lljdPpHkoKCQKSD+qZWXnyvkqXrKvjzxl3UNrYyKDudBSeMZsEJo5hTMEQ3gUlSURCIANv3NLCseBd/2rCL10p309waYXB2OhccP4KLZozktGPzSNepH0lSCgIJpX0Nzby+eQ/LN1WxvKSKzVX1ABQMzebzc8dx7tThzCkYovP+EgoKAgmFnTWNvLl1Lyu37GFF6R42VtTgDtkZqZx8zFC+cPJ4zjxuGBPy+gddqkivUxBI0mlobmVdeQ1rtu/j7dhH2d79AGSmpzBr/GBuPmcSJx8zlBPGDiIjTX/1S7gpCCRhuTu7apvY8EENxRW1rP+ghrU7qimtqsc92mb0oCxmjs3lS6dOYNb4wUwdmaM3fpEOFATS57W2RSjbu5/SqjpKK+t5v7KOTTvr2LSrjur9LR+2G5mbyfGjcvn0zFFMG5XLjLG5DBuYGWDlIokhrkFgZhcAPwVSgfvd/d86PN8PeBiYBewGLnf3LfGsSfoed6eqrpnyffvZsW8/2/c0ULZ3P1v3NLBtdz1le/fTGvEP2w/OTqdw+EA+NWMkhcMGMGVkDseNyCE3W9M6ixyJuAWBmaUCdwPnAmXASjNb7O7r2zVbBOx192PNbCHwA+DyeNUkvcvdqdnfSmVdE1V1TeyqbWJXTSOVtU1U1DRSUd3IzppGyqsbaW6NfORrc7PSGTckm+NH53LxjJGMH9qfifn9OSZvAIP7awoHkZ4UzyOCOUCJu5cCmNkTwAKgfRAsAL4T234G+E8zM3d3pM+IRJy65lZqG1upbWyhZn8rNftbqN7fwr7Y5731zextaGZfQwu765vZU9/E3voWmtsiH3u9jNQUhuf2Y2ROFtPHDOL84zMZNSiLkbmZjBmczZghWVq0RaQXxTMIRgPb2z0uA+YerI27t5pZNTAUqGrfyMyuA64DGDduXLzq7fMiEac14rRFnJZIhNY2p7UtQksk9rktQnOr09wWobk19tHWRlNLhKbWCE2tbTS2RGhsiX7e39JGY0sbDc2tNDS3sb+5jfrYdl1TK/VNrdQ3RbcPxSz6F/zg7AwGZaczelAm00fnMKR/P/IGZJA/sB9D+/djWE4/hg3sR25WOma6M1ekr0iIwWJ3vxe4F6CoqOiIjhbuf7mUu/5YTIpZ7ANSUv66bWYYfPSxRd/kjNg2f21H7H3swL5Oao5+/nBHdNvdcSDiTiT2x3LEnYg7bRFin51IxGk7sO3RAOjp46SMtBSy0lOjHxmpZGek0j8jjSH9Mxg7JJv+Gan075fGwMx0BvZLY0BmGrlZ6eRkppOTFd3OzUpnYGY6qZpyQSRhxTMIdgBj2z0eE9vXWZsyM0sDcokOGve440fl8sVTCj7yhuvuRBzaPPom67E3ZHeI+F/ftP/6+a9v5hB7k2/35uw4Rrs3RPvIpw9DxAxSY8liGKkp0QAyi26nxrbTUozUFCMlxUhPMVJTUkhNgbTUFNJSos+npaaQnmqkpaSQkZZCemoKGWlGv7RUMtJSyEhNoV96yoePM9NSyMpIpV9aqt68RQSIbxCsBArNbALRN/yFwJUd2iwGvgi8BlwK/Dle4wOnTBzKKROHxuOlRUQSWtyCIHbO/0ZgKdHLRx9093Vmdgewyt0XAw8Aj5hZCbCHaFiIiEgviusYgbsvAZZ02Hd7u+1G4LJ41iAiIoeme+1FREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiFniTa/m5lVAluP8Mvz6DCPUUiEsd9h7DOEs99h7DMcfr/Hu3t+Z08kXBAcDTNb5e5FQdfR28LY7zD2GcLZ7zD2GXq23zo1JCIScgoCEZGQC1sQ3Bt0AQEJY7/D2GcIZ7/D2GfowX6HaoxAREQ+LmxHBCIi0oGCQEQk5JIyCMzsAjMrNrMSM7utk+f7mdmTsedfN7OC3q+yZ3Wjz98ws/Vm9o6Z/cnMxgdRZ0/rqt/t2l1iZm5mCX+ZYXf6bGafi/2815nZ471dYzx043d8nJktM7PVsd/zi4KosyeZ2YNmtsvM1h7keTOz/4j9m7xjZicd0Tfy2JKNyfJBdBGc94FjgAxgDTC1Q5sbgHti2wuBJ4Ouuxf6fCaQHdu+PtH73N1+x9oNBF4CVgBFQdfdCz/rQmA1MDj2eFjQdfdSv+8Fro9tTwW2BF13D/T7k8BJwNqDPH8R8ALRFXFPBl4/ku+TjEcEc4ASdy9192bgCWBBhzYLgF/Ftp8BzrbOVqBPHF322d2XuXtD7OEKomtIJ7ru/KwBvgf8AGjszeLipDt9vha42933Arj7rl6uMR66028HcmLbuUB5L9YXF+7+EtHVGw9mAfCwR60ABpnZyMP9PskYBKOB7e0el8X2ddrG3VuBaiCRFzTuTp/bW0T0r4hE12W/Y4fKY939971ZWBx152c9CZhkZq+Y2Qozu6DXqouf7vT7O8BVZlZGdGXEm3qntEAd7v/9TsV1qUrpe8zsKqAIOCPoWuLNzFKAnwDXBFxKb0sjenpoHtEjv5fMbLq77wu0qvi7AnjI3X9sZqcQXQ99mrtHgi6sr0vGI4IdwNh2j8fE9nXaxszSiB5G7u6V6uKjO33GzM4Bvg3Md/emXqotnrrq90BgGvAXM9tC9Bzq4gQfMO7Oz7oMWOzuLe6+GXiPaDAksu70exHwFIC7vwZkEp2YLZl16/9+V5IxCFYChWY2wcwyiA4GL+7QZjHwxdj2pcCfPTbykqC67LOZnQj8gmgIJMM5Y+ii3+5e7e557l7g7gVEx0bmu/uqYMrtEd35/X6O6NEAZpZH9FRRaW8WGQfd6fc24GwAM5tCNAgqe7XK3rcYuDp29dDJQLW7f3C4L5J0p4bcvdXMbgSWEr3S4EF3X2dmdwCr3H0x8ADRw8YSogMxC4Or+Oh1s88/AgYAT9GQM8MAAAI6SURBVMfGxbe5+/zAiu4B3ex3Uulmn5cC55nZeqANuNXdE/mIt7v9vgW4z8xuJjpwfE2C/4GHmf2aaKjnxcY+/hlIB3D3e4iOhVwElAANwJeO6Psk+L+TiIgcpWQ8NSQiIodBQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiR8jM7ojdpCeS0HT5qMgRMLNUd28Lug6RnqAjApEOzKzAzDaa2WNmtsHMnjGzbDPbYmY/MLO3gMvM7CEzuzT2NbPN7FUzW2Nmb5jZQDNLNbMfmdnK2FzxX4m1HWlmL5nZ22a21sxOD7TDEnpJd2exSA+ZDCxy91fM7EGia1gA7Hb3kyC6UErscwbwJHC5u680sxxgP9G5b6rdfbaZ9QNeMbM/Ap8Flrr7nWaWCmT3btdEPkpBINK57e7+Smz7UeDrse0nO2k7GfjA3VcCuHsNgJmdB8w4cNRAdHLDQqLz5jxoZunAc+7+dpz6INItCgKRznUcPDvwuP4wXsOAm9x96ceeMPskcDHwkJn9xN0fPrIyRY6exghEOjcuNqc9wJXA8kO0LQZGmtlsgNj4QBrRCdKuj/3lj5lNMrP+Fl0veqe73wfcT3QpQpHAKAhEOlcMfM3MNgCDgZ8frGFs6cTLgZ+Z2Rrgv4lOgXw/sB54K7b4+C+IHoXPA9aY2erY1/00jv0Q6ZIuHxXpwMwKgN+5+7SASxHpFToiEBEJOR0RiIiEnI4IRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5P4X+vYdPeV8/l0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}