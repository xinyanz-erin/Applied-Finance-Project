{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "deep_learning_option_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/deep_learning_option_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "6f23e9aa-69cb-4bba-cc9a-20a4f9aa993b"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0  13057      0 --:--:-- --:--:-- --:--:-- 13166\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9MB 55kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.6MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. \n",
        "\n",
        "Loading all the necessary libraries:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6no5JzH-B6"
      },
      "source": [
        "# !pip install cupy-cuda101"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbkx3hXWnwi8"
      },
      "source": [
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBN3YFOnwi-"
      },
      "source": [
        "The CuPy version of batched barrier option pricing simulation is as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhj4DtLnwi-"
      },
      "source": [
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRjmX5zcnwi_"
      },
      "source": [
        "Note, the parameters (K, B, S0, sigma, mu, r) are passed in as an array with length of batch size. The output array is a two dimensional array flatten to 1-D. The first dimension is for Batch and the second dimension is for Path. \n",
        "\n",
        "Testing it out by entering two sets of option parameters:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4PMo7Inwi_"
      },
      "source": [
        "# N_PATHS = 2048000\n",
        "# N_STEPS = 365\n",
        "# N_BATCH = 2\n",
        "# T = 1.0\n",
        "\n",
        "# K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
        "# B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
        "# S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
        "# sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
        "# mu = cupy.array([0.15, 0.1], dtype=cupy.float32)\n",
        "# r =cupy.array([0.05, 0.05], dtype=cupy.float32)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpWK3wcEnwjA"
      },
      "source": [
        "Put everything into a simple function to launch this GPU kernel. The option prices for each batch is the average of the corresponding path terminal values. This can be computed easily by Cupy function `mean(axis=1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAb34NTnwjA"
      },
      "source": [
        "# def batch_run():\n",
        "#     number_of_threads = 256\n",
        "#     number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
        "#     randoms_gpu = cupy.random.normal(0, 1, N_BATCH*N_PATHS * N_STEPS, dtype=cupy.float32)\n",
        "#     output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     s = time.time()\n",
        "#     cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
        "#                        (output, np.float32(T), K, B, S0, sigma, mu, r,\n",
        "#                         randoms_gpu, N_STEPS, N_PATHS, N_BATCH))\n",
        "#     v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     e = time.time()\n",
        "#     print('time', e-s, 'v',v)\n",
        "# batch_run()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRgQCelnwjC"
      },
      "source": [
        "This produces the option prices $21.22$ and $0.848$ for these two sets of option parameters in $66ms$.\n",
        "\n",
        "It works efficiently hence we will construct an `OptionDataSet` class to wrap the above code so we can use it in Pytorch. For every `next` element, it generates uniform distributed random option parameters in the specified range, launches the GPU kernel to compute the option prices, convert the CuPy array to Pytorch tensors with zero copy via the DLPack. Note how we implemented the iterable Dataset interface:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1KUra7ZnwjC"
      },
      "source": [
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo46Vf4XnwjD"
      },
      "source": [
        "Put everything related to Pytorch dataset into a file `cupy_dataset.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQUGMBlnwjE"
      },
      "source": [
        "# #%%writefile cupy_dataset.py \n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "# cupy.cuda.set_allocator(None)\n",
        "\n",
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')\n",
        "\n",
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPAsh7JnwjF"
      },
      "source": [
        "Here is a test code to sample 10 data points with batch size 16:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLKxMF05nwjF"
      },
      "source": [
        "# from cupy_dataset import OptionDataSet\n",
        "# ds = OptionDataSet(10, number_path=100000, batch=16, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlzRTD0nwjG"
      },
      "source": [
        "We can implement the same code by using Numba to accelerate the calculation in GPU:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IsfSwVwnwjG"
      },
      "source": [
        "# import numba\n",
        "# from numba import cuda\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average)\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]:\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                               X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
        "#         o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
        "#         Y = o.mean(axis = 1) \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=1, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "01af1ec4-42a7-4eaa-8aa1-c1b3c1cb5888"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# batch - number of underlyings in one basket option\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "          # X = cupy.random.rand(self.N_STOCKS, 6, dtype=cupy.float32)\n",
        "          X = cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS, 6)\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          # X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32) # parameters\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op, i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "          stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "# ds = NumbaOptionDataSet(2, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBOv_RiBsCWa"
      },
      "source": [
        "### PUI TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BME87CgGsFrd"
      },
      "source": [
        "# %%writefile cupy_dataset.py\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n",
        "\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp2 = math.exp(-r*T)\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)    \n",
        "\n",
        "#     for i in range(ii, N_PATHS, stride): # for each path          \n",
        "#         running_average = 0.0\n",
        "\n",
        "#         for j in range(N_STOCKS): # initialize S0\n",
        "#             s_curr[j] = S0[j]\n",
        "\n",
        "#         for n in range(N_STEPS): # for each step\n",
        "#             s_curr_avg = 0.0\n",
        "\n",
        "#             for j in range(N_STOCKS): # for each stock\n",
        "#                 tmp1 = mu[j]*T/N_STEPS  \n",
        "#                 s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n",
        "#                 s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n",
        "\n",
        "#             # add stock average to running average\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n",
        "\n",
        "#             # compare to barrier\n",
        "#             if running_average <= B:\n",
        "#                 break\n",
        "\n",
        "#         payoff = running_average - K if running_average > K else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "    \n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, number_stocks = 3, batch=1, threads=512, seed=15, T=1):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_STOCKS = number_stocks\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(T)\n",
        "#         self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "\n",
        "#         ############ <new\n",
        "#         self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n",
        "#         self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#         cupy.fill_diagonal(self.Z_cov, 1)\n",
        "#         ############ new>\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "\n",
        "#         X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "\n",
        "#         for i in range(self.N_BATCH): # for each batch\n",
        "#           self.S0 = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 200\n",
        "#           self.K = 110.0\n",
        "#           self.B = 100.0\n",
        "#           self.sigma = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.mu = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.r = 0.05\n",
        "#           self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n",
        "\n",
        "#           ############ <new - add correlation between stocks\n",
        "#           all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "          \n",
        "#           single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n",
        "#                                                                                     self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n",
        "#           Y[i] = self.output.mean()\n",
        "\n",
        "#           ############ <new - combine to get X matrix\n",
        "#           X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n",
        "#                                 self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "        \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "# ds = NumbaOptionDataSet(max_len=10, number_path=100, batch=2, seed=15)\n",
        "# for i in ds:\n",
        "#   print(i)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQt8PqinwjI"
      },
      "source": [
        "# %%writefile model.py\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6, hidden)\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0,\n",
        "#                                            198.0,\n",
        "#                                            200.0,\n",
        "#                                            0.4,\n",
        "#                                            0.2,\n",
        "#                                            0.2,]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6fa8ba-0f93-4403-8017-9398614dcb86"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "                                           200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "                                           200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f2007b-93bc-4bff-d32b-331b5b4ed9c7"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/c3/f472843797b5ccbb2f0e806a6927f52c7c9522bfcea8e7e881d39258368b/pytorch_ignite-0.4.5-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 28.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 31.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 33.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 35.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 35.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 36.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 36.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102kB 36.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 36.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 36.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133kB 36.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 36.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 36.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163kB 36.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 36.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 36.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 36.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204kB 36.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 36.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70765bd-95cb-412d-d9b0-394f36d1d6e9"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.00267992215231061 average time 0.0758091790499958\n",
            "loss 0.0013253286015242338 average time 0.0035648968500015597\n",
            "loss 0.19589868187904358 average time 0.0035413368100029176\n",
            "loss 0.5146300792694092 average time 0.0034633600699999077\n",
            "loss 0.24934127926826477 average time 0.0035134376899981137\n",
            "loss 0.24542927742004395 average time 0.003516448139998829\n",
            "loss 0.6991642117500305 average time 0.003521334749999028\n",
            "loss 0.11588068306446075 average time 0.003468158319994927\n",
            "loss 0.058390479534864426 average time 0.003465099669999745\n",
            "loss 0.26754871010780334 average time 0.003500372050000351\n",
            "loss 0.025374287739396095 average time 0.0035278278100031456\n",
            "loss 0.951458215713501 average time 0.0034409594899972263\n",
            "loss 0.673078179359436 average time 0.0034951599199979453\n",
            "loss 0.1115506961941719 average time 0.0034808028800011924\n",
            "loss 0.2315910905599594 average time 0.0034626999000050775\n",
            "loss 0.05845922976732254 average time 0.0034720193499987316\n",
            "loss 0.35172128677368164 average time 0.00350894776000132\n",
            "loss 0.2233859747648239 average time 0.0034933193799975017\n",
            "loss 0.4409811496734619 average time 0.003586805070000878\n",
            "loss 0.5697194337844849 average time 0.003463944189999211\n",
            "loss 1.091957449913025 average time 0.0034712819600008515\n",
            "loss 0.03765171393752098 average time 0.0034228720300012584\n",
            "loss 0.41409632563591003 average time 0.003474129189999644\n",
            "loss 0.06328479200601578 average time 0.00347186118000252\n",
            "loss 0.8888922929763794 average time 0.003476184890004106\n",
            "loss 0.06430359929800034 average time 0.003497064670000327\n",
            "loss 0.08560744673013687 average time 0.0035406031799988112\n",
            "loss 0.6302568912506104 average time 0.0034864651100036783\n",
            "loss 1.75157630443573 average time 0.003488842030000114\n",
            "loss 0.690954327583313 average time 0.0035230645199999344\n",
            "loss 0.2674294412136078 average time 0.0034728504799971917\n",
            "loss 0.01280911359935999 average time 0.0034454597100005914\n",
            "loss 0.6038992404937744 average time 0.0034138735399994857\n",
            "loss 0.2796708345413208 average time 0.0034657947499977127\n",
            "loss 0.25314638018608093 average time 0.003527340089998461\n",
            "loss 0.12323247641324997 average time 0.003421203700000888\n",
            "loss 1.0753310918807983 average time 0.003445562390008945\n",
            "loss 0.015321476384997368 average time 0.0034415340699968057\n",
            "loss 0.3939591348171234 average time 0.0035133542300025055\n",
            "loss 0.007007244508713484 average time 0.003532663810011627\n",
            "loss 0.8551607131958008 average time 0.003557126830003199\n",
            "loss 0.49892473220825195 average time 0.0034976534400004765\n",
            "loss 0.192763552069664 average time 0.003489208690000396\n",
            "loss 0.01589532196521759 average time 0.0035541785700002036\n",
            "loss 0.4094453454017639 average time 0.003526355229995488\n",
            "loss 0.035797443240880966 average time 0.003469757609999533\n",
            "loss 0.984240710735321 average time 0.0035253573199986476\n",
            "loss 0.11918798834085464 average time 0.0034855931000038255\n",
            "loss 0.7041807174682617 average time 0.00354392515000427\n",
            "loss 0.39012446999549866 average time 0.0035225374499975714\n",
            "loss 0.09185025840997696 average time 0.003501691019998816\n",
            "loss 0.6708714365959167 average time 0.0034723414600000523\n",
            "loss 0.4718559980392456 average time 0.003458879680000564\n",
            "loss 0.1644434630870819 average time 0.003530525510000189\n",
            "loss 0.6135247945785522 average time 0.0034456168400060962\n",
            "loss 0.4542525112628937 average time 0.0035340489000020626\n",
            "loss 0.3865679204463959 average time 0.003718698900001982\n",
            "loss 0.007575576193630695 average time 0.003494099249996907\n",
            "loss 0.15176305174827576 average time 0.003538451410000789\n",
            "loss 0.2854021489620209 average time 0.0035259890199984055\n",
            "loss 0.21342948079109192 average time 0.0034846006499992654\n",
            "loss 0.016932420432567596 average time 0.003465907940001216\n",
            "loss 1.1022684574127197 average time 0.0035036411200098884\n",
            "loss 0.16662180423736572 average time 0.0034927607999998144\n",
            "loss 0.06622037291526794 average time 0.0034998297099969023\n",
            "loss 0.17588916420936584 average time 0.003479951839996147\n",
            "loss 0.8597815632820129 average time 0.0035987287599994035\n",
            "loss 0.018794627860188484 average time 0.0034745017899967933\n",
            "loss 0.3206161558628082 average time 0.0034978687500085925\n",
            "loss 0.5115024447441101 average time 0.0034672813299960127\n",
            "loss 0.06461464613676071 average time 0.0034989792000044415\n",
            "loss 0.08085425198078156 average time 0.003487186760003169\n",
            "loss 1.1660865545272827 average time 0.003535022029997208\n",
            "loss 0.34691593050956726 average time 0.003575435379996179\n",
            "loss 1.2459800243377686 average time 0.003510043019996374\n",
            "loss 0.03643195331096649 average time 0.003491010999999844\n",
            "loss 0.08741404116153717 average time 0.003438511780001363\n",
            "loss 0.1504928320646286 average time 0.003455562459997736\n",
            "loss 0.299678772687912 average time 0.0034885168899995733\n",
            "loss 0.14800545573234558 average time 0.003517130140002109\n",
            "loss 0.39330026507377625 average time 0.0035517836099938902\n",
            "loss 0.09077094495296478 average time 0.003439575499990042\n",
            "loss 0.09753459692001343 average time 0.003486827039995433\n",
            "loss 0.18092365562915802 average time 0.0035355718100004195\n",
            "loss 1.0391541719436646 average time 0.0035146122899993772\n",
            "loss 0.2022213488817215 average time 0.0035060292399998615\n",
            "loss 0.167124941945076 average time 0.0034780450799951268\n",
            "loss 0.10210953652858734 average time 0.003492980410006794\n",
            "loss 0.3228035867214203 average time 0.003478497840003456\n",
            "loss 0.28965023159980774 average time 0.0034802563000005194\n",
            "loss 0.4152640700340271 average time 0.0034652469699949506\n",
            "loss 0.0017611464718356729 average time 0.0034599674300056903\n",
            "loss 0.04396084323525429 average time 0.003468667750005352\n",
            "loss 0.4026414752006531 average time 0.003427275090001558\n",
            "loss 0.08081187307834625 average time 0.003485911280000664\n",
            "loss 0.677374541759491 average time 0.0035394843800008857\n",
            "loss 0.4635000228881836 average time 0.003495476629997256\n",
            "loss 0.02221427857875824 average time 0.0034827862699955857\n",
            "loss 0.06733430176973343 average time 0.0034314018800012036\n",
            "loss 0.09254780411720276 average time 0.0035264258899985633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 10000\n",
              "\tepoch: 100\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 100\n",
              "\toutput: 0.09254780411720276\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2HokSoVnwjJ"
      },
      "source": [
        "### TensorCore mixed precision training\n",
        "\n",
        "The V100 GPUs have 640 tensor cores that can accelerate half precision matrix multiplication calculation which is the core computation done by the DL model. [Apex library](https://github.com/NVIDIA/apex) developed by NVIDIA makes mixed precision and distributed training in Pytorch easy. By changing 3 lines of code, it can use the tensor cores to accelerate the training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWionrWjPRsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f8fc62-f22b-4cff-e6ae-bfb548e928ec"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8054, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 8054 (delta 68), reused 97 (delta 44), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8054/8054), 14.11 MiB | 26.76 MiB/s, done.\n",
            "Resolving deltas: 100% (5467/5467), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI1mX1trPZOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764b2a31-4a06-4b5b-f8b3-ccc67df044fa"
      },
      "source": [
        "cd apex"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/apex\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ERRfQDvPegW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72561d4-0154-4718-e372-8c4305252d8b"
      },
      "source": [
        "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-rpprn32q\n",
            "Created temporary directory: /tmp/pip-req-tracker-aeadi2b5\n",
            "Created requirements tracker '/tmp/pip-req-tracker-aeadi2b5'\n",
            "Created temporary directory: /tmp/pip-install-sw4mmisq\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-_85b4wsn\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-aeadi2b5'\n",
            "    Running setup.py (path:/tmp/pip-req-build-_85b4wsn/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.9.0+cu102\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-_85b4wsn/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-_85b4wsn/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-_85b4wsn/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-_85b4wsn/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-_85b4wsn/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-req-build-_85b4wsn/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-_85b4wsn/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-_85b4wsn has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-aeadi2b5'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-twzkowfg\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-_85b4wsn/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-_85b4wsn/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-twzkowfg/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.9.0+cu102\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-_85b4wsn/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "    Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "    Cuda compilation tools, release 11.0, V11.0.221\n",
            "    Build cuda_11.0_bu.TC445_37.28845127_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "      File \"<string>\", line 1, in <module>\n",
            "      File \"/tmp/pip-req-build-_85b4wsn/setup.py\", line 171, in <module>\n",
            "        check_cuda_torch_binary_vs_bare_metal(torch.utils.cpp_extension.CUDA_HOME)\n",
            "      File \"/tmp/pip-req-build-_85b4wsn/setup.py\", line 106, in check_cuda_torch_binary_vs_bare_metal\n",
            "        \"https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  \"\n",
            "    RuntimeError: Cuda extensions are being compiled with a version of Cuda that does not match the version used to compile Pytorch binaries.  Pytorch binaries were compiled with Cuda 10.2.\n",
            "    In some cases, a minor-version mismatch will not cause later errors:  https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  You can try commenting out this check (at your own risk).\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25herror\n",
            "Cleaning up...\n",
            "  Removing source in /tmp/pip-req-build-_85b4wsn\n",
            "Removed build tracker '/tmp/pip-req-tracker-aeadi2b5'\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-_85b4wsn/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-_85b4wsn/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-twzkowfg/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Exception information:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n",
            "    use_user_site=options.use_user_site,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/__init__.py\", line 62, in install_given_reqs\n",
            "    **kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py\", line 888, in install\n",
            "    cwd=self.unpacked_source_directory,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 275, in runner\n",
            "    spinner=spinner,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 242, in call_subprocess\n",
            "    raise InstallationError(exc_msg)\n",
            "pip._internal.exceptions.InstallationError: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-_85b4wsn/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-_85b4wsn/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-twzkowfg/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGGHIHS-nwjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3103fd21-1a9b-43c9-f256-323837f08474"
      },
      "source": [
        "from apex import amp\n",
        "from ignite.engine import Engine, Events\n",
        "from torch.nn import MSELoss\n",
        "from ignite.handlers import Timer\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# set the AMP optimization level to O1\n",
        "opt_level = 'O1'\n",
        "# wrap the optimizer and model\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
        "#dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    # amp handles the auto loss scaling\n",
        "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
            "loss 0.06063811480998993 average time 0.006230319420008072\n",
            "loss 0.001066660275682807 average time 0.006128368320004256\n",
            "loss 0.19295042753219604 average time 0.006067026359993406\n",
            "loss 0.5067887306213379 average time 0.006070645440007638\n",
            "loss 0.250715970993042 average time 0.006001979470003107\n",
            "loss 0.2519461512565613 average time 0.006021862310007009\n",
            "loss 0.6993328332901001 average time 0.006041894729999058\n",
            "loss 0.11660344153642654 average time 0.006211583790006898\n",
            "loss 0.05746414512395859 average time 0.006062029840003333\n",
            "loss 0.2677597999572754 average time 0.006151297789999717\n",
            "loss 0.02621569111943245 average time 0.006007147619997113\n",
            "loss 0.9495847225189209 average time 0.006121364570005881\n",
            "loss 0.67964768409729 average time 0.006163298550001172\n",
            "loss 0.11375847458839417 average time 0.006023901979999664\n",
            "loss 0.23856757581233978 average time 0.00612254574999838\n",
            "loss 0.05641895532608032 average time 0.005996596109996517\n",
            "loss 0.35834193229675293 average time 0.006222605360009083\n",
            "loss 0.22024698555469513 average time 0.006096300960000463\n",
            "loss 0.4409521222114563 average time 0.00607026845999826\n",
            "loss 0.5643376111984253 average time 0.006026711960009834\n",
            "loss 1.0929069519042969 average time 0.006033112619998064\n",
            "loss 0.03989917039871216 average time 0.006082763969997132\n",
            "loss 0.41125229001045227 average time 0.006026322339997705\n",
            "loss 0.06322555989027023 average time 0.00605984745000228\n",
            "loss 0.8981878757476807 average time 0.005970268030000625\n",
            "loss 0.0642775148153305 average time 0.006027747940004246\n",
            "loss 0.08264154195785522 average time 0.006099728570006846\n",
            "loss 0.6294618248939514 average time 0.006092341279995707\n",
            "loss 1.746427297592163 average time 0.005941704799997751\n",
            "loss 0.6902605295181274 average time 0.006064089879999983\n",
            "loss 0.27665019035339355 average time 0.006103775649993395\n",
            "loss 0.013936994597315788 average time 0.006088539590001574\n",
            "loss 0.6038936376571655 average time 0.006147464150002406\n",
            "loss 0.2906837463378906 average time 0.006047578189999285\n",
            "loss 0.25413915514945984 average time 0.006086056260007808\n",
            "loss 0.12174355238676071 average time 0.006097758480000266\n",
            "loss 1.0781440734863281 average time 0.006116111560008903\n",
            "loss 0.014829826541244984 average time 0.006005097329999671\n",
            "loss 0.389574259519577 average time 0.005995355919992562\n",
            "loss 0.007021288853138685 average time 0.006043752189998486\n",
            "loss 0.8545404076576233 average time 0.0062155227100072355\n",
            "loss 0.5031425356864929 average time 0.006067898520003609\n",
            "loss 0.19302226603031158 average time 0.006044122279998874\n",
            "loss 0.0156618133187294 average time 0.006068389999998089\n",
            "loss 0.41199105978012085 average time 0.006129917939998677\n",
            "loss 0.033786121755838394 average time 0.006087203480001335\n",
            "loss 0.95587557554245 average time 0.006042833949999249\n",
            "loss 0.11538783460855484 average time 0.005981008510007086\n",
            "loss 0.7027420997619629 average time 0.006058656410000367\n",
            "loss 0.39402106404304504 average time 0.006029853629999024\n",
            "loss 0.09143467247486115 average time 0.0059780317900026605\n",
            "loss 0.6642405390739441 average time 0.0061290232700037\n",
            "loss 0.46307146549224854 average time 0.0060096288799968535\n",
            "loss 0.1701323688030243 average time 0.006026914980000128\n",
            "loss 0.6205778121948242 average time 0.005978111020006054\n",
            "loss 0.44686639308929443 average time 0.006064806840000756\n",
            "loss 0.388960063457489 average time 0.006069441259999166\n",
            "loss 0.007307527586817741 average time 0.00613417239000114\n",
            "loss 0.15174180269241333 average time 0.006042334890004213\n",
            "loss 0.2866894006729126 average time 0.006089214680002897\n",
            "loss 0.21586138010025024 average time 0.00604916659999617\n",
            "loss 0.01630670204758644 average time 0.0060658727799989265\n",
            "loss 1.1025168895721436 average time 0.006192397380001467\n",
            "loss 0.16545596718788147 average time 0.006155654460001187\n",
            "loss 0.06642352044582367 average time 0.00625784884999689\n",
            "loss 0.18030405044555664 average time 0.006047451880003791\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "loss 0.8420208692550659 average time 0.006056942700002992\n",
            "loss 0.021149367094039917 average time 0.005972444470002074\n",
            "loss 0.3110746741294861 average time 0.006118816399996376\n",
            "loss 0.49788081645965576 average time 0.0060876479399962595\n",
            "loss 0.06705725193023682 average time 0.006059703140005012\n",
            "loss 0.08089345693588257 average time 0.006094331080000757\n",
            "loss 1.1746855974197388 average time 0.006065337479999471\n",
            "loss 0.35746750235557556 average time 0.006025025720001622\n",
            "loss 1.25910484790802 average time 0.0060236530900090205\n",
            "loss 0.03662481531500816 average time 0.006100104269997928\n",
            "loss 0.09634509682655334 average time 0.006023949159998665\n",
            "loss 0.15773527324199677 average time 0.006046982469999875\n",
            "loss 0.3038168251514435 average time 0.006046717320001563\n",
            "loss 0.15854395925998688 average time 0.0060401711200006505\n",
            "loss 0.39295509457588196 average time 0.006135306029993899\n",
            "loss 0.08792462199926376 average time 0.0060055477200023685\n",
            "loss 0.0998072475194931 average time 0.006226675960004968\n",
            "loss 0.1849805861711502 average time 0.006137335140010691\n",
            "loss 1.0436803102493286 average time 0.005997202269991248\n",
            "loss 0.20255598425865173 average time 0.006072177770017788\n",
            "loss 0.17178785800933838 average time 0.006023153330011155\n",
            "loss 0.09871138632297516 average time 0.0060504524000089075\n",
            "loss 0.3063816428184509 average time 0.006106292550016406\n",
            "loss 0.29392120242118835 average time 0.00607118947000572\n",
            "loss 0.4109807014465332 average time 0.005961532260014338\n",
            "loss 0.0020455550402402878 average time 0.006001418969999577\n",
            "loss 0.041161321103572845 average time 0.006105884609992245\n",
            "loss 0.3981459140777588 average time 0.005957057160001114\n",
            "loss 0.08065679669380188 average time 0.006021769979997771\n",
            "loss 0.7165994644165039 average time 0.006022940089999338\n",
            "loss 0.4619889557361603 average time 0.00599453570000378\n",
            "loss 0.022628463804721832 average time 0.006098283579997314\n",
            "loss 0.06459581851959229 average time 0.006059638709984938\n",
            "loss 0.08836936950683594 average time 0.006066256680007882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 10000\n",
              "\tepoch: 100\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 100\n",
              "\toutput: 0.08836936950683594\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPcuUUYwnwjK"
      },
      "source": [
        "It improves to compute each mini-batch in $8ms$. As we reduce the model weights to half precision for better performance, the loss need to be scaled to make sure the half precision dynamic range aligns with the computation. It is guessing what is the correct loss scaling factor and adjust it automatically if the gradient overflows. In the end, we will get the best hardware acceleration while maintaining the accuracy of model prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cPD5r6bnwjK"
      },
      "source": [
        "### Multiple GPU training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzLskEcSnwjK"
      },
      "source": [
        "Apex makes multiple GPU training easy. Working on the same training script, we need to take care of a few extra steps:\n",
        "\n",
        "1. Add the argument `--local_rank` which will be automatically set by the distributed launcher\n",
        "2. Initialize the process group\n",
        "2. Generate independent batched data based on process id in the dataset.\n",
        "3. Wrap the model and optimizer to handle distributed computation. \n",
        "4. Scale the loss and optimizer\n",
        "\n",
        "To launch distributed training, we need to put everything into a python file. Following is an example:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gH23WKAiFUf",
        "outputId": "cdc1cb7e-7851-4b18-ec71-08d603a28210"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhe-NMOknwjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c275e8e1-87f6-43be-c813-5ad65798f3ca"
      },
      "source": [
        "%%writefile distributed_train.py \n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from apex import amp\n",
        "from ignite.engine import Engine, Events\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from apex.parallel import DistributedDataParallel \n",
        "import argparse\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser = argparse.ArgumentParser()\n",
        "# this local_rank arg is automaticall set by distributed launch\n",
        "parser.add_argument(\"--local_rank\", default=0, type=int)\n",
        "args = parser.parse_args()\n",
        "\n",
        "args.distributed = False\n",
        "if 'WORLD_SIZE' in os.environ:\n",
        "    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n",
        "\n",
        "if args.distributed:\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl',\n",
        "                                         init_method='env://')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "opt_level = 'O1'\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
        "if args.distributed:\n",
        "    model = DistributedDataParallel(model)\n",
        "#dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=10240, seed=args.local_rank)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2, stocks=3, seed=args.local_rank)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing distributed_train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsWtauwknwjL"
      },
      "source": [
        "To launch multiple processes training, we need to run the following command:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Yzx7KPnwjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e49512-a7c0-4064-f836-0c92644ffafe"
      },
      "source": [
        "%reset -f\n",
        "\n",
        "!python -m torch.distributed.launch --nproc_per_node=4 distributed_train.py"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:164: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
            "  \"The module torch.distributed.launch is deprecated \"\n",
            "The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run\n",
            "*****************************************\n",
            "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "*****************************************\n",
            "WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.\n",
            " Please read local_rank from `os.environ('LOCAL_RANK')` instead.\n",
            "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\n",
            "  entrypoint       : distributed_train.py\n",
            "  min_nodes        : 1\n",
            "  max_nodes        : 1\n",
            "  nproc_per_node   : 4\n",
            "  run_id           : none\n",
            "  rdzv_backend     : static\n",
            "  rdzv_endpoint    : 127.0.0.1:29500\n",
            "  rdzv_configs     : {'rank': 0, 'timeout': 900}\n",
            "  max_restarts     : 3\n",
            "  monitor_interval : 5\n",
            "  log_dir          : None\n",
            "  metrics_cfg      : {}\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic__jykxyjq/none_h0owf7m3\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/utils/store.py:53: FutureWarning: This is an experimental API and will be changed in future.\n",
            "  \"This is an experimental API and will be changed in future.\", FutureWarning\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=0\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3]\n",
            "  role_ranks=[0, 1, 2, 3]\n",
            "  global_ranks=[0, 1, 2, 3]\n",
            "  role_world_sizes=[4, 4, 4, 4]\n",
            "  global_world_sizes=[4, 4, 4, 4]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_0/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_0/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_0/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_0/3/error.json\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 308) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=1\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3]\n",
            "  role_ranks=[0, 1, 2, 3]\n",
            "  global_ranks=[0, 1, 2, 3]\n",
            "  role_world_sizes=[4, 4, 4, 4]\n",
            "  global_world_sizes=[4, 4, 4, 4]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_1/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_1/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_1/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_1/3/error.json\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 320) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=2\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3]\n",
            "  role_ranks=[0, 1, 2, 3]\n",
            "  global_ranks=[0, 1, 2, 3]\n",
            "  role_world_sizes=[4, 4, 4, 4]\n",
            "  global_world_sizes=[4, 4, 4, 4]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_2/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_2/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_2/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_2/3/error.json\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 332) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=3\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3]\n",
            "  role_ranks=[0, 1, 2, 3]\n",
            "  global_ranks=[0, 1, 2, 3]\n",
            "  role_world_sizes=[4, 4, 4, 4]\n",
            "  global_world_sizes=[4, 4, 4, 4]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_3/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_3/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_3/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic__jykxyjq/none_h0owf7m3/attempt_3/3/error.json\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "Traceback (most recent call last):\n",
            "  File \"distributed_train.py\", line 11, in <module>\n",
            "    from apex import amp\n",
            "ImportError: cannot import name 'amp' from 'apex' (unknown location)\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 344) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/utils/store.py:71: FutureWarning: This is an experimental API and will be changed in future.\n",
            "  \"This is an experimental API and will be changed in future.\", FutureWarning\n",
            "INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00037026405334472656 seconds\n",
            "{\"name\": \"torchelastic.worker.status.FAILED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 0, \"group_rank\": 0, \"worker_id\": \"344\", \"role\": \"default\", \"hostname\": \"24eaf2afc215\", \"state\": \"FAILED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": \"{\\\"message\\\": \\\"<NONE>\\\"}\", \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [0], \\\"role_rank\\\": [0], \\\"role_world_size\\\": [4]}\", \"agent_restarts\": 3}}\n",
            "{\"name\": \"torchelastic.worker.status.FAILED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 1, \"group_rank\": 0, \"worker_id\": \"345\", \"role\": \"default\", \"hostname\": \"24eaf2afc215\", \"state\": \"FAILED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": \"{\\\"message\\\": \\\"<NONE>\\\"}\", \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [1], \\\"role_rank\\\": [1], \\\"role_world_size\\\": [4]}\", \"agent_restarts\": 3}}\n",
            "{\"name\": \"torchelastic.worker.status.FAILED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 2, \"group_rank\": 0, \"worker_id\": \"346\", \"role\": \"default\", \"hostname\": \"24eaf2afc215\", \"state\": \"FAILED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": \"{\\\"message\\\": \\\"<NONE>\\\"}\", \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [2], \\\"role_rank\\\": [2], \\\"role_world_size\\\": [4]}\", \"agent_restarts\": 3}}\n",
            "{\"name\": \"torchelastic.worker.status.FAILED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 3, \"group_rank\": 0, \"worker_id\": \"347\", \"role\": \"default\", \"hostname\": \"24eaf2afc215\", \"state\": \"FAILED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": \"{\\\"message\\\": \\\"<NONE>\\\"}\", \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [3], \\\"role_rank\\\": [3], \\\"role_world_size\\\": [4]}\", \"agent_restarts\": 3}}\n",
            "{\"name\": \"torchelastic.worker.status.SUCCEEDED\", \"source\": \"AGENT\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": null, \"group_rank\": 0, \"worker_id\": null, \"role\": \"default\", \"hostname\": \"24eaf2afc215\", \"state\": \"SUCCEEDED\", \"total_run_time\": 20, \"rdzv_backend\": \"static\", \"raw_error\": null, \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\"}\", \"agent_restarts\": 3}}\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: \n",
            "\n",
            "**********************************************************************\n",
            "               CHILD PROCESS FAILED WITH NO ERROR_FILE                \n",
            "**********************************************************************\n",
            "CHILD PROCESS FAILED WITH NO ERROR_FILE\n",
            "Child process 344 (local_rank 0) FAILED (exitcode 1)\n",
            "Error msg: Process failed with exitcode 1\n",
            "Without writing an error file to <N/A>.\n",
            "While this DOES NOT affect the correctness of your application,\n",
            "no trace information about the error will be available for inspection.\n",
            "Consider decorating your top level entrypoint function with\n",
            "torch.distributed.elastic.multiprocessing.errors.record. Example:\n",
            "\n",
            "  from torch.distributed.elastic.multiprocessing.errors import record\n",
            "\n",
            "  @record\n",
            "  def trainer_main(args):\n",
            "     # do train\n",
            "**********************************************************************\n",
            "  warnings.warn(_no_error_file_warning_msg(rank, failure))\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 173, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 169, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py\", line 624, in run\n",
            "    )(*cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 116, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 247, in launch_agent\n",
            "    failures=result.failures,\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "***************************************\n",
            "      distributed_train.py FAILED      \n",
            "=======================================\n",
            "Root Cause:\n",
            "[0]:\n",
            "  time: 2021-06-25_01:30:51\n",
            "  rank: 0 (local_rank: 0)\n",
            "  exitcode: 1 (pid: 344)\n",
            "  error_file: <N/A>\n",
            "  msg: \"Process failed with exitcode 1\"\n",
            "=======================================\n",
            "Other Failures:\n",
            "[1]:\n",
            "  time: 2021-06-25_01:30:51\n",
            "  rank: 1 (local_rank: 1)\n",
            "  exitcode: 1 (pid: 345)\n",
            "  error_file: <N/A>\n",
            "  msg: \"Process failed with exitcode 1\"\n",
            "[2]:\n",
            "  time: 2021-06-25_01:30:51\n",
            "  rank: 2 (local_rank: 2)\n",
            "  exitcode: 1 (pid: 346)\n",
            "  error_file: <N/A>\n",
            "  msg: \"Process failed with exitcode 1\"\n",
            "[3]:\n",
            "  time: 2021-06-25_01:30:51\n",
            "  rank: 3 (local_rank: 3)\n",
            "  exitcode: 1 (pid: 347)\n",
            "  error_file: <N/A>\n",
            "  msg: \"Process failed with exitcode 1\"\n",
            "***************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUAIQJKAnwjM"
      },
      "source": [
        "It works and all the GPUs are busy to train this network. However, it has a few problems:-\n",
        "   \n",
        "    1. There is no model serialization so the trained model is not saved\n",
        "    2. There is no validation dataset to check the training progress\n",
        "    3. Most of the time is spent in Monte Carlo simulation hence the training is slow\n",
        "    4. We use a few paths(1024) for each option parameter set which is noise and the model cannot converge to a low cost value.\n",
        "We will address these problems in the next notebook"
      ]
    }
  ]
}