{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Judy/European_Call_jax_Judy_1028.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "389c254d-8972-404e-f1ba-3fb5b4fac727"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 3\n",
        "numsteps = 50\n",
        "numpaths = 100000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([100.]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 110.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3353994\n",
            "100 loops, best of 5: 5.4 ms per loop\n",
            "[0.09293348 0.09306508 0.09328145]\n",
            "10 loops, best of 5: 45.1 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d6986f-cd61-451f-b83d-2325ad5f4019"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 1.0)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "          drift = jnp.array(np.random.random(self.N_STOCKS) * 0.1)\n",
        "\n",
        "          T = self.T\n",
        "          K = np.random.random(1) * 1.0\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 10000, batch = 2, seed = 15, stocks=3) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "760037dc-6e95-4899-cb8b-85f35683452e"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*3, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, hidden)\n",
        "        self.fc7 = nn.Linear(hidden, hidden)\n",
        "        self.fc8 = nn.Linear(hidden, hidden)\n",
        "        self.fc9 = nn.Linear(hidden, 1) # 4 outputs: price, delta1, delta2, delta3\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1, 1.0, 1.0, 0.3, 0.1, 0.1]*3)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        x = F.elu(self.fc6(x))\n",
        "        x = F.elu(self.fc7(x))\n",
        "        x = F.elu(self.fc8(x))\n",
        "        return self.fc9(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "fe9d56fb-dfb9-4b04-e8f9-db4bc95e626a"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQol7RECI7gw"
      },
      "source": [
        "from ignite.engine import Engine, Events\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "fbbfedd5-292e-412f-8834-be8b3eff1736"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    y_pred = model(x)\n",
        "    # print(y_pred)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2,8,14]]\n",
        "\n",
        "    # print(torch.unbind(x))\n",
        "    # print([compute_deltas(x) for x in torch.unbind(x)])\n",
        "    # print(torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0))\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # print(y_pred)\n",
        "\n",
        "    loss_weight = 1/(y.mean(axis=0)**2)\n",
        "    # print(y.mean(axis=0))\n",
        "    # print((y.mean(axis=0)**2))\n",
        "    # print(1/(y.mean(axis=0)**2))\n",
        "    \n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    # print(loss_weight)\n",
        "    # print(loss_weight_normalized)\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean() # compute weighted MSE between the 2 arrays\n",
        "    # print((y_pred - y) ** 2)\n",
        "    # print((y_pred - y) ** 2 * loss_weight_normalized)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 61.06236018240452 average time 0.2834701959499853 iter num 20\n",
            "loss 48.15610125660896 average time 0.16185898434998763 iter num 40\n",
            "loss 54.048579186201096 average time 0.12188879573333604 iter num 60\n",
            "loss 41.36764444410801 average time 0.10191533803749167 iter num 80\n",
            "loss 44.58911716938019 average time 0.08979454761998112 iter num 100\n",
            "loss 34.18606240302324 average time 0.09484096209994278 iter num 20\n",
            "loss 16.21469622477889 average time 0.06774112827497447 iter num 40\n",
            "loss 6.936944555491209 average time 0.05902920154997749 iter num 60\n",
            "loss 5.347756086848676 average time 0.05458311391248003 iter num 80\n",
            "loss 4.379288584459573 average time 0.05204863644999023 iter num 100\n",
            "loss 4.947562702000141 average time 0.09631357710002249 iter num 20\n",
            "loss 3.223855164833367 average time 0.06853779952502918 iter num 40\n",
            "loss 4.452631983440369 average time 0.059830910533347985 iter num 60\n",
            "loss 1.5186829841695726 average time 0.05497673186251291 iter num 80\n",
            "loss 1.938883651746437 average time 0.052103770490007265 iter num 100\n",
            "loss 4.030826967209578 average time 0.09488867325001138 iter num 20\n",
            "loss 4.513275052886456 average time 0.0680301698500216 iter num 40\n",
            "loss 2.9528274899348617 average time 0.05883816076669367 iter num 60\n",
            "loss 4.321321030147374 average time 0.054731132037511544 iter num 80\n",
            "loss 2.2467266535386443 average time 0.052184182770006374 iter num 100\n",
            "loss 3.8847970427013934 average time 0.09485080314996139 iter num 20\n",
            "loss 2.9801938217133284 average time 0.06835432132496636 iter num 40\n",
            "loss 3.0735231121070683 average time 0.059142663866653795 iter num 60\n",
            "loss 3.077071742154658 average time 0.05498501754997846 iter num 80\n",
            "loss 3.11651558149606 average time 0.05231833022997307 iter num 100\n",
            "loss 2.6743824128061533 average time 0.09511126894994959 iter num 20\n",
            "loss 3.31704446580261 average time 0.06876872584996363 iter num 40\n",
            "loss 2.589201321825385 average time 0.05973356926663958 iter num 60\n",
            "loss 1.2280902592465281 average time 0.05531649938747592 iter num 80\n",
            "loss 1.8851659842766821 average time 0.05260693510998408 iter num 100\n",
            "loss 3.158761828672141 average time 0.09530760239997563 iter num 20\n",
            "loss 2.117497060680762 average time 0.06860856044999082 iter num 40\n",
            "loss 2.9873481253162026 average time 0.05949706826664472 iter num 60\n",
            "loss 1.5365915896836668 average time 0.05504323703749492 iter num 80\n",
            "loss 3.490301896817982 average time 0.05249406798999189 iter num 100\n",
            "loss 3.2438477501273155 average time 0.09900834454999767 iter num 20\n",
            "loss 2.7944421162828803 average time 0.07031411017499067 iter num 40\n",
            "loss 2.4988126824609935 average time 0.06044860216666924 iter num 60\n",
            "loss 2.4953880347311497 average time 0.05593403923751339 iter num 80\n",
            "loss 1.7898828082252294 average time 0.053365377599998286 iter num 100\n",
            "loss 1.9307901675347239 average time 0.09572829159994853 iter num 20\n",
            "loss 2.613268152344972 average time 0.0693941405999908 iter num 40\n",
            "loss 2.3337380844168365 average time 0.06015471411666719 iter num 60\n",
            "loss 2.9615015955641866 average time 0.05540885975000265 iter num 80\n",
            "loss 1.2795683869626373 average time 0.052480841279998455 iter num 100\n",
            "loss 2.7707137633115053 average time 0.09643831030000456 iter num 20\n",
            "loss 3.985378425568342 average time 0.0688244873500139 iter num 40\n",
            "loss 1.5635835006833076 average time 0.05957962771669069 iter num 60\n",
            "loss 3.127031377516687 average time 0.05534162693747362 iter num 80\n",
            "loss 2.234338317066431 average time 0.05254288337000162 iter num 100\n",
            "loss 2.5224173441529274 average time 0.09599511069986874 iter num 20\n",
            "loss 2.092390932375565 average time 0.06859303714995804 iter num 40\n",
            "loss 1.66297861142084 average time 0.05957497146661505 iter num 60\n",
            "loss 2.5859431480057538 average time 0.055010488112463916 iter num 80\n",
            "loss 2.36439926084131 average time 0.05223263411994594 iter num 100\n",
            "loss 1.9954740128014237 average time 0.09581680394994692 iter num 20\n",
            "loss 2.3400841746479273 average time 0.06846455545000936 iter num 40\n",
            "loss 2.4610862601548433 average time 0.05910039363329815 iter num 60\n",
            "loss 0.826294781290926 average time 0.054492737012492395 iter num 80\n",
            "loss 1.3931306602898985 average time 0.051772116650008686 iter num 100\n",
            "loss 2.1940848091617227 average time 0.09456665504999365 iter num 20\n",
            "loss 2.573509991634637 average time 0.06775869879993479 iter num 40\n",
            "loss 2.154129761038348 average time 0.059211763366602096 iter num 60\n",
            "loss 1.0885314259212464 average time 0.05494594749993666 iter num 80\n",
            "loss 1.5262453234754503 average time 0.05246422270995936 iter num 100\n",
            "loss 2.073289215331897 average time 0.09466813150002054 iter num 20\n",
            "loss 1.8800116959027946 average time 0.06776303785006803 iter num 40\n",
            "loss 2.360323996981606 average time 0.059134021350079516 iter num 60\n",
            "loss 2.11932128877379 average time 0.054487773875018776 iter num 80\n",
            "loss 1.521382509963587 average time 0.05182842181002343 iter num 100\n",
            "loss 3.805824089795351 average time 0.09462227525004892 iter num 20\n",
            "loss 1.427599781891331 average time 0.06774702872501166 iter num 40\n",
            "loss 1.6282826254609972 average time 0.0588044747667027 iter num 60\n",
            "loss 2.5574042228981853 average time 0.054360002587498 iter num 80\n",
            "loss 2.679696772247553 average time 0.051691132810010455 iter num 100\n",
            "loss 2.7997640427201986 average time 0.09440431949992671 iter num 20\n",
            "loss 1.7312593990936875 average time 0.06770738585000799 iter num 40\n",
            "loss 2.6952321059070528 average time 0.05897457101665774 iter num 60\n",
            "loss 2.154915709979832 average time 0.054689840899993666 iter num 80\n",
            "loss 3.0063686426728964 average time 0.051893420289998174 iter num 100\n",
            "loss 2.45632603764534 average time 0.09458893760001956 iter num 20\n",
            "loss 2.1998543525114655 average time 0.06777213584996389 iter num 40\n",
            "loss 2.001490065595135 average time 0.05888087703331924 iter num 60\n",
            "loss 2.2483067004941404 average time 0.05430953940000336 iter num 80\n",
            "loss 1.6132730524986982 average time 0.051722007630005465 iter num 100\n",
            "loss 3.250188019592315 average time 0.09461856699999771 iter num 20\n",
            "loss 2.781862858682871 average time 0.06829517684993788 iter num 40\n",
            "loss 1.32206900161691 average time 0.05881866519994219 iter num 60\n",
            "loss 1.1652206012513489 average time 0.054563961537473914 iter num 80\n",
            "loss 1.4794447633903474 average time 0.05203344106998884 iter num 100\n",
            "loss 1.6560486983507872 average time 0.09485998240006666 iter num 20\n",
            "loss 1.0007795208366588 average time 0.06783564854999895 iter num 40\n",
            "loss 1.1698880553012714 average time 0.05887030483336275 iter num 60\n",
            "loss 1.0554160689935088 average time 0.054791525212522174 iter num 80\n",
            "loss 0.9532058902550489 average time 0.05220828979002363 iter num 100\n",
            "loss 1.2242488446645439 average time 0.09435404569999264 iter num 20\n",
            "loss 2.1204999939072877 average time 0.06833854999999858 iter num 40\n",
            "loss 0.7596408977406099 average time 0.05907841186670642 iter num 60\n",
            "loss 1.6982323722913861 average time 0.054567054900030595 iter num 80\n",
            "loss 1.525396219221875 average time 0.05182918466002775 iter num 100\n",
            "loss 1.7603507149033248 average time 0.09430779829995117 iter num 20\n",
            "loss 1.3456380111165345 average time 0.06773049307498695 iter num 40\n",
            "loss 0.7002741767792031 average time 0.058537050933318825 iter num 60\n",
            "loss 1.169824245152995 average time 0.05401838027495387 iter num 80\n",
            "loss 1.1436752538429573 average time 0.05127727676996983 iter num 100\n",
            "loss 1.8829853797797114 average time 0.09536435865011299 iter num 20\n",
            "loss 1.6387281357310712 average time 0.06858419742507067 iter num 40\n",
            "loss 1.1216531129321083 average time 0.059733305550086394 iter num 60\n",
            "loss 1.0495840979274362 average time 0.054935180275037965 iter num 80\n",
            "loss 2.530024794396013 average time 0.05221934109002177 iter num 100\n",
            "loss 0.4152666951995343 average time 0.09291465665010037 iter num 20\n",
            "loss 0.8006246207514778 average time 0.06707079250004426 iter num 40\n",
            "loss 1.4198716962710023 average time 0.05842526928336156 iter num 60\n",
            "loss 1.2412139039952308 average time 0.05408291948755277 iter num 80\n",
            "loss 0.6785834557376802 average time 0.05137199916004647 iter num 100\n",
            "loss 0.44915555918123573 average time 0.09463300649981647 iter num 20\n",
            "loss 0.7034373993519694 average time 0.06798866047490719 iter num 40\n",
            "loss 0.6201735232025385 average time 0.059081545383302604 iter num 60\n",
            "loss 1.4483644918072969 average time 0.054506323749967577 iter num 80\n",
            "loss 0.4886910392087884 average time 0.051640342289956606 iter num 100\n",
            "loss 1.7129097250290215 average time 0.09497736545004046 iter num 20\n",
            "loss 0.7928350532893091 average time 0.06816614267495424 iter num 40\n",
            "loss 1.0172203474212438 average time 0.0594574021665873 iter num 60\n",
            "loss 1.9326039182487875 average time 0.054971223987411125 iter num 80\n",
            "loss 1.3713576481677592 average time 0.052228015559940104 iter num 100\n",
            "loss 0.2273585050716065 average time 0.09280320655007017 iter num 20\n",
            "loss 1.14964583190158 average time 0.06716542637507246 iter num 40\n",
            "loss 0.8953281212598085 average time 0.05828356306672807 iter num 60\n",
            "loss 0.7245673623401672 average time 0.054175679337572544 iter num 80\n",
            "loss 0.9105639765039086 average time 0.05143846622004276 iter num 100\n",
            "loss 3.0993265681900084 average time 0.09404420350006149 iter num 20\n",
            "loss 0.30166695069056004 average time 0.06730222427502212 iter num 40\n",
            "loss 1.067642224370502 average time 0.058357891416699205 iter num 60\n",
            "loss 1.0401959298178554 average time 0.05459127475000969 iter num 80\n",
            "loss 0.37196292396401986 average time 0.05184398317000159 iter num 100\n",
            "loss 3.813911462202668 average time 0.09464617950006868 iter num 20\n",
            "loss 1.2565532233566046 average time 0.06796250942504685 iter num 40\n",
            "loss 0.981046978267841 average time 0.05884293641667379 iter num 60\n",
            "loss 0.5420753950602375 average time 0.054349841687485426 iter num 80\n",
            "loss 0.41684554162202403 average time 0.05161903222997353 iter num 100\n",
            "loss 0.5418743967311457 average time 0.09295600269992974 iter num 20\n",
            "loss 1.0055500752059743 average time 0.06694445609998638 iter num 40\n",
            "loss 0.6359699182212353 average time 0.058420339333315495 iter num 60\n",
            "loss 0.5463197885546833 average time 0.054002577712469704 iter num 80\n",
            "loss 1.6410490206908435 average time 0.0512222777599527 iter num 100\n",
            "loss 0.6089812086429447 average time 0.09454325030001201 iter num 20\n",
            "loss 0.36842004192294553 average time 0.06801964552503251 iter num 40\n",
            "loss 0.4050306961289607 average time 0.05887566279999798 iter num 60\n",
            "loss 0.6072792166378349 average time 0.05453714213747389 iter num 80\n",
            "loss 1.1585480388021097 average time 0.051606184419988495 iter num 100\n",
            "loss 0.8754277223488316 average time 0.09424785825008257 iter num 20\n",
            "loss 0.4177176742814481 average time 0.06775340757506002 iter num 40\n",
            "loss 1.2289138976484537 average time 0.05904449521670661 iter num 60\n",
            "loss 0.5135107130627148 average time 0.05455097102500304 iter num 80\n",
            "loss 0.4095982149010524 average time 0.051751665740021054 iter num 100\n",
            "loss 0.804626033641398 average time 0.0930153414999495 iter num 20\n",
            "loss 0.49336074880557135 average time 0.06739511809994383 iter num 40\n",
            "loss 0.46921846660552546 average time 0.058821363266618694 iter num 60\n",
            "loss 0.6311706965789199 average time 0.0544018354750051 iter num 80\n",
            "loss 0.3674861727631651 average time 0.05168977093004287 iter num 100\n",
            "loss 0.5826124106533825 average time 0.09477622289996361 iter num 20\n",
            "loss 1.5984565834514797 average time 0.06778157344997453 iter num 40\n",
            "loss 0.47177592932712287 average time 0.05916804601665717 iter num 60\n",
            "loss 1.0336645937059075 average time 0.05459333940000306 iter num 80\n",
            "loss 0.5867014988325536 average time 0.0517793100000199 iter num 100\n",
            "loss 0.46931418182794005 average time 0.09569807220000257 iter num 20\n",
            "loss 1.9917203462682664 average time 0.06866675037497316 iter num 40\n",
            "loss 0.46756431402172893 average time 0.05932277804995465 iter num 60\n",
            "loss 0.5673673149431124 average time 0.054687991024968595 iter num 80\n",
            "loss 0.15426005120389163 average time 0.05182535642997209 iter num 100\n",
            "loss 0.34266799048054963 average time 0.0950238411000555 iter num 20\n",
            "loss 0.532827980350703 average time 0.0677944228500337 iter num 40\n",
            "loss 0.9363505523651838 average time 0.0588628858666804 iter num 60\n",
            "loss 0.8104171138256788 average time 0.05449700688750454 iter num 80\n",
            "loss 0.3630060382420197 average time 0.051997565429983295 iter num 100\n",
            "loss 0.4989247099729255 average time 0.09582355269990331 iter num 20\n",
            "loss 0.4042347427457571 average time 0.06853412332491189 iter num 40\n",
            "loss 1.2700221850536764 average time 0.059083308683269324 iter num 60\n",
            "loss 0.5381735536502674 average time 0.05494811851244776 iter num 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c8c30e4018d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m           \u001b[0mEuropean_Call_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m           \u001b[0mgooptionvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptionvalueavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m           \u001b[0mDeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooptionvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0;31m###################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mgrad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0m_check_input_dtype_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholomorphic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       ans, vjp_py, aux = _vjp(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, reduce_axes, *primals)\u001b[0m\n\u001b[1;32m   2144\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun_nokwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m     out_primal, out_vjp = ad.vjp(\n\u001b[0;32m-> 2146\u001b[0;31m         flat_fun, primals_flat, reduce_axes=reduce_axes)\n\u001b[0m\u001b[1;32m   2147\u001b[0m     \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux, reduce_axes)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m   \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_primal_pval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout_primal_pval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    506\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJaxprTrace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36moptionvalueavg\u001b[0;34m(key, initial_stocks, numsteps, drift, r, cov, K, T, keys)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m###################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    540\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__rshift__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__rrshift__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rrshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m   5653\u001b[0m   \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_index_for_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5654\u001b[0m   return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[0;32m-> 5655\u001b[0;31m                  unique_indices)\n\u001b[0m\u001b[1;32m   5656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[0;31m# TODO(phawkins): re-enable jit after fixing excessive recompilation for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m   5661\u001b[0m             unique_indices):\n\u001b[1;32m   5662\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_static_and_dynamic_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5663\u001b[0;31m   \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_index_to_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shared with _scatter_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5664\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   5852\u001b[0m         \u001b[0;31m# XLA gives error when indexing into an axis of size 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5853\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"index is out of bounds for axis {x_axis} with size 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5854\u001b[0;31m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_axis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnormalize_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5855\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5856\u001b[0m       \u001b[0mgather_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgather_indices_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_normalize_index\u001b[0;34m(index, axis_size)\u001b[0m\n\u001b[1;32m   5436\u001b[0m   return lax.select(\n\u001b[1;32m   5437\u001b[0m     \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_constant_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5438\u001b[0;31m     \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5439\u001b[0m     index)\n\u001b[1;32m   5440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;34mr\"\"\"Elementwise addition: :math:`x + y`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0madd_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    266\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_execute_compiled_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_execute_compiled_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64716e4-36d0-4c0c-d66a-037bf704f9ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_test_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebab2a5-966c-424a-8316-7679d5234918"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758c5f5b-a96b-42de-c9c0-4f2681811917"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_test_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Judy/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias', 'fc7.weight', 'fc7.bias', 'fc8.weight', 'fc8.bias', 'fc9.weight', 'fc9.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38771e7-d280-4c19-8a92-a074ed96eb4e"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=18, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc8): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc9): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3b39835-19bf-4d40-84df-6f4f140ad607"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 8, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    \n",
        "    y_pred = model(x)\n",
        "    \n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2,8,14]]\n",
        "\n",
        "    # print(torch.unbind(x))\n",
        "    # print([compute_deltas(x) for x in torch.unbind(x)])\n",
        "    # print(torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0))\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    error = (y_pred - y)/y\n",
        "    error = error[abs(error) != float(\"Inf\")].mean()\n",
        "    print('error', error)\n",
        "\n",
        "\n",
        "    loss_weight = 1/(y.mean(axis=0)**2)\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean() # compute weighted MSE between the 2 arrays\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 10\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 50)\n",
        "\n",
        "# model_save_name = 'jax_european_test_3.pth'\n",
        "# path = F\"/content/drive/MyDrive/AFP Project/Lilian/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error tensor(0.3386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.2135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-748.1926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(219.5309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(3.7461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-127.6580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1414.2839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-3.2178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 1.1413099127821624 average time 0.1737452590999055 iter num 10\n",
            "error tensor(67.5247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1090.2649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-12.3441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-505.9640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-118.1467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.1451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-296.5696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(5693.0127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1587.8082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(21.6944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.30090495783952065 average time 0.09879784119996202 iter num 20\n",
            "error tensor(3.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(18483.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(3.1952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-2.8268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-83.3324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-281.4715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.4337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(22.2437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.11154785170219839 average time 0.073477441466639 iter num 30\n",
            "error tensor(1.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(3.3373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(69.4862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(66.7573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.1254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-4.4392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.9970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-60.4857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.9751407924341038 average time 0.06079207199996972 iter num 40\n",
            "error tensor(-945.6541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-2.5031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.2294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-3.3468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(2.1672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.2625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(62.7958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(172.3886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 3.5028447746299207 average time 0.053463448019983845 iter num 50\n",
            "error tensor(4.0473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(23183.4355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-16.5942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.2200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-275.7495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-9.3963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.9398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-6294.8799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-223.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.3332092455821112 average time 0.04840394808331894 iter num 60\n",
            "error tensor(-2.0790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(13.3271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.2857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-224.7225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-4.0549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.0321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(26.4161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.3891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.48986537876771763 average time 0.044728670957147186 iter num 70\n",
            "error tensor(0.0378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.5728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-3.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-3.2279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-8.6591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-22.9132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(17.5021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.3078107692999765 average time 0.04190516488751541 iter num 80\n",
            "error tensor(-0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(490.8815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(53.3476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-13.7693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(6.0251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(4.4902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.3666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.07121037469914882 average time 0.0397626040889059 iter num 90\n",
            "error tensor(0.1978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-268.1203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.9100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-71.5855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-295.4438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-6.9164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.4182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.5816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.5267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.9348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.6526515790028498 average time 0.037994880270016435 iter num 100\n",
            "error tensor(0.9800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-57.5314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-9.7088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-2972.9548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.7726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-327.5245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.4201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-109.6172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.1451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.25745561288204044 average time 0.1634554188000493 iter num 10\n",
            "error tensor(51.4139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-14.9118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-37.2935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-3.4596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(5.5538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(65.5843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(5.0707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(431.4372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(7.4002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(4.5836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 1.1007038847310469 average time 0.09289540125007534 iter num 20\n",
            "error tensor(0.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(946.5854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(66.7693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-186.1680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-59.4497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.3404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-2.5882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1252.4271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1691.8123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.516105501446873 average time 0.06913877446671297 iter num 30\n",
            "error tensor(38.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(4.8285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(50.7203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(64.8865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(2.3208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-7.7764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-8.0436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.07616129096277291 average time 0.05763088350001908 iter num 40\n",
            "error tensor(-112.4136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.7623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.2907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-13.4251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-30.8163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.9563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.3038943941646721 average time 0.05051315854002496 iter num 50\n",
            "error tensor(-3.6935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(21.3050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-190.5442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.1391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.3942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.7390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-4.2806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(141.3323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.08445953426416963 average time 0.04594017313336281 iter num 60\n",
            "error tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.1987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.4519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(2.9552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-7.7687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.9294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.5561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-22.9799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.1482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.23516939108958468 average time 0.04262241408574222 iter num 70\n",
            "error tensor(18.4452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-42.4095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(13.9204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-9.5543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(22.7663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-217.8385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-645.4666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.13571218005381525 average time 0.04003161475002344 iter num 80\n",
            "error tensor(-2.0983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.5391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(257.4189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.4156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(2.8904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-43.4787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-113.7304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.09230157957063057 average time 0.03801689048890593 iter num 90\n",
            "error tensor(9.2421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-2.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.9187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-6.7374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.4412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-50.9342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.2113749724230729 average time 0.03649525633001758 iter num 100\n",
            "error tensor(19.9264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-33.4039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.8003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-14.9757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.2089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.5991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-240.4391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.9889318607747555 average time 0.1618179417999727 iter num 10\n",
            "error tensor(-0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(4.5095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.3100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(59.2960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-37.6523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-6.9131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.3264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-10.3435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.35529443266568705 average time 0.09226199965000888 iter num 20\n",
            "error tensor(0.2881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(12.9936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(12.8208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.2274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.8578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.6069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-93.4798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-9.5586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.47682107833679765 average time 0.06879468010001802 iter num 30\n",
            "error tensor(1169.5435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(3.4710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-5.0357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(23.6741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-480.6625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-637.4567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.8028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(2.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 2.155312104150653 average time 0.057083881824996754 iter num 40\n",
            "error tensor(0.3813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-2.6144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(40.3400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.1339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-10.3219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.1952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-19.1643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(69.5771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.13923153346695472 average time 0.050002854580006896 iter num 50\n",
            "error tensor(-115.1678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-65.5077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-17.7264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.9199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(258.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.1124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-3.3561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-5126.9609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.0262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 1.7850814037956297 average time 0.045346785683341294 iter num 60\n",
            "error tensor(-347.8956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-62.0466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.3405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-10.7757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-2.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-2.6983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-32.8465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.2567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.1073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 1.2368753959890455 average time 0.04200145657144146 iter num 70\n",
            "error tensor(0.8720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-5.7217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-13.2586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-1.2485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-152.2097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-0.1069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(0.6553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(4.4560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.1992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "loss 0.3602642027544789 average time 0.0395277366000073 iter num 80\n",
            "error tensor(0.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(21.4857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(73.1289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(1.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-35.1504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-24.6496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "error tensor(-4.3194, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-2fe2019f18a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# model_save_name = 'jax_european_test_3.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m           \u001b[0;31m################################################################################################### store input and output numbers in X and Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m           \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEuropean_Call_price\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m           \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray.__setitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_routines_indexing.pyx\u001b[0m in \u001b[0;36mcupy._core._routines_indexing._ndarray_setitem\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_routines_indexing.pyx\u001b[0m in \u001b[0;36mcupy._core._routines_indexing._scatter_op\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray.fill\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_forward_method\u001b[0;34m(attrname, self, fun, *args)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0m_forward_to_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_forward_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtxsoZ9U9M8T"
      },
      "source": [
        "model_save_name = 'jax_european_test_4.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Lilian/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a14062-9df3-4a98-bb6c-f6b05c4611aa"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 0.8, 0.8, 0.25, 0.05, 0.05]*3]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2,8,14]]\n",
        "\n",
        "# price, delta1, delta2, delta3\n",
        "# should be around (0.067710705, 0.22125466 , 0.22136934 , 0.22104672)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.0557]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2467, 0.2465, 0.2462], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_2AXrPt7bNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a623e39-9b62-4882-c064-b5f209b9829a"
      },
      "source": [
        "numstocks = 3\n",
        "numsteps = 50\n",
        "numpaths = 100000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([0.8]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.8\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06718459\n",
            "[0.21998082 0.21983796 0.22022645]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "80d9e213-5764-4257-bd40-a480a61e76e9"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.8, S, 0.25, 0.05, 0.05] + ([1, 0.8, 0.8, 0.25, 0.05, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(0, 1, 0.01)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f89f3683710>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}