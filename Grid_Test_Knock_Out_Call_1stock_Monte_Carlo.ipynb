{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid Test_Knock Out Call 1stock Monte Carlo",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/Grid_Test_Knock_Out_Call_1stock_Monte_Carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYigDkiy0HU9",
        "outputId": "e0c3b860-737f-466a-875b-80441b966805"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 8)\n",
        "K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "B_range = jnp.linspace(0.5, 1.0, 5)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "\n",
        "print(S_range)\n",
        "print(K_range)\n",
        "print(B_range)\n",
        "print(sigma_range)\n",
        "print(r_range)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75       0.82142854 0.89285713 0.9642857  1.0357143  1.1071429\n",
            " 1.1785713  1.25      ]\n",
            "[0.75  0.875 1.    1.125 1.25 ]\n",
            "[0.5   0.625 0.75  0.875 1.   ]\n",
            "[0.15       0.25       0.35000002 0.45      ]\n",
            "[0.01  0.025 0.04 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQxpJqK6OZr"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "\n",
        "goptionvalueavg = jax.grad(optionvalueavg, argnums=1)\n",
        "\n",
        "#################################################################### Adjust all parameters here (not inside class)\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 8)\n",
        "K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "B_range = jnp.linspace(0.5, 1.0, 5)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "T = 1.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "####################################################################\n",
        "\n",
        "call = []\n",
        "count = 0\n",
        "\n",
        "for S in S_range:\n",
        "  for K in K_range:\n",
        "    for B in B_range:\n",
        "      for r in r_range:\n",
        "        for sigma in sigma_range:    \n",
        "\n",
        "          initial_stocks = jnp.array([S]*numstocks) # must be float\n",
        "          r_tmp = jnp.array([r]*numstocks)\n",
        "          drift = r_tmp\n",
        "          cov = jnp.identity(numstocks)*sigma*sigma\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "          Deltas = goptionvalueavg(keys, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "          call.append([T, K, B, S, sigma, r, r, European_Call_price] + list(Deltas)) #T, K, B, S, sigma, mu, r, price, delta\n",
        "          \n",
        "          count += 1\n",
        "          print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "e_OUtP8GUwj5",
        "outputId": "9c73750d-5f32-428c-9697-23ecc7102976"
      },
      "source": [
        "Thedataset = pd.DataFrame(call)\n",
        "Thedataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.04829822</td>\n",
              "      <td>0.555157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.07776406</td>\n",
              "      <td>0.563845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.10653644</td>\n",
              "      <td>0.572229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.13306102</td>\n",
              "      <td>0.569618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.053997554</td>\n",
              "      <td>0.594301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2395</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.19972013</td>\n",
              "      <td>0.471407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.10002679</td>\n",
              "      <td>0.631672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2397</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.1442415</td>\n",
              "      <td>0.583131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2398</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.17972831</td>\n",
              "      <td>0.529840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2399</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.20697877</td>\n",
              "      <td>0.482881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2400 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6            7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010   0.04829822  0.555157\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010   0.07776406  0.563845\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010   0.10653644  0.572229\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010   0.13306102  0.569618\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.053997554  0.594301\n",
              "...   ...   ...  ...   ...   ...    ...    ...          ...       ...\n",
              "2395  1.0  1.25  1.0  1.25  0.45  0.025  0.025   0.19972013  0.471407\n",
              "2396  1.0  1.25  1.0  1.25  0.15  0.040  0.040   0.10002679  0.631672\n",
              "2397  1.0  1.25  1.0  1.25  0.25  0.040  0.040    0.1442415  0.583131\n",
              "2398  1.0  1.25  1.0  1.25  0.35  0.040  0.040   0.17972831  0.529840\n",
              "2399  1.0  1.25  1.0  1.25  0.45  0.040  0.040   0.20697877  0.482881\n",
              "\n",
              "[2400 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQKnflf6peX"
      },
      "source": [
        "# save to csv\n",
        "Thedataset.to_csv('Knock_Out_Call_1stock_MC_Datset_v2.csv', index=False, header=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "skGWSSsG8TGG",
        "outputId": "cab9e8fc-10ca-4c5b-e899-a25ffd96705f"
      },
      "source": [
        "# read csv\n",
        "import pandas as pd\n",
        "\n",
        "Thedataset = pd.read_csv('Knock_Out_Call_1stock_MC_Datset_v2.csv', header=None)\n",
        "Thedataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.048298</td>\n",
              "      <td>0.555157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.077764</td>\n",
              "      <td>0.563845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.106536</td>\n",
              "      <td>0.572229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.133061</td>\n",
              "      <td>0.569618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.053998</td>\n",
              "      <td>0.594301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2395</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.199720</td>\n",
              "      <td>0.471407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.100027</td>\n",
              "      <td>0.631672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2397</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.144241</td>\n",
              "      <td>0.583131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2398</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.179728</td>\n",
              "      <td>0.529840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2399</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.206979</td>\n",
              "      <td>0.482881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2400 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6         7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010  0.048298  0.555157\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010  0.077764  0.563845\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010  0.106536  0.572229\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010  0.133061  0.569618\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.053998  0.594301\n",
              "...   ...   ...  ...   ...   ...    ...    ...       ...       ...\n",
              "2395  1.0  1.25  1.0  1.25  0.45  0.025  0.025  0.199720  0.471407\n",
              "2396  1.0  1.25  1.0  1.25  0.15  0.040  0.040  0.100027  0.631672\n",
              "2397  1.0  1.25  1.0  1.25  0.25  0.040  0.040  0.144241  0.583131\n",
              "2398  1.0  1.25  1.0  1.25  0.35  0.040  0.040  0.179728  0.529840\n",
              "2399  1.0  1.25  1.0  1.25  0.45  0.040  0.040  0.206979  0.482881\n",
              "\n",
              "[2400 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4HV-G44th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f440896d-fc2d-4f1a-d36d-9800f3423254"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch\n",
        "torch.set_printoptions(precision=6)\n",
        "\n",
        "Thedataset_X = Thedataset.iloc[:,:7]\n",
        "Thedataset_Y = Thedataset.iloc[:,7:]\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.X = cupy.array(Thedataset_X)\n",
        "        self.Y = cupy.array(Thedataset_Y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(self.X.toDlpack()), from_dlpack(self.Y.toDlpack()))\n",
        "\n",
        "# print\n",
        "ds = OptionDataSet(max_len = 1)\n",
        "for i in ds:\n",
        "    print(i[0])\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    print(i[1].shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.000000, 0.750000, 0.500000,  ..., 0.150000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.500000,  ..., 0.250000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.500000,  ..., 0.350000, 0.010000, 0.010000],\n",
            "        ...,\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.250000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.350000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.450000, 0.040000, 0.040000]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([2400, 7])\n",
            "tensor([[0.048298, 0.555157],\n",
            "        [0.077764, 0.563845],\n",
            "        [0.106536, 0.572229],\n",
            "        ...,\n",
            "        [0.144241, 0.583131],\n",
            "        [0.179728, 0.529840],\n",
            "        [0.206979, 0.482881]], device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([2400, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "44e106d4-f144-43b4-9639-5a8fbb7a5c02"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(7*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.5, 0.3, 0.03, 0.03]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, B, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.5, 0.75, 0.15, 0.01, 0.01]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "0a2a6fcf-8c08-4acf-bc46-982ba47f4731"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 33.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeLVZiiaDS4y",
        "outputId": "4764621b-f37d-4342-9ad2-c203a314b668"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CyULkENYKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d64938a-544d-44f9-a3f4-ce12bbcb5368"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-4, amsgrad=True) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    # print(x.shape)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "    y_pred = model(x.float())\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]  # Now index 3 is stock price, not 2\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.14523550327004037 average time 2.1992292456000313 iter num 20\n",
            "loss 0.12577864186479512 average time 2.1997673753000297 iter num 40\n",
            "loss 0.11293679799165811 average time 2.1954604720500055 iter num 60\n",
            "loss 0.10703413124038802 average time 2.192272550512507 iter num 80\n",
            "loss 0.10579795717180672 average time 2.191238213610004 iter num 100\n",
            "loss 0.08307395139242776 average time 2.2116000942500023 iter num 20\n",
            "loss 0.06557160711712753 average time 2.2039175137750133 iter num 40\n",
            "loss 0.05724081835638322 average time 2.213295871449979 iter num 60\n",
            "loss 0.05464241316147082 average time 2.2214319026125056 iter num 80\n",
            "loss 0.05420184583205156 average time 2.219873935989999 iter num 100\n",
            "loss 0.051226648004523666 average time 2.190934604750032 iter num 20\n",
            "loss 0.0547272207823712 average time 2.185492518775027 iter num 40\n",
            "loss 0.05726100442147599 average time 2.1925301883000126 iter num 60\n",
            "loss 0.058072819950748776 average time 2.1923267427500206 iter num 80\n",
            "loss 0.058208571029285464 average time 2.1907213842700184 iter num 100\n",
            "loss 0.06040618929459182 average time 2.204819555449967 iter num 20\n",
            "loss 0.06145002504078274 average time 2.2019154108249834 iter num 40\n",
            "loss 0.061693472310792065 average time 2.2004330737499838 iter num 60\n",
            "loss 0.061730362801342815 average time 2.200390648774987 iter num 80\n",
            "loss 0.061733687104863384 average time 2.2047289257399916 iter num 100\n",
            "loss 0.06174225984921263 average time 2.206094979699924 iter num 20\n",
            "loss 0.061654582723560405 average time 2.2089505989749796 iter num 40\n",
            "loss 0.06154894936743247 average time 2.2112780850666542 iter num 60\n",
            "loss 0.06149087883308531 average time 2.2085130211999853 iter num 80\n",
            "loss 0.06147793654562264 average time 2.206553369409985 iter num 100\n",
            "loss 0.06122204442500371 average time 2.2010887770999488 iter num 20\n",
            "loss 0.060977193487269994 average time 2.209872465674971 iter num 40\n",
            "loss 0.06080896959543581 average time 2.205012255316645 iter num 60\n",
            "loss 0.06073156598893699 average time 2.203299548687471 iter num 80\n",
            "loss 0.0607153053776446 average time 2.201415237219985 iter num 100\n",
            "loss 0.06040775021546746 average time 2.1982016766499783 iter num 20\n",
            "loss 0.06013702581738401 average time 2.2030000428499763 iter num 40\n",
            "loss 0.059960548550262525 average time 2.2041938697000054 iter num 60\n",
            "loss 0.05988116395342005 average time 2.2068859775250074 iter num 80\n",
            "loss 0.059864628284769 average time 2.2074565059499944 iter num 100\n",
            "loss 0.05955444844321923 average time 2.2121260657000676 iter num 20\n",
            "loss 0.05928481995955796 average time 2.209699296825056 iter num 40\n",
            "loss 0.05911055645934717 average time 2.215382836566687 iter num 60\n",
            "loss 0.059032566929267186 average time 2.215835328050025 iter num 80\n",
            "loss 0.05901633611842121 average time 2.21997458765999 iter num 100\n",
            "loss 0.05871217622014397 average time 2.2602908935999038 iter num 20\n",
            "loss 0.058448204704329036 average time 2.2440548612999693 iter num 40\n",
            "loss 0.058278137065128084 average time 2.235320443949998 iter num 60\n",
            "loss 0.05820211130747233 average time 2.231186273237506 iter num 80\n",
            "loss 0.058186309241676074 average time 2.2259950781500084 iter num 100\n",
            "loss 0.05789096229347361 average time 2.2038972514999386 iter num 20\n",
            "loss 0.057635223226223484 average time 2.209835444600003 iter num 40\n",
            "loss 0.05747033388722934 average time 2.2156009882500256 iter num 60\n",
            "loss 0.05739665985184825 average time 2.214733392625044 iter num 80\n",
            "loss 0.057381344233836576 average time 2.212545532849999 iter num 100\n",
            "loss 0.05709477283532814 average time 2.209537684400084 iter num 20\n",
            "loss 0.056846198930014365 average time 2.2017834511750607 iter num 40\n",
            "loss 0.05668586376658329 average time 2.206510704700031 iter num 60\n",
            "loss 0.056614200068074945 average time 2.208364429162555 iter num 80\n",
            "loss 0.056599315649909505 average time 2.21469683236005 iter num 100\n",
            "loss 0.05632083654689147 average time 2.207781862300044 iter num 20\n",
            "loss 0.05607961464452976 average time 2.2023655243500797 iter num 40\n",
            "loss 0.055924152623472716 average time 2.202221036916687 iter num 60\n",
            "loss 0.05585464002756495 average time 2.2024269984374882 iter num 80\n",
            "loss 0.05584018866772089 average time 2.2034073582999736 iter num 100\n",
            "loss 0.055569981088959365 average time 2.1919308415999694 iter num 20\n",
            "loss 0.055335354345026164 average time 2.201307527574977 iter num 40\n",
            "loss 0.055183885039768316 average time 2.2016776286500166 iter num 60\n",
            "loss 0.05511611621659118 average time 2.2051061272250196 iter num 80\n",
            "loss 0.055102031802131066 average time 2.2064577797200036 iter num 100\n",
            "loss 0.05483832141082594 average time 2.2063697543500895 iter num 20\n",
            "loss 0.054609150008957184 average time 2.2122376709750826 iter num 40\n",
            "loss 0.054461169436323204 average time 2.210642853833421 iter num 60\n",
            "loss 0.05439492123864296 average time 2.2164363178500595 iter num 80\n",
            "loss 0.05438114955533056 average time 2.2163498400900608 iter num 100\n",
            "loss 0.054123339609411195 average time 2.216107589250032 iter num 20\n",
            "loss 0.05389935419388178 average time 2.2167616081999766 iter num 40\n",
            "loss 0.05375459248930348 average time 2.214264320983345 iter num 60\n",
            "loss 0.05368974415923358 average time 2.209646163787511 iter num 80\n",
            "loss 0.053676259333327606 average time 2.2072624042400095 iter num 100\n",
            "loss 0.05342370837382509 average time 2.2353712002499377 iter num 20\n",
            "loss 0.05320359827048248 average time 2.2224219821499447 iter num 40\n",
            "loss 0.05306107069883943 average time 2.214416568633351 iter num 60\n",
            "loss 0.052997140755690446 average time 2.211491461837511 iter num 80\n",
            "loss 0.052983840153961834 average time 2.2083973021500243 iter num 100\n",
            "loss 0.05273456732268469 average time 2.1909738904500045 iter num 20\n",
            "loss 0.052517289048268495 average time 2.1914060105500313 iter num 40\n",
            "loss 0.05237652407386449 average time 2.1959537168666885 iter num 60\n",
            "loss 0.05231335977980314 average time 2.2009086357125627 iter num 80\n",
            "loss 0.052300225744022044 average time 2.2073907532200336 iter num 100\n",
            "loss 0.05205398772951932 average time 2.2285320198001046 iter num 20\n",
            "loss 0.05183894447838082 average time 2.2156753577750576 iter num 40\n",
            "loss 0.05169912246961549 average time 2.2080698266334062 iter num 60\n",
            "loss 0.05163631535226224 average time 2.2046771267375336 iter num 80\n",
            "loss 0.05162323803566143 average time 2.20569805703004 iter num 100\n",
            "loss 0.05137791556149105 average time 2.222027508149904 iter num 20\n",
            "loss 0.051163395944844876 average time 2.21622506449994 iter num 40\n",
            "loss 0.05102402821327913 average time 2.2135446962166347 iter num 60\n",
            "loss 0.050961373180873494 average time 2.2158819681999944 iter num 80\n",
            "loss 0.05094833072635794 average time 2.213709323080002 iter num 100\n",
            "loss 0.05070344964217238 average time 2.212283445250159 iter num 20\n",
            "loss 0.05048867894880489 average time 2.2260400444750985 iter num 40\n",
            "loss 0.050348778549185284 average time 2.2294305208333904 iter num 60\n",
            "loss 0.05028584559569451 average time 2.2252668669625337 iter num 80\n",
            "loss 0.050272739702405464 average time 2.222857873200064 iter num 100\n",
            "loss 0.05002632882957825 average time 2.2151987616000497 iter num 20\n",
            "loss 0.049810004452672745 average time 2.2174363352750106 iter num 40\n",
            "loss 0.04966888417266864 average time 2.218200049800013 iter num 60\n",
            "loss 0.0496053095289237 average time 2.2212841593000574 iter num 80\n",
            "loss 0.04959205417811852 average time 2.224443434900031 iter num 100\n",
            "loss 0.049342764543563546 average time 2.212346384400098 iter num 20\n",
            "loss 0.049123274041913564 average time 2.2059898264750473 iter num 40\n",
            "loss 0.04898009053386743 average time 2.2055688039333896 iter num 60\n",
            "loss 0.048915536727619155 average time 2.206769014312556 iter num 80\n",
            "loss 0.048902068583084324 average time 2.2053990294900268 iter num 100\n",
            "loss 0.04864852063461721 average time 2.226964855099868 iter num 20\n",
            "loss 0.048425357835811235 average time 2.2260779563998767 iter num 40\n",
            "loss 0.048279422541747635 average time 2.2231895896165343 iter num 60\n",
            "loss 0.048213575368360635 average time 2.2220927291373984 iter num 80\n",
            "loss 0.048199851134801956 average time 2.2201281223799105 iter num 100\n",
            "loss 0.04794134733550971 average time 2.2117411760000323 iter num 20\n",
            "loss 0.04771357782274408 average time 2.2200401520000694 iter num 40\n",
            "loss 0.047564797448540384 average time 2.2210542872667247 iter num 60\n",
            "loss 0.047497794321768316 average time 2.220397665387543 iter num 80\n",
            "loss 0.04748384367843901 average time 2.217270466340033 iter num 100\n",
            "loss 0.04722082291189268 average time 2.207065608650055 iter num 20\n",
            "loss 0.04698966047588495 average time 2.210549222225063 iter num 40\n",
            "loss 0.046838915492541947 average time 2.21150429461674 iter num 60\n",
            "loss 0.04677103025135108 average time 2.211717878850061 iter num 80\n",
            "loss 0.04675689498247692 average time 2.2126342141401163 iter num 100\n",
            "loss 0.04649059941522653 average time 2.2206547932500142 iter num 20\n",
            "loss 0.04625687474670974 average time 2.2080462615000216 iter num 40\n",
            "loss 0.046104818320547976 average time 2.199515213566701 iter num 60\n",
            "loss 0.046036446388087 average time 2.199031245325 iter num 80\n",
            "loss 0.04602220085571518 average time 2.19758293158995 iter num 100\n",
            "loss 0.045754497745744006 average time 2.1980677474000005 iter num 20\n",
            "loss 0.045520106424392345 average time 2.201338019524928 iter num 40\n",
            "loss 0.04536791644790895 average time 2.205621653033298 iter num 60\n",
            "loss 0.04529958454843221 average time 2.2032085671999537 iter num 80\n",
            "loss 0.04528536105376788 average time 2.199990786419985 iter num 100\n",
            "loss 0.04501794470817196 average time 2.1968499669500487 iter num 20\n",
            "loss 0.04478435842285791 average time 2.194915020750068 iter num 40\n",
            "loss 0.044632992052586515 average time 2.203183275166642 iter num 60\n",
            "loss 0.0445650632688305 average time 2.2033189599125307 iter num 80\n",
            "loss 0.044550947455326215 average time 2.2069554476700795 iter num 100\n",
            "loss 0.04428569750103823 average time 2.207251467249898 iter num 20\n",
            "loss 0.04405420339297135 average time 2.2065920944998654 iter num 40\n",
            "loss 0.0439045063646865 average time 2.2067582180000196 iter num 60\n",
            "loss 0.04383739090133625 average time 2.209788152662486 iter num 80\n",
            "loss 0.043823430362591266 average time 2.210805148639993 iter num 100\n",
            "loss 0.04356111786332238 average time 2.2242368180000085 iter num 20\n",
            "loss 0.04333269573446729 average time 2.223694957499947 iter num 40\n",
            "loss 0.04318502786487017 average time 2.219095162199998 iter num 60\n",
            "loss 0.04311882201993662 average time 2.2176603204749883 iter num 80\n",
            "loss 0.04310505968970035 average time 2.2157011987900113 iter num 100\n",
            "loss 0.042846705677408385 average time 2.2140330640000685 iter num 20\n",
            "loss 0.04262194266999729 average time 2.2119712623000396 iter num 40\n",
            "loss 0.0424767364254017 average time 2.2130075639833495 iter num 60\n",
            "loss 0.042411651670653666 average time 2.2162719122 iter num 80\n",
            "loss 0.04239812633309681 average time 2.2148665810900274 iter num 100\n",
            "loss 0.042144239121498464 average time 2.2066860655999334 iter num 20\n",
            "loss 0.04192374730345409 average time 2.2040092620749876 iter num 40\n",
            "loss 0.041781474387248975 average time 2.2039945330666644 iter num 60\n",
            "loss 0.0417177660498247 average time 2.203957553712553 iter num 80\n",
            "loss 0.04170451929845821 average time 2.2050661495900568 iter num 100\n",
            "loss 0.04145611167028177 average time 2.226011725250191 iter num 20\n",
            "loss 0.04124046657962909 average time 2.2129985753751953 iter num 40\n",
            "loss 0.041101406828806135 average time 2.2091683195001677 iter num 60\n",
            "loss 0.041039151865079684 average time 2.210879708175116 iter num 80\n",
            "loss 0.04102621529153815 average time 2.2098080209001636 iter num 100\n",
            "loss 0.040783566792609256 average time 2.2051774083000963 iter num 20\n",
            "loss 0.04057335582933348 average time 2.20631199417503 iter num 40\n",
            "loss 0.04043800039682081 average time 2.2135384447501565 iter num 60\n",
            "loss 0.04037744997610197 average time 2.2090253884000957 iter num 80\n",
            "loss 0.040364872119720674 average time 2.208871173320058 iter num 100\n",
            "loss 0.04012917342946492 average time 2.209660134099613 iter num 20\n",
            "loss 0.03992555458378039 average time 2.206408489174828 iter num 40\n",
            "loss 0.03979475346130369 average time 2.2093560091498756 iter num 60\n",
            "loss 0.03973627324211823 average time 2.208914510149884 iter num 80\n",
            "loss 0.0397241253631881 average time 2.2128439554799115 iter num 100\n",
            "loss 0.03949681776017624 average time 2.204046953249599 iter num 20\n",
            "loss 0.03930097434022761 average time 2.1995900783997513 iter num 40\n",
            "loss 0.039175525089163894 average time 2.1870433717164817 iter num 60\n",
            "loss 0.03911953948978215 average time 2.186378248712413 iter num 80\n",
            "loss 0.039107924915884565 average time 2.1869861793199563 iter num 100\n",
            "loss 0.038890805722106354 average time 2.18329500334994 iter num 20\n",
            "loss 0.03870451415207541 average time 2.1894592199748786 iter num 40\n",
            "loss 0.03858553703433848 average time 2.182444086216583 iter num 60\n",
            "loss 0.038532537051564696 average time 2.1790176326249138 iter num 80\n",
            "loss 0.03852155219018541 average time 2.1761007600199447 iter num 100\n",
            "loss 0.03831641386504301 average time 2.1815654547499435 iter num 20\n",
            "loss 0.03814132910681565 average time 2.173163187500086 iter num 40\n",
            "loss 0.038029936088514675 average time 2.174248018800154 iter num 60\n",
            "loss 0.03798043867472367 average time 2.1775549690501066 iter num 80\n",
            "loss 0.037970203644378864 average time 2.175738429190078 iter num 100\n",
            "loss 0.03777912710623396 average time 2.1825625158998263 iter num 20\n",
            "loss 0.03761699788111099 average time 2.1841967505249613 iter num 40\n",
            "loss 0.03751427816746535 average time 2.180862223500026 iter num 60\n",
            "loss 0.03746875908836865 average time 2.1780955904374877 iter num 80\n",
            "loss 0.03745934563842944 average time 2.178910689759996 iter num 100\n",
            "loss 0.037284277066115375 average time 2.198162640300052 iter num 20\n",
            "loss 0.03713663729503027 average time 2.1817482379000923 iter num 40\n",
            "loss 0.037043587290742155 average time 2.17626175973334 iter num 60\n",
            "loss 0.03700248183538235 average time 2.184140359812477 iter num 80\n",
            "loss 0.03699401355558892 average time 2.1846426436699584 iter num 100\n",
            "loss 0.03683658326741527 average time 2.1717419422497186 iter num 20\n",
            "loss 0.03670485429703226 average time 2.1697140900500473 iter num 40\n",
            "loss 0.03662239443147958 average time 2.1780839506832916 iter num 60\n",
            "loss 0.03658609558417258 average time 2.1775872634374536 iter num 80\n",
            "loss 0.036578622016233374 average time 2.175621451090028 iter num 100\n",
            "loss 0.036440293348848844 average time 2.1749754697501884 iter num 20\n",
            "loss 0.03632558101930793 average time 2.1745743568002127 iter num 40\n",
            "loss 0.03625425043407453 average time 2.1759137788667733 iter num 60\n",
            "loss 0.03622299898276299 average time 2.17578234133764 iter num 80\n",
            "loss 0.03621658961943059 average time 2.179607138450119 iter num 100\n",
            "loss 0.03609831685626623 average time 2.1755982196499644 iter num 20\n",
            "loss 0.03600132753808086 average time 2.172069312850135 iter num 40\n",
            "loss 0.03594154970494474 average time 2.169620011750127 iter num 60\n",
            "loss 0.03591551267015362 average time 2.171600200362673 iter num 80\n",
            "loss 0.03591018592716548 average time 2.171348934470152 iter num 100\n",
            "loss 0.03581233566642964 average time 2.1767825389998507 iter num 20\n",
            "loss 0.03573319780442094 average time 2.186002116224927 iter num 40\n",
            "loss 0.03568498195021754 average time 2.1834074045833405 iter num 60\n",
            "loss 0.03566413952453041 average time 2.18249223357509 iter num 80\n",
            "loss 0.03565990549880006 average time 2.18140195849006 iter num 100\n",
            "loss 0.03558208069209484 average time 2.17191807349991 iter num 20\n",
            "loss 0.035520185663102215 average time 2.1692722373000377 iter num 40\n",
            "loss 0.035483046173105694 average time 2.1725223452833538 iter num 60\n",
            "loss 0.03546719653454118 average time 2.176741839899955 iter num 80\n",
            "loss 0.035463969256317965 average time 2.1783504172500034 iter num 100\n",
            "loss 0.035405471396014444 average time 2.170949198100061 iter num 20\n",
            "loss 0.035360066239811104 average time 2.174416277249975 iter num 40\n",
            "loss 0.035333423840712606 average time 2.1745626257166806 iter num 60\n",
            "loss 0.035322165570704424 average time 2.173204858637496 iter num 80\n",
            "loss 0.03531989985924518 average time 2.1717567101099484 iter num 100\n",
            "loss 0.03527900795715265 average time 2.1688786984001127 iter num 20\n",
            "loss 0.03524854009287443 average time 2.1804430053500257 iter num 40\n",
            "loss 0.0352311448045108 average time 2.1750677496500734 iter num 60\n",
            "loss 0.03522398831550579 average time 2.1770311057000526 iter num 80\n",
            "loss 0.035222554717715904 average time 2.181594150660039 iter num 100\n",
            "loss 0.03519721618511091 average time 2.1906598820498404 iter num 20\n",
            "loss 0.03517940772212621 average time 2.182078969399845 iter num 40\n",
            "loss 0.03516992623938536 average time 2.180569602399919 iter num 60\n",
            "loss 0.03516620743618228 average time 2.1860100760625074 iter num 80\n",
            "loss 0.03516548412590928 average time 2.1867814638599885 iter num 100\n",
            "loss 0.03515301819296738 average time 2.185703193599875 iter num 20\n",
            "loss 0.035145900866469536 average time 2.1866772574500373 iter num 40\n",
            "loss 0.0351430649761133 average time 2.1840820942333568 iter num 60\n",
            "loss 0.03514224541890014 average time 2.189666111562519 iter num 80\n",
            "loss 0.03514209822888084 average time 2.2029835636900135 iter num 100\n",
            "loss 0.035140499801125415 average time 2.2411246523000043 iter num 20\n",
            "loss 0.03514199450709819 average time 2.216865635124941 iter num 40\n",
            "loss 0.0351443627309064 average time 2.2060425241832187 iter num 60\n",
            "loss 0.035145768884830154 average time 2.199459111499914 iter num 80\n",
            "loss 0.03514609703197584 average time 2.1926419775499015 iter num 100\n",
            "loss 0.03515254149099296 average time 2.1903535648002617 iter num 20\n",
            "loss 0.03516021421043723 average time 2.1923457464501097 iter num 40\n",
            "loss 0.035166205898713374 average time 2.2006610508168403 iter num 60\n",
            "loss 0.03516911201822829 average time 2.2023148657875935 iter num 80\n",
            "loss 0.035169734173386 average time 2.200494925540097 iter num 100\n",
            "loss 0.03518183463466877 average time 2.2124441756998747 iter num 20\n",
            "loss 0.0351941595839501 average time 2.20895718722486 iter num 40\n",
            "loss 0.03520289215818346 average time 2.2066784288332806 iter num 60\n",
            "loss 0.035206957409763096 average time 2.2043207188000222 iter num 80\n",
            "loss 0.03520783133623405 average time 2.203873182100033 iter num 100\n",
            "loss 0.035224059560875494 average time 2.1821837285501715 iter num 20\n",
            "loss 0.03523942359315253 average time 2.180294204975053 iter num 40\n",
            "loss 0.03524991204022683 average time 2.1910980968500855 iter num 60\n",
            "loss 0.035254685268025235 average time 2.194545185225047 iter num 80\n",
            "loss 0.03525569636524712 average time 2.194193998130049 iter num 100\n",
            "loss 0.03527421834980809 average time 2.1746507757499787 iter num 20\n",
            "loss 0.03529098009169169 average time 2.1783019034250173 iter num 40\n",
            "loss 0.03530215190360351 average time 2.170985229516797 iter num 60\n",
            "loss 0.035307242067525384 average time 2.1694835354625868 iter num 80\n",
            "loss 0.0353083005969888 average time 2.1702536442601197 iter num 100\n",
            "loss 0.03532808527911224 average time 2.164368635100163 iter num 20\n",
            "loss 0.035345959341518775 average time 2.16376451480005 iter num 40\n",
            "loss 0.035357635154824374 average time 2.167872841616645 iter num 60\n",
            "loss 0.035362890012505886 average time 2.1727048585375086 iter num 80\n",
            "loss 0.03536397375747305 average time 2.1732509996900626 iter num 100\n",
            "loss 0.035384025488296666 average time 2.178348854450178 iter num 20\n",
            "loss 0.03540204313427543 average time 2.1752288553500874 iter num 40\n",
            "loss 0.03541394036841683 average time 2.1753231614000166 iter num 60\n",
            "loss 0.0354192956698677 average time 2.1726383675874787 iter num 80\n",
            "loss 0.03542042448078678 average time 2.172608643240019 iter num 100\n",
            "loss 0.035440865786758556 average time 2.1949166300000797 iter num 20\n",
            "loss 0.03545884219120401 average time 2.188867200450113 iter num 40\n",
            "loss 0.03547048516195266 average time 2.1849156000334613 iter num 60\n",
            "loss 0.03547574973096625 average time 2.1878786659375464 iter num 80\n",
            "loss 0.03547685808392296 average time 2.187166141100006 iter num 100\n",
            "loss 0.03549674898710214 average time 2.1815125586495925 iter num 20\n",
            "loss 0.03551431824411664 average time 2.181413201499754 iter num 40\n",
            "loss 0.035525693145238395 average time 2.1871330243830926 iter num 60\n",
            "loss 0.03553078296644487 average time 2.1869084202123985 iter num 80\n",
            "loss 0.035531835409404314 average time 2.1848241158099335 iter num 100\n",
            "loss 0.03555106052207217 average time 2.1836470828999155 iter num 20\n",
            "loss 0.035567964065470435 average time 2.182915190750009 iter num 40\n",
            "loss 0.03557904033828609 average time 2.181989705633411 iter num 60\n",
            "loss 0.0355839957600922 average time 2.1808220849250572 iter num 80\n",
            "loss 0.035585033066162425 average time 2.183431524100015 iter num 100\n",
            "loss 0.035603966897167875 average time 2.184344866000083 iter num 20\n",
            "loss 0.03562071051464464 average time 2.1817304072249497 iter num 40\n",
            "loss 0.035631565475315 average time 2.177255362733346 iter num 60\n",
            "loss 0.035636468174963985 average time 2.1763428769250366 iter num 80\n",
            "loss 0.035637477364011155 average time 2.176321153090066 iter num 100\n",
            "loss 0.03565596244470125 average time 2.183136044299954 iter num 20\n",
            "loss 0.03567215676106652 average time 2.1911286754749653 iter num 40\n",
            "loss 0.03568263705181735 average time 2.1917745474332455 iter num 60\n",
            "loss 0.035687346889193707 average time 2.190023577324928 iter num 80\n",
            "loss 0.03568834060755003 average time 2.1854209335099224 iter num 100\n",
            "loss 0.03570609018775135 average time 2.1720041335498537 iter num 20\n",
            "loss 0.0357217400735175 average time 2.1734747389748916 iter num 40\n",
            "loss 0.03573187625441133 average time 2.173981098483212 iter num 60\n",
            "loss 0.03573639467402191 average time 2.173886749274902 iter num 80\n",
            "loss 0.035737347962677864 average time 2.1778124584099716 iter num 100\n",
            "loss 0.03575443858301335 average time 2.1822060953500113 iter num 20\n",
            "loss 0.03576922777542108 average time 2.176084706225038 iter num 40\n",
            "loss 0.035778800824389397 average time 2.171849195383432 iter num 60\n",
            "loss 0.03578311685989408 average time 2.1730642020749884 iter num 80\n",
            "loss 0.03578400479406967 average time 2.1728213405599126 iter num 100\n",
            "loss 0.03580039397769095 average time 2.188617203549802 iter num 20\n",
            "loss 0.03581467231132692 average time 2.1986743845499404 iter num 40\n",
            "loss 0.035824036936304 average time 2.1964925900500627 iter num 60\n",
            "loss 0.035828251669688345 average time 2.192683270174939 iter num 80\n",
            "loss 0.035829116356738334 average time 2.18953206397 iter num 100\n",
            "loss 0.03584467051047753 average time 2.182622713299679 iter num 20\n",
            "loss 0.035857934910149866 average time 2.1757878699248066 iter num 40\n",
            "loss 0.035866603740457116 average time 2.172497489966554 iter num 60\n",
            "loss 0.0358705223773114 average time 2.1769674667249546 iter num 80\n",
            "loss 0.035871346446142024 average time 2.177153365150007 iter num 100\n",
            "loss 0.03588621396085766 average time 2.2404323400003703 iter num 20\n",
            "loss 0.035899310512125415 average time 2.2222583151504294 iter num 40\n",
            "loss 0.03590775251015749 average time 2.209404209416971 iter num 60\n",
            "loss 0.03591153075779717 average time 2.1996992748127466 iter num 80\n",
            "loss 0.03591231142312888 average time 2.1929049213102187 iter num 100\n",
            "loss 0.03592663994172874 average time 2.187518539099801 iter num 20\n",
            "loss 0.03593912453013737 average time 2.1811825143247914 iter num 40\n",
            "loss 0.03594714444755376 average time 2.1739472364666046 iter num 60\n",
            "loss 0.035950695281173485 average time 2.1722531242000516 iter num 80\n",
            "loss 0.03595143474361821 average time 2.172690764920044 iter num 100\n",
            "loss 0.035964746556976256 average time 2.168274785950416 iter num 20\n",
            "loss 0.0359764744534054 average time 2.172792263749943 iter num 40\n",
            "loss 0.03598404023761929 average time 2.181503929999599 iter num 60\n",
            "loss 0.03598741997678857 average time 2.1820105509621954 iter num 80\n",
            "loss 0.03598813073889098 average time 2.1807279652697615 iter num 100\n",
            "loss 0.036000940100857956 average time 2.1699450789501498 iter num 20\n",
            "loss 0.03601200141576432 average time 2.1699836689750556 iter num 40\n",
            "loss 0.036019141903057834 average time 2.167132133583315 iter num 60\n",
            "loss 0.03602230955252132 average time 2.1704999868374673 iter num 80\n",
            "loss 0.03602297209629601 average time 2.1780082571300228 iter num 100\n",
            "loss 0.036034795261852985 average time 2.18074957359986 iter num 20\n",
            "loss 0.0360450077408537 average time 2.1874763014498058 iter num 40\n",
            "loss 0.03605150778412209 average time 2.182679270016585 iter num 60\n",
            "loss 0.03605437286741702 average time 2.180356338999991 iter num 80\n",
            "loss 0.03605495661520568 average time 2.1803024737400483 iter num 100\n",
            "loss 0.03606570123669542 average time 2.1929879954999705 iter num 20\n",
            "loss 0.03607495982358293 average time 2.2018250157500914 iter num 40\n",
            "loss 0.03608090876418327 average time 2.1951826384000013 iter num 60\n",
            "loss 0.036083570043114806 average time 2.1926517872124807 iter num 80\n",
            "loss 0.03608412050472152 average time 2.1892384543100705 iter num 100\n",
            "loss 0.03609412384398616 average time 2.172013117950337 iter num 20\n",
            "loss 0.03610277094802582 average time 2.174631876800049 iter num 40\n",
            "loss 0.03610827881487476 average time 2.1778594727834086 iter num 60\n",
            "loss 0.036110729723135976 average time 2.180444055812541 iter num 80\n",
            "loss 0.03611122935446237 average time 2.1845422750899526 iter num 100\n",
            "loss 0.0361204144729244 average time 2.1990372446492983 iter num 20\n",
            "loss 0.0361284084051819 average time 2.1896197918498728 iter num 40\n",
            "loss 0.03613349790486694 average time 2.1905711592832326 iter num 60\n",
            "loss 0.03613576082633628 average time 2.1926424691874216 iter num 80\n",
            "loss 0.03613622244039602 average time 2.1927597058600803 iter num 100\n",
            "loss 0.03614466853456739 average time 2.2111780491501123 iter num 20\n",
            "loss 0.036151949840125465 average time 2.206350069399923 iter num 40\n",
            "loss 0.03615658688057473 average time 2.199708365966641 iter num 60\n",
            "loss 0.03615862432077674 average time 2.19858646520006 iter num 80\n",
            "loss 0.03615903338163781 average time 2.194513385400096 iter num 100\n",
            "loss 0.03616655815309909 average time 2.175966833050006 iter num 20\n",
            "loss 0.03617292474950493 average time 2.183015432175125 iter num 40\n",
            "loss 0.03617681067424957 average time 2.1855437046335284 iter num 60\n",
            "loss 0.03617850378916302 average time 2.187451259700083 iter num 80\n",
            "loss 0.03617884572559738 average time 2.1863403487600954 iter num 100\n",
            "loss 0.03618523520575651 average time 2.1694569559002046 iter num 20\n",
            "loss 0.036190666037983286 average time 2.169627835975098 iter num 40\n",
            "loss 0.03619412666537624 average time 2.1727023053167915 iter num 60\n",
            "loss 0.03619563960102901 average time 2.170822910900051 iter num 80\n",
            "loss 0.03619593942769945 average time 2.1712499280700284 iter num 100\n",
            "loss 0.036201383784999056 average time 2.1904310075999094 iter num 20\n",
            "loss 0.036205864808035 average time 2.177614412124876 iter num 40\n",
            "loss 0.03620860726079405 average time 2.1753202796334636 iter num 60\n",
            "loss 0.03620977707599044 average time 2.173508502937557 iter num 80\n",
            "loss 0.036210007415448574 average time 2.1734159358001124 iter num 100\n",
            "loss 0.036214337490458326 average time 2.1660743295498834 iter num 20\n",
            "loss 0.036217985864892256 average time 2.16614293752491 iter num 40\n",
            "loss 0.036220206421291846 average time 2.1746173606832597 iter num 60\n",
            "loss 0.03622115342497258 average time 2.173455793087487 iter num 80\n",
            "loss 0.03622135286382908 average time 2.171780028930007 iter num 100\n",
            "loss 0.03622470876951568 average time 2.1637492841500716 iter num 20\n",
            "loss 0.03622749107757297 average time 2.1655090699998256 iter num 40\n",
            "loss 0.0362291973492425 average time 2.1662583878833783 iter num 60\n",
            "loss 0.036229899122772885 average time 2.166506194312569 iter num 80\n",
            "loss 0.03623005213856135 average time 2.1702938286400966 iter num 100\n",
            "loss 0.03623253011044629 average time 2.160909930550224 iter num 20\n",
            "loss 0.03623443127216426 average time 2.1631325712251055 iter num 40\n",
            "loss 0.036235521930917225 average time 2.1610911123333305 iter num 60\n",
            "loss 0.03623597048213086 average time 2.1621469068626085 iter num 80\n",
            "loss 0.03623607430037461 average time 2.162797886570188 iter num 100\n",
            "loss 0.036237483102917946 average time 2.163079874849791 iter num 20\n",
            "loss 0.036238509663668615 average time 2.168732393524897 iter num 40\n",
            "loss 0.03623907942675121 average time 2.1657301779498086 iter num 60\n",
            "loss 0.036239261181658114 average time 2.165794668037279 iter num 80\n",
            "loss 0.036239316817752616 average time 2.167018921089875 iter num 100\n",
            "loss 0.036239905391243755 average time 2.1664726858498398 iter num 20\n",
            "loss 0.03624024636910939 average time 2.167096936074631 iter num 40\n",
            "loss 0.03624032544594387 average time 2.1673749052832743 iter num 60\n",
            "loss 0.03624030982575284 average time 2.171098754774948 iter num 80\n",
            "loss 0.03624030307717112 average time 2.171692868769969 iter num 100\n",
            "loss 0.0362399693039488 average time 2.1619305611500748 iter num 20\n",
            "loss 0.036239427077406905 average time 2.166357472074742 iter num 40\n",
            "loss 0.036238932939275874 average time 2.171103087233011 iter num 60\n",
            "loss 0.03623866085072317 average time 2.172924224937424 iter num 80\n",
            "loss 0.036238595826238874 average time 2.173075918600007 iter num 100\n",
            "loss 0.03623726194859545 average time 2.1982423155006474 iter num 20\n",
            "loss 0.036235896687742346 average time 2.1917949926756592 iter num 40\n",
            "loss 0.036234857510425066 average time 2.1871344835674487 iter num 60\n",
            "loss 0.03623433912079925 average time 2.1858992669755026 iter num 80\n",
            "loss 0.03623423201013755 average time 2.183828865390351 iter num 100\n",
            "loss 0.036231958205122324 average time 2.1748764678501176 iter num 20\n",
            "loss 0.036229818381509335 average time 2.1739445238250483 iter num 40\n",
            "loss 0.036228215552272076 average time 2.173059752950212 iter num 60\n",
            "loss 0.036227471810117026 average time 2.1764624104500854 iter num 80\n",
            "loss 0.0362273010778662 average time 2.1737709318399356 iter num 100\n",
            "loss 0.03622414913682469 average time 2.1624542078498052 iter num 20\n",
            "loss 0.036221303773022955 average time 2.169060426449596 iter num 40\n",
            "loss 0.03621929484647444 average time 2.1697786032164004 iter num 60\n",
            "loss 0.03621837209392781 average time 2.1713051304747752 iter num 80\n",
            "loss 0.036218191271093665 average time 2.1698039540899483 iter num 100\n",
            "loss 0.03621438736402708 average time 2.1927321699002276 iter num 20\n",
            "loss 0.036210813560454726 average time 2.1839573870252935 iter num 40\n",
            "loss 0.03620839777680432 average time 2.179822555050305 iter num 60\n",
            "loss 0.03620725500362251 average time 2.177669287512754 iter num 80\n",
            "loss 0.03620701292796957 average time 2.177334888650112 iter num 100\n",
            "loss 0.03620235619985911 average time 2.1835532736497045 iter num 20\n",
            "loss 0.03619811316963556 average time 2.1864891107750735 iter num 40\n",
            "loss 0.03619524251153812 average time 2.192668130133461 iter num 60\n",
            "loss 0.03619389937209881 average time 2.1929152533876275 iter num 80\n",
            "loss 0.036193621734836666 average time 2.1927541749000374 iter num 100\n",
            "loss 0.036188155877975366 average time 2.1839479192005458 iter num 20\n",
            "loss 0.03618326581875198 average time 2.185139191075268 iter num 40\n",
            "loss 0.03617995692632514 average time 2.188516803183605 iter num 60\n",
            "loss 0.03617843288117958 average time 2.1884545375876767 iter num 80\n",
            "loss 0.03617810493991799 average time 2.1925230823901076 iter num 100\n",
            "loss 0.03617192696707183 average time 2.194638915200085 iter num 20\n",
            "loss 0.036166320241320986 average time 2.1923199435999776 iter num 40\n",
            "loss 0.036162565580632224 average time 2.18993441111658 iter num 60\n",
            "loss 0.03616087063304333 average time 2.1901044716626075 iter num 80\n",
            "loss 0.03616050529398915 average time 2.190376033780085 iter num 100\n",
            "loss 0.03615370681358485 average time 2.2029007728498984 iter num 20\n",
            "loss 0.036147598271946725 average time 2.204766389974793 iter num 40\n",
            "loss 0.03614353465224979 average time 2.2022811622001126 iter num 60\n",
            "loss 0.03614167417701706 average time 2.2025227953751254 iter num 80\n",
            "loss 0.036141274677198784 average time 2.2020647782700324 iter num 100\n",
            "loss 0.036133999823329065 average time 2.2008688581501703 iter num 20\n",
            "loss 0.03612752235638662 average time 2.1994717704749744 iter num 40\n",
            "loss 0.03612319980006927 average time 2.197652585316731 iter num 60\n",
            "loss 0.03612122551536288 average time 2.2036158918750517 iter num 80\n",
            "loss 0.03612081709412556 average time 2.202203142520084 iter num 100\n",
            "loss 0.03611290381366447 average time 2.19299684939906 iter num 20\n",
            "loss 0.03610592567267783 average time 2.196336601099574 iter num 40\n",
            "loss 0.03610130142030554 average time 2.195636728999792 iter num 60\n",
            "loss 0.03609918951853399 average time 2.1962117811498048 iter num 80\n",
            "loss 0.036098758170816904 average time 2.1965390463998484 iter num 100\n",
            "loss 0.03609044404240939 average time 2.2236575287997766 iter num 20\n",
            "loss 0.03608307165309238 average time 2.212398504150042 iter num 40\n",
            "loss 0.03607818475393885 average time 2.206564191100006 iter num 60\n",
            "loss 0.03607596887863544 average time 2.206644924787497 iter num 80\n",
            "loss 0.0360754859333254 average time 2.2043679033299486 iter num 100\n",
            "loss 0.036066830620662155 average time 2.188911451050444 iter num 20\n",
            "loss 0.03605916126648885 average time 2.1844935450503726 iter num 40\n",
            "loss 0.036054070277158325 average time 2.188307865900242 iter num 60\n",
            "loss 0.036051732818105676 average time 2.18119314566261 iter num 80\n",
            "loss 0.03605124733334107 average time 2.179851050210091 iter num 100\n",
            "loss 0.03604209617919165 average time 2.163427274249807 iter num 20\n",
            "loss 0.036034044325426076 average time 2.162322796574972 iter num 40\n",
            "loss 0.03602874370479947 average time 2.1642568638670734 iter num 60\n",
            "loss 0.036026342144193826 average time 2.166029149475162 iter num 80\n",
            "loss 0.036025838967358524 average time 2.16931527321005 iter num 100\n",
            "loss 0.036016339582707196 average time 2.169607832200563 iter num 20\n",
            "loss 0.036007960110934156 average time 2.1709420997754934 iter num 40\n",
            "loss 0.036002494397774934 average time 2.171285954083578 iter num 60\n",
            "loss 0.03600003100857814 average time 2.1726125247002073 iter num 80\n",
            "loss 0.03599952784969504 average time 2.171908450260198 iter num 100\n",
            "loss 0.035989773882681174 average time 2.1674076905004767 iter num 20\n",
            "loss 0.03598124584852237 average time 2.1717477765754665 iter num 40\n",
            "loss 0.03597559656192374 average time 2.1716379678170292 iter num 60\n",
            "loss 0.03597306406367572 average time 2.169063317937753 iter num 80\n",
            "loss 0.03597254299299966 average time 2.1688298776902957 iter num 100\n",
            "loss 0.03596257874349972 average time 2.163866612899801 iter num 20\n",
            "loss 0.0359538328806276 average time 2.1688615629248487 iter num 40\n",
            "loss 0.035948095792918223 average time 2.1661489921498287 iter num 60\n",
            "loss 0.035945505329465215 average time 2.170104508074792 iter num 80\n",
            "loss 0.03594496886733356 average time 2.1753274697799134 iter num 100\n",
            "loss 0.0359348108284738 average time 2.1782758506995377 iter num 20\n",
            "loss 0.03592596615440901 average time 2.1786431686497054 iter num 40\n",
            "loss 0.03592016104845889 average time 2.176131307399616 iter num 60\n",
            "loss 0.03591755717504867 average time 2.1759322434245405 iter num 80\n",
            "loss 0.03591701376718816 average time 2.174834268119521 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_fEzULvKwR-"
      },
      "source": [
        "# 0:55\n",
        "# 7:00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3be0cc-7ec7-4b6f-f8b8-dc8109dcbf5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df2b39b-c8f3-4553-87df-67ffdfc6dc90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b63240-f835-49a9-d002-bd595a2dbf30"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0758c7-9bd1-4a88-eea2-92fd19038c70"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=7, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3b18a4-2063-4632-b756-7d2d283c3877"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-4, amsgrad=True) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.035923793568878065 average time 2.1161620507499945 iter num 20\n",
            "loss 0.0359088227884686 average time 2.117986477250014 iter num 40\n",
            "loss 0.035898869725666846 average time 2.119290947166682 iter num 60\n",
            "loss 0.03589445991231862 average time 2.1167064351750016 iter num 80\n",
            "loss 0.035893525670518685 average time 2.1163014866000003 iter num 100\n",
            "loss 0.035876145018231706 average time 2.130255113499993 iter num 20\n",
            "loss 0.03586071681304882 average time 2.1185614623999696 iter num 40\n",
            "loss 0.035850518773534526 average time 2.122669476816653 iter num 60\n",
            "loss 0.03584589810408316 average time 2.1214107102499953 iter num 80\n",
            "loss 0.03584492107817007 average time 2.124587830999994 iter num 100\n",
            "loss 0.035826937819412166 average time 2.1230099534999907 iter num 20\n",
            "loss 0.035811238762083926 average time 2.1204493999750014 iter num 40\n",
            "loss 0.03580098220655523 average time 2.119429244066661 iter num 60\n",
            "loss 0.035796379485267794 average time 2.1215808921000017 iter num 80\n",
            "loss 0.03579542667373802 average time 2.1204596576900006 iter num 100\n",
            "loss 0.03577758185549446 average time 2.1165278543000112 iter num 20\n",
            "loss 0.03576217669123945 average time 2.111668071449998 iter num 40\n",
            "loss 0.035752115621639165 average time 2.1213781003166523 iter num 60\n",
            "loss 0.035747600964277704 average time 2.1208633587374806 iter num 80\n",
            "loss 0.03574666240920758 average time 2.1188522083899786 iter num 100\n",
            "loss 0.035729204391899125 average time 2.1060923174500203 iter num 20\n",
            "loss 0.03571422009974365 average time 2.1051727680500107 iter num 40\n",
            "loss 0.0357045521344185 average time 2.118224651883338 iter num 60\n",
            "loss 0.035700219184156834 average time 2.117615505000009 iter num 80\n",
            "loss 0.035699338460336974 average time 2.119036032519998 iter num 100\n",
            "loss 0.03568263676629199 average time 2.118593621749983 iter num 20\n",
            "loss 0.03566825399692997 average time 2.106464945925006 iter num 40\n",
            "loss 0.03565900997557502 average time 2.1070089071500204 iter num 60\n",
            "loss 0.03565487149787711 average time 2.101505268325025 iter num 80\n",
            "loss 0.03565399643302266 average time 2.1009717139000257 iter num 100\n",
            "loss 0.03563797484149057 average time 2.112051137599974 iter num 20\n",
            "loss 0.03562420192104143 average time 2.111943496674962 iter num 40\n",
            "loss 0.03561533402179883 average time 2.1136256896499996 iter num 60\n",
            "loss 0.03561140521294283 average time 2.1165285636375075 iter num 80\n",
            "loss 0.035610588241606386 average time 2.106577718380022 iter num 100\n",
            "loss 0.035595392009629995 average time 2.0981741081500105 iter num 20\n",
            "loss 0.035582316349927846 average time 2.1042400940999983 iter num 40\n",
            "loss 0.03557388945367844 average time 2.105065682733349 iter num 60\n",
            "loss 0.03557009696372333 average time 2.1003893676375016 iter num 80\n",
            "loss 0.03556931715939621 average time 2.1024357412100105 iter num 100\n",
            "loss 0.03555475714683309 average time 2.090482409050014 iter num 20\n",
            "loss 0.035542219799572526 average time 2.082796974149994 iter num 40\n",
            "loss 0.035534152465715924 average time 2.088302393949986 iter num 60\n",
            "loss 0.03553054525288692 average time 2.084488032675 iter num 80\n",
            "loss 0.035529779392147985 average time 2.0894968644999743 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQs-OZHGEwac"
      },
      "source": [
        "# 12:00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0093aa-7b7e-4637-89b3-36e71b5e34a7"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 0.8, 1, 0.25, 0.02, 0.02]]).cuda() # T, K, B, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[3]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.10632345, 0.5543747)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.101666]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.591832], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqpasxVi0hx3",
        "outputId": "91788b74-ac25-43d2-d57a-9dedd3c42b76"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "        dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx2)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.02]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.]*numstocks) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "B = 0.8 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10632345\n",
            "[0.5543747]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovJwXo3-YEu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1a688e44-e63c-4519-f43f-226b45759239"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "correct_call_prices = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    correct_call_prices.append(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, correct_call_prices, label = \"correct_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(correct_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "77bf560d-5ade-49c7-938f-066562b0857a"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.0, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgD1NjP7DOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "4014d547-8d24-4a76-c5bd-3919eb48b2c9"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.775, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 0.775, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7MfujMQ7oij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "db38961a-0b40-4ffa-ff90-3d8fb8671537"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.225, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.225, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}