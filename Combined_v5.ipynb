{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Combined_v5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/Combined_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "65384b98-732e-42ea-e56b-5304e13860b8"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   6752      0 --:--:-- --:--:-- --:--:--  6752\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9MB 50kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 48.3MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. \n",
        "\n",
        "Loading all the necessary libraries:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6no5JzH-B6"
      },
      "source": [
        "# !pip install cupy-cuda101"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbkx3hXWnwi8"
      },
      "source": [
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBN3YFOnwi-"
      },
      "source": [
        "The CuPy version of batched barrier option pricing simulation is as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhj4DtLnwi-"
      },
      "source": [
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRjmX5zcnwi_"
      },
      "source": [
        "Note, the parameters (K, B, S0, sigma, mu, r) are passed in as an array with length of batch size. The output array is a two dimensional array flatten to 1-D. The first dimension is for Batch and the second dimension is for Path. \n",
        "\n",
        "Testing it out by entering two sets of option parameters:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4PMo7Inwi_"
      },
      "source": [
        "# N_PATHS = 2048000\n",
        "# N_STEPS = 365\n",
        "# N_BATCH = 2\n",
        "# T = 1.0\n",
        "\n",
        "# K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
        "# B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
        "# S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
        "# sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
        "# mu = cupy.array([0.15, 0.1], dtype=cupy.float32)\n",
        "# r =cupy.array([0.05, 0.05], dtype=cupy.float32)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpWK3wcEnwjA"
      },
      "source": [
        "Put everything into a simple function to launch this GPU kernel. The option prices for each batch is the average of the corresponding path terminal values. This can be computed easily by Cupy function `mean(axis=1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAb34NTnwjA"
      },
      "source": [
        "# def batch_run():\n",
        "#     number_of_threads = 256\n",
        "#     number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
        "#     randoms_gpu = cupy.random.normal(0, 1, N_BATCH*N_PATHS * N_STEPS, dtype=cupy.float32)\n",
        "#     output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     s = time.time()\n",
        "#     cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
        "#                        (output, np.float32(T), K, B, S0, sigma, mu, r,\n",
        "#                         randoms_gpu, N_STEPS, N_PATHS, N_BATCH))\n",
        "#     v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     e = time.time()\n",
        "#     print('time', e-s, 'v',v)\n",
        "# batch_run()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRgQCelnwjC"
      },
      "source": [
        "This produces the option prices $21.22$ and $0.848$ for these two sets of option parameters in $66ms$.\n",
        "\n",
        "It works efficiently hence we will construct an `OptionDataSet` class to wrap the above code so we can use it in Pytorch. For every `next` element, it generates uniform distributed random option parameters in the specified range, launches the GPU kernel to compute the option prices, convert the CuPy array to Pytorch tensors with zero copy via the DLPack. Note how we implemented the iterable Dataset interface:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1KUra7ZnwjC"
      },
      "source": [
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo46Vf4XnwjD"
      },
      "source": [
        "Put everything related to Pytorch dataset into a file `cupy_dataset.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQUGMBlnwjE"
      },
      "source": [
        "# #%%writefile cupy_dataset.py \n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "# cupy.cuda.set_allocator(None)\n",
        "\n",
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')\n",
        "\n",
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPAsh7JnwjF"
      },
      "source": [
        "Here is a test code to sample 10 data points with batch size 16:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLKxMF05nwjF"
      },
      "source": [
        "# from cupy_dataset import OptionDataSet\n",
        "# ds = OptionDataSet(10, number_path=100000, batch=16, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlzRTD0nwjG"
      },
      "source": [
        "We can implement the same code by using Numba to accelerate the calculation in GPU:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IsfSwVwnwjG"
      },
      "source": [
        "# import numba\n",
        "# from numba import cuda\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average)\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]:\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                               X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
        "#         o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
        "#         Y = o.mean(axis = 1) \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=1, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "f60f989b-ed8c-4959-8e26-96a6513d990f"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "\n",
        "          X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          stocks_randoms_cov = cupy.array([0.9] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i[0])\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBOv_RiBsCWa"
      },
      "source": [
        "### PUI TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BME87CgGsFrd"
      },
      "source": [
        "# %%writefile cupy_dataset.py\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n",
        "\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp2 = math.exp(-r*T)\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)    \n",
        "\n",
        "#     for i in range(ii, N_PATHS, stride): # for each path          \n",
        "#         running_average = 0.0\n",
        "\n",
        "#         for j in range(N_STOCKS): # initialize S0\n",
        "#             s_curr[j] = S0[j]\n",
        "\n",
        "#         for n in range(N_STEPS): # for each step\n",
        "#             s_curr_avg = 0.0\n",
        "\n",
        "#             for j in range(N_STOCKS): # for each stock\n",
        "#                 tmp1 = mu[j]*T/N_STEPS  \n",
        "#                 s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n",
        "#                 s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n",
        "\n",
        "#             # add stock average to running average\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n",
        "\n",
        "#             # compare to barrier\n",
        "#             if running_average <= B:\n",
        "#                 break\n",
        "\n",
        "#         payoff = running_average - K if running_average > K else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "    \n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, number_stocks = 3, batch=1, threads=512, seed=15, T=1):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_STOCKS = number_stocks\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(T)\n",
        "#         self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "\n",
        "#         ############ <new\n",
        "#         self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n",
        "#         self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#         cupy.fill_diagonal(self.Z_cov, 1)\n",
        "#         ############ new>\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "\n",
        "#         X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "\n",
        "#         for i in range(self.N_BATCH): # for each batch\n",
        "#           self.S0 = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 200\n",
        "#           self.K = 110.0\n",
        "#           self.B = 100.0\n",
        "#           self.sigma = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.mu = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.r = 0.05\n",
        "#           self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n",
        "\n",
        "#           ############ <new - add correlation between stocks\n",
        "#           all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "          \n",
        "#           single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n",
        "#                                                                                     self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n",
        "#           Y[i] = self.output.mean()\n",
        "\n",
        "#           ############ <new - combine to get X matrix\n",
        "#           X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n",
        "#                                 self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "        \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "# ds = NumbaOptionDataSet(max_len=10, number_path=100, batch=2, seed=15)\n",
        "# for i in ds:\n",
        "#   print(i)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQt8PqinwjI"
      },
      "source": [
        "# %%writefile model.py\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6, hidden)\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0,\n",
        "#                                            198.0,\n",
        "#                                            200.0,\n",
        "#                                            0.4,\n",
        "#                                            0.2,\n",
        "#                                            0.2,]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94008edf-ae18-4ca0-f8b8-df72b2732550"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(300, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm', torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2] * 50))\n",
        "        # self.register_buffer('norm',\n",
        "        #                      torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8454443f-4b0d-4893-f4ed-02fff00c5194"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/c3/f472843797b5ccbb2f0e806a6927f52c7c9522bfcea8e7e881d39258368b/pytorch_ignite-0.4.5-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 21.1MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 15.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75dfc1f3-d98d-4676-e1c3-d76e39796278"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=8, stocks=3)\n",
        "dataset = NumbaOptionDataSet(max_len=200, number_path = 1024, batch=1, stocks=50)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "        \n",
        "trainer.run(dataset, max_epochs=300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 35.43681335449219 average time 0.07534065216000045\n",
            "loss 65.33242797851562 average time 0.039536002539999233\n",
            "loss 6.3768463134765625 average time 0.0054300466600011536\n",
            "loss 32.22458267211914 average time 0.004586981490002699\n",
            "loss 16.819419860839844 average time 0.005314801619999798\n",
            "loss 41.68236541748047 average time 0.00451329514499605\n",
            "loss 49.49589157104492 average time 0.005319764050002504\n",
            "loss 1.2699803113937378 average time 0.004491229909998538\n",
            "loss 125.24279022216797 average time 0.005315602250001348\n",
            "loss 6.0875244140625 average time 0.004533006834998901\n",
            "loss 93.68501281738281 average time 0.005288479429999597\n",
            "loss 79.62922668457031 average time 0.0045359526049998065\n",
            "loss 11.912220001220703 average time 0.005344315810002626\n",
            "loss 25.772478103637695 average time 0.0045647297549982115\n",
            "loss 26.537212371826172 average time 0.005320568809999031\n",
            "loss 105.55554962158203 average time 0.004553561914997886\n",
            "loss 81.27977752685547 average time 0.005409446959998831\n",
            "loss 10.198372840881348 average time 0.00459654743000101\n",
            "loss 2.92876935005188 average time 0.0054956669099999545\n",
            "loss 43.33232498168945 average time 0.004653045054999439\n",
            "loss 20.704940795898438 average time 0.005446443929992028\n",
            "loss 617.130615234375 average time 0.004662683939999397\n",
            "loss 1.2028695344924927 average time 0.0054417026600003734\n",
            "loss 16.926584243774414 average time 0.004627080985002863\n",
            "loss 17.13762092590332 average time 0.005392579109998224\n",
            "loss 53.82476806640625 average time 0.004591745149998588\n",
            "loss 5.370033264160156 average time 0.005404887649997363\n",
            "loss 4.267465591430664 average time 0.004569017664997546\n",
            "loss 1.515405535697937 average time 0.005365650819992424\n",
            "loss 14.601069450378418 average time 0.004568295659996693\n",
            "loss 22.858619689941406 average time 0.005461827200000471\n",
            "loss 1.8657194375991821 average time 0.004671485170005667\n",
            "loss 0.21572521328926086 average time 0.005403428470021936\n",
            "loss 38.34489440917969 average time 0.004631837550020918\n",
            "loss 2.82082462310791 average time 0.005455098889983674\n",
            "loss 13.722926139831543 average time 0.004644139579989996\n",
            "loss 63.90308380126953 average time 0.005433578479992321\n",
            "loss 21.172815322875977 average time 0.004623096209995765\n",
            "loss 20.903554916381836 average time 0.005345864370005984\n",
            "loss 0.866369903087616 average time 0.004573969604999775\n",
            "loss 40.22726821899414 average time 0.005383250530003352\n",
            "loss 2.3627521991729736 average time 0.004609516220001524\n",
            "loss 0.17952682077884674 average time 0.005376091579987588\n",
            "loss 4.776590347290039 average time 0.004603620474995296\n",
            "loss 1.0400352478027344 average time 0.005352215809998597\n",
            "loss 28.466386795043945 average time 0.004558644014998663\n",
            "loss 59.62433624267578 average time 0.00536079099001654\n",
            "loss 23.401348114013672 average time 0.004601868769999556\n",
            "loss 2.2815351486206055 average time 0.005427223389990559\n",
            "loss 50.32421112060547 average time 0.00458959194498334\n",
            "loss 4.04685115814209 average time 0.005427203520011972\n",
            "loss 11.84823989868164 average time 0.004612042925004971\n",
            "loss 0.0004052232252433896 average time 0.005619027320008172\n",
            "loss 4.16200065612793 average time 0.004702904150003633\n",
            "loss 27.645193099975586 average time 0.005470218920011121\n",
            "loss 1.5112007856369019 average time 0.004639934919998723\n",
            "loss 8.953802108764648 average time 0.0053937534799933925\n",
            "loss 0.8463404774665833 average time 0.00461228853000307\n",
            "loss 3.7076785564422607 average time 0.005419129019996944\n",
            "loss 38.48027420043945 average time 0.0046306966800068496\n",
            "loss 3.0760934352874756 average time 0.005480609240016747\n",
            "loss 15.53012752532959 average time 0.004650981330009927\n",
            "loss 4.541407108306885 average time 0.005504894670002614\n",
            "loss 2.3110857009887695 average time 0.004673100790007538\n",
            "loss 20.331998825073242 average time 0.00557409655000356\n",
            "loss 0.746974527835846 average time 0.004689513074999922\n",
            "loss 24.304546356201172 average time 0.005454407639974761\n",
            "loss 0.03894232586026192 average time 0.004660085494987243\n",
            "loss 2.2558517456054688 average time 0.005335351539999919\n",
            "loss 1.8878356218338013 average time 0.004548113210007614\n",
            "loss 17.165788650512695 average time 0.005459446719992229\n",
            "loss 0.18515624105930328 average time 0.004638839854995922\n",
            "loss 0.10556847602128983 average time 0.0054840266500150395\n",
            "loss 9.17825698852539 average time 0.004651617305017908\n",
            "loss 22.67888832092285 average time 0.0054102757499913424\n",
            "loss 44.67457962036133 average time 0.004617606429995931\n",
            "loss 13.239460945129395 average time 0.0054342887900293135\n",
            "loss 15.95102596282959 average time 0.004632676420019379\n",
            "loss 5.875738143920898 average time 0.0053589079399989714\n",
            "loss 0.3221570551395416 average time 0.004595189884996671\n",
            "loss 6.431621551513672 average time 0.005408460340006514\n",
            "loss 18.2901554107666 average time 0.004585679170006642\n",
            "loss 49.04375076293945 average time 0.00533061607999798\n",
            "loss 3.111675500869751 average time 0.004536926345001575\n",
            "loss 24.12694549560547 average time 0.0053680902899964165\n",
            "loss 0.7229384779930115 average time 0.004633162424999\n",
            "loss 0.24696892499923706 average time 0.005368638570005259\n",
            "loss 0.0009486317285336554 average time 0.0045793864100051\n",
            "loss 0.15857921540737152 average time 0.005454675519995362\n",
            "loss 1.7999070882797241 average time 0.0046950447399956375\n",
            "loss 7.012119770050049 average time 0.0054277478900144165\n",
            "loss 3.565539836883545 average time 0.004621205065012646\n",
            "loss 47.871700286865234 average time 0.005417197350009246\n",
            "loss 0.6948683857917786 average time 0.004591249514996889\n",
            "loss 7.20432710647583 average time 0.005459070570027507\n",
            "loss 12.588035583496094 average time 0.0045896393000111856\n",
            "loss 38.334266662597656 average time 0.005295760630015138\n",
            "loss 10.677851676940918 average time 0.0045409554250136356\n",
            "loss 0.013699849136173725 average time 0.005312682799985851\n",
            "loss 2.042752265930176 average time 0.00452831827498585\n",
            "loss 8.156425476074219 average time 0.0053566074999935155\n",
            "loss 7.277587890625 average time 0.004540693035010008\n",
            "loss 12.238734245300293 average time 0.005332661099973848\n",
            "loss 1.530620813369751 average time 0.004544347204991936\n",
            "loss 5.034380592405796e-05 average time 0.0053348525200181025\n",
            "loss 1.752575397491455 average time 0.004537035874993762\n",
            "loss 0.591503381729126 average time 0.005447484959991016\n",
            "loss 0.980073094367981 average time 0.0046234763749930605\n",
            "loss 0.12783221900463104 average time 0.005362232450002011\n",
            "loss 0.38336819410324097 average time 0.004546927944993513\n",
            "loss 0.1477130502462387 average time 0.005330941730007908\n",
            "loss 43.873138427734375 average time 0.004608480055014752\n",
            "loss 4.942838191986084 average time 0.005637127700001656\n",
            "loss 2.0739312171936035 average time 0.004702644614990276\n",
            "loss 0.03286732733249664 average time 0.005378323709996948\n",
            "loss 6.5238847732543945 average time 0.004568722005010386\n",
            "loss 61.927066802978516 average time 0.005416113150017736\n",
            "loss 6.0088419914245605 average time 0.004614698665016022\n",
            "loss 8.118783950805664 average time 0.0053539748200137184\n",
            "loss 2.727315902709961 average time 0.004554436319997421\n",
            "loss 6.026397228240967 average time 0.005321553650010174\n",
            "loss 0.009425312280654907 average time 0.0045559564550057986\n",
            "loss 0.07846662402153015 average time 0.005389840119996734\n",
            "loss 1.1698342561721802 average time 0.004572199545007152\n",
            "loss 10.142362594604492 average time 0.0053257699100186075\n",
            "loss 0.9515595436096191 average time 0.004562564515001668\n",
            "loss 1.0695513486862183 average time 0.005340368559991475\n",
            "loss 7.399278163909912 average time 0.0045763281599965925\n",
            "loss 16.077104568481445 average time 0.005381700330012791\n",
            "loss 0.002215542597696185 average time 0.0045863143249926\n",
            "loss 34.24007034301758 average time 0.005325861160017666\n",
            "loss 24.985200881958008 average time 0.004552306219998172\n",
            "loss 60.48090362548828 average time 0.00533459166000739\n",
            "loss 37.738929748535156 average time 0.0045496610300119755\n",
            "loss 4.861486434936523 average time 0.005395759250004631\n",
            "loss 6.299000263214111 average time 0.004580517710001004\n",
            "loss 23.600811004638672 average time 0.005370643019969066\n",
            "loss 0.18543539941310883 average time 0.004597627054986333\n",
            "loss 54.4135856628418 average time 0.005400306260003163\n",
            "loss 18.686662673950195 average time 0.004616997810010162\n",
            "loss 4.3177947998046875 average time 0.0054204850300038744\n",
            "loss 30.747575759887695 average time 0.004590312089997042\n",
            "loss 39.56391906738281 average time 0.005332428910005547\n",
            "loss 1.616556167602539 average time 0.004590000564999173\n",
            "loss 3.2922332286834717 average time 0.005403333790022771\n",
            "loss 0.23080256581306458 average time 0.004599481040017963\n",
            "loss 1.8061151504516602 average time 0.005376928400010001\n",
            "loss 5.802106857299805 average time 0.004589593145012714\n",
            "loss 0.9177424311637878 average time 0.0053887922300100395\n",
            "loss 92.44921112060547 average time 0.004591446604999874\n",
            "loss 3.5636096000671387 average time 0.005666010289974111\n",
            "loss 13.011428833007812 average time 0.004743304464984704\n",
            "loss 4.880395889282227 average time 0.005466491610022785\n",
            "loss 3.464996814727783 average time 0.004620615240019106\n",
            "loss 17.698678970336914 average time 0.005384199990007801\n",
            "loss 1.9609763622283936 average time 0.004592455770011839\n",
            "loss 5.4741902351379395 average time 0.005313670079972326\n",
            "loss 12.868032455444336 average time 0.004566899624981033\n",
            "loss 25.985340118408203 average time 0.005401045910020912\n",
            "loss 10.315656661987305 average time 0.004623363170012453\n",
            "loss 9.247369766235352 average time 0.005412107789989022\n",
            "loss 8.737740516662598 average time 0.004581445049980175\n",
            "loss 0.37446868419647217 average time 0.005410865629996806\n",
            "loss 49.35324478149414 average time 0.0045822458400061805\n",
            "loss 16.171478271484375 average time 0.005296522270000424\n",
            "loss 14.97266960144043 average time 0.004572766870010128\n",
            "loss 17.060279846191406 average time 0.005552217599993128\n",
            "loss 2.90038800239563 average time 0.004637870114977432\n",
            "loss 2.598112106323242 average time 0.005801475950001986\n",
            "loss 9.033103942871094 average time 0.004777174059997833\n",
            "loss 3.1378190517425537 average time 0.005659529400004431\n",
            "loss 8.044618606567383 average time 0.0047269227349943325\n",
            "loss 4.138631820678711 average time 0.005556584830014799\n",
            "loss 21.390766143798828 average time 0.004643949944997985\n",
            "loss 61.99732971191406 average time 0.005337672530004056\n",
            "loss 39.891544342041016 average time 0.004525780654996652\n",
            "loss 0.872217059135437 average time 0.005412425830008942\n",
            "loss 1.4831115007400513 average time 0.004581589209992671\n",
            "loss 1.9989147186279297 average time 0.005343562580023899\n",
            "loss 0.5690593123435974 average time 0.004567770249998375\n",
            "loss 10.752311706542969 average time 0.005323198000005505\n",
            "loss 7.314414024353027 average time 0.004544633425000484\n",
            "loss 0.6621413826942444 average time 0.005601380399998562\n",
            "loss 9.690216064453125 average time 0.004679544685020573\n",
            "loss 11.312020301818848 average time 0.005345424360020843\n",
            "loss 12.522857666015625 average time 0.00457244585500348\n",
            "loss 2.8411004543304443 average time 0.00526911113002825\n",
            "loss 0.533693253993988 average time 0.0045127939550229715\n",
            "loss 5.846628189086914 average time 0.005352926039977319\n",
            "loss 5.677359104156494 average time 0.004578581490011402\n",
            "loss 0.08815062046051025 average time 0.005337744760017813\n",
            "loss 0.6698494553565979 average time 0.00453672348000282\n",
            "loss 78.23458862304688 average time 0.005349996520026252\n",
            "loss 0.0011818504426628351 average time 0.004532933465020506\n",
            "loss 0.12345407158136368 average time 0.005375383059995329\n",
            "loss 0.06965569406747818 average time 0.004576138995009842\n",
            "loss 3.5003209114074707 average time 0.0053398628700142585\n",
            "loss 2.4661688804626465 average time 0.004546864970020579\n",
            "loss 13.76707935333252 average time 0.0054607576799935486\n",
            "loss 6.784671306610107 average time 0.004585815545003698\n",
            "loss 11.838472366333008 average time 0.005274753540038546\n",
            "loss 27.226055145263672 average time 0.004550573435017214\n",
            "loss 4.681515693664551 average time 0.005343845839993264\n",
            "loss 4.443663120269775 average time 0.004538255115003267\n",
            "loss 7.130927562713623 average time 0.005377350439998736\n",
            "loss 8.22826862335205 average time 0.004575209455003915\n",
            "loss 6.778452396392822 average time 0.005328966179990857\n",
            "loss 13.836450576782227 average time 0.00451162522498862\n",
            "loss 6.489273548126221 average time 0.005351855539984172\n",
            "loss 14.223662376403809 average time 0.004544909925009506\n",
            "loss 44.25783920288086 average time 0.0052733835100070795\n",
            "loss 0.9757651090621948 average time 0.004484798500004672\n",
            "loss 1.2878228425979614 average time 0.005521368219983742\n",
            "loss 27.561840057373047 average time 0.00466052313999171\n",
            "loss 6.299373626708984 average time 0.00543039036000664\n",
            "loss 7.928448677062988 average time 0.00460795957499613\n",
            "loss 22.14088249206543 average time 0.005331274230006784\n",
            "loss 0.011138951405882835 average time 0.004539861450020908\n",
            "loss 10.370434761047363 average time 0.005369729939966419\n",
            "loss 1.2405930757522583 average time 0.004541731389986125\n",
            "loss 5.9146294593811035 average time 0.005511463330030893\n",
            "loss 22.3237361907959 average time 0.004624728955018326\n",
            "loss 2.012848377227783 average time 0.005428837079916775\n",
            "loss 0.27692389488220215 average time 0.004575535489971116\n",
            "loss 0.3965217173099518 average time 0.005508266660026493\n",
            "loss 10.06781005859375 average time 0.004614325895008733\n",
            "loss 7.673325061798096 average time 0.005378554099997927\n",
            "loss 1.1330116987228394 average time 0.004605741950022093\n",
            "loss 21.576597213745117 average time 0.0053782596100518274\n",
            "loss 18.696855545043945 average time 0.004551099755021823\n",
            "loss 5.800930500030518 average time 0.005380784679973658\n",
            "loss 0.13267263770103455 average time 0.004573702199959371\n",
            "loss 1.3354412317276 average time 0.005336839300025531\n",
            "loss 2.046024799346924 average time 0.004543765905041255\n",
            "loss 4.713048458099365 average time 0.00553508062003857\n",
            "loss 11.88720417022705 average time 0.004618428625008164\n",
            "loss 30.8975830078125 average time 0.005342837219968715\n",
            "loss 16.762474060058594 average time 0.00452330503997473\n",
            "loss 11.53799819946289 average time 0.00533152248002807\n",
            "loss 0.6531577706336975 average time 0.004527468290002616\n",
            "loss 22.444730758666992 average time 0.0053685648599639535\n",
            "loss 19.983400344848633 average time 0.004549967794955592\n",
            "loss 0.06023026257753372 average time 0.005356772869981796\n",
            "loss 4.793880462646484 average time 0.004545716595007434\n",
            "loss 0.7533842921257019 average time 0.005363708310096626\n",
            "loss 0.23589396476745605 average time 0.004561564055056806\n",
            "loss 42.13671112060547 average time 0.005371629010041943\n",
            "loss 1.5385124683380127 average time 0.004550888660019154\n",
            "loss 0.33007556200027466 average time 0.005439068350015077\n",
            "loss 5.87640380859375 average time 0.0045711337399689\n",
            "loss 1.3054803609848022 average time 0.005381734110042089\n",
            "loss 5.824824810028076 average time 0.004590245520030294\n",
            "loss 5.263375282287598 average time 0.0053345172499939504\n",
            "loss 17.887216567993164 average time 0.004540370869995059\n",
            "loss 19.430404663085938 average time 0.005370087969940869\n",
            "loss 3.1798949241638184 average time 0.004632394699965517\n",
            "loss 30.94189453125 average time 0.005424920869927519\n",
            "loss 1.984271764755249 average time 0.004586376739948719\n",
            "loss 9.696535110473633 average time 0.0053045266599383465\n",
            "loss 35.49822235107422 average time 0.004564556724972135\n",
            "loss 0.00019069411791861057 average time 0.005298629979970429\n",
            "loss 8.571002960205078 average time 0.004507166034995862\n",
            "loss 7.0796990394592285 average time 0.0054456704399490265\n",
            "loss 2.6991658210754395 average time 0.004565439574998891\n",
            "loss 12.766969680786133 average time 0.005297384699924805\n",
            "loss 32.30302047729492 average time 0.0045075088199882886\n",
            "loss 31.137910842895508 average time 0.005340907550043994\n",
            "loss 0.00010569092410150915 average time 0.004539553045037792\n",
            "loss 111.88308715820312 average time 0.0053324711800269145\n",
            "loss 4.316471099853516 average time 0.004556550905026597\n",
            "loss 21.033430099487305 average time 0.005483766450051917\n",
            "loss 4.052639484405518 average time 0.004600889030011785\n",
            "loss 0.6898215413093567 average time 0.005282710889978262\n",
            "loss 32.11955642700195 average time 0.004504118619970541\n",
            "loss 1.0024199485778809 average time 0.005364028299991333\n",
            "loss 5.000668525695801 average time 0.00454023811497791\n",
            "loss 32.20600509643555 average time 0.005337058279956182\n",
            "loss 20.24563980102539 average time 0.004558115280001402\n",
            "loss 18.372112274169922 average time 0.005434937279987935\n",
            "loss 7.603362560272217 average time 0.0045787716149652625\n",
            "loss 3.6562936305999756 average time 0.005328368819928073\n",
            "loss 7.624752044677734 average time 0.004551339740000913\n",
            "loss 0.6431061625480652 average time 0.005323993040065034\n",
            "loss 0.43880313634872437 average time 0.004541945260029934\n",
            "loss 53.01688003540039 average time 0.005309376539962614\n",
            "loss 0.7452841401100159 average time 0.004518709749945628\n",
            "loss 0.5532968640327454 average time 0.005372047010014285\n",
            "loss 9.170100212097168 average time 0.004547708570007671\n",
            "loss 4.965372085571289 average time 0.005353743890009355\n",
            "loss 1.0856602191925049 average time 0.004534100779987966\n",
            "loss 6.945628643035889 average time 0.005306213109988676\n",
            "loss 0.5429770946502686 average time 0.004505199860013818\n",
            "loss 12.695469856262207 average time 0.005283345290026773\n",
            "loss 10.09259033203125 average time 0.004502301195016116\n",
            "loss 0.846684455871582 average time 0.005290527270044549\n",
            "loss 12.055315017700195 average time 0.004509233365024556\n",
            "loss 20.144155502319336 average time 0.005322043479991407\n",
            "loss 0.11640564352273941 average time 0.004613378370013379\n",
            "loss 5.764301776885986 average time 0.005437255890028609\n",
            "loss 0.591344952583313 average time 0.004614753780001593\n",
            "loss 99.3692855834961 average time 0.00532423344999188\n",
            "loss 12.0986909866333 average time 0.004535726785002225\n",
            "loss 8.903090476989746 average time 0.005367900539968104\n",
            "loss 2.667011022567749 average time 0.004601824504993602\n",
            "loss 0.1807151585817337 average time 0.0053604358600023265\n",
            "loss 4.795600891113281 average time 0.004562361915004658\n",
            "loss 6.255856990814209 average time 0.005415712379999604\n",
            "loss 0.41067415475845337 average time 0.004618704619992968\n",
            "loss 2.131697654724121 average time 0.005363884389971645\n",
            "loss 0.7570574283599854 average time 0.004532700724989808\n",
            "loss 59.767608642578125 average time 0.005282059999999546\n",
            "loss 38.69733810424805 average time 0.004528496375005488\n",
            "loss 7.991754531860352 average time 0.0054219652600113475\n",
            "loss 0.03378304839134216 average time 0.004568239160007579\n",
            "loss 18.220884323120117 average time 0.005252976440006023\n",
            "loss 2.067662239074707 average time 0.004500692150031682\n",
            "loss 50.37131118774414 average time 0.005409044560028633\n",
            "loss 20.17877197265625 average time 0.004565095189987005\n",
            "loss 7.640201568603516 average time 0.005305867229990327\n",
            "loss 6.54080057144165 average time 0.004501396954983647\n",
            "loss 8.638343811035156 average time 0.005291164229965943\n",
            "loss 0.23520338535308838 average time 0.004516390340022554\n",
            "loss 22.50572967529297 average time 0.005343055410021407\n",
            "loss 24.298416137695312 average time 0.004561536555015664\n",
            "loss 2.9621715545654297 average time 0.005349420430056853\n",
            "loss 0.33618974685668945 average time 0.004571075385038057\n",
            "loss 4.590134143829346 average time 0.005443257220003943\n",
            "loss 0.9398000240325928 average time 0.004603284775021166\n",
            "loss 6.085500717163086 average time 0.00532081656997434\n",
            "loss 8.233193397521973 average time 0.004532987369998409\n",
            "loss 20.980348587036133 average time 0.005354150190050859\n",
            "loss 4.4303483963012695 average time 0.004564623890037183\n",
            "loss 4.782686710357666 average time 0.005358448329952807\n",
            "loss 50.5609245300293 average time 0.004557396819968744\n",
            "loss 14.324433326721191 average time 0.005359517119941302\n",
            "loss 8.520987510681152 average time 0.004593388459957168\n",
            "loss 3.8153231143951416 average time 0.005435921469997993\n",
            "loss 2.022010564804077 average time 0.0045677678849915535\n",
            "loss 5.171796798706055 average time 0.005402058799982115\n",
            "loss 40.9124755859375 average time 0.004559887605028052\n",
            "loss 0.5676156282424927 average time 0.005331232699927568\n",
            "loss 3.2592105865478516 average time 0.004588861339962023\n",
            "loss 13.743240356445312 average time 0.005426365190005527\n",
            "loss 1.3412578105926514 average time 0.004586944480024613\n",
            "loss 3.9875357151031494 average time 0.00534429314992849\n",
            "loss 46.52842330932617 average time 0.004567973799958054\n",
            "loss 7.230531692504883 average time 0.005342185789950236\n",
            "loss 7.674233913421631 average time 0.004568559969980015\n",
            "loss 0.01892324909567833 average time 0.005341162010026892\n",
            "loss 8.767975807189941 average time 0.004537804635015163\n",
            "loss 6.2451372146606445 average time 0.005310138670029119\n",
            "loss 11.178878784179688 average time 0.004512503310029388\n",
            "loss 49.739288330078125 average time 0.005339419599977191\n",
            "loss 14.181224822998047 average time 0.004545989024995834\n",
            "loss 14.284728050231934 average time 0.005352560910041575\n",
            "loss 21.684572219848633 average time 0.004564389775027849\n",
            "loss 11.786946296691895 average time 0.005359200370075996\n",
            "loss 3.855036497116089 average time 0.004534524040045653\n",
            "loss 24.018875122070312 average time 0.005323644999971293\n",
            "loss 27.68785285949707 average time 0.004551447354970151\n",
            "loss 1.019526481628418 average time 0.0053744185300274696\n",
            "loss 14.106809616088867 average time 0.004547114490028434\n",
            "loss 14.239550590515137 average time 0.005348047570005292\n",
            "loss 14.108485221862793 average time 0.004545267260009495\n",
            "loss 24.250545501708984 average time 0.005338983170031497\n",
            "loss 2.82651686668396 average time 0.004545033260019409\n",
            "loss 0.5991175770759583 average time 0.0054156676699858505\n",
            "loss 5.652390956878662 average time 0.004605723249987932\n",
            "loss 39.35922622680664 average time 0.005333573449961477\n",
            "loss 0.8175252079963684 average time 0.004561273359963707\n",
            "loss 4.590019702911377 average time 0.005370966400005272\n",
            "loss 4.263258457183838 average time 0.004575155210027333\n",
            "loss 9.069231986999512 average time 0.005339455739976984\n",
            "loss 6.478277683258057 average time 0.004532698690009056\n",
            "loss 12.254539489746094 average time 0.005353806799939775\n",
            "loss 0.3476411998271942 average time 0.004554139779975231\n",
            "loss 2.359762668609619 average time 0.0052967408099448225\n",
            "loss 0.020163513720035553 average time 0.004522421744945859\n",
            "loss 40.15971755981445 average time 0.005276141459999053\n",
            "loss 6.987998008728027 average time 0.004501509005012849\n",
            "loss 0.5630208253860474 average time 0.005308110149999265\n",
            "loss 21.297483444213867 average time 0.004509084850019463\n",
            "loss 6.142264366149902 average time 0.005350331180006833\n",
            "loss 2.743555784225464 average time 0.004576057374983975\n",
            "loss 3.122053861618042 average time 0.005333295580012418\n",
            "loss 28.72046661376953 average time 0.004544399030010027\n",
            "loss 5.263471603393555 average time 0.005307863850039212\n",
            "loss 42.648956298828125 average time 0.004528726910025398\n",
            "loss 5.756876468658447 average time 0.0054918908399668\n",
            "loss 3.4606950283050537 average time 0.004642407649994311\n",
            "loss 3.5940780639648438 average time 0.0053619579199585135\n",
            "loss 13.587305068969727 average time 0.0045558085349739486\n",
            "loss 0.29831963777542114 average time 0.005335072929956369\n",
            "loss 19.24855613708496 average time 0.004522694089978359\n",
            "loss 18.259366989135742 average time 0.005295618489963089\n",
            "loss 0.1567555069923401 average time 0.004525119310019363\n",
            "loss 16.108030319213867 average time 0.0053602853899519685\n",
            "loss 4.581638336181641 average time 0.0045984508249512146\n",
            "loss 0.902874767780304 average time 0.005301985670048453\n",
            "loss 7.457667827606201 average time 0.004551715080019676\n",
            "loss 52.27907180786133 average time 0.005369403380000221\n",
            "loss 14.876290321350098 average time 0.004585025690039402\n",
            "loss 3.2047793865203857 average time 0.005405131369971059\n",
            "loss 19.186614990234375 average time 0.00457214216502507\n",
            "loss 0.7285670638084412 average time 0.005373902949950207\n",
            "loss 0.43162089586257935 average time 0.004560715094962689\n",
            "loss 18.833118438720703 average time 0.005309476349984834\n",
            "loss 10.452077865600586 average time 0.004560770114981096\n",
            "loss 3.078770160675049 average time 0.005475469599959979\n",
            "loss 14.272735595703125 average time 0.00461705367002196\n",
            "loss 5.03385591506958 average time 0.005311874509989139\n",
            "loss 4.0314106941223145 average time 0.004530791384954682\n",
            "loss 15.29648208618164 average time 0.005438533960050336\n",
            "loss 62.687767028808594 average time 0.004583974425045199\n",
            "loss 14.799145698547363 average time 0.005416771740019612\n",
            "loss 0.015919148921966553 average time 0.004611899600026845\n",
            "loss 14.960037231445312 average time 0.005324441220054723\n",
            "loss 1.1613091230392456 average time 0.0045499785150514074\n",
            "loss 0.19216302037239075 average time 0.0053337832800662\n",
            "loss 0.575575590133667 average time 0.0045212862300240885\n",
            "loss 28.486722946166992 average time 0.005298447219965965\n",
            "loss 40.67020034790039 average time 0.0045253062149686225\n",
            "loss 35.2315788269043 average time 0.005451118310011225\n",
            "loss 22.234283447265625 average time 0.004585621120022552\n",
            "loss 0.6152250170707703 average time 0.005440479280068757\n",
            "loss 0.11142583191394806 average time 0.0045731389150478205\n",
            "loss 4.4899001121521 average time 0.005315384690056817\n",
            "loss 0.4292508065700531 average time 0.004540911350059105\n",
            "loss 0.27761486172676086 average time 0.005456175369945413\n",
            "loss 32.3443603515625 average time 0.004638870279982257\n",
            "loss 1.5674725770950317 average time 0.005471510110010058\n",
            "loss 2.6583774089813232 average time 0.004617058970025028\n",
            "loss 34.987552642822266 average time 0.005539356730050713\n",
            "loss 1.857339859008789 average time 0.0046095995150199085\n",
            "loss 8.35136890411377 average time 0.005158503020038552\n",
            "loss 0.027864735573530197 average time 0.004335264820010707\n",
            "loss 88.14505004882812 average time 0.0050335696100046335\n",
            "loss 6.68522834777832 average time 0.004255149615005394\n",
            "loss 3.279475212097168 average time 0.005049395390033169\n",
            "loss 0.518286406993866 average time 0.004283788350035138\n",
            "loss 2.5336532592773438 average time 0.005096686099996077\n",
            "loss 5.261135101318359 average time 0.004250220579965571\n",
            "loss 50.42815399169922 average time 0.00510500346002118\n",
            "loss 7.2744598388671875 average time 0.00425768750998941\n",
            "loss 3.104997158050537 average time 0.005227117260010346\n",
            "loss 24.75861930847168 average time 0.004328901829999268\n",
            "loss 15.257567405700684 average time 0.00530561897008738\n",
            "loss 1.158351182937622 average time 0.004495625045074121\n",
            "loss 5.460276126861572 average time 0.005438303880055173\n",
            "loss 10.587347030639648 average time 0.004460934090029695\n",
            "loss 0.1184908002614975 average time 0.005019716340029845\n",
            "loss 4.848752021789551 average time 0.004206800929996462\n",
            "loss 23.870084762573242 average time 0.005033283690054304\n",
            "loss 0.05864924192428589 average time 0.004224570369997309\n",
            "loss 16.592979431152344 average time 0.005033756159964469\n",
            "loss 0.6606429815292358 average time 0.004283380004985702\n",
            "loss 8.203600883483887 average time 0.005099253199969098\n",
            "loss 27.553327560424805 average time 0.00442039034499885\n",
            "loss 5.547891616821289 average time 0.0055541770599757\n",
            "loss 1.7156938314437866 average time 0.004634797649955545\n",
            "loss 13.266763687133789 average time 0.005120951680000871\n",
            "loss 9.41000747680664 average time 0.004360448039992661\n",
            "loss 6.565370082855225 average time 0.005317882509943956\n",
            "loss 0.6589542031288147 average time 0.004503124219977508\n",
            "loss 0.13955986499786377 average time 0.005555054829983419\n",
            "loss 0.8936517834663391 average time 0.004711763754976346\n",
            "loss 30.679903030395508 average time 0.0055554950499481495\n",
            "loss 9.107458114624023 average time 0.004710889024963762\n",
            "loss 11.285067558288574 average time 0.005788685779998559\n",
            "loss 0.10135997831821442 average time 0.004774149794943696\n",
            "loss 0.6081303358078003 average time 0.0053244704399185135\n",
            "loss 0.026661762967705727 average time 0.004505758090008385\n",
            "loss 69.19351959228516 average time 0.0050339150299441825\n",
            "loss 0.0023541064001619816 average time 0.004256129305013019\n",
            "loss 10.467944145202637 average time 0.005186775820038747\n",
            "loss 12.393142700195312 average time 0.004317120980049367\n",
            "loss 0.002423086203634739 average time 0.005126816890133341\n",
            "loss 9.734679222106934 average time 0.004277598665094046\n",
            "loss 0.18015125393867493 average time 0.005053040649981994\n",
            "loss 0.00013777650019619614 average time 0.004243727949979075\n",
            "loss 17.89613914489746 average time 0.00507684516000154\n",
            "loss 16.98853874206543 average time 0.004270498884989138\n",
            "loss 2.410385847091675 average time 0.004990380319977703\n",
            "loss 7.239027500152588 average time 0.004225920704930104\n",
            "loss 4.104633808135986 average time 0.0050890273100048945\n",
            "loss 2.666774272918701 average time 0.00430191952498717\n",
            "loss 0.7021310329437256 average time 0.005117055470054766\n",
            "loss 1.024763822555542 average time 0.004269404950091485\n",
            "loss 0.07404591888189316 average time 0.005014166450055199\n",
            "loss 25.472871780395508 average time 0.004264150404960674\n",
            "loss 60.979976654052734 average time 0.0050467518900222785\n",
            "loss 0.7880164980888367 average time 0.004231663910013594\n",
            "loss 6.8115458488464355 average time 0.005153746140149451\n",
            "loss 48.830501556396484 average time 0.0043214127300780095\n",
            "loss 13.34273624420166 average time 0.0050440235299174675\n",
            "loss 2.163057565689087 average time 0.004234580194915907\n",
            "loss 0.19467280805110931 average time 0.005000578840063099\n",
            "loss 10.253950119018555 average time 0.004206652015091095\n",
            "loss 11.0355863571167 average time 0.00507721517005848\n",
            "loss 0.00981743074953556 average time 0.004278530500068882\n",
            "loss 0.3159772455692291 average time 0.005038467429967568\n",
            "loss 3.469083458185196e-05 average time 0.004238362509922809\n",
            "loss 32.332122802734375 average time 0.005047246630092559\n",
            "loss 16.227487564086914 average time 0.00423823552997419\n",
            "loss 0.11134180426597595 average time 0.005039239790039574\n",
            "loss 1.5141425132751465 average time 0.004276812139951289\n",
            "loss 2.394120693206787 average time 0.0051007591900452095\n",
            "loss 17.74683952331543 average time 0.004270882995033389\n",
            "loss 0.04882022365927696 average time 0.004989448340129456\n",
            "loss 16.732282638549805 average time 0.00423003257507844\n",
            "loss 38.167110443115234 average time 0.005075767870021081\n",
            "loss 8.901166915893555 average time 0.0043231194450345355\n",
            "loss 2.583707094192505 average time 0.005003277849991719\n",
            "loss 0.9017224907875061 average time 0.004231908415122234\n",
            "loss 1.385180950164795 average time 0.0050646157999472054\n",
            "loss 40.511497497558594 average time 0.0042875272599576415\n",
            "loss 0.035056184977293015 average time 0.005248704309924506\n",
            "loss 2.3442184925079346 average time 0.004462363644952347\n",
            "loss 23.83466911315918 average time 0.005288957290085819\n",
            "loss 9.643083572387695 average time 0.004533007855088727\n",
            "loss 1.0674850940704346 average time 0.005366667919952306\n",
            "loss 9.166866302490234 average time 0.004419905534969075\n",
            "loss 1.2797187566757202 average time 0.005048959869945975\n",
            "loss 0.9914923310279846 average time 0.004293960585000605\n",
            "loss 49.28817367553711 average time 0.005158477639979537\n",
            "loss 6.8732523918151855 average time 0.00449163503996715\n",
            "loss 4.613112449645996 average time 0.005325317639981222\n",
            "loss 65.53694915771484 average time 0.004557059745020524\n",
            "loss 1.352866530418396 average time 0.005334563780015742\n",
            "loss 15.180152893066406 average time 0.004581892209998841\n",
            "loss 13.433489799499512 average time 0.005226101039934292\n",
            "loss 12.958642959594727 average time 0.0043872408950301174\n",
            "loss 3.4232001304626465 average time 0.005079768820014579\n",
            "loss 20.52113151550293 average time 0.004263366425020649\n",
            "loss 11.77906322479248 average time 0.005068559019982786\n",
            "loss 3.5453851222991943 average time 0.004271190049948927\n",
            "loss 3.0614452362060547 average time 0.005036904190092173\n",
            "loss 60.95289993286133 average time 0.004240696200004095\n",
            "loss 17.494462966918945 average time 0.004984411350105802\n",
            "loss 1.518821358680725 average time 0.0042412877750393815\n",
            "loss 38.34364318847656 average time 0.005054002959986974\n",
            "loss 15.923325538635254 average time 0.004231896474975656\n",
            "loss 0.09844221919775009 average time 0.005060799889943155\n",
            "loss 2.005035400390625 average time 0.0042808786499244885\n",
            "loss 8.140461921691895 average time 0.005093730080043315\n",
            "loss 7.396040916442871 average time 0.004255501929983438\n",
            "loss 10.974198341369629 average time 0.005019393519978621\n",
            "loss 3.4382712841033936 average time 0.004241171449975809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu"
      },
      "source": [
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp"
      },
      "source": [
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, S, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 300, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, S, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 100.0, 120.0, sigma, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}