{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/European_Call_jax_1stock_v3_priceonly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "8359008e-3b9a-46f3-faff-146ff13db383"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0807]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.2597*0.2597\n",
        "initial_stocks = jnp.array([0.7178]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.2106\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5235998\n",
            "[1.0001004]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177bb330-fb01-46b9-a4f9-592d2865ae91"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "# version 1, 2, 6\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(0.75 + np.random.random(self.N_STOCKS) * 0.5)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(0.15 + np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(0.25 + np.random.random(1) * 0.35), self.N_STOCKS)\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          K = 0.75 + np.random.random(1) * 0.5\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32) # remember to change this!\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "a4496bef-6fac-499e-f276-28d1974ab5d2"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.35, 0.35]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.25, 0.25]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "9eb086e7-471c-4b6a-bf02-79002de1816f"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "96eef241-5492-4639-9a15-3526c96dba94"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.04367094114422798 average time 0.5912234553500013 iter num 20\n",
            "loss 0.040439706295728683 average time 0.3209088441250003 iter num 40\n",
            "loss 0.019552085548639297 average time 0.23108287316666748 iter num 60\n",
            "loss 0.02303377538919449 average time 0.1868861621750007 iter num 80\n",
            "loss 0.019472695887088776 average time 0.15979123976999973 iter num 100\n",
            "loss 0.021109173074364662 average time 0.14715574040000376 iter num 20\n",
            "loss 0.02306990697979927 average time 0.09935025800001256 iter num 40\n",
            "loss 0.013570882380008698 average time 0.08371536236667415 iter num 60\n",
            "loss 0.016068110242486 average time 0.07569062680000513 iter num 80\n",
            "loss 0.016461433842778206 average time 0.07053868729000101 iter num 100\n",
            "loss 0.013583388179540634 average time 0.14726887365000607 iter num 20\n",
            "loss 0.008109602145850658 average time 0.1000830599000011 iter num 40\n",
            "loss 0.011103606782853603 average time 0.08399280368333659 iter num 60\n",
            "loss 0.00822502002120018 average time 0.07619210409999795 iter num 80\n",
            "loss 0.012723824009299278 average time 0.07186471713999822 iter num 100\n",
            "loss 0.006729885004460812 average time 0.1414389899000014 iter num 20\n",
            "loss 0.004065180663019419 average time 0.09721371870000439 iter num 40\n",
            "loss 0.003519211895763874 average time 0.08316595300000244 iter num 60\n",
            "loss 0.0025459749158471823 average time 0.07517130948750435 iter num 80\n",
            "loss 0.0022022027987986803 average time 0.07127157586000521 iter num 100\n",
            "loss 0.000997511437162757 average time 0.13931496099999094 iter num 20\n",
            "loss 0.0008560613496229053 average time 0.09526800585001069 iter num 40\n",
            "loss 0.000855022924952209 average time 0.08102691413334166 iter num 60\n",
            "loss 0.001025969977490604 average time 0.07443978361249606 iter num 80\n",
            "loss 0.0005569452187046409 average time 0.06989435916999127 iter num 100\n",
            "loss 0.0005388597492128611 average time 0.1412831947500422 iter num 20\n",
            "loss 0.0003878998104482889 average time 0.09725875715001848 iter num 40\n",
            "loss 0.0003682239621412009 average time 0.08226991238335586 iter num 60\n",
            "loss 0.0005056161317043006 average time 0.07456714695000528 iter num 80\n",
            "loss 0.0008087889291346073 average time 0.0702710309300096 iter num 100\n",
            "loss 0.0003414314705878496 average time 0.1404926378499681 iter num 20\n",
            "loss 0.00042726838728412986 average time 0.09769403610000041 iter num 40\n",
            "loss 0.00042940975981764495 average time 0.08246572946666599 iter num 60\n",
            "loss 0.00033540785079821944 average time 0.0753785258125049 iter num 80\n",
            "loss 0.0005012794863432646 average time 0.07108052375000852 iter num 100\n",
            "loss 0.0007196074584499002 average time 0.14424649890004276 iter num 20\n",
            "loss 0.0003814144292846322 average time 0.09837472635002768 iter num 40\n",
            "loss 0.00026979486574418843 average time 0.08324412646668028 iter num 60\n",
            "loss 0.00027245888486504555 average time 0.07572127174999821 iter num 80\n",
            "loss 0.0004388177767395973 average time 0.07225060532999805 iter num 100\n",
            "loss 0.00044804494245909154 average time 0.14377725225003815 iter num 20\n",
            "loss 0.0005171962548047304 average time 0.09831091587502101 iter num 40\n",
            "loss 0.0004951973096467555 average time 0.08391963645001017 iter num 60\n",
            "loss 0.00026583598810248077 average time 0.07642985335001243 iter num 80\n",
            "loss 0.00044462832738645375 average time 0.07279259601000604 iter num 100\n",
            "loss 0.00040509001701138914 average time 0.14979128034999575 iter num 20\n",
            "loss 0.00040826000622473657 average time 0.10028355592499452 iter num 40\n",
            "loss 0.0004045687965117395 average time 0.08557414854999858 iter num 60\n",
            "loss 0.00016936499741859734 average time 0.07830762951249426 iter num 80\n",
            "loss 0.0002755157183855772 average time 0.07340338010998494 iter num 100\n",
            "loss 0.0004111534508410841 average time 0.14164636355003496 iter num 20\n",
            "loss 0.0003812761860899627 average time 0.09810192110003299 iter num 40\n",
            "loss 0.0003063976764678955 average time 0.08402365741668898 iter num 60\n",
            "loss 0.0006048189825378358 average time 0.076319031037508 iter num 80\n",
            "loss 0.0002555797982495278 average time 0.07119157720001112 iter num 100\n",
            "loss 0.0006243965472094715 average time 0.1435716408499502 iter num 20\n",
            "loss 0.00024949992075562477 average time 0.09852623297499577 iter num 40\n",
            "loss 0.0003798056859523058 average time 0.0837545737999714 iter num 60\n",
            "loss 0.00021219138579908758 average time 0.0768767024499823 iter num 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-60d4d1cd22ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_european_1stock_test_1.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m           \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m           \u001b[0mEuropean_Call_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m           \u001b[0mgooptionvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptionvalueavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m           \u001b[0mDeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooptionvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36moptionvalueavg\u001b[0;34m(key, initial_stocks, numsteps, drift, r, cov, K, T, keys)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m###################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m   5653\u001b[0m   \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_index_for_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5654\u001b[0m   return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[0;32m-> 5655\u001b[0;31m                  unique_indices)\n\u001b[0m\u001b[1;32m   5656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[0;31m# TODO(phawkins): re-enable jit after fixing excessive recompilation for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m   5661\u001b[0m             unique_indices):\n\u001b[1;32m   5662\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_static_and_dynamic_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5663\u001b[0;31m   \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_index_to_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shared with _scatter_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5664\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   5852\u001b[0m         \u001b[0;31m# XLA gives error when indexing into an axis of size 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5853\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"index is out of bounds for axis {x_axis} with size 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5854\u001b[0;31m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_axis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnormalize_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5855\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5856\u001b[0m       \u001b[0mgather_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgather_indices_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_normalize_index\u001b[0;34m(index, axis_size)\u001b[0m\n\u001b[1;32m   5436\u001b[0m   return lax.select(\n\u001b[1;32m   5437\u001b[0m     \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_constant_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5438\u001b[0;31m     \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5439\u001b[0m     index)\n\u001b[1;32m   5440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;34mr\"\"\"Elementwise addition: :math:`x + y`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0madd_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    266\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cd78e5-f56b-46b9-8ed8-3a51b0d87feb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_test_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58206fe0-aa56-4fab-d958-de126cadc0e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1106fd84-ad99-44b2-e245-e10d0e926348"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_test_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93312a3-4e05-4c8e-9851-f4a556f754e8"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763accc5-f5e4-4a51-e85e-17148b3abae3"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1000000, batch = 16, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 10\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 20)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_test_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.0003371097263880074 average time 0.3695418690003862 iter num 10\n",
            "loss 1.65747114806436e-05 average time 0.19877912855026808 iter num 20\n",
            "loss 2.7993793992209248e-05 average time 0.14245014933355074 iter num 30\n",
            "loss 2.985338869621046e-05 average time 0.11396422715015433 iter num 40\n",
            "loss 2.2593023459194228e-05 average time 0.09662082618015119 iter num 50\n",
            "loss 1.4428145732381381e-05 average time 0.0849570476168689 iter num 60\n",
            "loss 2.2074871594668366e-05 average time 0.07682620735738185 iter num 70\n",
            "loss 4.197245289105922e-05 average time 0.07051296936283506 iter num 80\n",
            "loss 1.990869532164652e-05 average time 0.06585930705587088 iter num 90\n",
            "loss 6.209993443917483e-06 average time 0.06233861934022571 iter num 100\n",
            "loss 4.576627543428913e-05 average time 0.36713456090001273 iter num 10\n",
            "loss 4.017864557681605e-05 average time 0.1975041710998994 iter num 20\n",
            "loss 3.739510430023074e-05 average time 0.1410590825664258 iter num 30\n",
            "loss 4.53769534942694e-05 average time 0.11273821709983167 iter num 40\n",
            "loss 2.933954237960279e-05 average time 0.0959427075998974 iter num 50\n",
            "loss 1.828931999625638e-05 average time 0.08469393206651148 iter num 60\n",
            "loss 2.061160012090113e-05 average time 0.07638205385696242 iter num 70\n",
            "loss 2.295724698342383e-05 average time 0.07047010987475914 iter num 80\n",
            "loss 1.5064757462823763e-05 average time 0.06577090309979718 iter num 90\n",
            "loss 2.0688334188889712e-05 average time 0.06209885050982848 iter num 100\n",
            "loss 1.3689139450434595e-05 average time 0.36804858360083015 iter num 10\n",
            "loss 1.7569738702150062e-05 average time 0.19843921980045706 iter num 20\n",
            "loss 6.511100218631327e-05 average time 0.14156700743369582 iter num 30\n",
            "loss 4.2855819629039615e-05 average time 0.11303327930054366 iter num 40\n",
            "loss 1.2319823326834012e-05 average time 0.09564112136031326 iter num 50\n",
            "loss 2.3706637875875458e-05 average time 0.08475271645026321 iter num 60\n",
            "loss 2.9979031751281582e-05 average time 0.0767181925859664 iter num 70\n",
            "loss 2.8666607249761e-05 average time 0.07069440351274352 iter num 80\n",
            "loss 1.7808697521104477e-05 average time 0.06599158146689459 iter num 90\n",
            "loss 4.064312452101149e-05 average time 0.062221320160206234 iter num 100\n",
            "loss 1.4734727301402017e-05 average time 0.3697347356999671 iter num 10\n",
            "loss 2.8917984309373423e-05 average time 0.1996423540998876 iter num 20\n",
            "loss 4.4885480747325346e-05 average time 0.1417218895665428 iter num 30\n",
            "loss 1.745901317917742e-05 average time 0.11281340932464445 iter num 40\n",
            "loss 2.5223380362149328e-05 average time 0.0963582641598623 iter num 50\n",
            "loss 8.193294343072921e-05 average time 0.08541530836658542 iter num 60\n",
            "loss 7.460801043635001e-06 average time 0.07702805158566792 iter num 70\n",
            "loss 1.7327931345789693e-05 average time 0.07069078351255484 iter num 80\n",
            "loss 1.2449811947590206e-05 average time 0.06596894198895219 iter num 90\n",
            "loss 2.492535531928297e-05 average time 0.062152588219942115 iter num 100\n",
            "loss 2.922479689004831e-05 average time 0.3682547922002414 iter num 10\n",
            "loss 9.379471521242522e-06 average time 0.19868682570031523 iter num 20\n",
            "loss 1.2339462955424096e-05 average time 0.14198449283358058 iter num 30\n",
            "loss 3.826358442893252e-05 average time 0.11314528697521382 iter num 40\n",
            "loss 2.1365405700635165e-05 average time 0.09573972406018584 iter num 50\n",
            "loss 1.0736722288129386e-05 average time 0.08455283908357766 iter num 60\n",
            "loss 2.2552900190930814e-05 average time 0.07629211391447045 iter num 70\n",
            "loss 3.421500150579959e-05 average time 0.06997160496275683 iter num 80\n",
            "loss 2.67185787379276e-05 average time 0.06526333178909327 iter num 90\n",
            "loss 3.290533277322538e-05 average time 0.06162124229023903 iter num 100\n",
            "loss 4.2108607885893434e-05 average time 0.36994177509986914 iter num 10\n",
            "loss 2.4907387341954745e-05 average time 0.19976537754973833 iter num 20\n",
            "loss 2.603547545731999e-05 average time 0.1425376443330606 iter num 30\n",
            "loss 1.580826392455492e-05 average time 0.11447513464972872 iter num 40\n",
            "loss 2.8089983970858157e-05 average time 0.09792672677991504 iter num 50\n",
            "loss 2.6116376830032095e-05 average time 0.08645593615001416 iter num 60\n",
            "loss 1.6145550034707412e-05 average time 0.07806412375710871 iter num 70\n",
            "loss 2.9444065148709342e-05 average time 0.07153776337499948 iter num 80\n",
            "loss 1.5711044397903606e-05 average time 0.06667091937771248 iter num 90\n",
            "loss 2.143069650628604e-05 average time 0.06278298522986006 iter num 100\n",
            "loss 1.8795764844981022e-05 average time 0.3665538159002608 iter num 10\n",
            "loss 3.292335895821452e-05 average time 0.1967897793503653 iter num 20\n",
            "loss 1.9402539692237042e-05 average time 0.14074923000031656 iter num 30\n",
            "loss 1.371668440697249e-05 average time 0.1128782600503655 iter num 40\n",
            "loss 1.960665758815594e-05 average time 0.09569880754032056 iter num 50\n",
            "loss 2.5780400392250158e-05 average time 0.08424394308349292 iter num 60\n",
            "loss 1.832979978644289e-05 average time 0.07617926862855841 iter num 70\n",
            "loss 2.0245148334652185e-05 average time 0.0700375076999535 iter num 80\n",
            "loss 1.0471530913491733e-05 average time 0.06543554155549726 iter num 90\n",
            "loss 7.698607987549622e-06 average time 0.061657414519941084 iter num 100\n",
            "loss 4.707675543613732e-05 average time 0.3681150420998165 iter num 10\n",
            "loss 0.00010836878936970606 average time 0.1988846868500332 iter num 20\n",
            "loss 1.3744776879320852e-05 average time 0.1426026378664877 iter num 30\n",
            "loss 7.02157831256045e-06 average time 0.11407183987485041 iter num 40\n",
            "loss 2.879061685234774e-05 average time 0.09709486849991662 iter num 50\n",
            "loss 1.09378215711331e-05 average time 0.08547332948334467 iter num 60\n",
            "loss 3.0357816285686567e-05 average time 0.07764569590014747 iter num 70\n",
            "loss 2.8168902645120397e-05 average time 0.07161143167513728 iter num 80\n",
            "loss 1.1347307918185834e-05 average time 0.0666325964445908 iter num 90\n",
            "loss 1.6580735973548144e-05 average time 0.06281816160022573 iter num 100\n",
            "loss 3.4242028050357476e-05 average time 0.37179746389956564 iter num 10\n",
            "loss 2.974694325530436e-05 average time 0.19940740714973798 iter num 20\n",
            "loss 1.5024654203443788e-05 average time 0.14307555366673722 iter num 30\n",
            "loss 2.4387267330894247e-05 average time 0.11403163335007775 iter num 40\n",
            "loss 1.4224156984710135e-05 average time 0.09725160351998056 iter num 50\n",
            "loss 2.6642406737664714e-05 average time 0.08559067459997702 iter num 60\n",
            "loss 2.470062463544309e-05 average time 0.07736240547154531 iter num 70\n",
            "loss 1.0054694939753972e-05 average time 0.07103782882513769 iter num 80\n",
            "loss 8.612690180598292e-06 average time 0.06613193984457111 iter num 90\n",
            "loss 8.839382644509897e-06 average time 0.06243588177010679 iter num 100\n",
            "loss 2.5054301659110934e-05 average time 0.37109078750036134 iter num 10\n",
            "loss 1.2432461517164484e-05 average time 0.20068584475011447 iter num 20\n",
            "loss 3.885923797497526e-05 average time 0.14327055470009023 iter num 30\n",
            "loss 3.332814230816439e-05 average time 0.11456250430019281 iter num 40\n",
            "loss 7.874827861087397e-05 average time 0.09705990782029403 iter num 50\n",
            "loss 1.099616383726243e-05 average time 0.0853846873168853 iter num 60\n",
            "loss 1.1148852536280174e-05 average time 0.07700701134308474 iter num 70\n",
            "loss 4.8178394536080305e-06 average time 0.0711369935377661 iter num 80\n",
            "loss 1.6162368410732597e-05 average time 0.06623320814461396 iter num 90\n",
            "loss 2.3391419745166786e-05 average time 0.06211567100017419 iter num 100\n",
            "loss 1.7335483789793216e-05 average time 0.3693487443000777 iter num 10\n",
            "loss 2.302818393218331e-05 average time 0.19854133190019638 iter num 20\n",
            "loss 1.5260551663232036e-05 average time 0.14201753063340827 iter num 30\n",
            "loss 4.845135845243931e-05 average time 0.11386142870005642 iter num 40\n",
            "loss 2.3812946892576292e-05 average time 0.09660373962004087 iter num 50\n",
            "loss 1.0511869731999468e-05 average time 0.08463223198341438 iter num 60\n",
            "loss 1.2519784831965808e-05 average time 0.07629162164287534 iter num 70\n",
            "loss 1.2401566891639959e-05 average time 0.06995374869989064 iter num 80\n",
            "loss 1.579483796376735e-05 average time 0.06536292922214165 iter num 90\n",
            "loss 1.03884094642126e-05 average time 0.06142518423999718 iter num 100\n",
            "loss 4.875585000263527e-05 average time 0.3696663972004899 iter num 10\n",
            "loss 3.015749462065287e-05 average time 0.19817467680004483 iter num 20\n",
            "loss 8.438172699243296e-06 average time 0.14116502086665908 iter num 30\n",
            "loss 1.9467715901555493e-05 average time 0.11248608307487303 iter num 40\n",
            "loss 1.0644476788002066e-05 average time 0.09573575125985372 iter num 50\n",
            "loss 1.3750519428867847e-05 average time 0.08523143506651347 iter num 60\n",
            "loss 8.412012903136201e-06 average time 0.07690252104262722 iter num 70\n",
            "loss 9.674846296547912e-06 average time 0.07069674089980253 iter num 80\n",
            "loss 1.2084830814274028e-05 average time 0.06582899206651847 iter num 90\n",
            "loss 1.0215325346507598e-05 average time 0.06197574464986246 iter num 100\n",
            "loss 8.367441478185356e-05 average time 0.36635917999992673 iter num 10\n",
            "loss 1.4197702512319665e-05 average time 0.19663559694981814 iter num 20\n",
            "loss 1.694797356321942e-05 average time 0.14025177356646357 iter num 30\n",
            "loss 1.9164257537340745e-05 average time 0.1121165868999924 iter num 40\n",
            "loss 5.113984116178472e-06 average time 0.09552480303995253 iter num 50\n",
            "loss 1.374442672386067e-05 average time 0.08426225418328007 iter num 60\n",
            "loss 1.5928086213534698e-05 average time 0.07622429472851633 iter num 70\n",
            "loss 8.098546459223144e-06 average time 0.07033987848749348 iter num 80\n",
            "loss 8.859145964379422e-06 average time 0.06565102075554831 iter num 90\n",
            "loss 6.963667146919761e-06 average time 0.061861258249919046 iter num 100\n",
            "loss 9.074368790606968e-06 average time 0.36956360719959774 iter num 10\n",
            "loss 8.054789759626146e-06 average time 0.19900881574958476 iter num 20\n",
            "loss 1.4783223377889954e-05 average time 0.14168056969962586 iter num 30\n",
            "loss 1.981197601708118e-05 average time 0.11337892762485353 iter num 40\n",
            "loss 1.1120037015643902e-05 average time 0.09619203823989665 iter num 50\n",
            "loss 1.77651563717518e-05 average time 0.08469653764987015 iter num 60\n",
            "loss 4.049724157084711e-05 average time 0.07638401979992133 iter num 70\n",
            "loss 1.5492976672248915e-05 average time 0.07026094096249835 iter num 80\n",
            "loss 1.4671674762212206e-05 average time 0.06538600638887146 iter num 90\n",
            "loss 2.987419611599762e-05 average time 0.06162738584996987 iter num 100\n",
            "loss 6.857492735434789e-06 average time 0.3657825857993885 iter num 10\n",
            "loss 3.235431722714566e-05 average time 0.19702750329997798 iter num 20\n",
            "loss 1.3154476619092748e-05 average time 0.14053584809977718 iter num 30\n",
            "loss 2.586967275419738e-05 average time 0.11245095887488787 iter num 40\n",
            "loss 6.170297638163902e-06 average time 0.09534790915997292 iter num 50\n",
            "loss 1.1335514500387944e-05 average time 0.08412418813313707 iter num 60\n",
            "loss 1.0230810403299984e-05 average time 0.0759877727570191 iter num 70\n",
            "loss 8.785074896877632e-06 average time 0.06983541364984376 iter num 80\n",
            "loss 8.056631486397237e-06 average time 0.06529287198872125 iter num 90\n",
            "loss 2.2181613530847244e-05 average time 0.06147922738993657 iter num 100\n",
            "loss 2.5099861886701547e-05 average time 0.3678724133005744 iter num 10\n",
            "loss 2.150116597476881e-05 average time 0.19814689035029004 iter num 20\n",
            "loss 1.3731656508753076e-05 average time 0.14129171710046648 iter num 30\n",
            "loss 1.8933907995233312e-05 average time 0.11276407525037939 iter num 40\n",
            "loss 1.6687150491634384e-05 average time 0.09617762634028622 iter num 50\n",
            "loss 1.6864696590346284e-05 average time 0.08468662386676443 iter num 60\n",
            "loss 9.424411473446526e-06 average time 0.07657482184305471 iter num 70\n",
            "loss 1.1826667105196975e-05 average time 0.07062350026267268 iter num 80\n",
            "loss 6.973625204409473e-06 average time 0.06582185966674135 iter num 90\n",
            "loss 1.8117567378794774e-05 average time 0.0620944977399995 iter num 100\n",
            "loss 1.2728933143080212e-05 average time 0.3689730739002698 iter num 10\n",
            "loss 6.5252143031102605e-06 average time 0.19827261450009245 iter num 20\n",
            "loss 6.745803602825617e-06 average time 0.1409885924000264 iter num 30\n",
            "loss 1.3583310646936297e-05 average time 0.11225932227489466 iter num 40\n",
            "loss 7.689774065511301e-06 average time 0.0953728561400203 iter num 50\n",
            "loss 1.1142145012854598e-05 average time 0.08405039903330665 iter num 60\n",
            "loss 5.259150384517852e-06 average time 0.07638399119989897 iter num 70\n",
            "loss 8.952894859248772e-06 average time 0.07010376156235906 iter num 80\n",
            "loss 1.2451669135771226e-05 average time 0.06526173651097149 iter num 90\n",
            "loss 8.250361133832484e-06 average time 0.06165306868988409 iter num 100\n",
            "loss 1.3501588910003193e-05 average time 0.3679527768999833 iter num 10\n",
            "loss 1.7780021153157577e-05 average time 0.19772806224973466 iter num 20\n",
            "loss 8.987301953311544e-06 average time 0.1415161061000011 iter num 30\n",
            "loss 2.5469989850535057e-05 average time 0.11336399084984805 iter num 40\n",
            "loss 8.598124622949399e-06 average time 0.09641067932003353 iter num 50\n",
            "loss 6.364377441059332e-06 average time 0.08484906149994155 iter num 60\n",
            "loss 5.801151019113604e-06 average time 0.07670195995711505 iter num 70\n",
            "loss 5.990450517856516e-06 average time 0.07057767732499087 iter num 80\n",
            "loss 6.103199666540604e-06 average time 0.06581037576664434 iter num 90\n",
            "loss 9.894212780636735e-06 average time 0.061951307959898255 iter num 100\n",
            "loss 7.958779860928189e-06 average time 0.36878302500044813 iter num 10\n",
            "loss 9.5880932349246e-06 average time 0.19737423775040952 iter num 20\n",
            "loss 5.894880814594217e-06 average time 0.14105005430028542 iter num 30\n",
            "loss 6.5998456193483435e-06 average time 0.112642637350109 iter num 40\n",
            "loss 2.0157462131464854e-05 average time 0.09581094878012664 iter num 50\n",
            "loss 5.2720322855748236e-05 average time 0.08423279831664937 iter num 60\n",
            "loss 7.224430191854481e-06 average time 0.07622384219991675 iter num 70\n",
            "loss 6.596583261853084e-06 average time 0.07030143150000186 iter num 80\n",
            "loss 1.2246346159372479e-05 average time 0.0654307301222515 iter num 90\n",
            "loss 2.8754084269166924e-05 average time 0.06155145432003337 iter num 100\n",
            "loss 4.871387318416964e-06 average time 0.3672753363993252 iter num 10\n",
            "loss 1.3580504855781328e-05 average time 0.19856500269943353 iter num 20\n",
            "loss 2.269866490678396e-05 average time 0.14096781526617028 iter num 30\n",
            "loss 1.148699811892584e-05 average time 0.11283924252466022 iter num 40\n",
            "loss 9.0942376118619e-06 average time 0.0960784968797816 iter num 50\n",
            "loss 2.733010842348449e-05 average time 0.08488597869963996 iter num 60\n",
            "loss 8.35123319120612e-06 average time 0.07691635388530682 iter num 70\n",
            "loss 4.516891294770176e-06 average time 0.07059413701203994 iter num 80\n",
            "loss 3.619856443037861e-06 average time 0.06586073302185266 iter num 90\n",
            "loss 4.249741778039606e-06 average time 0.062217272569687336 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5ca505-58b7-4652-fed8-d7844de4be90"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.3, 0.3]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.27130044, 0.90763223)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.2729]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9052], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2AXrPt7bNj",
        "outputId": "c1756329-bcc5-4d94-ad8d-77d31f50f79a"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.3]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.0]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27130044\n",
            "[0.90763223]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovJwXo3-YEu",
        "outputId": "757f6b0f-fcca-442e-ab28-b8c22501be84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "BS_call_prices = []\n",
        "for p in prices:\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    BS_call_prices.append(bs_call(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, BS_call_prices, label = \"BS_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(BS_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lwApH0GT9bBK",
        "outputId": "3bda01a7-de9f-4cef-c2eb-97e570dd8991"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.75, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 0.75, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}