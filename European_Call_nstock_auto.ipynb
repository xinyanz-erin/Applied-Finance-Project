{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "European_Call_nstock_auto.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "TY_9g3tbdLiY",
        "JIa4c_aHz15a",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/European_Call_nstock_auto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xk52L4czj5x"
      },
      "source": [
        "nstock = 1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "33c97605-445f-4bcf-930b-e9c9cd58091a"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   6171      0 --:--:-- --:--:-- --:--:--  6171\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9 MB 46 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 71.7 MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "### Train(Erin Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxT9Eida-c_"
      },
      "source": [
        "# ################################# TEST ########################################\n",
        "# %%writefile cupy_dataset.py\n",
        "\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import random\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "#         paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           X = cupy.array([])\n",
        "#           K_rand = cupy.random.rand(1)[0]\n",
        "#           B_rand = cupy.random.rand(1)[0]\n",
        "#           r_rand = cupy.random.rand(1)[0]\n",
        "#           for i in range(0,self.N_STOCKS):\n",
        "#             X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "#           X = X.reshape(self.N_STOCKS,6)\n",
        "#           X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "#           #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "#           #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "#           # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#           #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "#           # make sure the Barrier is smaller than the Strike price\n",
        "#           # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#           for i in range(self.N_STOCKS):\n",
        "#             paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "#           stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "#           rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "#           #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "#           #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "#           #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#           stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "#           cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "#           num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "#           randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                                         num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "#           b1_r = randoms_gpu[:,0]\n",
        "#           b2_r = randoms_gpu[:,1]\n",
        "#           randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "#           for i in range(interval):\n",
        "#             if i % 2 == 0:\n",
        "#                 ind = int(i/2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "#             else:\n",
        "#                 ind = int(i//2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "#           randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "#           o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "#           Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# # ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# # for i in ds:\n",
        "# #     print(i[0])\n",
        "# ################################# TEST ########################################"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dZnWTTfbf1"
      },
      "source": [
        "### Train (European Call option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeREuPw0fguQ",
        "outputId": "f77e80a6-cf5f-4e98-d68e-7f869d78420c"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def European_call_option(d_s, T, K, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    #tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        h = T[batch_id] / N_STEPS\n",
        "        tmp1 = r[batch_id]*T[batch_id]/N_STEPS \n",
        "        tmp2 = math.exp(-r[batch_id]*T[batch_id]) # discount\n",
        "        tmp3 = math.sqrt(T[batch_id]/N_STEPS)\n",
        "        #running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "          s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "          #s_curr = s_curr * math.exp((r[batch_id] - (1/2)*sigma[batch_id]**2)*h + sigma[batch_id] * tmp3 * d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH])\n",
        "          #running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "          #if i==0 and batch_id == 2:\n",
        "          #    print(s_curr)\n",
        "          #if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "          #    break\n",
        "        #payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        #payoff = s_curr - K[batch_id] if s_curr > K[batch_id] else 0\n",
        "        #d_s[i] = tmp2 * payoff\n",
        "        d_s[i] = s_curr\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        #self.N_STEPS = 365\n",
        "        self.N_STEPS = 10000\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        #self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32)\n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        #paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 5), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          X = cupy.array([])\n",
        "          #T_rand = cupy.random.rand(1)[0]\n",
        "          K_rand = cupy.random.rand(1)\n",
        "          #B_rand = cupy.random.rand(1)[0]\n",
        "          r_rand = cupy.random.rand(1)\n",
        "          for i in range(0, self.N_STOCKS):\n",
        "            #X =  cupy.concatenate((X, cupy.array([K_rand,B_rand]), cupy.random.rand(3), cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "            X = cupy.concatenate((X, cupy.array([1.0]), K_rand, cupy.random.rand(3), r_rand))\n",
        "          \n",
        "          X = X.reshape(self.N_STOCKS, 6)\n",
        "          #X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #[T, K, S0, sigma, mu, r]\n",
        "          X = X * ((cupy.array([1, 150.0, 150.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "            #paras[op, i*5:(i+1)*5] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          if self.N_STOCKS != 1:\n",
        "            randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "            # stocks_randoms_cov = cupy.array([0] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "            # cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "            # num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "            # randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "            #                                               num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "            # b1_r = randoms_gpu[:,0]\n",
        "            # b2_r = randoms_gpu[:,1]\n",
        "            # randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "            # interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "            # for i in range(interval):\n",
        "            #   if i % 2 == 0:\n",
        "            #       ind = int(i/2)\n",
        "            #       randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            #   else:\n",
        "            #       ind = int(i//2)\n",
        "            #       randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "          if self.N_STOCKS == 1:\n",
        "            randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          \n",
        "          European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS) # this contains prices for each stock for each path at time T\n",
        "          K = K_rand * 150\n",
        "          r = r_rand * 0.2\n",
        "          o = o.mean(axis = 0) # average across stocks end prices\n",
        "          payoff = np.maximum(o - K, 0) # compute payoff\n",
        "          payoff = payoff * np.exp(-r*1) # T=1, discount\n",
        "          Y[op] = payoff.mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(1, number_path = 10000, batch = 3, seed = random.randint(0,100), stocks=nstock)\n",
        "# for i in ds:\n",
        "#     print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIa4c_aHz15a"
      },
      "source": [
        "### For verification (Monte Carlo for n-stocks basket)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DTuXR1ANZPC"
      },
      "source": [
        "# def test(array):\n",
        "#   S1 = array[2]\n",
        "#   S2 = array[8]\n",
        "#   S3 = array[14]\n",
        "#   sigma1 = array[3]\n",
        "#   sigma2 = array[9]\n",
        "#   sigma3 = array[15]\n",
        "#   T = 1  \n",
        "#   K = array[1]\n",
        "#   r = array[5]\n",
        "\n",
        "#   dt = 1/1000\n",
        "#   N = round(T/dt)\n",
        "#   t = np.linspace(0, T, N)\n",
        "\n",
        "#   out = []\n",
        "\n",
        "#   for i in range(10000):\n",
        "#     W1 = np.random.standard_normal(size = N)\n",
        "#     W1 = np.cumsum(W1) * np.sqrt(dt)\n",
        "#     W2 = np.random.standard_normal(size = N)\n",
        "#     W2 = np.cumsum(W2) * np.sqrt(dt)\n",
        "#     W3 = np.random.standard_normal(size = N)\n",
        "#     W3 = np.cumsum(W3) * np.sqrt(dt)\n",
        "\n",
        "#     # W = np.random.standard_normal(size = N)\n",
        "#     # W = np.cumsum(W) * np.sqrt(dt)\n",
        "    \n",
        "#     P1_T = (S1 * np.exp((r - 0.5*sigma1**2) *t + sigma1 * W1))[-1]\n",
        "#     P2_T = (S2 * np.exp((r - 0.5*sigma2**2) *t + sigma2 * W2))[-1]\n",
        "#     P3_T = (S3 * np.exp((r - 0.5*sigma3**2) *t + sigma3 * W3))[-1]\n",
        "#     payoff = (P1_T+P2_T+P3_T)/3 - K if (P1_T+P2_T+P3_T)/3 > K else 0\n",
        "#     out.append(payoff * np.exp(-r*T))\n",
        "  \n",
        "#   return(np.array(out).mean())\n",
        "\n",
        "# print(test(np.array([1.0000e+00, 5.8549e+00, 3.6871e+01, 2.6866e-01, 1.9026e-01, 1.8305e-01,\n",
        "#          1.0000e+00, 5.8549e+00, 4.1110e+00, 2.3714e-01, 5.9547e-02, 1.8305e-01,\n",
        "#          1.0000e+00, 5.8549e+00, 1.6189e+01, 7.3936e-02, 1.1713e-01, 1.8305e-01])))\n",
        "# print(test(np.array([1.0000e+00, 1.1296e+02, 7.4318e+01, 2.0409e-01, 1.9133e-01, 5.3904e-02,\n",
        "#          1.0000e+00, 1.1296e+02, 5.3453e+01, 4.8439e-02, 5.8557e-02, 5.3904e-02,\n",
        "#          1.0000e+00, 1.1296e+02, 3.8721e+01, 2.0259e-01, 6.0423e-02, 5.3904e-02])))\n",
        "# print(test(np.array([1.0000e+00, 3.2307e+01, 3.8874e+01, 2.5820e-01, 1.9522e-01, 8.6353e-02,\n",
        "#          1.0000e+00, 3.2307e+01, 3.6077e+01, 3.7123e-01, 1.8810e-01, 8.6353e-02,\n",
        "#          1.0000e+00, 3.2307e+01, 6.9274e+01, 2.4100e-01, 1.9839e-01, 8.6353e-02])))\n",
        "# print(test(np.array([1.0000e+00, 2.4764e+01, 6.6956e+01, 2.0769e-01, 9.8127e-02, 1.7302e-01,\n",
        "#          1.0000e+00, 2.4764e+01, 3.6483e+01, 8.9075e-02, 1.9375e-01, 1.7302e-01,\n",
        "#          1.0000e+00, 2.4764e+01, 9.4748e+01, 2.6420e-01, 1.2015e-01, 1.7302e-01])))\n",
        "# print(test(np.array([1.0000e+00, 8.4591e+01, 1.2730e+02, 2.8949e-02, 1.0954e-01, 1.3706e-01,\n",
        "#          1.0000e+00, 8.4591e+01, 1.2859e+02, 1.5262e-01, 1.4064e-03, 1.3706e-01,\n",
        "#          1.0000e+00, 8.4591e+01, 4.6679e+01, 2.6688e-01, 1.5895e-01, 1.3706e-01])))\n",
        "# print(test(np.array([1.0000e+00, 6.8433e+01, 3.5593e+01, 6.5870e-02, 6.8068e-02, 1.3578e-02,\n",
        "#          1.0000e+00, 6.8433e+01, 2.3864e+01, 9.5815e-02, 1.6075e-01, 1.3578e-02,\n",
        "#          1.0000e+00, 6.8433e+01, 1.1863e+02, 3.0400e-01, 5.6958e-02, 1.3578e-02])))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqx6hNuucM5U"
      },
      "source": [
        "# array = np.array([1.0000e+00, 8.3754e+01, 1.2787e+02, 2.7420e-01, 1.2507e-01, 1.7700e-01,\n",
        "#          1.0000e+00, 8.3754e+01, 4.0869e+01, 3.3302e-01, 1.7439e-01, 1.7700e-01,\n",
        "#          1.0000e+00, 8.3754e+01, 1.1031e+02, 3.5945e-01, 1.5382e-01, 1.7700e-01])\n",
        "\n",
        "# S1 = array[2]\n",
        "# S2 = array[8]\n",
        "# S3 = array[14]\n",
        "# sigma1 = array[3]\n",
        "# sigma2 = array[9]\n",
        "# sigma3 = array[15]\n",
        "# T = 1  \n",
        "# K = array[1]\n",
        "# r = array[5]\n",
        "\n",
        "# dt = 1/100\n",
        "# N = round(T/dt)\n",
        "# t = np.linspace(0, T, N)\n",
        "\n",
        "# out = []\n",
        "\n",
        "# #for i in range(1000):\n",
        "# W = np.random.standard_normal(size = N)\n",
        "# W = np.cumsum(W) * np.sqrt(dt)\n",
        "# P1_T = (S1 * np.exp((r - 0.5*sigma1**2) *t + sigma1 * W))\n",
        "# P2_T = (S2 * np.exp((r - 0.5*sigma2**2) *t + sigma2 * W))[-1]\n",
        "# P3_T = (S3 * np.exp((r - 0.5*sigma3**2) *t + sigma3 * W))[-1]\n",
        "# #ayoff = (P1_T+P2_T+P3_T)/3 - K if (P1_T+P2_T+P3_T)/3 > K else 0\n",
        "# #out.append(payoff * np.exp(-r*T))\n",
        "\n",
        "# #return(np.array(out).mean())\n",
        "# #P1_T\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(t, P1_T)\n",
        "# plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tpLw9WDhx8b"
      },
      "source": [
        "# stocks_randoms_cov = cupy.array([0.99999] * 3 * 3, dtype = cupy.float32).reshape(3,3)  #Covariance\n",
        "# cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "# stocks_randoms_mean = cupy.zeros(3, dtype = cupy.float32)\n",
        "\n",
        "# num_of_randoms_each_stock = 2 * 3\n",
        "# randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                               num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "# randoms_gpu"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7acbc6b3-50bf-442d-acb8-b89460ef7f03"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024, nstock=3):\n",
        "        super(Net, self).__init__()\n",
        "        self.nstock = nstock\n",
        "        self.fc1 = nn.Linear(6 * self.nstock, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1, 150.0, 150.0, 0.4, 0.2, 0.2] * self.nstock)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e2b8f3-4f63-496a-d918-f0594356c33f"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.6-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 39.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30 kB 45.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 61 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 92 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 112 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 122 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 143 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 153 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 184 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 194 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 204 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 215 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 225 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 232 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "265b9434-f721-4dd0-c3ef-d525528c8981"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net(nstock = nstock).cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 1000, number_path = 1048, batch = 32, stocks = nstock)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 500)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 1169.7003173828125 average time 0.2105443869000169 iter num 20\n",
            "loss 1132.545166015625 average time 0.10679875760002915 iter num 40\n",
            "loss 494.0757141113281 average time 0.07213646295001581 iter num 60\n",
            "loss 45.18058395385742 average time 0.05485932352500526 iter num 80\n",
            "loss 11.069539070129395 average time 0.04444896874000051 iter num 100\n",
            "loss 11.31775188446045 average time 0.037505184549998676 iter num 120\n",
            "loss 9.249517440795898 average time 0.03256693114285392 iter num 140\n",
            "loss 5.791199207305908 average time 0.028847591593745392 iter num 160\n",
            "loss 7.341282367706299 average time 0.02595841094444419 iter num 180\n",
            "loss 1.6332147121429443 average time 0.02364045062500054 iter num 200\n",
            "loss 8.69856071472168 average time 0.021752368604548294 iter num 220\n",
            "loss 2.7190070152282715 average time 0.020202378191667474 iter num 240\n",
            "loss 4.617489337921143 average time 0.018875622288462287 iter num 260\n",
            "loss 4.35109281539917 average time 0.017727472385714853 iter num 280\n",
            "loss 3.6671829223632812 average time 0.016735885330000807 iter num 300\n",
            "loss 5.361028671264648 average time 0.015864460703124906 iter num 320\n",
            "loss 6.741949081420898 average time 0.01509839532352945 iter num 340\n",
            "loss 3.5009427070617676 average time 0.01442349706388768 iter num 360\n",
            "loss 4.719311714172363 average time 0.01380850001052606 iter num 380\n",
            "loss 5.682141304016113 average time 0.01325555555000051 iter num 400\n",
            "loss 5.269976615905762 average time 0.012768289345238026 iter num 420\n",
            "loss 4.465554714202881 average time 0.012317125179546648 iter num 440\n",
            "loss 7.931415557861328 average time 0.011903857232610077 iter num 460\n",
            "loss 1.590311050415039 average time 0.011523562352084108 iter num 480\n",
            "loss 1.4564642906188965 average time 0.011174347682002463 iter num 500\n",
            "loss 4.3420610427856445 average time 0.010853215625001685 iter num 520\n",
            "loss 2.4409191608428955 average time 0.010557019000000563 iter num 540\n",
            "loss 1.99904465675354 average time 0.010278827814286729 iter num 560\n",
            "loss 1.4752869606018066 average time 0.010024644624138938 iter num 580\n",
            "loss 3.647813558578491 average time 0.009782313810000763 iter num 600\n",
            "loss 3.3428244590759277 average time 0.009556075956451452 iter num 620\n",
            "loss 7.1011271476745605 average time 0.009343008471876501 iter num 640\n",
            "loss 3.4167404174804688 average time 0.00914656589848612 iter num 660\n",
            "loss 4.390002250671387 average time 0.00895918373970684 iter num 680\n",
            "loss 5.418116569519043 average time 0.00878322165571717 iter num 700\n",
            "loss 4.1157965660095215 average time 0.00861930011250283 iter num 720\n",
            "loss 3.5358238220214844 average time 0.008461865445948772 iter num 740\n",
            "loss 2.783719062805176 average time 0.008312083431581197 iter num 760\n",
            "loss 1.7525174617767334 average time 0.008170142926924957 iter num 780\n",
            "loss 3.227769374847412 average time 0.008036939892502062 iter num 800\n",
            "loss 4.860308647155762 average time 0.007907902321953382 iter num 820\n",
            "loss 3.574676752090454 average time 0.007785893126192821 iter num 840\n",
            "loss 4.103114128112793 average time 0.007668616275583285 iter num 860\n",
            "loss 3.066157579421997 average time 0.007559584334091793 iter num 880\n",
            "loss 1.5503472089767456 average time 0.007456366630000654 iter num 900\n",
            "loss 1.2940092086791992 average time 0.007353718644565254 iter num 920\n",
            "loss 3.1229207515716553 average time 0.0072569855223402185 iter num 940\n",
            "loss 4.467825889587402 average time 0.007170548732291356 iter num 960\n",
            "loss 3.4593000411987305 average time 0.007082194822447835 iter num 980\n",
            "loss 2.6003224849700928 average time 0.006995199509998656 iter num 1000\n",
            "loss 2.8751211166381836 average time 0.017987492549980288 iter num 20\n",
            "loss 2.6690688133239746 average time 0.010453270974988983 iter num 40\n",
            "loss 4.1194257736206055 average time 0.007942111733329208 iter num 60\n",
            "loss 7.087533950805664 average time 0.0066465424749878824 iter num 80\n",
            "loss 2.4204659461975098 average time 0.005868219459980537 iter num 100\n",
            "loss 4.443394660949707 average time 0.005349148324980509 iter num 120\n",
            "loss 2.427778482437134 average time 0.004983243249984883 iter num 140\n",
            "loss 2.0891482830047607 average time 0.004709495412487286 iter num 160\n",
            "loss 2.0106146335601807 average time 0.004499593127765264 iter num 180\n",
            "loss 3.6497642993927 average time 0.00432415720498966 iter num 200\n",
            "loss 2.6717734336853027 average time 0.00418280770453314 iter num 220\n",
            "loss 2.194211483001709 average time 0.004066719866654959 iter num 240\n",
            "loss 3.289504289627075 average time 0.003968231288452164 iter num 260\n",
            "loss 2.700199604034424 average time 0.0038896295464204353 iter num 280\n",
            "loss 1.6773024797439575 average time 0.0038144728133261197 iter num 300\n",
            "loss 1.6124545335769653 average time 0.003750295443742857 iter num 320\n",
            "loss 1.28285813331604 average time 0.003692804517640311 iter num 340\n",
            "loss 4.778318881988525 average time 0.003640175055547464 iter num 360\n",
            "loss 3.0659890174865723 average time 0.0035954187947296354 iter num 380\n",
            "loss 2.50714111328125 average time 0.0035523993174928136 iter num 400\n",
            "loss 1.36639404296875 average time 0.0035166800285640508 iter num 420\n",
            "loss 2.0599355697631836 average time 0.0034825879454481105 iter num 440\n",
            "loss 3.3308238983154297 average time 0.003462307002167148 iter num 460\n",
            "loss 2.2271552085876465 average time 0.0034319539270768473 iter num 480\n",
            "loss 2.4355831146240234 average time 0.0034054670739933498 iter num 500\n",
            "loss 2.802163600921631 average time 0.003381812865377644 iter num 520\n",
            "loss 1.694718837738037 average time 0.0033696429388812617 iter num 540\n",
            "loss 2.573885917663574 average time 0.00334881719106355 iter num 560\n",
            "loss 1.9660048484802246 average time 0.0033288508999923016 iter num 580\n",
            "loss 1.8700439929962158 average time 0.0033106972816585767 iter num 600\n",
            "loss 1.1467385292053223 average time 0.0032925926854762023 iter num 620\n",
            "loss 1.1524319648742676 average time 0.003276063176556221 iter num 640\n",
            "loss 1.647033452987671 average time 0.0032604546045380436 iter num 660\n",
            "loss 2.390554666519165 average time 0.0032446320073461266 iter num 680\n",
            "loss 0.6502736210823059 average time 0.0032364481785647774 iter num 700\n",
            "loss 1.196122407913208 average time 0.003226053462492789 iter num 720\n",
            "loss 1.2092454433441162 average time 0.003214189362155151 iter num 740\n",
            "loss 1.3100314140319824 average time 0.0032027440026243573 iter num 760\n",
            "loss 0.571618378162384 average time 0.003191611237172928 iter num 780\n",
            "loss 1.8254716396331787 average time 0.003184677783743268 iter num 800\n",
            "loss 0.7612893581390381 average time 0.0031741317853585514 iter num 820\n",
            "loss 2.4114294052124023 average time 0.003163839159516274 iter num 840\n",
            "loss 2.001711368560791 average time 0.003154592191852972 iter num 860\n",
            "loss 1.9491832256317139 average time 0.0031473066181743805 iter num 880\n",
            "loss 1.5620745420455933 average time 0.0031416038244371216 iter num 900\n",
            "loss 1.0500985383987427 average time 0.0031348721565146445 iter num 920\n",
            "loss 1.8183027505874634 average time 0.0031275681574398015 iter num 940\n",
            "loss 1.8223192691802979 average time 0.003123014587493363 iter num 960\n",
            "loss 0.5255853533744812 average time 0.0031159825499933467 iter num 980\n",
            "loss 0.9589937925338745 average time 0.0031101100419928117 iter num 1000\n",
            "loss 1.82861328125 average time 0.0178148976500097 iter num 20\n",
            "loss 1.1186084747314453 average time 0.010384972424986927 iter num 40\n",
            "loss 3.119866371154785 average time 0.007828841166660066 iter num 60\n",
            "loss 2.025116205215454 average time 0.006601000499998122 iter num 80\n",
            "loss 2.20691180229187 average time 0.0058483942100019705 iter num 100\n",
            "loss 1.0731123685836792 average time 0.005360866700002968 iter num 120\n",
            "loss 0.9977614879608154 average time 0.004995244385715328 iter num 140\n",
            "loss 0.8203743696212769 average time 0.004712566843748789 iter num 160\n",
            "loss 1.872633457183838 average time 0.004496367688886949 iter num 180\n",
            "loss 0.6403850317001343 average time 0.004325218800000812 iter num 200\n",
            "loss 0.9367018342018127 average time 0.004190208627273312 iter num 220\n",
            "loss 1.7930560111999512 average time 0.004068143425001836 iter num 240\n",
            "loss 3.9254777431488037 average time 0.003967377738461682 iter num 260\n",
            "loss 1.8933181762695312 average time 0.003882373174999592 iter num 280\n",
            "loss 1.762537956237793 average time 0.003814678063334516 iter num 300\n",
            "loss 2.116462230682373 average time 0.0037493652406269005 iter num 320\n",
            "loss 1.1838001012802124 average time 0.0036906524705908423 iter num 340\n",
            "loss 1.1672632694244385 average time 0.0036393215000026027 iter num 360\n",
            "loss 1.1467413902282715 average time 0.0035920302526338483 iter num 380\n",
            "loss 1.1584705114364624 average time 0.0035505605225023373 iter num 400\n",
            "loss 0.7420145273208618 average time 0.0035231826595274168 iter num 420\n",
            "loss 0.9481808543205261 average time 0.0034993941954584527 iter num 440\n",
            "loss 0.8513085842132568 average time 0.0034669203239168977 iter num 460\n",
            "loss 1.1640242338180542 average time 0.0034378274812529716 iter num 480\n",
            "loss 0.6626380681991577 average time 0.003411919900002886 iter num 500\n",
            "loss 1.19266676902771 average time 0.0033863012269259446 iter num 520\n",
            "loss 1.4513649940490723 average time 0.0033627984925955288 iter num 540\n",
            "loss 1.111729621887207 average time 0.0033412299696436777 iter num 560\n",
            "loss 1.5568311214447021 average time 0.00332316184138006 iter num 580\n",
            "loss 0.8137780427932739 average time 0.0033059014199992966 iter num 600\n",
            "loss 0.9141581058502197 average time 0.003291370493548496 iter num 620\n",
            "loss 1.480181097984314 average time 0.0032735919812513004 iter num 640\n",
            "loss 0.8261655569076538 average time 0.003261003951515497 iter num 660\n",
            "loss 0.9062436819076538 average time 0.0032450203544094752 iter num 680\n",
            "loss 0.9904125928878784 average time 0.0032290841457120224 iter num 700\n",
            "loss 0.7277248501777649 average time 0.003214594504163099 iter num 720\n",
            "loss 0.4152345061302185 average time 0.0032024234608075933 iter num 740\n",
            "loss 0.5633557438850403 average time 0.0031892712986808947 iter num 760\n",
            "loss 0.798072874546051 average time 0.00317653156794601 iter num 780\n",
            "loss 0.7307133674621582 average time 0.0031648693124972736 iter num 800\n",
            "loss 1.3169357776641846 average time 0.0031532276390238644 iter num 820\n",
            "loss 0.6539804935455322 average time 0.0031489967297599853 iter num 840\n",
            "loss 0.42121872305870056 average time 0.0031434343337193326 iter num 860\n",
            "loss 0.44798409938812256 average time 0.003134292656816807 iter num 880\n",
            "loss 0.7182132005691528 average time 0.00312931676999723 iter num 900\n",
            "loss 0.7326266169548035 average time 0.0031218000989109325 iter num 920\n",
            "loss 0.891323447227478 average time 0.003113511606381672 iter num 940\n",
            "loss 0.22559854388237 average time 0.003105080857291635 iter num 960\n",
            "loss 0.4094446003437042 average time 0.0030965425704094947 iter num 980\n",
            "loss 0.6646175384521484 average time 0.0030897648660004507 iter num 1000\n",
            "loss 0.8118062019348145 average time 0.01760173919999488 iter num 20\n",
            "loss 2.0918612480163574 average time 0.010195168824975554 iter num 40\n",
            "loss 3.507858991622925 average time 0.00769397734999681 iter num 60\n",
            "loss 1.5862157344818115 average time 0.006448994612480874 iter num 80\n",
            "loss 0.8156772255897522 average time 0.005747115619979013 iter num 100\n",
            "loss 1.027379035949707 average time 0.005239795466635163 iter num 120\n",
            "loss 5.196028709411621 average time 0.004898810914252343 iter num 140\n",
            "loss 2.3405396938323975 average time 0.004644239237472902 iter num 160\n",
            "loss 2.3444042205810547 average time 0.004427247349978339 iter num 180\n",
            "loss 1.327298641204834 average time 0.0042590082249762415 iter num 200\n",
            "loss 1.4031720161437988 average time 0.004124220522707715 iter num 220\n",
            "loss 0.7498452663421631 average time 0.0040119918708152605 iter num 240\n",
            "loss 2.3080618381500244 average time 0.003914459246140303 iter num 260\n",
            "loss 0.968519926071167 average time 0.0038354088964199814 iter num 280\n",
            "loss 0.5060453414916992 average time 0.0037602302133291233 iter num 300\n",
            "loss 0.6695543527603149 average time 0.0036938617343693636 iter num 320\n",
            "loss 0.6076561212539673 average time 0.0036410922088139963 iter num 340\n",
            "loss 2.519228935241699 average time 0.0035887209999916395 iter num 360\n",
            "loss 1.3685566186904907 average time 0.003551790384201587 iter num 380\n",
            "loss 0.5589702129364014 average time 0.003515524592492056 iter num 400\n",
            "loss 0.6281355023384094 average time 0.0034776441118960326 iter num 420\n",
            "loss 0.6041491031646729 average time 0.0034414760113546353 iter num 440\n",
            "loss 0.4767180383205414 average time 0.0034131485934692494 iter num 460\n",
            "loss 0.5941442847251892 average time 0.003385041458326062 iter num 480\n",
            "loss 0.4965124726295471 average time 0.003360652903994378 iter num 500\n",
            "loss 1.1503350734710693 average time 0.0033370236230710433 iter num 520\n",
            "loss 0.7331395745277405 average time 0.0033131744148089705 iter num 540\n",
            "loss 0.7745355367660522 average time 0.0032939159892797372 iter num 560\n",
            "loss 0.602294921875 average time 0.0032729029482699594 iter num 580\n",
            "loss 0.44067487120628357 average time 0.0032576644166624645 iter num 600\n",
            "loss 0.4491690397262573 average time 0.0032447468709652905 iter num 620\n",
            "loss 0.7430290579795837 average time 0.0032313482874954504 iter num 640\n",
            "loss 0.5425419807434082 average time 0.0032155351757527377 iter num 660\n",
            "loss 0.8608068227767944 average time 0.003199952969114025 iter num 680\n",
            "loss 0.5364676117897034 average time 0.0031851126942813476 iter num 700\n",
            "loss 0.33880722522735596 average time 0.0031715846708297904 iter num 720\n",
            "loss 0.2203265130519867 average time 0.003158877410805257 iter num 740\n",
            "loss 0.3045388460159302 average time 0.003150774027625702 iter num 760\n",
            "loss 0.7584109306335449 average time 0.003139745825636708 iter num 780\n",
            "loss 0.2694140076637268 average time 0.003133605469993199 iter num 800\n",
            "loss 0.7610135674476624 average time 0.0031275777268225595 iter num 820\n",
            "loss 0.647080659866333 average time 0.0031191048821353562 iter num 840\n",
            "loss 0.4744759798049927 average time 0.00311117034068978 iter num 860\n",
            "loss 0.876693844795227 average time 0.0031079587818081482 iter num 880\n",
            "loss 0.27900806069374084 average time 0.0031029198099890385 iter num 900\n",
            "loss 0.30393320322036743 average time 0.0030986763108582795 iter num 920\n",
            "loss 0.740535318851471 average time 0.003099089940416134 iter num 940\n",
            "loss 0.36890560388565063 average time 0.003092626418739523 iter num 960\n",
            "loss 0.778326153755188 average time 0.003084543873459162 iter num 980\n",
            "loss 0.5546058416366577 average time 0.0030776199289880425 iter num 1000\n",
            "loss 3.7108144760131836 average time 0.017661013099973387 iter num 20\n",
            "loss 3.5390095710754395 average time 0.010303536899971277 iter num 40\n",
            "loss 19.745574951171875 average time 0.007772720166659989 iter num 60\n",
            "loss 0.8182997107505798 average time 0.006516276224994044 iter num 80\n",
            "loss 1.2655748128890991 average time 0.005764642440012722 iter num 100\n",
            "loss 0.8425243496894836 average time 0.0052856006250181055 iter num 120\n",
            "loss 0.5024225115776062 average time 0.00491589406429804 iter num 140\n",
            "loss 1.6244672536849976 average time 0.004637345450009889 iter num 160\n",
            "loss 1.1079983711242676 average time 0.004429260094459804 iter num 180\n",
            "loss 1.3437215089797974 average time 0.00425515560501708 iter num 200\n",
            "loss 1.3381708860397339 average time 0.004113653627283301 iter num 220\n",
            "loss 0.6730674505233765 average time 0.0039961209708413515 iter num 240\n",
            "loss 1.3515874147415161 average time 0.003897382796166172 iter num 260\n",
            "loss 0.4728381931781769 average time 0.003811065557152103 iter num 280\n",
            "loss 3.2132301330566406 average time 0.0037460813500092625 iter num 300\n",
            "loss 1.5779263973236084 average time 0.0036818453593809863 iter num 320\n",
            "loss 0.6217597723007202 average time 0.0036262040500051784 iter num 340\n",
            "loss 0.31177666783332825 average time 0.0035767836444531693 iter num 360\n",
            "loss 0.7418402433395386 average time 0.003530631728956809 iter num 380\n",
            "loss 0.6442376375198364 average time 0.003491380865011706 iter num 400\n",
            "loss 1.0314544439315796 average time 0.0034599224976307684 iter num 420\n",
            "loss 0.3775859475135803 average time 0.0034271133522865553 iter num 440\n",
            "loss 0.3774236738681793 average time 0.0033997848130589456 iter num 460\n",
            "loss 0.27868786454200745 average time 0.0033735262750127504 iter num 480\n",
            "loss 0.8188768625259399 average time 0.0033488364320128313 iter num 500\n",
            "loss 0.662987232208252 average time 0.0033251388846268198 iter num 520\n",
            "loss 1.4350874423980713 average time 0.0033079988814952903 iter num 540\n",
            "loss 0.573306679725647 average time 0.0032860650357268437 iter num 560\n",
            "loss 0.4822832942008972 average time 0.0032650129620809946 iter num 580\n",
            "loss 0.7465680837631226 average time 0.0032539471766775325 iter num 600\n",
            "loss 0.6840914487838745 average time 0.0032357552016220883 iter num 620\n",
            "loss 0.47068142890930176 average time 0.003220519679695144 iter num 640\n",
            "loss 0.41443172097206116 average time 0.0032043700212198295 iter num 660\n",
            "loss 0.4385542571544647 average time 0.0031905809514776538 iter num 680\n",
            "loss 0.845551073551178 average time 0.0031765168728634307 iter num 700\n",
            "loss 0.9164688587188721 average time 0.0031652318805615526 iter num 720\n",
            "loss 0.9105029106140137 average time 0.0031555824121658194 iter num 740\n",
            "loss 0.26010870933532715 average time 0.0031444950460557887 iter num 760\n",
            "loss 0.3818953037261963 average time 0.003131714620515315 iter num 780\n",
            "loss 0.524730384349823 average time 0.0031203849475019752 iter num 800\n",
            "loss 0.9457225799560547 average time 0.003109057179269102 iter num 820\n",
            "loss 0.34483522176742554 average time 0.0030988727369032987 iter num 840\n",
            "loss 0.7402986288070679 average time 0.003089257406976114 iter num 860\n",
            "loss 0.2201155126094818 average time 0.0030802541022715312 iter num 880\n",
            "loss 0.6389913558959961 average time 0.0030725798777772677 iter num 900\n",
            "loss 0.42655307054519653 average time 0.003064889992390363 iter num 920\n",
            "loss 0.43988972902297974 average time 0.0030568313765957584 iter num 940\n",
            "loss 0.6996641755104065 average time 0.0030511086052091704 iter num 960\n",
            "loss 0.1390630304813385 average time 0.003050243131633115 iter num 980\n",
            "loss 0.2037472277879715 average time 0.003049640660001387 iter num 1000\n",
            "loss 0.24655336141586304 average time 0.017940365449976525 iter num 20\n",
            "loss 0.5274486541748047 average time 0.010303781224968134 iter num 40\n",
            "loss 0.809669017791748 average time 0.007786544199946851 iter num 60\n",
            "loss 1.4335265159606934 average time 0.00656476941247206 iter num 80\n",
            "loss 5.92632532119751 average time 0.005802039699956367 iter num 100\n",
            "loss 2.5405118465423584 average time 0.005287902174954221 iter num 120\n",
            "loss 1.02363121509552 average time 0.004939563878526444 iter num 140\n",
            "loss 0.5439808964729309 average time 0.00466318533746346 iter num 160\n",
            "loss 3.0639748573303223 average time 0.004443983611074348 iter num 180\n",
            "loss 0.769583523273468 average time 0.004281933679963003 iter num 200\n",
            "loss 1.8858625888824463 average time 0.004133657509059544 iter num 220\n",
            "loss 0.9746308922767639 average time 0.004013811720803536 iter num 240\n",
            "loss 0.5325934886932373 average time 0.003910002084584308 iter num 260\n",
            "loss 0.5134751796722412 average time 0.003821199889253941 iter num 280\n",
            "loss 0.9079407453536987 average time 0.0037469972633061843 iter num 300\n",
            "loss 1.6821651458740234 average time 0.0036883754937250047 iter num 320\n",
            "loss 1.6604299545288086 average time 0.0036294461617457914 iter num 340\n",
            "loss 0.8377584218978882 average time 0.003580444316649947 iter num 360\n",
            "loss 1.6033411026000977 average time 0.0035328401736634124 iter num 380\n",
            "loss 0.8393138647079468 average time 0.0034894855199809173 iter num 400\n",
            "loss 0.6944992542266846 average time 0.003452456142833528 iter num 420\n",
            "loss 1.0205340385437012 average time 0.0034176406613441526 iter num 440\n",
            "loss 1.01041579246521 average time 0.0033866097195453656 iter num 460\n",
            "loss 1.070425033569336 average time 0.0033555712708145317 iter num 480\n",
            "loss 0.19116736948490143 average time 0.0033304906519806537 iter num 500\n",
            "loss 0.5862888693809509 average time 0.003307023282675695 iter num 520\n",
            "loss 0.33839744329452515 average time 0.0032862284999846786 iter num 540\n",
            "loss 0.26266539096832275 average time 0.0032689775106981805 iter num 560\n",
            "loss 0.31493180990219116 average time 0.0032506951465354084 iter num 580\n",
            "loss 0.5626850724220276 average time 0.0032305823699857682 iter num 600\n",
            "loss 1.035574197769165 average time 0.0032150809080495756 iter num 620\n",
            "loss 0.687343180179596 average time 0.003201365506235021 iter num 640\n",
            "loss 0.3509528934955597 average time 0.003185266007562859 iter num 660\n",
            "loss 0.3903118968009949 average time 0.0031697640779264156 iter num 680\n",
            "loss 0.4184359014034271 average time 0.0031602413185575122 iter num 700\n",
            "loss 0.5264201164245605 average time 0.0031482554472076017 iter num 720\n",
            "loss 0.7193588018417358 average time 0.003138461317555723 iter num 740\n",
            "loss 0.30778273940086365 average time 0.003126281542091348 iter num 760\n",
            "loss 0.14186783134937286 average time 0.0031143201140885605 iter num 780\n",
            "loss 0.5070206522941589 average time 0.0031037975699865685 iter num 800\n",
            "loss 0.13538780808448792 average time 0.003093403736571839 iter num 820\n",
            "loss 1.1620326042175293 average time 0.003088500871414605 iter num 840\n",
            "loss 0.5557003617286682 average time 0.0030810663755690172 iter num 860\n",
            "loss 0.45344841480255127 average time 0.003071755093169899 iter num 880\n",
            "loss 1.081344723701477 average time 0.0030630157244316554 iter num 900\n",
            "loss 0.30882513523101807 average time 0.0030550690239015636 iter num 920\n",
            "loss 0.47808632254600525 average time 0.003047536854245799 iter num 940\n",
            "loss 0.801129937171936 average time 0.003040461123948527 iter num 960\n",
            "loss 0.3878006041049957 average time 0.0030324982755018937 iter num 980\n",
            "loss 0.5926847457885742 average time 0.0030245586239911974 iter num 1000\n",
            "loss 0.1533060371875763 average time 0.01761173774996223 iter num 20\n",
            "loss 1.0935192108154297 average time 0.01014439559995708 iter num 40\n",
            "loss 0.9466938972473145 average time 0.0076705268832862805 iter num 60\n",
            "loss 0.7475818395614624 average time 0.0064185465249636305 iter num 80\n",
            "loss 0.34411513805389404 average time 0.005671793359961157 iter num 100\n",
            "loss 1.5138269662857056 average time 0.00518195654997271 iter num 120\n",
            "loss 0.8856347799301147 average time 0.004824288014261973 iter num 140\n",
            "loss 1.5060088634490967 average time 0.004554709599977969 iter num 160\n",
            "loss 0.5641430020332336 average time 0.004347080949977074 iter num 180\n",
            "loss 0.7037665247917175 average time 0.004224365234986181 iter num 200\n",
            "loss 0.8054636716842651 average time 0.004088174404531938 iter num 220\n",
            "loss 1.3527500629425049 average time 0.003976545320824698 iter num 240\n",
            "loss 0.8939277529716492 average time 0.0038818854192256298 iter num 260\n",
            "loss 0.8510808944702148 average time 0.003800088414282397 iter num 280\n",
            "loss 0.5775414109230042 average time 0.0037243353033287953 iter num 300\n",
            "loss 0.45286500453948975 average time 0.0036584523937492008 iter num 320\n",
            "loss 0.7996018528938293 average time 0.003623231538235546 iter num 340\n",
            "loss 0.39553672075271606 average time 0.0035720169749993147 iter num 360\n",
            "loss 0.9744472503662109 average time 0.0035277383052636268 iter num 380\n",
            "loss 1.0058870315551758 average time 0.0034886358499989 iter num 400\n",
            "loss 0.29633596539497375 average time 0.0034663593142871224 iter num 420\n",
            "loss 0.5423746109008789 average time 0.0034294250522717367 iter num 440\n",
            "loss 0.33497291803359985 average time 0.0033976928108695177 iter num 460\n",
            "loss 0.4245820939540863 average time 0.003367145018747427 iter num 480\n",
            "loss 0.3534162938594818 average time 0.0033409095919978427 iter num 500\n",
            "loss 0.5617520809173584 average time 0.0033148644769233635 iter num 520\n",
            "loss 0.22919867932796478 average time 0.003293423194442632 iter num 540\n",
            "loss 0.44636067748069763 average time 0.00327298643392656 iter num 560\n",
            "loss 0.41601869463920593 average time 0.0032526689568934167 iter num 580\n",
            "loss 0.6624085307121277 average time 0.0032326194933292147 iter num 600\n",
            "loss 0.543663501739502 average time 0.0032166366693511536 iter num 620\n",
            "loss 0.41936811804771423 average time 0.003199131695308921 iter num 640\n",
            "loss 0.45671510696411133 average time 0.003182970036361136 iter num 660\n",
            "loss 0.5323200225830078 average time 0.00316719295587839 iter num 680\n",
            "loss 0.5674782991409302 average time 0.00315251339570945 iter num 700\n",
            "loss 0.47263771295547485 average time 0.0031393896638854182 iter num 720\n",
            "loss 0.4678042531013489 average time 0.0031372478432380425 iter num 740\n",
            "loss 0.18174687027931213 average time 0.00312561217367901 iter num 760\n",
            "loss 0.33842143416404724 average time 0.0031141124679426213 iter num 780\n",
            "loss 0.3710266053676605 average time 0.0031027881637425026 iter num 800\n",
            "loss 0.1665530800819397 average time 0.003091595801212928 iter num 820\n",
            "loss 0.40996885299682617 average time 0.0030816986630903946 iter num 840\n",
            "loss 0.19192221760749817 average time 0.0030722268662741973 iter num 860\n",
            "loss 0.4979689419269562 average time 0.0030635864761315835 iter num 880\n",
            "loss 0.2156720757484436 average time 0.003057735701106847 iter num 900\n",
            "loss 0.23586472868919373 average time 0.003049380331517766 iter num 920\n",
            "loss 0.3366571068763733 average time 0.0030421335627632755 iter num 940\n",
            "loss 0.2192775309085846 average time 0.00303437730312055 iter num 960\n",
            "loss 0.7516754865646362 average time 0.0030268094663221503 iter num 980\n",
            "loss 0.28008556365966797 average time 0.003019367138994312 iter num 1000\n",
            "loss 0.5056381225585938 average time 0.017685926099989046 iter num 20\n",
            "loss 1.6733982563018799 average time 0.010189468424994175 iter num 40\n",
            "loss 1.2439348697662354 average time 0.0076976079166797716 iter num 60\n",
            "loss 2.324575901031494 average time 0.006442773850005778 iter num 80\n",
            "loss 0.7056481838226318 average time 0.005690744280004764 iter num 100\n",
            "loss 8.722265243530273 average time 0.0051837240916635585 iter num 120\n",
            "loss 4.309625625610352 average time 0.004833491878574543 iter num 140\n",
            "loss 0.9817649126052856 average time 0.004577024825013609 iter num 160\n",
            "loss 2.467456817626953 average time 0.004374010105566918 iter num 180\n",
            "loss 1.1108061075210571 average time 0.00420303724001542 iter num 200\n",
            "loss 0.23750931024551392 average time 0.004075816827286293 iter num 220\n",
            "loss 1.5346300601959229 average time 0.003958087158343915 iter num 240\n",
            "loss 0.5475564002990723 average time 0.003859452215390145 iter num 260\n",
            "loss 0.5112953186035156 average time 0.0037756898000062783 iter num 280\n",
            "loss 0.4737262725830078 average time 0.003702390980006385 iter num 300\n",
            "loss 0.4505443572998047 average time 0.0036372532531331104 iter num 320\n",
            "loss 0.6031097173690796 average time 0.003579764623536368 iter num 340\n",
            "loss 0.6539362668991089 average time 0.003533802980563097 iter num 360\n",
            "loss 0.5343151688575745 average time 0.0034990431236900277 iter num 380\n",
            "loss 0.5812458992004395 average time 0.003460728062502767 iter num 400\n",
            "loss 0.4138556122779846 average time 0.003423173578570034 iter num 420\n",
            "loss 0.23485252261161804 average time 0.003391032911359947 iter num 440\n",
            "loss 1.09274160861969 average time 0.003362177413039361 iter num 460\n",
            "loss 0.9156290888786316 average time 0.0033334595749948902 iter num 480\n",
            "loss 0.22540350258350372 average time 0.0033110144099955507 iter num 500\n",
            "loss 0.7111459970474243 average time 0.003286083776917901 iter num 520\n",
            "loss 0.7610171437263489 average time 0.003262061059257138 iter num 540\n",
            "loss 0.796779215335846 average time 0.0032401438499984384 iter num 560\n",
            "loss 0.4491838812828064 average time 0.003221346801721885 iter num 580\n",
            "loss 0.720284640789032 average time 0.003203059443330858 iter num 600\n",
            "loss 0.4961097538471222 average time 0.003189495482257048 iter num 620\n",
            "loss 0.35153210163116455 average time 0.0031745943453110213 iter num 640\n",
            "loss 0.4682822823524475 average time 0.003163287562117809 iter num 660\n",
            "loss 0.25524085760116577 average time 0.00315613870440944 iter num 680\n",
            "loss 0.5877289772033691 average time 0.003150662214284239 iter num 700\n",
            "loss 0.49683716893196106 average time 0.0031392681611098145 iter num 720\n",
            "loss 0.3796716332435608 average time 0.0031275843040524756 iter num 740\n",
            "loss 0.5507780909538269 average time 0.0031156283999980348 iter num 760\n",
            "loss 0.3221968710422516 average time 0.0031113504833298906 iter num 780\n",
            "loss 0.4826437830924988 average time 0.003102224522497181 iter num 800\n",
            "loss 0.4449973702430725 average time 0.003092397265850639 iter num 820\n",
            "loss 0.5179898738861084 average time 0.0030821927595211275 iter num 840\n",
            "loss 0.21680721640586853 average time 0.00307214744767132 iter num 860\n",
            "loss 0.5403221249580383 average time 0.0030663399715887283 iter num 880\n",
            "loss 0.13951696455478668 average time 0.0030604736666651256 iter num 900\n",
            "loss 0.22948099672794342 average time 0.0030522040771732307 iter num 920\n",
            "loss 0.5810307264328003 average time 0.0030446733255327305 iter num 940\n",
            "loss 0.1633569598197937 average time 0.003038226713542258 iter num 960\n",
            "loss 0.20506034791469574 average time 0.0030318666510223066 iter num 980\n",
            "loss 0.21979928016662598 average time 0.003023650077002003 iter num 1000\n",
            "loss 1.2886052131652832 average time 0.0175757426500013 iter num 20\n",
            "loss 1.5463175773620605 average time 0.010111889200004498 iter num 40\n",
            "loss 1.3668397665023804 average time 0.007611623816668119 iter num 60\n",
            "loss 0.518772542476654 average time 0.006351329224997926 iter num 80\n",
            "loss 0.9071668386459351 average time 0.005639608590004172 iter num 100\n",
            "loss 0.8574888706207275 average time 0.005143889158345397 iter num 120\n",
            "loss 0.23598027229309082 average time 0.004779147464299578 iter num 140\n",
            "loss 7.506735801696777 average time 0.004510275793768415 iter num 160\n",
            "loss 1.4883381128311157 average time 0.004297013577807876 iter num 180\n",
            "loss 0.5825511813163757 average time 0.004137770670029113 iter num 200\n",
            "loss 0.5053329467773438 average time 0.004000093600023485 iter num 220\n",
            "loss 0.739202618598938 average time 0.003881370779191684 iter num 240\n",
            "loss 0.7481204271316528 average time 0.0037844413538690646 iter num 260\n",
            "loss 2.0488407611846924 average time 0.0037104097071619436 iter num 280\n",
            "loss 0.2100907862186432 average time 0.003636276820020612 iter num 300\n",
            "loss 0.3388410210609436 average time 0.0035695154125178876 iter num 320\n",
            "loss 0.4173395037651062 average time 0.003509866247077298 iter num 340\n",
            "loss 0.5575771331787109 average time 0.003458521775015318 iter num 360\n",
            "loss 0.9689794778823853 average time 0.0034120384921236206 iter num 380\n",
            "loss 0.84854656457901 average time 0.0033763871225210096 iter num 400\n",
            "loss 0.8009929656982422 average time 0.003339075428594437 iter num 420\n",
            "loss 0.3718148469924927 average time 0.003308035327295949 iter num 440\n",
            "loss 1.0035626888275146 average time 0.003278696093501742 iter num 460\n",
            "loss 0.8319190740585327 average time 0.0032529500291881655 iter num 480\n",
            "loss 0.8999118804931641 average time 0.0032295038140173348 iter num 500\n",
            "loss 0.3739932179450989 average time 0.0032097911923214127 iter num 520\n",
            "loss 0.42971712350845337 average time 0.003189289483347474 iter num 540\n",
            "loss 0.3757965862751007 average time 0.0031696505482264976 iter num 560\n",
            "loss 0.5118988752365112 average time 0.0031515145913905648 iter num 580\n",
            "loss 0.4691682457923889 average time 0.0031352880083462273 iter num 600\n",
            "loss 0.24719004333019257 average time 0.003127059570980354 iter num 620\n",
            "loss 0.5812763571739197 average time 0.0031118735828258083 iter num 640\n",
            "loss 0.630784273147583 average time 0.0030995686666783788 iter num 660\n",
            "loss 0.5620240569114685 average time 0.003085748825012883 iter num 680\n",
            "loss 0.39858365058898926 average time 0.003073655988585285 iter num 700\n",
            "loss 0.3232249319553375 average time 0.0030618918916799328 iter num 720\n",
            "loss 0.4352498948574066 average time 0.0030539371581223373 iter num 740\n",
            "loss 0.3330906629562378 average time 0.0030443772789615314 iter num 760\n",
            "loss 0.36600461602211 average time 0.003035619546169936 iter num 780\n",
            "loss 0.4332655072212219 average time 0.003026409526268594 iter num 800\n",
            "loss 0.43045246601104736 average time 0.0030184510219691575 iter num 820\n",
            "loss 0.26333117485046387 average time 0.0030126252678742494 iter num 840\n",
            "loss 0.21679140627384186 average time 0.0030042794848991116 iter num 860\n",
            "loss 0.33855003118515015 average time 0.0030037988022875405 iter num 880\n",
            "loss 0.1695459485054016 average time 0.0029957766122354064 iter num 900\n",
            "loss 0.7652883529663086 average time 0.002993720706533919 iter num 920\n",
            "loss 0.3521748185157776 average time 0.0029871004946921565 iter num 940\n",
            "loss 0.6642037630081177 average time 0.0029835294656360626 iter num 960\n",
            "loss 0.33270779252052307 average time 0.0029777375551132035 iter num 980\n",
            "loss 0.3223319947719574 average time 0.0029713613000094483 iter num 1000\n",
            "loss 0.9102282524108887 average time 0.017743616200027644 iter num 20\n",
            "loss 0.8534078598022461 average time 0.01019134965002877 iter num 40\n",
            "loss 1.9980239868164062 average time 0.0077100305333488 iter num 60\n",
            "loss 2.6011385917663574 average time 0.00649467167498301 iter num 80\n",
            "loss 6.2392144203186035 average time 0.005722142059985344 iter num 100\n",
            "loss 1.3141295909881592 average time 0.005210331041659326 iter num 120\n",
            "loss 0.3518041968345642 average time 0.00484351721427499 iter num 140\n",
            "loss 0.5268913507461548 average time 0.004570724974996665 iter num 160\n",
            "loss 0.542535662651062 average time 0.004365708416660002 iter num 180\n",
            "loss 0.6497135162353516 average time 0.004196414189991628 iter num 200\n",
            "loss 1.2781195640563965 average time 0.004063521331809997 iter num 220\n",
            "loss 1.539604902267456 average time 0.003945465516655607 iter num 240\n",
            "loss 0.36622244119644165 average time 0.003845642719219401 iter num 260\n",
            "loss 0.34998154640197754 average time 0.0037613739107038263 iter num 280\n",
            "loss 0.4032090902328491 average time 0.003687494699991779 iter num 300\n",
            "loss 0.9382665157318115 average time 0.0036268556843694457 iter num 320\n",
            "loss 0.6926198601722717 average time 0.0035707885911653433 iter num 340\n",
            "loss 0.38833028078079224 average time 0.003525384630543411 iter num 360\n",
            "loss 0.8000444173812866 average time 0.003480059831569729 iter num 380\n",
            "loss 0.565775990486145 average time 0.003438897442493953 iter num 400\n",
            "loss 0.5877475738525391 average time 0.0034006916357081233 iter num 420\n",
            "loss 0.8023093342781067 average time 0.0033660123090850326 iter num 440\n",
            "loss 0.5855758190155029 average time 0.0033406197804306034 iter num 460\n",
            "loss 0.5569949150085449 average time 0.0033156452916604925 iter num 480\n",
            "loss 0.8708247542381287 average time 0.0032946344679903633 iter num 500\n",
            "loss 0.2753797173500061 average time 0.003271498457679802 iter num 520\n",
            "loss 2.2763092517852783 average time 0.003249116666658362 iter num 540\n",
            "loss 0.9733174443244934 average time 0.0032281328446312533 iter num 560\n",
            "loss 0.7880064845085144 average time 0.0032094202568832145 iter num 580\n",
            "loss 0.8689000606536865 average time 0.0031905973016538762 iter num 600\n",
            "loss 0.3742898404598236 average time 0.003173787985470118 iter num 620\n",
            "loss 0.31372490525245667 average time 0.0031567659999915064 iter num 640\n",
            "loss 0.5328225493431091 average time 0.003143994019689506 iter num 660\n",
            "loss 0.5041279792785645 average time 0.00312912167057742 iter num 680\n",
            "loss 0.23587217926979065 average time 0.0031185049585610615 iter num 700\n",
            "loss 0.5991791486740112 average time 0.003112590470821412 iter num 720\n",
            "loss 0.4845857620239258 average time 0.0031028772432345913 iter num 740\n",
            "loss 0.3810223340988159 average time 0.003090856111835269 iter num 760\n",
            "loss 0.6770473718643188 average time 0.003079031685890699 iter num 780\n",
            "loss 0.2770143151283264 average time 0.0030707709487450075 iter num 800\n",
            "loss 0.23796837031841278 average time 0.0030612318109694194 iter num 820\n",
            "loss 0.5463100075721741 average time 0.003051811923808145 iter num 840\n",
            "loss 0.3837769329547882 average time 0.0030432252011593827 iter num 860\n",
            "loss 0.35533609986305237 average time 0.0030360749988605305 iter num 880\n",
            "loss 0.27712124586105347 average time 0.0030272704788810896 iter num 900\n",
            "loss 0.230000838637352 average time 0.0030192254467341743 iter num 920\n",
            "loss 0.09323933720588684 average time 0.0030115071734004 iter num 940\n",
            "loss 0.3772507309913635 average time 0.0030038210979119145 iter num 960\n",
            "loss 0.507702112197876 average time 0.002996460239791193 iter num 980\n",
            "loss 0.24330557882785797 average time 0.0029900149349973617 iter num 1000\n",
            "loss 0.7696163654327393 average time 0.01780264925005213 iter num 20\n",
            "loss 1.0484838485717773 average time 0.010220559125036744 iter num 40\n",
            "loss 0.8789676427841187 average time 0.007699323450045388 iter num 60\n",
            "loss 0.7203856110572815 average time 0.0064335257750144596 iter num 80\n",
            "loss 0.434559166431427 average time 0.005676397490015006 iter num 100\n",
            "loss 0.352311909198761 average time 0.005188557183350895 iter num 120\n",
            "loss 2.5704259872436523 average time 0.00482852783573305 iter num 140\n",
            "loss 1.7365198135375977 average time 0.004571558575037216 iter num 160\n",
            "loss 0.27601051330566406 average time 0.004357315750030466 iter num 180\n",
            "loss 0.8031179904937744 average time 0.004184705205029786 iter num 200\n",
            "loss 0.6505004167556763 average time 0.004057847095480776 iter num 220\n",
            "loss 0.4438568353652954 average time 0.0039405953500325575 iter num 240\n",
            "loss 0.41401880979537964 average time 0.0038416483846469 iter num 260\n",
            "loss 0.5699741840362549 average time 0.003756638607176553 iter num 280\n",
            "loss 0.9808322787284851 average time 0.0036872345700248842 iter num 300\n",
            "loss 1.316290259361267 average time 0.0036270558656440245 iter num 320\n",
            "loss 0.6088119745254517 average time 0.003586964729428848 iter num 340\n",
            "loss 0.4991854131221771 average time 0.003534043116691363 iter num 360\n",
            "loss 0.4594552516937256 average time 0.0034915727658196708 iter num 380\n",
            "loss 1.021702527999878 average time 0.0034512793400335797 iter num 400\n",
            "loss 0.6916643381118774 average time 0.0034131917595546144 iter num 420\n",
            "loss 0.3357625901699066 average time 0.003384308804571777 iter num 440\n",
            "loss 0.852847695350647 average time 0.003351599030457178 iter num 460\n",
            "loss 0.2903071641921997 average time 0.0033221672937732666 iter num 480\n",
            "loss 0.7781392335891724 average time 0.0032953113400180883 iter num 500\n",
            "loss 0.28355544805526733 average time 0.0032703175827114486 iter num 520\n",
            "loss 0.5376418828964233 average time 0.003246625768536062 iter num 540\n",
            "loss 0.2837139368057251 average time 0.0032242540767989727 iter num 560\n",
            "loss 0.49696826934814453 average time 0.0032042615810433774 iter num 580\n",
            "loss 1.426847219467163 average time 0.0031863997533400834 iter num 600\n",
            "loss 0.4306800663471222 average time 0.003170912595167817 iter num 620\n",
            "loss 0.6505234837532043 average time 0.0031540627265698616 iter num 640\n",
            "loss 0.19440725445747375 average time 0.0031388518257672057 iter num 660\n",
            "loss 0.2744278311729431 average time 0.00312734738236165 iter num 680\n",
            "loss 0.19133105874061584 average time 0.0031140097942937116 iter num 700\n",
            "loss 0.2918647527694702 average time 0.0031018984458379842 iter num 720\n",
            "loss 0.7531378865242004 average time 0.003088806178380469 iter num 740\n",
            "loss 0.18406201899051666 average time 0.0030768729631577836 iter num 760\n",
            "loss 0.1445920616388321 average time 0.003066166661539263 iter num 780\n",
            "loss 0.3921801447868347 average time 0.0030573102674998154 iter num 800\n",
            "loss 0.20935484766960144 average time 0.003047332241460869 iter num 820\n",
            "loss 0.5945223569869995 average time 0.0030373380357072424 iter num 840\n",
            "loss 0.48647964000701904 average time 0.0030277557372004973 iter num 860\n",
            "loss 0.3688119649887085 average time 0.003018727993170697 iter num 880\n",
            "loss 0.9528979063034058 average time 0.0030104865144322604 iter num 900\n",
            "loss 0.24968305230140686 average time 0.003007174634767975 iter num 920\n",
            "loss 0.38681599497795105 average time 0.0030002258191338927 iter num 940\n",
            "loss 0.3577670156955719 average time 0.002993473369776704 iter num 960\n",
            "loss 0.16357937455177307 average time 0.0029862901367184295 iter num 980\n",
            "loss 0.5534391403198242 average time 0.0029810499639816045 iter num 1000\n",
            "loss 0.5266631841659546 average time 0.017607118850037295 iter num 20\n",
            "loss 0.7851548194885254 average time 0.010140578550044665 iter num 40\n",
            "loss 1.0053571462631226 average time 0.007676175216753715 iter num 60\n",
            "loss 0.5856863260269165 average time 0.006428823162536901 iter num 80\n",
            "loss 0.6109719276428223 average time 0.005665640360020916 iter num 100\n",
            "loss 0.7858264446258545 average time 0.00517145291671568 iter num 120\n",
            "loss 1.5770224332809448 average time 0.00480896509289518 iter num 140\n",
            "loss 1.0700089931488037 average time 0.00456034966254606 iter num 160\n",
            "loss 0.2906627655029297 average time 0.0043538607389261115 iter num 180\n",
            "loss 0.43761104345321655 average time 0.004184247660027722 iter num 200\n",
            "loss 0.8406885862350464 average time 0.004042628068203736 iter num 220\n",
            "loss 0.40647485852241516 average time 0.003928199987505347 iter num 240\n",
            "loss 0.6162540316581726 average time 0.0038321334615410217 iter num 260\n",
            "loss 0.5691617131233215 average time 0.0037481156392816825 iter num 280\n",
            "loss 1.264479398727417 average time 0.0036798075066690216 iter num 300\n",
            "loss 0.559546947479248 average time 0.003614224262506127 iter num 320\n",
            "loss 0.8326045274734497 average time 0.003576925494125509 iter num 340\n",
            "loss 0.5749506950378418 average time 0.0035299109694506116 iter num 360\n",
            "loss 0.40675878524780273 average time 0.003486392468427144 iter num 380\n",
            "loss 0.557206392288208 average time 0.003444511390007392 iter num 400\n",
            "loss 0.22915154695510864 average time 0.0034067681238151896 iter num 420\n",
            "loss 0.8839998245239258 average time 0.003373462343190361 iter num 440\n",
            "loss 0.2112453281879425 average time 0.0033426244065317997 iter num 460\n",
            "loss 0.560571551322937 average time 0.003315989622927873 iter num 480\n",
            "loss 0.20846103131771088 average time 0.0032908683580080834 iter num 500\n",
            "loss 0.3557388186454773 average time 0.0032674059730862398 iter num 520\n",
            "loss 0.4582051932811737 average time 0.003246204301854892 iter num 540\n",
            "loss 0.1733623743057251 average time 0.0032261768482174246 iter num 560\n",
            "loss 0.3509272038936615 average time 0.003213502115516443 iter num 580\n",
            "loss 0.5025051236152649 average time 0.003198125345002154 iter num 600\n",
            "loss 0.07108338177204132 average time 0.003184362537102413 iter num 620\n",
            "loss 0.9665669798851013 average time 0.003169859112503559 iter num 640\n",
            "loss 0.46804529428482056 average time 0.0031558879409120974 iter num 660\n",
            "loss 0.20911797881126404 average time 0.0031436004911791528 iter num 680\n",
            "loss 0.399658203125 average time 0.0031308003471414328 iter num 700\n",
            "loss 0.2157793492078781 average time 0.0031186124069462395 iter num 720\n",
            "loss 0.40394267439842224 average time 0.0031074699202678392 iter num 740\n",
            "loss 0.15410056710243225 average time 0.0030977626565764265 iter num 760\n",
            "loss 0.25559723377227783 average time 0.0030875277038422866 iter num 780\n",
            "loss 0.26167237758636475 average time 0.0030807872949969806 iter num 800\n",
            "loss 0.20916876196861267 average time 0.0030714459987792927 iter num 820\n",
            "loss 0.47497883439064026 average time 0.0030621246357159615 iter num 840\n",
            "loss 0.2460426539182663 average time 0.003056610959302541 iter num 860\n",
            "loss 0.1047375351190567 average time 0.003050434257955782 iter num 880\n",
            "loss 0.26175257563591003 average time 0.003043476658892056 iter num 900\n",
            "loss 0.11444568634033203 average time 0.003036322990222921 iter num 920\n",
            "loss 0.33498477935791016 average time 0.0030295888223453734 iter num 940\n",
            "loss 0.18103039264678955 average time 0.003023398959378672 iter num 960\n",
            "loss 0.20113813877105713 average time 0.003018909167351049 iter num 980\n",
            "loss 0.13641342520713806 average time 0.003014072956002565 iter num 1000\n",
            "loss 0.6504711508750916 average time 0.017730631599988556 iter num 20\n",
            "loss 0.6502863168716431 average time 0.010233742925015577 iter num 40\n",
            "loss 0.625652551651001 average time 0.007769763449975168 iter num 60\n",
            "loss 0.3398439586162567 average time 0.006507834349974928 iter num 80\n",
            "loss 0.8793641328811646 average time 0.005766100219952932 iter num 100\n",
            "loss 0.9547542929649353 average time 0.005255417708296287 iter num 120\n",
            "loss 1.2478058338165283 average time 0.0049003232642397575 iter num 140\n",
            "loss 0.4371083080768585 average time 0.004642266806212092 iter num 160\n",
            "loss 1.3051751852035522 average time 0.004432950999959454 iter num 180\n",
            "loss 0.4181789755821228 average time 0.004259615034966373 iter num 200\n",
            "loss 1.6532624959945679 average time 0.004119044595427113 iter num 220\n",
            "loss 0.515357494354248 average time 0.0040028673541542045 iter num 240\n",
            "loss 0.6501368880271912 average time 0.0039125053153769 iter num 260\n",
            "loss 0.5829649567604065 average time 0.0038399085357208864 iter num 280\n",
            "loss 1.0362274646759033 average time 0.0037677533766721657 iter num 300\n",
            "loss 0.4833603799343109 average time 0.0037033617593749566 iter num 320\n",
            "loss 0.3929331302642822 average time 0.0036523376941097335 iter num 340\n",
            "loss 1.7343074083328247 average time 0.0036013982000011107 iter num 360\n",
            "loss 2.033449649810791 average time 0.0035544608710575917 iter num 380\n",
            "loss 0.48700058460235596 average time 0.0035167532750028842 iter num 400\n",
            "loss 0.3444962501525879 average time 0.003482808642859579 iter num 420\n",
            "loss 0.8085938096046448 average time 0.0034464642863688577 iter num 440\n",
            "loss 0.5463239550590515 average time 0.0034144784065278065 iter num 460\n",
            "loss 0.4056660532951355 average time 0.0033852930229253766 iter num 480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-15684524a3dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m           European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n\u001b[0;32m--> 128\u001b[0;31m                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m           \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this contains prices for each stock for each path at time T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         return self.dispatcher.call(args, self.griddim, self.blockdim,\n\u001b[0;32m--> 770\u001b[0;31m                                     self.stream, self.sharedmem)\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    860\u001b[0m             [self.typingctx.resolve_argument_type(a) for a in args])\n\u001b[1;32m    861\u001b[0m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharedmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspecialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mkernelargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margument_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Configure kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36m_prepare_args\u001b[0;34m(self, ty, val, stream, retr, kernelargs)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m             \u001b[0mdevary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0mc_intp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_ssize_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/args.py\u001b[0m in \u001b[0;36mto_device\u001b[0;34m(self, retr, stream)\u001b[0m\n\u001b[1;32m     61\u001b[0m         devary, conv = auto_device(\n\u001b[1;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             stream=stream)\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mretr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_host\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36mauto_device\u001b[0;34m(obj, stream, copy)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__cuda_array_interface__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_cuda_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/api.py\u001b[0m in \u001b[0;36mas_cuda_array\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         return from_cuda_array_interface(obj.__cuda_array_interface__,\n\u001b[0;32m---> 67\u001b[0;31m                                          owner=obj)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36m_require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36mensure_context\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0many\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mNumba\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_active_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0moldctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_attached_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mnewctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mhctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrvapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcu_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuCtxGetCurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mhctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhctx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fabcf7-fed4-450a-b176-d771fe63cb97"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "import torch\n",
        "model_save_name = f'EuCall_{str(nstock)}_v2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HCgRfQm46W8"
      },
      "source": [
        "model_save_name = 'Judy.pth'"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec80a847-e216-4335-da6b-3ccbe1094cf1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387525a8-4947-4fbb-a56b-7bc84e8fbd0a"
      },
      "source": [
        "import torch\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a7aa45-dc4b-4223-c519-5c5ca3a4b7af"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net(nstock = nstock).cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "### Continue to train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b3e1b6-9774-421b-d5e6-95e62b86a95e"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "#model = Net(nstock = nstock).cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=500, number_path = 1024, batch=32, stocks=nstock)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 12.92512321472168 average time 0.018425430449997292 iter num 20\n",
            "loss 0.9737306833267212 average time 0.010558371724982862 iter num 40\n",
            "loss 0.7903856635093689 average time 0.007945787699994374 iter num 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KTvu9N4oSY"
      },
      "source": [
        "# model_save_name = f'EuCall_{str(nstock)}_v2_Erin.pth'\n",
        "model_save_name = 'Judy.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F"
      },
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05] * nstock]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu"
      },
      "source": [
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05] * nstock]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwgeVDsA_Mr"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi_2Dqnx3_oP"
      },
      "source": [
        "#### Using gradient, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp"
      },
      "source": [
        "#### Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S, ith):\n",
        "    inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MfZKJlv13SC"
      },
      "source": [
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 150, 0.1)\n",
        "  deltas = []\n",
        "  for p in prices:\n",
        "      deltas.append(compute_delta(p, i).item())\n",
        "  fig = pylab.plot(prices, deltas, label = f'{i}th stock')\n",
        "  pylab.legend(loc = 'lower right')\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Delta')\n",
        "  fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXixHQic35iI"
      },
      "source": [
        "#### Using Finite Difference, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGk5Hw64fMdh"
      },
      "source": [
        "## Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S, ith):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S + epsilon, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "\n",
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 150, 0.1)\n",
        "  deltas = []\n",
        "  for p in prices:\n",
        "      deltas.append(compute_delta(p, i).item())\n",
        "  fig = pylab.plot(prices, deltas, label = f'{i}th stock')\n",
        "  pylab.legend(loc = 'lower right')\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Delta')\n",
        "  fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIJe36fJ4JFO"
      },
      "source": [
        "#### Using Finite Difference, Change 3 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1I8COnUxnz"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "\n",
        "\n",
        "prices = np.arange(0, 150, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DfOK_a4nuEt"
      },
      "source": [
        "compute_delta(110)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB8LPwfBMHQ"
      },
      "source": [
        "# Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlXVePiK4UH5"
      },
      "source": [
        "#### Using gradient, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S, ith):\n",
        "  inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "  # inputs = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05] + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "  inputs.requires_grad = True\n",
        "  x = model(inputs.float())\n",
        "  #x = model(inputs)\n",
        "  loss_grads = grad(x, inputs, create_graph=True)\n",
        "  drv = grad(loss_grads[0][0][2], inputs)\n",
        "  return drv[0][0][2]\n",
        "\n",
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 150, 0.1)\n",
        "  gammas = []\n",
        "  for p in prices:\n",
        "      gammas.append(compute_gamma(p, i).item())\n",
        "  fig2 = pylab.plot(prices, gammas, label = f'{i}th stock')\n",
        "  pylab.legend(loc = 'upper right')\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Gamma')\n",
        "  fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDEmn97t4xxJ"
      },
      "source": [
        "#### Using Finite Difference, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoaOyCDxQy0"
      },
      "source": [
        "##Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S, ith):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S+epsilon, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs3 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S-epsilon, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    # inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]  + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    # inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]  + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    # inputs3 = torch.tensor([[1, 110.0, S - epsilon, 0.35, 0.1, 0.05]  + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "\n",
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 150, 0.1)\n",
        "  gammas = []\n",
        "  for p in prices:\n",
        "      gammas.append(compute_gamma(p, i).item())\n",
        "  fig = pylab.plot(prices, gammas, label = f'{i}th stock')\n",
        "  pylab.legend(loc = 'upper right')\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Gamma')\n",
        "  fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnRaSM685NCq"
      },
      "source": [
        "#### Using Finite Difference, Change 3 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsXgOwWZ_ru"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    inputs3 = torch.tensor([[1, 110.0, S - epsilon, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "\n",
        "prices = np.arange(0, 150, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lca2xrBh9a"
      },
      "source": [
        "# Vega"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuETeg-e53wz"
      },
      "source": [
        "#### Using finite difference, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muozc-hzhSGA"
      },
      "source": [
        "##Using finite difference, Change only 1 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S, ith):\n",
        "    epsilon = 0.5\n",
        "\n",
        "    inputs1 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35+epsilon, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "\n",
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 150, 0.1)\n",
        "  vegas = []\n",
        "  for p in prices:\n",
        "      vegas.append(compute_vega(p, i).item())\n",
        "  fig = pylab.plot(prices, vegas, label = f'{i}th stock')\n",
        "  pylab.legend(loc = 'upper left')\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Vega')\n",
        "  fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_BYyL9d584w"
      },
      "source": [
        "#### Using Finite Difference, Change 3 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KATxBCAdlFt"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S, 0.35 + epsilon, 0.1, 0.05]*nstock]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "\n",
        "\n",
        "prices = np.arange(0, 150, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "# import pylab\n",
        "# import numpy as np\n",
        "# def compute_price(sigma):\n",
        "#     inputs = torch.tensor([[1, 110.0, 110.0, sigma, 0.1, 0.05]]).cuda()\n",
        "#     x = model(inputs.float())\n",
        "#     #x = model(inputs)\n",
        "#     return x.item()\n",
        "# sigmas = np.arange(0, 0.5, 0.1)\n",
        "# prices = []\n",
        "# for s in sigmas:\n",
        "#     prices.append(compute_price(s))\n",
        "# fig3 = pylab.plot(sigmas, prices)\n",
        "# pylab.xlabel('Sigma')\n",
        "# pylab.ylabel('Price')\n",
        "# fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "# def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "#     if fun(large) - target < 0:\n",
        "#         print('upper bound is too small')\n",
        "#         return None\n",
        "#     if fun(small) - target > 0:\n",
        "#         print('lower bound is too large')\n",
        "#         return None\n",
        "#     while large - small > EPS:\n",
        "#         mid = (large + small) / 2.0\n",
        "#         if fun(mid) - target >= 0:\n",
        "#             large = mid\n",
        "#         else:\n",
        "#             small = mid\n",
        "#     mid = (large + small) / 2.0\n",
        "#     return mid, abs(fun(mid) - target)\n",
        "# quoted_price = 16.0\n",
        "# sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "# print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEiAredqQGxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}