{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "European_Call.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "TY_9g3tbdLiY",
        "u2_89jOknwjH",
        "rXT4Bg0wdL7l"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/European_Call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "9d3631ae-7e5e-4396-f279-49eae56533f2"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   6396      0 --:--:-- --:--:-- --:--:--  6422\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9 MB 33 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 56.5 MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "### Train(Erin Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxT9Eida-c_"
      },
      "source": [
        "# ################################# TEST ########################################\n",
        "# %%writefile cupy_dataset.py\n",
        "\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import random\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "#         paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           X = cupy.array([])\n",
        "#           K_rand = cupy.random.rand(1)[0]\n",
        "#           B_rand = cupy.random.rand(1)[0]\n",
        "#           r_rand = cupy.random.rand(1)[0]\n",
        "#           for i in range(0,self.N_STOCKS):\n",
        "#             X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "#           X = X.reshape(self.N_STOCKS,6)\n",
        "#           X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "#           #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "#           #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "#           # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#           #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "#           # make sure the Barrier is smaller than the Strike price\n",
        "#           # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#           for i in range(self.N_STOCKS):\n",
        "#             paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "#           stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "#           rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "#           #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "#           #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "#           #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#           stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "#           cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "#           num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "#           randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                                         num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "#           b1_r = randoms_gpu[:,0]\n",
        "#           b2_r = randoms_gpu[:,1]\n",
        "#           randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "#           for i in range(interval):\n",
        "#             if i % 2 == 0:\n",
        "#                 ind = int(i/2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "#             else:\n",
        "#                 ind = int(i//2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "#           randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "#           o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "#           Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# # ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# # for i in ds:\n",
        "# #     print(i[0])\n",
        "# ################################# TEST ########################################"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dZnWTTfbf1"
      },
      "source": [
        "### Train (European Call option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeREuPw0fguQ",
        "outputId": "8fdeea9c-b4c3-4c44-822d-fb5eddb8ab62"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def European_call_option(d_s, T, K, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    #tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        h = T[batch_id] / N_STEPS\n",
        "        tmp1 = mu[batch_id]*T[batch_id]/N_STEPS \n",
        "        tmp2 = math.exp(-r[batch_id]*T[batch_id]) # discount\n",
        "        tmp3 = math.sqrt(T[batch_id]/N_STEPS)\n",
        "        #running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "          #s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "          s_curr = s_curr * math.exp((r[batch_id] - (1/2)*sigma[batch_id]**2)*h + sigma[batch_id] * tmp3 * d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH])\n",
        "          #running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "          #if i==0 and batch_id == 2:\n",
        "          #    print(s_curr)\n",
        "          #if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "          #    break\n",
        "        #payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        payoff = s_curr - K[batch_id] if s_curr > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        #self.N_STEPS = 365\n",
        "        self.N_STEPS = 10000\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        #self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        #paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 5), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          X = cupy.array([])\n",
        "          #T_rand = cupy.random.rand(1)[0]\n",
        "          #K_rand = cupy.random.rand(1)[0]\n",
        "          #B_rand = cupy.random.rand(1)[0]\n",
        "          #r_rand = cupy.random.rand(1)[0]\n",
        "          for i in range(0, self.N_STOCKS):\n",
        "            #X =  cupy.concatenate((X, cupy.array([K_rand,B_rand]), cupy.random.rand(3), cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "            X = cupy.concatenate((X, cupy.random.rand(6))) #[T, K, S0, sigma, mu, r]\n",
        "          \n",
        "          X = X.reshape(self.N_STOCKS, 6)\n",
        "          #X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #[T, K, S0, sigma, mu, r]\n",
        "          X = X * ((cupy.array([3, 200.0, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "            #paras[op, i*5:(i+1)*5] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          if self.N_STOCKS != 1:\n",
        "            stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "            cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "            num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "            randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                          num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "            b1_r = randoms_gpu[:,0]\n",
        "            b2_r = randoms_gpu[:,1]\n",
        "            randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "            interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "            for i in range(interval):\n",
        "              if i % 2 == 0:\n",
        "                  ind = int(i/2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "              else:\n",
        "                  ind = int(i//2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "          if self.N_STOCKS == 1:\n",
        "            randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          \n",
        "          European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(2, number_path = 100000, batch = 1, seed = random.randint(0,100), stocks=1)\n",
        "# for i in ds:\n",
        "#     print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43d6074-0f47-405a-bee2-86647b260931"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0, 0.1, 200.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([3, 200.0, 200.0, 0.4, 0.2, 0.2]*1)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19f34c6-2a65-491d-c38e-d747a0c38d47"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.6-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 35.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 61 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 112 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 122 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 143 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 153 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 184 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 194 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 204 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 215 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 225 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 232 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b195f79-6237-4ea6-dd34-4ecd89be18aa"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 1000, number_path = 1024, batch = 1, stocks = 1)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=1000)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 22.7678279876709 average time 0.12757542479999984 iter num 20\n",
            "loss 1658.3956298828125 average time 0.06503163990001895 iter num 40\n",
            "loss 4.402946949005127 average time 0.04410188120001143 iter num 60\n",
            "loss 1329.0072021484375 average time 0.033652722137512116 iter num 80\n",
            "loss 3937.8759765625 average time 0.027406589390013777 iter num 100\n",
            "loss 9997.2138671875 average time 0.023216938991676747 iter num 120\n",
            "loss 1098.6044921875 average time 0.02021841405000779 iter num 140\n",
            "loss 751.1159057617188 average time 0.018009933800003354 iter num 160\n",
            "loss 2482.378662109375 average time 0.016260185333339904 iter num 180\n",
            "loss 244.85018920898438 average time 0.014854227045004791 iter num 200\n",
            "loss 5.176673412322998 average time 0.013707708545457949 iter num 220\n",
            "loss 5557.03955078125 average time 0.012784559920838016 iter num 240\n",
            "loss 326.3426513671875 average time 0.011981368188466708 iter num 260\n",
            "loss 570.961181640625 average time 0.011289012132151097 iter num 280\n",
            "loss 4544.69189453125 average time 0.01068363427667388 iter num 300\n",
            "loss 171.17916870117188 average time 0.010171828706256747 iter num 320\n",
            "loss 813.2120361328125 average time 0.00970244235588646 iter num 340\n",
            "loss 655.548583984375 average time 0.009299831616668698 iter num 360\n",
            "loss 736.50048828125 average time 0.008925277555266276 iter num 380\n",
            "loss 779.0355834960938 average time 0.008605967510002302 iter num 400\n",
            "loss 150.39016723632812 average time 0.008321648721430175 iter num 420\n",
            "loss 697.626953125 average time 0.008048882336363373 iter num 440\n",
            "loss 365.51177978515625 average time 0.007797409334782811 iter num 460\n",
            "loss 506.96710205078125 average time 0.007569282654167845 iter num 480\n",
            "loss 183.313232421875 average time 0.007360745128001781 iter num 500\n",
            "loss 5.494093894958496 average time 0.007165980386541221 iter num 520\n",
            "loss 951.4226684570312 average time 0.0069935222222233685 iter num 540\n",
            "loss 237.37640380859375 average time 0.006834697491072461 iter num 560\n",
            "loss 1.477220892906189 average time 0.0066819473913793 iter num 580\n",
            "loss 47.27199935913086 average time 0.0065452618116656446 iter num 600\n",
            "loss 4.7032647132873535 average time 0.006407256725804972 iter num 620\n",
            "loss 221.10064697265625 average time 0.006287456287498827 iter num 640\n",
            "loss 5.397229194641113 average time 0.00616531703333294 iter num 660\n",
            "loss 0.9519763588905334 average time 0.006050941883822749 iter num 680\n",
            "loss 298.947021484375 average time 0.005942132379998384 iter num 700\n",
            "loss 13.381501197814941 average time 0.005842102261109024 iter num 720\n",
            "loss 225.58242797851562 average time 0.0057458390310791176 iter num 740\n",
            "loss 122.40692901611328 average time 0.005655687839472241 iter num 760\n",
            "loss 7.582612991333008 average time 0.005575040794871 iter num 780\n",
            "loss 96.08702087402344 average time 0.005493496631249286 iter num 800\n",
            "loss 65.75738525390625 average time 0.005423197114633949 iter num 820\n",
            "loss 65.77507781982422 average time 0.005349146007142735 iter num 840\n",
            "loss 3.046750545501709 average time 0.005277675141860791 iter num 860\n",
            "loss 0.08173098415136337 average time 0.0052206122659095465 iter num 880\n",
            "loss 0.1277385801076889 average time 0.005153900353333635 iter num 900\n",
            "loss 22.585935592651367 average time 0.005090336090216851 iter num 920\n",
            "loss 0.2240370512008667 average time 0.005029912030850851 iter num 940\n",
            "loss 3.119802474975586 average time 0.0049857320395832495 iter num 960\n",
            "loss 24.61317253112793 average time 0.004931932142856762 iter num 980\n",
            "loss 283.00439453125 average time 0.004877572968999175 iter num 1000\n",
            "loss 3.0332770347595215 average time 0.0028261432000022068 iter num 20\n",
            "loss 0.22674383223056793 average time 0.0025392423000198507 iter num 40\n",
            "loss 29.874107360839844 average time 0.0025275260166646757 iter num 60\n",
            "loss 553.1593627929688 average time 0.0024677061624998942 iter num 80\n",
            "loss 268.6900939941406 average time 0.002425820669998302 iter num 100\n",
            "loss 78.8939208984375 average time 0.0024556310083312383 iter num 120\n",
            "loss 851.1825561523438 average time 0.002423838771431162 iter num 140\n",
            "loss 378.0414123535156 average time 0.0023964772812519186 iter num 160\n",
            "loss 82.39668273925781 average time 0.0023758640166698167 iter num 180\n",
            "loss 139.6711883544922 average time 0.0023640792700041404 iter num 200\n",
            "loss 217.08180236816406 average time 0.002348887195457585 iter num 220\n",
            "loss 222.0371551513672 average time 0.0023502461541710317 iter num 240\n",
            "loss 481.62689208984375 average time 0.0023410269846207343 iter num 260\n",
            "loss 5.367917537689209 average time 0.002330810971432941 iter num 280\n",
            "loss 2.1803832054138184 average time 0.002336114070005048 iter num 300\n",
            "loss 246.69668579101562 average time 0.002336162025004285 iter num 320\n",
            "loss 131.5521697998047 average time 0.0023304460029454706 iter num 340\n",
            "loss 135.31967163085938 average time 0.002337412683337763 iter num 360\n",
            "loss 42.3172721862793 average time 0.0023519429052660263 iter num 380\n",
            "loss 59.94560241699219 average time 0.0023503030475023933 iter num 400\n",
            "loss 1.1363519430160522 average time 0.002344036354764696 iter num 420\n",
            "loss 17.225234985351562 average time 0.0023504791613664687 iter num 440\n",
            "loss 3.717252731323242 average time 0.0023448346413068107 iter num 460\n",
            "loss 15.6583890914917 average time 0.002349516627085772 iter num 480\n",
            "loss 0.06356498599052429 average time 0.0023441323380029642 iter num 500\n",
            "loss 6.93795108795166 average time 0.002343684338464632 iter num 520\n",
            "loss 11.513593673706055 average time 0.0023402180685210354 iter num 540\n",
            "loss 413.6015625 average time 0.0023375943892874926 iter num 560\n",
            "loss 6.30535888671875 average time 0.00233351236724212 iter num 580\n",
            "loss 885.751708984375 average time 0.0023394016483348425 iter num 600\n",
            "loss 54.30299758911133 average time 0.0023412098032277724 iter num 620\n",
            "loss 27.672760009765625 average time 0.002339650100001833 iter num 640\n",
            "loss 12.180296897888184 average time 0.002344976692427281 iter num 660\n",
            "loss 13.838109016418457 average time 0.002346753804415116 iter num 680\n",
            "loss 413.6735534667969 average time 0.0023452449871459585 iter num 700\n",
            "loss 2.6828205585479736 average time 0.002342812130559145 iter num 720\n",
            "loss 29.536949157714844 average time 0.0023398074418952168 iter num 740\n",
            "loss 0.2774561047554016 average time 0.0023445323157925725 iter num 760\n",
            "loss 2.5795881748199463 average time 0.0023504537269262143 iter num 780\n",
            "loss 0.7081423997879028 average time 0.002347447965002516 iter num 800\n",
            "loss 200.84909057617188 average time 0.002344528071953724 iter num 820\n",
            "loss 68.173828125 average time 0.0023533243773840697 iter num 840\n",
            "loss 74.98601531982422 average time 0.0023525981395381636 iter num 860\n",
            "loss 240.30392456054688 average time 0.002350711473866599 iter num 880\n",
            "loss 152.0897216796875 average time 0.002348520841113542 iter num 900\n",
            "loss 142.23043823242188 average time 0.0023504750271760477 iter num 920\n",
            "loss 725.781005859375 average time 0.00234815414042748 iter num 940\n",
            "loss 12.928812026977539 average time 0.0023450877593764593 iter num 960\n",
            "loss 94.79920196533203 average time 0.0023439689255114063 iter num 980\n",
            "loss 6.528780937194824 average time 0.0023455497390004894 iter num 1000\n",
            "loss 31.90932846069336 average time 0.0029369492999876457 iter num 20\n",
            "loss 440.3358154296875 average time 0.0025736809749901113 iter num 40\n",
            "loss 3.105992078781128 average time 0.002448858199996569 iter num 60\n",
            "loss 1690.980712890625 average time 0.0024545364499914514 iter num 80\n",
            "loss 29.912755966186523 average time 0.002462617179996869 iter num 100\n",
            "loss 75.36148071289062 average time 0.002469613658329687 iter num 120\n",
            "loss 134.7243194580078 average time 0.002451484328568248 iter num 140\n",
            "loss 2.163865327835083 average time 0.00244110408124385 iter num 160\n",
            "loss 4.045921802520752 average time 0.0024156329444432836 iter num 180\n",
            "loss 575.4121704101562 average time 0.002426170524998952 iter num 200\n",
            "loss 119.84663391113281 average time 0.0024422188818189997 iter num 220\n",
            "loss 3.8600714206695557 average time 0.0024280570083353346 iter num 240\n",
            "loss 208.58682250976562 average time 0.0024151252307695777 iter num 260\n",
            "loss 606.896484375 average time 0.0023990637142869025 iter num 280\n",
            "loss 0.6396967172622681 average time 0.0023882951466680426 iter num 300\n",
            "loss 206.9390411376953 average time 0.0023911148656246438 iter num 320\n",
            "loss 0.2153373658657074 average time 0.0023850185441166532 iter num 340\n",
            "loss 625.1571655273438 average time 0.002375297725000615 iter num 360\n",
            "loss 497.0989990234375 average time 0.0023862023684231596 iter num 380\n",
            "loss 1292.9217529296875 average time 0.002386503805002462 iter num 400\n",
            "loss 1266.205322265625 average time 0.0023958393190501477 iter num 420\n",
            "loss 179.94178771972656 average time 0.002386224409093529 iter num 440\n",
            "loss 36.174766540527344 average time 0.002394713473915999 iter num 460\n",
            "loss 8.606529235839844 average time 0.0023908397812519412 iter num 480\n",
            "loss 2342.004150390625 average time 0.0023929084020005574 iter num 500\n",
            "loss 1.313902735710144 average time 0.0023857884288453046 iter num 520\n",
            "loss 356.9953918457031 average time 0.002385610892591743 iter num 540\n",
            "loss 11.888769149780273 average time 0.002394725710714088 iter num 560\n",
            "loss 226.5889434814453 average time 0.002391949622413308 iter num 580\n",
            "loss 123.3048324584961 average time 0.0023934474949995396 iter num 600\n",
            "loss 1.805582046508789 average time 0.0023957744903214005 iter num 620\n",
            "loss 4.239504814147949 average time 0.0023913108531242954 iter num 640\n",
            "loss 889.7493286132812 average time 0.00239441755908963 iter num 660\n",
            "loss 169.33103942871094 average time 0.0023982639764690648 iter num 680\n",
            "loss 25.275402069091797 average time 0.002398939941426761 iter num 700\n",
            "loss 221.64279174804688 average time 0.0024001577861096418 iter num 720\n",
            "loss 181.6406707763672 average time 0.002400149609458232 iter num 740\n",
            "loss 262.83282470703125 average time 0.0024011241539459373 iter num 760\n",
            "loss 28.83256721496582 average time 0.002402726374357255 iter num 780\n",
            "loss 86.06429290771484 average time 0.002404398549998632 iter num 800\n",
            "loss 4.2276997566223145 average time 0.002406337590242324 iter num 820\n",
            "loss 10.444112777709961 average time 0.002407229883331887 iter num 840\n",
            "loss 37.09651184082031 average time 0.0024092712383712964 iter num 860\n",
            "loss 4.631532669067383 average time 0.002413227095454121 iter num 880\n",
            "loss 45.82283401489258 average time 0.0024133698022218496 iter num 900\n",
            "loss 7.964577674865723 average time 0.002418591594563988 iter num 920\n",
            "loss 24.197599411010742 average time 0.002419337094680399 iter num 940\n",
            "loss 16.695796966552734 average time 0.0024175784729159487 iter num 960\n",
            "loss 340.72552490234375 average time 0.0024249956489788187 iter num 980\n",
            "loss 161.7388458251953 average time 0.002425746488999721 iter num 1000\n",
            "loss 145.28900146484375 average time 0.003099726350012588 iter num 20\n",
            "loss 4.517497539520264 average time 0.0027587797750015853 iter num 40\n",
            "loss 6.635983467102051 average time 0.0026556195000087732 iter num 60\n",
            "loss 169.5240936279297 average time 0.0026012269500000685 iter num 80\n",
            "loss 209.97706604003906 average time 0.0025963063699919074 iter num 100\n",
            "loss 172.28590393066406 average time 0.00256718851665975 iter num 120\n",
            "loss 104.18909454345703 average time 0.002564842128572309 iter num 140\n",
            "loss 191.94400024414062 average time 0.0025473717250051207 iter num 160\n",
            "loss 247.8097381591797 average time 0.0025318307166723266 iter num 180\n",
            "loss 142.51710510253906 average time 0.0025245970900027712 iter num 200\n",
            "loss 384.158447265625 average time 0.002570304581820437 iter num 220\n",
            "loss 362.7793884277344 average time 0.002565151954169664 iter num 240\n",
            "loss 156.20327758789062 average time 0.0025596217846187894 iter num 260\n",
            "loss 62.7304801940918 average time 0.002552831725003963 iter num 280\n",
            "loss 1.2999117374420166 average time 0.002549787373337343 iter num 300\n",
            "loss 5.299283027648926 average time 0.0025392449031294007 iter num 320\n",
            "loss 895.1337280273438 average time 0.0025319547029452596 iter num 340\n",
            "loss 2236.31103515625 average time 0.0025287923333381615 iter num 360\n",
            "loss 96.97526550292969 average time 0.002527042578953785 iter num 380\n",
            "loss 140.0298309326172 average time 0.0025324795000068435 iter num 400\n",
            "loss 479.9576721191406 average time 0.002534648128578066 iter num 420\n",
            "loss 67.2348861694336 average time 0.0025331098045507424 iter num 440\n",
            "loss 24.846494674682617 average time 0.002529909623919356 iter num 460\n",
            "loss 477.7966003417969 average time 0.002529014893757638 iter num 480\n",
            "loss 7.954598426818848 average time 0.0025258789840072494 iter num 500\n",
            "loss 718.848388671875 average time 0.002523640715392398 iter num 520\n",
            "loss 191.73391723632812 average time 0.0025229281055632984 iter num 540\n",
            "loss 585.9826049804688 average time 0.00251855961607857 iter num 560\n",
            "loss 6.636356830596924 average time 0.002516219977593241 iter num 580\n",
            "loss 264.4722900390625 average time 0.0025151645766739723 iter num 600\n",
            "loss 6.335689544677734 average time 0.002508026974199736 iter num 620\n",
            "loss 148.20004272460938 average time 0.002505832854693324 iter num 640\n",
            "loss 90.25920867919922 average time 0.002505553674247742 iter num 660\n",
            "loss 23.993345260620117 average time 0.0025042962161812873 iter num 680\n",
            "loss 3.607020378112793 average time 0.0025034136971472825 iter num 700\n",
            "loss 2171.678955078125 average time 0.002502332138893356 iter num 720\n",
            "loss 271.5183410644531 average time 0.002500522245949887 iter num 740\n",
            "loss 32.7577018737793 average time 0.0024984310144774507 iter num 760\n",
            "loss 510.3048400878906 average time 0.0024970070448760524 iter num 780\n",
            "loss 43.7763671875 average time 0.002502391760004059 iter num 800\n",
            "loss 1.3052091598510742 average time 0.0025025166963454156 iter num 820\n",
            "loss 615.3035888671875 average time 0.002501072584528261 iter num 840\n",
            "loss 10.578153610229492 average time 0.0025004040069810545 iter num 860\n",
            "loss 8.023577690124512 average time 0.002499556513640238 iter num 880\n",
            "loss 385.0238342285156 average time 0.002498582402226172 iter num 900\n",
            "loss 5.299943923950195 average time 0.002499019217394937 iter num 920\n",
            "loss 0.019417142495512962 average time 0.00249739677979099 iter num 940\n",
            "loss 285.9809265136719 average time 0.002502780476044819 iter num 960\n",
            "loss 314.9915466308594 average time 0.0025006291224529804 iter num 980\n",
            "loss 125.47240447998047 average time 0.0025018056310041176 iter num 1000\n",
            "loss 449.7409973144531 average time 0.003069398249965616 iter num 20\n",
            "loss 1428.5692138671875 average time 0.002866048649974573 iter num 40\n",
            "loss 8.017760276794434 average time 0.002755498049987182 iter num 60\n",
            "loss 836.7586669921875 average time 0.0026713893999897207 iter num 80\n",
            "loss 60.83088302612305 average time 0.0026280098799907137 iter num 100\n",
            "loss 10.321195602416992 average time 0.0026114566833257413 iter num 120\n",
            "loss 10.445296287536621 average time 0.0025899293357075034 iter num 140\n",
            "loss 6.892247200012207 average time 0.0025705668374968126 iter num 160\n",
            "loss 1077.46044921875 average time 0.002563040311108327 iter num 180\n",
            "loss 27.195232391357422 average time 0.002558251219996919 iter num 200\n",
            "loss 55.78467559814453 average time 0.0025512790272686782 iter num 220\n",
            "loss 0.46038708090782166 average time 0.002539811037498415 iter num 240\n",
            "loss 58.0984001159668 average time 0.0025400633346145274 iter num 260\n",
            "loss 361.65667724609375 average time 0.002532992592856382 iter num 280\n",
            "loss 259.9462585449219 average time 0.0025291711599970765 iter num 300\n",
            "loss 29.43446159362793 average time 0.002536987581248695 iter num 320\n",
            "loss 89.49536895751953 average time 0.0025352991588199394 iter num 340\n",
            "loss 0.7321139574050903 average time 0.0025284181388858288 iter num 360\n",
            "loss 0.47052907943725586 average time 0.0025240677421020574 iter num 380\n",
            "loss 0.3900291919708252 average time 0.0025177554099985854 iter num 400\n",
            "loss 11.649572372436523 average time 0.0025145632499997883 iter num 420\n",
            "loss 117.75516510009766 average time 0.002511501775001079 iter num 440\n",
            "loss 0.8046507239341736 average time 0.0025084075847837044 iter num 460\n",
            "loss 17.197160720825195 average time 0.002503857183334901 iter num 480\n",
            "loss 14.966851234436035 average time 0.002504396166000788 iter num 500\n",
            "loss 7.460228443145752 average time 0.0025054361865399573 iter num 520\n",
            "loss 42.208553314208984 average time 0.002504572512963966 iter num 540\n",
            "loss 36.512447357177734 average time 0.0025082256267873098 iter num 560\n",
            "loss 221.70436096191406 average time 0.002511788505174097 iter num 580\n",
            "loss 203.68092346191406 average time 0.0025109910816695446 iter num 600\n",
            "loss 77.05388641357422 average time 0.0025105122193581925 iter num 620\n",
            "loss 302.3446960449219 average time 0.0025062930718785736 iter num 640\n",
            "loss 734.3197631835938 average time 0.002501449919700325 iter num 660\n",
            "loss 16.8516788482666 average time 0.0024941882352971546 iter num 680\n",
            "loss 292.22955322265625 average time 0.002487847905716275 iter num 700\n",
            "loss 1.2205156087875366 average time 0.0024803842569462455 iter num 720\n",
            "loss 184.75804138183594 average time 0.0024799362959485637 iter num 740\n",
            "loss 67.52571868896484 average time 0.0024732120184245633 iter num 760\n",
            "loss 107.29625701904297 average time 0.0024715819076956717 iter num 780\n",
            "loss 67.43365478515625 average time 0.002478837952504023 iter num 800\n",
            "loss 147.54356384277344 average time 0.002480881950004193 iter num 820\n",
            "loss 73.02149200439453 average time 0.0024761131690516844 iter num 840\n",
            "loss 425.08856201171875 average time 0.0024777816511671116 iter num 860\n",
            "loss 23.50879669189453 average time 0.002481041878413686 iter num 880\n",
            "loss 309.1653747558594 average time 0.002480250498892676 iter num 900\n",
            "loss 17.489788055419922 average time 0.0024788494228296853 iter num 920\n",
            "loss 1.0051946640014648 average time 0.002473022969152129 iter num 940\n",
            "loss 594.77978515625 average time 0.0024707297125026173 iter num 960\n",
            "loss 52.79446029663086 average time 0.0024714114163284 iter num 980\n",
            "loss 5.2179646492004395 average time 0.0024691950410017397 iter num 1000\n",
            "loss 1183.8719482421875 average time 0.0031705351500022514 iter num 20\n",
            "loss 328.6360778808594 average time 0.0027271779000045625 iter num 40\n",
            "loss 27.64796257019043 average time 0.002564719316671926 iter num 60\n",
            "loss 1401.6278076171875 average time 0.002481387862499673 iter num 80\n",
            "loss 23.922168731689453 average time 0.0024872480199951497 iter num 100\n",
            "loss 29.32876205444336 average time 0.0024379329583325672 iter num 120\n",
            "loss 167.47120666503906 average time 0.002416674764286394 iter num 140\n",
            "loss 24.885042190551758 average time 0.002389681818753786 iter num 160\n",
            "loss 0.0003331947955302894 average time 0.002376903116669382 iter num 180\n",
            "loss 95.81066131591797 average time 0.0023764268150023326 iter num 200\n",
            "loss 0.0035253113601356745 average time 0.00238683266364164 iter num 220\n",
            "loss 515.4048461914062 average time 0.0023758686958406847 iter num 240\n",
            "loss 570.602294921875 average time 0.0023650847230818235 iter num 260\n",
            "loss 56.337581634521484 average time 0.002359312789292289 iter num 280\n",
            "loss 36.25588607788086 average time 0.002378000530006072 iter num 300\n",
            "loss 179.17636108398438 average time 0.0023716578406300926 iter num 320\n",
            "loss 0.9500875473022461 average time 0.0023711741088304197 iter num 340\n",
            "loss 23.603071212768555 average time 0.0023646031138961614 iter num 360\n",
            "loss 872.38427734375 average time 0.002369188913165712 iter num 380\n",
            "loss 51.71696090698242 average time 0.0023586330825065716 iter num 400\n",
            "loss 0.001642465591430664 average time 0.0023560490452452887 iter num 420\n",
            "loss 157.0827178955078 average time 0.002349360631825448 iter num 440\n",
            "loss 5.9799017906188965 average time 0.0023565454456601124 iter num 460\n",
            "loss 215.77975463867188 average time 0.002350769679172989 iter num 480\n",
            "loss 73.52376556396484 average time 0.002344049814005757 iter num 500\n",
            "loss 23.460468292236328 average time 0.00233842075577487 iter num 520\n",
            "loss 1987.467041015625 average time 0.002336331312967054 iter num 540\n",
            "loss 5.057254791259766 average time 0.0023323575910735275 iter num 560\n",
            "loss 3.598038911819458 average time 0.0023334266482783247 iter num 580\n",
            "loss 27.157407760620117 average time 0.0023382257550031227 iter num 600\n",
            "loss 32.488487243652344 average time 0.0023382597354868503 iter num 620\n",
            "loss 1.7481306791305542 average time 0.002342015020315813 iter num 640\n",
            "loss 134.7887725830078 average time 0.0023518103045479287 iter num 660\n",
            "loss 490.3609924316406 average time 0.00235680064412169 iter num 680\n",
            "loss 2.6276001930236816 average time 0.00236104644571794 iter num 700\n",
            "loss 29.760873794555664 average time 0.002356460102781297 iter num 720\n",
            "loss 1.5615777969360352 average time 0.0023614808148691565 iter num 740\n",
            "loss 183.4320526123047 average time 0.00236532662368811 iter num 760\n",
            "loss 416.3674011230469 average time 0.0023659972692348093 iter num 780\n",
            "loss 366.7941589355469 average time 0.002361840060003999 iter num 800\n",
            "loss 16.24017906188965 average time 0.0023590923865889992 iter num 820\n",
            "loss 317.741943359375 average time 0.0023594465702417656 iter num 840\n",
            "loss 2.0617854595184326 average time 0.002359676479073208 iter num 860\n",
            "loss 13.803011894226074 average time 0.002358122412503731 iter num 880\n",
            "loss 13.85805606842041 average time 0.002355427775559191 iter num 900\n",
            "loss 354.2544250488281 average time 0.002352958827176953 iter num 920\n",
            "loss 37.82908630371094 average time 0.002351743391492367 iter num 940\n",
            "loss 79.02021789550781 average time 0.0023489254489613624 iter num 960\n",
            "loss 2.6581618785858154 average time 0.0023512309234721513 iter num 980\n",
            "loss 0.012873510830104351 average time 0.0023498264950023895 iter num 1000\n",
            "loss 643.2518920898438 average time 0.003093581850009741 iter num 20\n",
            "loss 185.5310821533203 average time 0.002663629325013517 iter num 40\n",
            "loss 151.90554809570312 average time 0.0026133984333379582 iter num 60\n",
            "loss 545.0218505859375 average time 0.0025290856375050907 iter num 80\n",
            "loss 26.56473159790039 average time 0.0025561799800038897 iter num 100\n",
            "loss 350.5206298828125 average time 0.002500689633334711 iter num 120\n",
            "loss 336.9114685058594 average time 0.0024635890857163238 iter num 140\n",
            "loss 11.741205215454102 average time 0.002436112237502641 iter num 160\n",
            "loss 363.9504089355469 average time 0.002427810300004138 iter num 180\n",
            "loss 165.69766235351562 average time 0.0024096691750037282 iter num 200\n",
            "loss 3.548971176147461 average time 0.002408450509095804 iter num 220\n",
            "loss 205.4008331298828 average time 0.0023967451416713934 iter num 240\n",
            "loss 140.80592346191406 average time 0.0023968419807733183 iter num 260\n",
            "loss 613.4329223632812 average time 0.0023903416178630518 iter num 280\n",
            "loss 583.2752685546875 average time 0.0023953505100075745 iter num 300\n",
            "loss 0.24123947322368622 average time 0.0023838979937558236 iter num 320\n",
            "loss 27.75640296936035 average time 0.0023880901382413227 iter num 340\n",
            "loss 34.39958572387695 average time 0.0023967833250051426 iter num 360\n",
            "loss 243.6575927734375 average time 0.0023871430473736597 iter num 380\n",
            "loss 2.364816665649414 average time 0.0023805211975042083 iter num 400\n",
            "loss 47.49795913696289 average time 0.0023799124500047107 iter num 420\n",
            "loss 18.232187271118164 average time 0.0023705399977307644 iter num 440\n",
            "loss 668.3040771484375 average time 0.0023666335543517246 iter num 460\n",
            "loss 1.2321902513504028 average time 0.002361751287502993 iter num 480\n",
            "loss 50.53236389160156 average time 0.0023636412200039557 iter num 500\n",
            "loss 5.0795087814331055 average time 0.0023641014326966133 iter num 520\n",
            "loss 466.51611328125 average time 0.002370027140743871 iter num 540\n",
            "loss 10.400086402893066 average time 0.00236566603214458 iter num 560\n",
            "loss 2160.486572265625 average time 0.0023719473827609676 iter num 580\n",
            "loss 503.0771179199219 average time 0.0023749246833347113 iter num 600\n",
            "loss 267.72003173828125 average time 0.0023711587580655465 iter num 620\n",
            "loss 472.9974670410156 average time 0.002366222887500058 iter num 640\n",
            "loss 3.9162847995758057 average time 0.0023670613166666154 iter num 660\n",
            "loss 3.572559118270874 average time 0.00236412930882399 iter num 680\n",
            "loss 96.32768249511719 average time 0.0023626986242863625 iter num 700\n",
            "loss 5.093278408050537 average time 0.002358695820834416 iter num 720\n",
            "loss 1264.04248046875 average time 0.0023616034716229554 iter num 740\n",
            "loss 96.67675018310547 average time 0.002357762889475249 iter num 760\n",
            "loss 28.502056121826172 average time 0.0023572161205146214 iter num 780\n",
            "loss 305.8108215332031 average time 0.002354836778752656 iter num 800\n",
            "loss 9.416982650756836 average time 0.0023552413231727326 iter num 820\n",
            "loss 26.42767333984375 average time 0.002351804492859682 iter num 840\n",
            "loss 33.571983337402344 average time 0.002351004866281348 iter num 860\n",
            "loss 132.6398468017578 average time 0.002348244817047823 iter num 880\n",
            "loss 297.3500061035156 average time 0.002348732806668876 iter num 900\n",
            "loss 200.7747344970703 average time 0.0023456082467414093 iter num 920\n",
            "loss 0.007071488071233034 average time 0.0023505230478750934 iter num 940\n",
            "loss 66.6410140991211 average time 0.0023502855875030095 iter num 960\n",
            "loss 10.023515701293945 average time 0.0023474487938805006 iter num 980\n",
            "loss 205.5276336669922 average time 0.0023447761460024595 iter num 1000\n",
            "loss 188.60791015625 average time 0.0027805088499803787 iter num 20\n",
            "loss 272.5078125 average time 0.002522040649984092 iter num 40\n",
            "loss 262.1018371582031 average time 0.0024836907833181007 iter num 60\n",
            "loss 1065.8472900390625 average time 0.002448074224986385 iter num 80\n",
            "loss 174.1305389404297 average time 0.0024289834799787967 iter num 100\n",
            "loss 232.04434204101562 average time 0.0024240102166572797 iter num 120\n",
            "loss 1742.2525634765625 average time 0.0024234721285649485 iter num 140\n",
            "loss 26.53659439086914 average time 0.0024281367249969323 iter num 160\n",
            "loss 4.528532028198242 average time 0.0024257455666656822 iter num 180\n",
            "loss 293.29083251953125 average time 0.002404523819998303 iter num 200\n",
            "loss 5.532344818115234 average time 0.002398165295453096 iter num 220\n",
            "loss 2.7545042037963867 average time 0.0024070530708343086 iter num 240\n",
            "loss 726.294921875 average time 0.002423194765384988 iter num 260\n",
            "loss 5.197207450866699 average time 0.002411400707144042 iter num 280\n",
            "loss 2.5632431507110596 average time 0.0024204822300028656 iter num 300\n",
            "loss 23.4986515045166 average time 0.0024106891437519094 iter num 320\n",
            "loss 548.9495239257812 average time 0.0023973902058829107 iter num 340\n",
            "loss 9.215193748474121 average time 0.0024029500916684455 iter num 360\n",
            "loss 153.02867126464844 average time 0.002400621171052894 iter num 380\n",
            "loss 6.393235683441162 average time 0.0024001140599978045 iter num 400\n",
            "loss 109.60850524902344 average time 0.0023972562857114185 iter num 420\n",
            "loss 80.0575180053711 average time 0.0023891995840890764 iter num 440\n",
            "loss 6.61091947555542 average time 0.0023945924326070505 iter num 460\n",
            "loss 83.22177124023438 average time 0.002397961274997821 iter num 480\n",
            "loss 158.4112548828125 average time 0.0023913613359977715 iter num 500\n",
            "loss 0.042019493877887726 average time 0.0023847490749994028 iter num 520\n",
            "loss 1437.348876953125 average time 0.002387720444443807 iter num 540\n",
            "loss 710.1417846679688 average time 0.0023889784357144046 iter num 560\n",
            "loss 192.2017822265625 average time 0.002383732289654447 iter num 580\n",
            "loss 83.02352905273438 average time 0.0023812945599994842 iter num 600\n",
            "loss 75.59320068359375 average time 0.0023819377161291206 iter num 620\n",
            "loss 283.6736145019531 average time 0.0023773645281249143 iter num 640\n",
            "loss 5.383691787719727 average time 0.002386328266666651 iter num 660\n",
            "loss 122.56216430664062 average time 0.002384239682353783 iter num 680\n",
            "loss 0.7481703758239746 average time 0.002389498365715943 iter num 700\n",
            "loss 1.6117597818374634 average time 0.0023858739500026457 iter num 720\n",
            "loss 68.0617446899414 average time 0.0023830824905432466 iter num 740\n",
            "loss 4.3815202713012695 average time 0.0023794300644756038 iter num 760\n",
            "loss 201.8525848388672 average time 0.0023861698282061274 iter num 780\n",
            "loss 4.6754913330078125 average time 0.0023882147462518334 iter num 800\n",
            "loss 91.38319396972656 average time 0.002390868997562803 iter num 820\n",
            "loss 5.874868869781494 average time 0.00239604988452535 iter num 840\n",
            "loss 8.224387168884277 average time 0.002398669436047385 iter num 860\n",
            "loss 5.246209621429443 average time 0.0024033408261374236 iter num 880\n",
            "loss 81.01359558105469 average time 0.0024044859844454753 iter num 900\n",
            "loss 4.411767959594727 average time 0.002408058145653305 iter num 920\n",
            "loss 5.884361267089844 average time 0.002413372289363161 iter num 940\n",
            "loss 0.2546154260635376 average time 0.0024127418010432204 iter num 960\n",
            "loss 80.5378646850586 average time 0.002413691419389709 iter num 980\n",
            "loss 0.01236678659915924 average time 0.002417094316001794 iter num 1000\n",
            "loss 398.9386901855469 average time 0.003145156099992619 iter num 20\n",
            "loss 48.61378479003906 average time 0.002793316775000676 iter num 40\n",
            "loss 16.81699562072754 average time 0.00267557869999185 iter num 60\n",
            "loss 204.93153381347656 average time 0.0026490879749928808 iter num 80\n",
            "loss 55.07550811767578 average time 0.002618095829993763 iter num 100\n",
            "loss 283.9156494140625 average time 0.0025952781833268545 iter num 120\n",
            "loss 19.653642654418945 average time 0.0025855806142820775 iter num 140\n",
            "loss 1360.23095703125 average time 0.0025719017249983267 iter num 160\n",
            "loss 6.798251152038574 average time 0.002571219555557036 iter num 180\n",
            "loss 72.53536224365234 average time 0.0025593170299998747 iter num 200\n",
            "loss 34.215484619140625 average time 0.002551166240913393 iter num 220\n",
            "loss 154.6788787841797 average time 0.0025313784583403274 iter num 240\n",
            "loss 76.62639617919922 average time 0.002541950696159417 iter num 260\n",
            "loss 1177.9530029296875 average time 0.002538731267860125 iter num 280\n",
            "loss 0.009160548448562622 average time 0.0025333842266703262 iter num 300\n",
            "loss 5.487264633178711 average time 0.00253146200937735 iter num 320\n",
            "loss 1.2818492650985718 average time 0.002524074829414878 iter num 340\n",
            "loss 100.8602066040039 average time 0.0025246744833345423 iter num 360\n",
            "loss 1.6160516738891602 average time 0.002519289999999704 iter num 380\n",
            "loss 202.5568389892578 average time 0.0025131754625010673 iter num 400\n",
            "loss 8.004348754882812 average time 0.0025064408095252996 iter num 420\n",
            "loss 35.9970703125 average time 0.002507611270454969 iter num 440\n",
            "loss 108.67996978759766 average time 0.002512003947829864 iter num 460\n",
            "loss 2.5438015460968018 average time 0.0025085715833374897 iter num 480\n",
            "loss 82.59081268310547 average time 0.002507067460003327 iter num 500\n",
            "loss 82.88490295410156 average time 0.00250674192500399 iter num 520\n",
            "loss 35.275020599365234 average time 0.002515452018521787 iter num 540\n",
            "loss 38.32685089111328 average time 0.0025132688339311927 iter num 560\n",
            "loss 511.1573486328125 average time 0.0025123046258633216 iter num 580\n",
            "loss 3.1180226802825928 average time 0.002510464705001899 iter num 600\n",
            "loss 375.121337890625 average time 0.0025126890774202133 iter num 620\n",
            "loss 1.0564229488372803 average time 0.0025114007359384517 iter num 640\n",
            "loss 1.0370806455612183 average time 0.00250917198939388 iter num 660\n",
            "loss 7.605959892272949 average time 0.0025085968161764572 iter num 680\n",
            "loss 526.2703247070312 average time 0.002508128258571755 iter num 700\n",
            "loss 3.2185404300689697 average time 0.0025076131013877204 iter num 720\n",
            "loss 5.640432357788086 average time 0.002506020702701089 iter num 740\n",
            "loss 10.624139785766602 average time 0.002506674311840279 iter num 760\n",
            "loss 96.07797241210938 average time 0.0025086890076916183 iter num 780\n",
            "loss 18.227985382080078 average time 0.0025172409162490792 iter num 800\n",
            "loss 201.1483612060547 average time 0.0025173550207311076 iter num 820\n",
            "loss 88.75289916992188 average time 0.0025153543333328258 iter num 840\n",
            "loss 601.3798828125 average time 0.0025167665220935708 iter num 860\n",
            "loss 122.9151840209961 average time 0.0025149846102276675 iter num 880\n",
            "loss 164.32891845703125 average time 0.0025189012800006416 iter num 900\n",
            "loss 14.301313400268555 average time 0.002517030128261843 iter num 920\n",
            "loss 0.0021668393164873123 average time 0.0025153168191500395 iter num 940\n",
            "loss 299.6844787597656 average time 0.0025139163291676425 iter num 960\n",
            "loss 19.335649490356445 average time 0.0025126001285721775 iter num 980\n",
            "loss 1.3682405948638916 average time 0.002510223401000758 iter num 1000\n",
            "loss 1.534601092338562 average time 0.0031375552000554308 iter num 20\n",
            "loss 468.0290832519531 average time 0.0028163058250243013 iter num 40\n",
            "loss 502.78839111328125 average time 0.0027060684833334864 iter num 60\n",
            "loss 16.010347366333008 average time 0.0026929914125020105 iter num 80\n",
            "loss 0.1021125465631485 average time 0.0026720277200070084 iter num 100\n",
            "loss 66.91889190673828 average time 0.0026431170000118225 iter num 120\n",
            "loss 4.0949602127075195 average time 0.002626654885726306 iter num 140\n",
            "loss 694.235595703125 average time 0.0025994593187604665 iter num 160\n",
            "loss 183.89112854003906 average time 0.0025803114722268423 iter num 180\n",
            "loss 0.7537750601768494 average time 0.002571442495004703 iter num 200\n",
            "loss 1.4217950105667114 average time 0.002557313736368046 iter num 220\n",
            "loss 29.944120407104492 average time 0.0025536955208385354 iter num 240\n",
            "loss 1.6209369897842407 average time 0.002556713988465512 iter num 260\n",
            "loss 33.736480712890625 average time 0.002553848732146337 iter num 280\n",
            "loss 6.5369038581848145 average time 0.002546684513336004 iter num 300\n",
            "loss 124.98694610595703 average time 0.0025472287843761876 iter num 320\n",
            "loss 12.4047212600708 average time 0.002542542100000259 iter num 340\n",
            "loss 90.59584045410156 average time 0.0025368671194415686 iter num 360\n",
            "loss 74.38404846191406 average time 0.002525693652627955 iter num 380\n",
            "loss 1.5744891166687012 average time 0.0025409783274949406 iter num 400\n",
            "loss 388.3138427734375 average time 0.002534261795233847 iter num 420\n",
            "loss 2214.6240234375 average time 0.0025356548749951714 iter num 440\n",
            "loss 81.85478210449219 average time 0.0025291021326053023 iter num 460\n",
            "loss 0.11356764286756516 average time 0.0025274273083311983 iter num 480\n",
            "loss 8.069696426391602 average time 0.002524019571997087 iter num 500\n",
            "loss 80.78727722167969 average time 0.0025212510576890367 iter num 520\n",
            "loss 0.07490984350442886 average time 0.002517886574070063 iter num 540\n",
            "loss 12.600255012512207 average time 0.002518621828568257 iter num 560\n",
            "loss 0.6196816563606262 average time 0.0025284782396519058 iter num 580\n",
            "loss 11.522842407226562 average time 0.0025254899899978984 iter num 600\n",
            "loss 1.3819071054458618 average time 0.0025272840419347633 iter num 620\n",
            "loss 403.6395263671875 average time 0.002522763357812252 iter num 640\n",
            "loss 3.446884870529175 average time 0.0025217428424232848 iter num 660\n",
            "loss 0.05934334546327591 average time 0.002520852576469679 iter num 680\n",
            "loss 0.7137773633003235 average time 0.002519792177142465 iter num 700\n",
            "loss 1.6379992961883545 average time 0.0025197883041657735 iter num 720\n",
            "loss 2.9368832111358643 average time 0.0025194453797285936 iter num 740\n",
            "loss 0.9077699780464172 average time 0.0025175803144727046 iter num 760\n",
            "loss 24.380714416503906 average time 0.0025177233474346977 iter num 780\n",
            "loss 1.34666907787323 average time 0.002530864729998825 iter num 800\n",
            "loss 447.71282958984375 average time 0.0025230233902431526 iter num 820\n",
            "loss 1.1851333379745483 average time 0.002515096292855679 iter num 840\n",
            "loss 4.948355674743652 average time 0.0025094453476732946 iter num 860\n",
            "loss 7.715314865112305 average time 0.002509208944316945 iter num 880\n",
            "loss 19.799348831176758 average time 0.0025017786055546924 iter num 900\n",
            "loss 6.062570571899414 average time 0.0024972070782600104 iter num 920\n",
            "loss 0.7151921987533569 average time 0.0024921757670211533 iter num 940\n",
            "loss 16.12014389038086 average time 0.0025020342854167174 iter num 960\n",
            "loss 11.663992881774902 average time 0.002496593378571663 iter num 980\n",
            "loss 67.4312744140625 average time 0.002490886045000025 iter num 1000\n",
            "loss 25.378082275390625 average time 0.0028233507499749065 iter num 20\n",
            "loss 8.96236515045166 average time 0.0025786017249743052 iter num 40\n",
            "loss 14.165229797363281 average time 0.002462083766658907 iter num 60\n",
            "loss 0.3732626736164093 average time 0.002485002612496601 iter num 80\n",
            "loss 101.46928405761719 average time 0.002451798059992143 iter num 100\n",
            "loss 0.5914395451545715 average time 0.002434080366665133 iter num 120\n",
            "loss 23.48601531982422 average time 0.0024467095714287876 iter num 140\n",
            "loss 278.1651611328125 average time 0.002443873156248344 iter num 160\n",
            "loss 54.367897033691406 average time 0.0024198480333312644 iter num 180\n",
            "loss 25.734148025512695 average time 0.002437827474999494 iter num 200\n",
            "loss 22.482120513916016 average time 0.0024169091500001825 iter num 220\n",
            "loss 527.3045043945312 average time 0.0023983285458333134 iter num 240\n",
            "loss 3.8525240421295166 average time 0.002396000826922773 iter num 260\n",
            "loss 2.3025803565979004 average time 0.0024032686928567924 iter num 280\n",
            "loss 3.3560659885406494 average time 0.0023925861899995954 iter num 300\n",
            "loss 12.019209861755371 average time 0.00240186902499957 iter num 320\n",
            "loss 169.50462341308594 average time 0.0024040581235273265 iter num 340\n",
            "loss 149.8662109375 average time 0.0024082466972199907 iter num 360\n",
            "loss 44.884132385253906 average time 0.002413997268420668 iter num 380\n",
            "loss 1.199625015258789 average time 0.002421145435000085 iter num 400\n",
            "loss 0.9661179780960083 average time 0.0024262157190486546 iter num 420\n",
            "loss 85.134521484375 average time 0.0024322989477267047 iter num 440\n",
            "loss 0.5052351951599121 average time 0.002433184936954485 iter num 460\n",
            "loss 4.312269687652588 average time 0.0024353467312460946 iter num 480\n",
            "loss 138.72100830078125 average time 0.002440821085996504 iter num 500\n",
            "loss 795.5447387695312 average time 0.002441717603841198 iter num 520\n",
            "loss 11.216458320617676 average time 0.002445064414809992 iter num 540\n",
            "loss 280.3238220214844 average time 0.0024472539142814348 iter num 560\n",
            "loss 59.412559509277344 average time 0.0024502807086166506 iter num 580\n",
            "loss 5.353034496307373 average time 0.0024496132333289705 iter num 600\n",
            "loss 21.12778091430664 average time 0.0024497498822535317 iter num 620\n",
            "loss 142.37388610839844 average time 0.002452926296870217 iter num 640\n",
            "loss 51.22822570800781 average time 0.0024575178848432657 iter num 660\n",
            "loss 7.657461643218994 average time 0.00245676758970202 iter num 680\n",
            "loss 161.55804443359375 average time 0.0024565433385672414 iter num 700\n",
            "loss 86.94532012939453 average time 0.0024561468583287806 iter num 720\n",
            "loss 1.9383814334869385 average time 0.0024557540202657627 iter num 740\n",
            "loss 29.29082489013672 average time 0.0024569341868378182 iter num 760\n",
            "loss 0.452597051858902 average time 0.0024552046256379376 iter num 780\n",
            "loss 0.21168407797813416 average time 0.0024562322524974436 iter num 800\n",
            "loss 2.8618063926696777 average time 0.002455058713411656 iter num 820\n",
            "loss 19.461929321289062 average time 0.002463265941663838 iter num 840\n",
            "loss 111.56536102294922 average time 0.0024626197372069414 iter num 860\n",
            "loss 0.1429721713066101 average time 0.0024637181477239456 iter num 880\n",
            "loss 442.3500671386719 average time 0.00246275429777471 iter num 900\n",
            "loss 1.0827641487121582 average time 0.002463798394562327 iter num 920\n",
            "loss 1.1484286785125732 average time 0.0024644171734019226 iter num 940\n",
            "loss 1.784606695175171 average time 0.0024666843635396656 iter num 960\n",
            "loss 1.2071648836135864 average time 0.0024657608438747977 iter num 980\n",
            "loss 0.2943912446498871 average time 0.002466271588997415 iter num 1000\n",
            "loss 385.3574523925781 average time 0.0031359544999986612 iter num 20\n",
            "loss 6.088361740112305 average time 0.002802732899988314 iter num 40\n",
            "loss 204.14852905273438 average time 0.0026820981666598226 iter num 60\n",
            "loss 481.6121520996094 average time 0.0026292168250023453 iter num 80\n",
            "loss 37.051631927490234 average time 0.002647559309998542 iter num 100\n",
            "loss 8.436487197875977 average time 0.002622743808327262 iter num 120\n",
            "loss 91.2795181274414 average time 0.0025798257928581086 iter num 140\n",
            "loss 55.34861373901367 average time 0.0025768814124987214 iter num 160\n",
            "loss 0.05568162724375725 average time 0.0025533566166624647 iter num 180\n",
            "loss 229.5405731201172 average time 0.0025957559249968655 iter num 200\n",
            "loss 45.41127014160156 average time 0.0025716139272700194 iter num 220\n",
            "loss 24.91649055480957 average time 0.0025631941999951853 iter num 240\n",
            "loss 37.20355987548828 average time 0.002554775246148193 iter num 260\n",
            "loss 2.0269453525543213 average time 0.0025427567499938116 iter num 280\n",
            "loss 6.932203769683838 average time 0.0025399460566616957 iter num 300\n",
            "loss 22.701330184936523 average time 0.002542425881246402 iter num 320\n",
            "loss 0.3413442075252533 average time 0.002544810394113697 iter num 340\n",
            "loss 115.68949890136719 average time 0.002545767180552932 iter num 360\n",
            "loss 2.9874987602233887 average time 0.0025422263499978582 iter num 380\n",
            "loss 12.73307991027832 average time 0.0025353572849979855 iter num 400\n",
            "loss 9.056071281433105 average time 0.0025302093380939064 iter num 420\n",
            "loss 0.01838867738842964 average time 0.002522567204544595 iter num 440\n",
            "loss 2.215582847595215 average time 0.0025213682065214015 iter num 460\n",
            "loss 109.991943359375 average time 0.0025190065604168885 iter num 480\n",
            "loss 219.4056396484375 average time 0.00251764892600022 iter num 500\n",
            "loss 0.16445069015026093 average time 0.0025199485269231797 iter num 520\n",
            "loss 59.324031829833984 average time 0.0025174743129633094 iter num 540\n",
            "loss 37.23902893066406 average time 0.002522310307142967 iter num 560\n",
            "loss 41.43354797363281 average time 0.0025344935189662403 iter num 580\n",
            "loss 2.5632989406585693 average time 0.002529692353334288 iter num 600\n",
            "loss 53.401214599609375 average time 0.0025264586064508874 iter num 620\n",
            "loss 41.05753707885742 average time 0.0025251261562491576 iter num 640\n",
            "loss 39.25107955932617 average time 0.002521994571210681 iter num 660\n",
            "loss 8.393479347229004 average time 0.0025211658558803533 iter num 680\n",
            "loss 1.041751503944397 average time 0.0025236076471398 iter num 700\n",
            "loss 91.29234313964844 average time 0.0025217283944420563 iter num 720\n",
            "loss 1.6804559230804443 average time 0.0025209722040515133 iter num 740\n",
            "loss 137.37380981445312 average time 0.0025289928368396743 iter num 760\n",
            "loss 1.1451197862625122 average time 0.0025270128487156797 iter num 780\n",
            "loss 3.2749037742614746 average time 0.002535441102497771 iter num 800\n",
            "loss 4.4485297203063965 average time 0.002534623463412661 iter num 820\n",
            "loss 17.202484130859375 average time 0.0025318480833323607 iter num 840\n",
            "loss 189.04550170898438 average time 0.002531602484882463 iter num 860\n",
            "loss 1.7200731039047241 average time 0.0025311362568172318 iter num 880\n",
            "loss 218.6644744873047 average time 0.002536451461110624 iter num 900\n",
            "loss 1.0349994897842407 average time 0.0025360475967389725 iter num 920\n",
            "loss 432.8839111328125 average time 0.002538095323404832 iter num 940\n",
            "loss 76.31209564208984 average time 0.0025383129968749546 iter num 960\n",
            "loss 0.9774538278579712 average time 0.0025346847704095033 iter num 980\n",
            "loss 8.145295143127441 average time 0.0025344399640011943 iter num 1000\n",
            "loss 14.256569862365723 average time 0.00302560145003099 iter num 20\n",
            "loss 0.10439921170473099 average time 0.002751956825011348 iter num 40\n",
            "loss 27.43295669555664 average time 0.0026770278999985447 iter num 60\n",
            "loss 378.1406555175781 average time 0.002653865775002373 iter num 80\n",
            "loss 346.1861877441406 average time 0.002619056220003131 iter num 100\n",
            "loss 38.152618408203125 average time 0.0025921193166671703 iter num 120\n",
            "loss 145.4715118408203 average time 0.0025642466785728043 iter num 140\n",
            "loss 2.64192271232605 average time 0.0026032433749989536 iter num 160\n",
            "loss 0.6513708829879761 average time 0.002586211899999095 iter num 180\n",
            "loss 0.14434315264225006 average time 0.002575289474998499 iter num 200\n",
            "loss 33.190704345703125 average time 0.0025690528909091304 iter num 220\n",
            "loss 38.29563522338867 average time 0.002576667366667114 iter num 240\n",
            "loss 3.7807981967926025 average time 0.0025704794076913317 iter num 260\n",
            "loss 63.173221588134766 average time 0.0025617498785720726 iter num 280\n",
            "loss 105.67617797851562 average time 0.0025575297999989744 iter num 300\n",
            "loss 0.026430554687976837 average time 0.0025488508218742822 iter num 320\n",
            "loss 393.4948425292969 average time 0.002530635685293954 iter num 340\n",
            "loss 33.402740478515625 average time 0.0025255130972210637 iter num 360\n",
            "loss 68.57379150390625 average time 0.0025220531842089595 iter num 380\n",
            "loss 1.6995428800582886 average time 0.002520416697498149 iter num 400\n",
            "loss 131.80343627929688 average time 0.0025080734166641333 iter num 420\n",
            "loss 57.14643859863281 average time 0.0024951208795440296 iter num 440\n",
            "loss 544.7775268554688 average time 0.002490243413040824 iter num 460\n",
            "loss 255.67222595214844 average time 0.0024868071166636698 iter num 480\n",
            "loss 9.527719497680664 average time 0.0024798486159975253 iter num 500\n",
            "loss 78.96102142333984 average time 0.0024693865865366696 iter num 520\n",
            "loss 217.70071411132812 average time 0.0024639306425906734 iter num 540\n",
            "loss 12.234944343566895 average time 0.0024564009249985475 iter num 560\n",
            "loss 1.3105270862579346 average time 0.002463171736206371 iter num 580\n",
            "loss 12.987905502319336 average time 0.002456110354999434 iter num 600\n",
            "loss 347.7304382324219 average time 0.002451225383870884 iter num 620\n",
            "loss 2.5800414085388184 average time 0.0024432137437496947 iter num 640\n",
            "loss 1.7841098308563232 average time 0.002439266442424028 iter num 660\n",
            "loss 33.64844512939453 average time 0.0024335450352937723 iter num 680\n",
            "loss 0.003974058199673891 average time 0.002435150464284886 iter num 700\n",
            "loss 0.09095864742994308 average time 0.0024319900374993922 iter num 720\n",
            "loss 269.5044860839844 average time 0.002434400836485516 iter num 740\n",
            "loss 88.41210174560547 average time 0.002428786359209162 iter num 760\n",
            "loss 0.0022396682761609554 average time 0.0024260439102552784 iter num 780\n",
            "loss 2.313739538192749 average time 0.002437012432498875 iter num 800\n",
            "loss 0.5412806272506714 average time 0.002435529734145248 iter num 820\n",
            "loss 3.974684000015259 average time 0.0024305581523798364 iter num 840\n",
            "loss 4.687856674194336 average time 0.002427396656976059 iter num 860\n",
            "loss 47.706703186035156 average time 0.00242419156704469 iter num 880\n",
            "loss 5.887598514556885 average time 0.002423275468887975 iter num 900\n",
            "loss 78.88226318359375 average time 0.002425147522825702 iter num 920\n",
            "loss 11.972121238708496 average time 0.0024260713180841234 iter num 940\n",
            "loss 28.637527465820312 average time 0.0024228484229157723 iter num 960\n",
            "loss 13.049185752868652 average time 0.0024280766448972996 iter num 980\n",
            "loss 15.221261024475098 average time 0.002424026089000108 iter num 1000\n",
            "loss 3.29634428024292 average time 0.0030839204000244536 iter num 20\n",
            "loss 141.61610412597656 average time 0.002672630750021199 iter num 40\n",
            "loss 176.8979949951172 average time 0.002518908016683478 iter num 60\n",
            "loss 0.045491475611925125 average time 0.0024621523375031986 iter num 80\n",
            "loss 125.50077819824219 average time 0.0024807855800054314 iter num 100\n",
            "loss 8.758027076721191 average time 0.002449569391673852 iter num 120\n",
            "loss 718.4102783203125 average time 0.0024411200785769845 iter num 140\n",
            "loss 1.0005773305892944 average time 0.00244517040625567 iter num 160\n",
            "loss 3.099141836166382 average time 0.0024326031166727563 iter num 180\n",
            "loss 527.65283203125 average time 0.002409975525004029 iter num 200\n",
            "loss 62.03875732421875 average time 0.0023932782954592434 iter num 220\n",
            "loss 978.5478515625 average time 0.0023855801000062606 iter num 240\n",
            "loss 9.912910172715783e-07 average time 0.0023825362807735053 iter num 260\n",
            "loss 1.2464027404785156 average time 0.0023935499142891917 iter num 280\n",
            "loss 107.40495300292969 average time 0.0023845223533354933 iter num 300\n",
            "loss 348.38909912109375 average time 0.002387817412503779 iter num 320\n",
            "loss 391.9724426269531 average time 0.002397417223531808 iter num 340\n",
            "loss 3.0533807277679443 average time 0.002388913100002002 iter num 360\n",
            "loss 50.552974700927734 average time 0.0023796351421083274 iter num 380\n",
            "loss 3.153989791870117 average time 0.002375979305002147 iter num 400\n",
            "loss 0.0030386734288185835 average time 0.002370692497620331 iter num 420\n",
            "loss 141.48939514160156 average time 0.002367033302274732 iter num 440\n",
            "loss 1.5284974575042725 average time 0.002368360989132033 iter num 460\n",
            "loss 56.20760726928711 average time 0.0023641051145839734 iter num 480\n",
            "loss 65.44120788574219 average time 0.002370758722001483 iter num 500\n",
            "loss 37.34488296508789 average time 0.0023773820557708475 iter num 520\n",
            "loss 175.77767944335938 average time 0.0023780030481504954 iter num 540\n",
            "loss 142.56948852539062 average time 0.0023775138017880083 iter num 560\n",
            "loss 9.898731231689453 average time 0.0023766294413811576 iter num 580\n",
            "loss 177.6248779296875 average time 0.002370943345000948 iter num 600\n",
            "loss 28.647632598876953 average time 0.0023659297241949624 iter num 620\n",
            "loss 7.930768966674805 average time 0.0023613649953130532 iter num 640\n",
            "loss 238.93939208984375 average time 0.002359720254547469 iter num 660\n",
            "loss 152.17591857910156 average time 0.002368773879413766 iter num 680\n",
            "loss 0.23093156516551971 average time 0.0023639061428584033 iter num 700\n",
            "loss 118.1048812866211 average time 0.0023620859597248984 iter num 720\n",
            "loss 57.40196228027344 average time 0.0023605770932467627 iter num 740\n",
            "loss 357.36688232421875 average time 0.0023565198447398103 iter num 760\n",
            "loss 1.5286747217178345 average time 0.0023534361512842082 iter num 780\n",
            "loss 11.761784553527832 average time 0.0023500759600025845 iter num 800\n",
            "loss 155.4222412109375 average time 0.0023468975817098263 iter num 820\n",
            "loss 5.856261730194092 average time 0.0023530520309553334 iter num 840\n",
            "loss 20.533885955810547 average time 0.002349651958142796 iter num 860\n",
            "loss 24.503686904907227 average time 0.0023496130420485625 iter num 880\n",
            "loss 5.365304946899414 average time 0.002350022435558559 iter num 900\n",
            "loss 15.951940536499023 average time 0.002347817643481389 iter num 920\n",
            "loss 0.08818460255861282 average time 0.0023515232936202144 iter num 940\n",
            "loss 81.87770080566406 average time 0.0023486078635438427 iter num 960\n",
            "loss 7.49473237991333 average time 0.002350362285716649 iter num 980\n",
            "loss 9.780794143676758 average time 0.0023529339880026326 iter num 1000\n",
            "loss 16.949159622192383 average time 0.0028390030999958073 iter num 20\n",
            "loss 90.04891204833984 average time 0.002592811900012748 iter num 40\n",
            "loss 202.3849334716797 average time 0.002568297783329854 iter num 60\n",
            "loss 1.2551603317260742 average time 0.0025406910250012514 iter num 80\n",
            "loss 344.56500244140625 average time 0.0024947831800045607 iter num 100\n",
            "loss 199.50323486328125 average time 0.0024972563083338175 iter num 120\n",
            "loss 0.22581928968429565 average time 0.0024630553928562094 iter num 140\n",
            "loss 172.98153686523438 average time 0.002465455987497478 iter num 160\n",
            "loss 304.5460510253906 average time 0.0024392940777741186 iter num 180\n",
            "loss 6.814144611358643 average time 0.0024298716099997366 iter num 200\n",
            "loss 10.644956588745117 average time 0.0024112100454544818 iter num 220\n",
            "loss 92.77854919433594 average time 0.002429584024996719 iter num 240\n",
            "loss 17.875247955322266 average time 0.00242862602307416 iter num 260\n",
            "loss 0.6631401181221008 average time 0.0024410120999959874 iter num 280\n",
            "loss 753.0516967773438 average time 0.0024266185433293687 iter num 300\n",
            "loss 0.12363709509372711 average time 0.0024195553093683485 iter num 320\n",
            "loss 5.29171085357666 average time 0.0024236345117568656 iter num 340\n",
            "loss 4.671286582946777 average time 0.0024165423111027293 iter num 360\n",
            "loss 125.62252044677734 average time 0.002420374823675861 iter num 380\n",
            "loss 25.978477478027344 average time 0.002414623099991218 iter num 400\n",
            "loss 5.676050186157227 average time 0.0024236213952316837 iter num 420\n",
            "loss 18.26926612854004 average time 0.00242705632044869 iter num 440\n",
            "loss 12.734386444091797 average time 0.0024173102521687093 iter num 460\n",
            "loss 1053.84423828125 average time 0.0024153517416626376 iter num 480\n",
            "loss 0.07798543572425842 average time 0.0024127974959947097 iter num 500\n",
            "loss 13.962924003601074 average time 0.0024092259346109412 iter num 520\n",
            "loss 135.01364135742188 average time 0.0024023552499940065 iter num 540\n",
            "loss 0.039083223789930344 average time 0.002398988024994862 iter num 560\n",
            "loss 3.368985891342163 average time 0.0023952925568923532 iter num 580\n",
            "loss 23.32578468322754 average time 0.002404345066662472 iter num 600\n",
            "loss 0.165485680103302 average time 0.00239937005644716 iter num 620\n",
            "loss 1.793785572052002 average time 0.0023966086749958038 iter num 640\n",
            "loss 23.02014923095703 average time 0.002391009174237807 iter num 660\n",
            "loss 1.1077296733856201 average time 0.002401238924995355 iter num 680\n",
            "loss 2.06640625 average time 0.002401731861423936 iter num 700\n",
            "loss 9.066681861877441 average time 0.0023991332680512617 iter num 720\n",
            "loss 14.602424621582031 average time 0.002394679739184638 iter num 740\n",
            "loss 16.954341888427734 average time 0.0023937579131535058 iter num 760\n",
            "loss 20.043642044067383 average time 0.0023984994192262138 iter num 780\n",
            "loss 0.14093324542045593 average time 0.0023948183987465652 iter num 800\n",
            "loss 23.53070068359375 average time 0.002390678287801853 iter num 820\n",
            "loss 3.425926685333252 average time 0.0023962902559492793 iter num 840\n",
            "loss 2.9205753803253174 average time 0.002394428116275945 iter num 860\n",
            "loss 53.61378479003906 average time 0.0023920555295420215 iter num 880\n",
            "loss 3.171764373779297 average time 0.002389298933330287 iter num 900\n",
            "loss 125.82372283935547 average time 0.002388621397823173 iter num 920\n",
            "loss 88.21823120117188 average time 0.0023890280851037943 iter num 940\n",
            "loss 309.5976867675781 average time 0.002391895119788501 iter num 960\n",
            "loss 126.171630859375 average time 0.0023946824367315884 iter num 980\n",
            "loss 209.12246704101562 average time 0.0024017893549967083 iter num 1000\n",
            "loss 64.05580139160156 average time 0.0029520141000148213 iter num 20\n",
            "loss 10.398695945739746 average time 0.002570357975008619 iter num 40\n",
            "loss 0.36168619990348816 average time 0.002460984183346682 iter num 60\n",
            "loss 3.4353504180908203 average time 0.0024526776125100014 iter num 80\n",
            "loss 152.0601806640625 average time 0.0024083308700096496 iter num 100\n",
            "loss 143.59854125976562 average time 0.0023707043666737113 iter num 120\n",
            "loss 19.902847290039062 average time 0.002421434100006081 iter num 140\n",
            "loss 272.5386047363281 average time 0.0024058029312570284 iter num 160\n",
            "loss 987.6698608398438 average time 0.002385540772229433 iter num 180\n",
            "loss 2.4588682651519775 average time 0.002366784565006128 iter num 200\n",
            "loss 497.1679992675781 average time 0.002351400140914641 iter num 220\n",
            "loss 293.51220703125 average time 0.0023409987375051363 iter num 240\n",
            "loss 23.643564224243164 average time 0.0023302548192366255 iter num 260\n",
            "loss 890.4117431640625 average time 0.0023301247642889067 iter num 280\n",
            "loss 0.00541344378143549 average time 0.0023290459166681408 iter num 300\n",
            "loss 8.942140579223633 average time 0.002336157859375021 iter num 320\n",
            "loss 6.128012180328369 average time 0.0023414993382382763 iter num 340\n",
            "loss 0.00025893226847983897 average time 0.0023354058500027956 iter num 360\n",
            "loss 281.2307434082031 average time 0.0023483135052665125 iter num 380\n",
            "loss 195.76828002929688 average time 0.002354661132501974 iter num 400\n",
            "loss 544.1323852539062 average time 0.002358669157144093 iter num 420\n",
            "loss 73.57532501220703 average time 0.0023612433886378106 iter num 440\n",
            "loss 3.1049656867980957 average time 0.0023588625956533532 iter num 460\n",
            "loss 499.81304931640625 average time 0.0023632472916683394 iter num 480\n",
            "loss 478.9663391113281 average time 0.002379866604000199 iter num 500\n",
            "loss 174.71266174316406 average time 0.0023750408230770203 iter num 520\n",
            "loss 0.7905930876731873 average time 0.002370006325926018 iter num 540\n",
            "loss 11.080327033996582 average time 0.0023716845785721878 iter num 560\n",
            "loss 16.386491775512695 average time 0.0023751496396561505 iter num 580\n",
            "loss 7.745136737823486 average time 0.0023787937350008784 iter num 600\n",
            "loss 0.5701342821121216 average time 0.002381580035484471 iter num 620\n",
            "loss 81.2080307006836 average time 0.0023858958359387118 iter num 640\n",
            "loss 232.27171325683594 average time 0.002389566007576432 iter num 660\n",
            "loss 26.733299255371094 average time 0.002391711161764346 iter num 680\n",
            "loss 19.63970947265625 average time 0.0023963689157140574 iter num 700\n",
            "loss 110.69039916992188 average time 0.0024020782458327025 iter num 720\n",
            "loss 58.109100341796875 average time 0.002403810045945426 iter num 740\n",
            "loss 66.51837921142578 average time 0.002405148868420706 iter num 760\n",
            "loss 254.9215545654297 average time 0.0024069758897428237 iter num 780\n",
            "loss 101.83035278320312 average time 0.002416723648749155 iter num 800\n",
            "loss 10.204476356506348 average time 0.0024221020085370007 iter num 820\n",
            "loss 21.79021644592285 average time 0.0024233756654772217 iter num 840\n",
            "loss 3.8302102088928223 average time 0.0024231329232562997 iter num 860\n",
            "loss 121.92154693603516 average time 0.0024229344659100994 iter num 880\n",
            "loss 54.90329360961914 average time 0.002426627082222947 iter num 900\n",
            "loss 80.8542251586914 average time 0.002431228863044364 iter num 920\n",
            "loss 247.29945373535156 average time 0.00244093217446876 iter num 940\n",
            "loss 0.9706320762634277 average time 0.0024411947781255586 iter num 960\n",
            "loss 79.80528259277344 average time 0.002442067615306444 iter num 980\n",
            "loss 25.36209487915039 average time 0.002446457739000607 iter num 1000\n",
            "loss 0.008630051277577877 average time 0.0030564096500143023 iter num 20\n",
            "loss 13.849976539611816 average time 0.002758339824981704 iter num 40\n",
            "loss 116.87319946289062 average time 0.002681201899997632 iter num 60\n",
            "loss 0.0884481742978096 average time 0.002635326974998975 iter num 80\n",
            "loss 2.992868423461914 average time 0.0026160378400004445 iter num 100\n",
            "loss 0.435611754655838 average time 0.002573594091668004 iter num 120\n",
            "loss 47.29343032836914 average time 0.0025596564857112624 iter num 140\n",
            "loss 8.333585739135742 average time 0.002552420175000236 iter num 160\n",
            "loss 33.32332992553711 average time 0.002546195344446763 iter num 180\n",
            "loss 0.34427034854888916 average time 0.002553420965003852 iter num 200\n",
            "loss 0.6764219403266907 average time 0.0025504971590986645 iter num 220\n",
            "loss 160.34915161132812 average time 0.0025467927625082137 iter num 240\n",
            "loss 166.35748291015625 average time 0.0025398965346224802 iter num 260\n",
            "loss 2.244041681289673 average time 0.0025664899214348225 iter num 280\n",
            "loss 355.9791259765625 average time 0.0025601797133382813 iter num 300\n",
            "loss 3.8940482139587402 average time 0.0025530314312533164 iter num 320\n",
            "loss 9.890838623046875 average time 0.002546505864705416 iter num 340\n",
            "loss 6.752031326293945 average time 0.002562559869445143 iter num 360\n",
            "loss 39.9769287109375 average time 0.002560765652632606 iter num 380\n",
            "loss 9.185193061828613 average time 0.002554705250000211 iter num 400\n",
            "loss 578.600830078125 average time 0.002552944876191261 iter num 420\n",
            "loss 17.995508193969727 average time 0.0025483249636372627 iter num 440\n",
            "loss 24.519807815551758 average time 0.0025458960478253493 iter num 460\n",
            "loss 8.281206130981445 average time 0.002542650168750053 iter num 480\n",
            "loss 0.0026297385338693857 average time 0.0025399207219993516 iter num 500\n",
            "loss 7.802837371826172 average time 0.002541047707692839 iter num 520\n",
            "loss 7.107801914215088 average time 0.0025390755240731846 iter num 540\n",
            "loss 184.732177734375 average time 0.0025393949785711 iter num 560\n",
            "loss 21.98674964904785 average time 0.0025386073137940064 iter num 580\n",
            "loss 60.48161697387695 average time 0.002536821865000244 iter num 600\n",
            "loss 120.82499694824219 average time 0.0025387172387097727 iter num 620\n",
            "loss 21.140600204467773 average time 0.002540598599999555 iter num 640\n",
            "loss 4.637275218963623 average time 0.002537629445453838 iter num 660\n",
            "loss 1.0382153987884521 average time 0.0025351112852928614 iter num 680\n",
            "loss 81.98342895507812 average time 0.0025345752357134187 iter num 700\n",
            "loss 31.896379470825195 average time 0.002535852694443482 iter num 720\n",
            "loss 0.8731579780578613 average time 0.0025335984162146866 iter num 740\n",
            "loss 91.82820892333984 average time 0.002532196169735662 iter num 760\n",
            "loss 14.145907402038574 average time 0.00252971109230727 iter num 780\n",
            "loss 3.5848727226257324 average time 0.0025323947312494966 iter num 800\n",
            "loss 173.07608032226562 average time 0.002531065468291195 iter num 820\n",
            "loss 29.955644607543945 average time 0.002532328159522768 iter num 840\n",
            "loss 0.13963967561721802 average time 0.0025367536999989317 iter num 860\n",
            "loss 36.13076400756836 average time 0.002534384198862809 iter num 880\n",
            "loss 212.44476318359375 average time 0.0025344886944440584 iter num 900\n",
            "loss 0.5439107418060303 average time 0.0025314239347823254 iter num 920\n",
            "loss 39.00968551635742 average time 0.0025302034053187854 iter num 940\n",
            "loss 0.6434909105300903 average time 0.002529629802082193 iter num 960\n",
            "loss 2.842397451400757 average time 0.0025275392295906365 iter num 980\n",
            "loss 24.842092514038086 average time 0.0025257820939989413 iter num 1000\n",
            "loss 1.249813199043274 average time 0.003124842099998659 iter num 20\n",
            "loss 0.5235357880592346 average time 0.002808191024990947 iter num 40\n",
            "loss 250.98924255371094 average time 0.0027073081666571853 iter num 60\n",
            "loss 0.8852602243423462 average time 0.0026456574874913485 iter num 80\n",
            "loss 0.09749255329370499 average time 0.002605962769996495 iter num 100\n",
            "loss 164.1849822998047 average time 0.002573609466662674 iter num 120\n",
            "loss 1.2967474460601807 average time 0.00255531322142945 iter num 140\n",
            "loss 177.4990234375 average time 0.002558687950001115 iter num 160\n",
            "loss 0.03266065567731857 average time 0.002566209655559406 iter num 180\n",
            "loss 0.3933040499687195 average time 0.0025535980050074157 iter num 200\n",
            "loss 6.908290386199951 average time 0.0025500073954607284 iter num 220\n",
            "loss 103.5052261352539 average time 0.00255243164167022 iter num 240\n",
            "loss 19.37874984741211 average time 0.0025614187153870753 iter num 260\n",
            "loss 48.31126022338867 average time 0.0025535150214295753 iter num 280\n",
            "loss 129.62692260742188 average time 0.002552645789999133 iter num 300\n",
            "loss 34.039981842041016 average time 0.002543203971871577 iter num 320\n",
            "loss 12.751517295837402 average time 0.0025444077882331626 iter num 340\n",
            "loss 32.243770599365234 average time 0.0025428521277761876 iter num 360\n",
            "loss 32.78740310668945 average time 0.0025377825157869138 iter num 380\n",
            "loss 664.4934692382812 average time 0.0025386136974981355 iter num 400\n",
            "loss 44.867637634277344 average time 0.0025354093380932294 iter num 420\n",
            "loss 4.556540012359619 average time 0.002532559613634207 iter num 440\n",
            "loss 1.3029204607009888 average time 0.0025307186456495364 iter num 460\n",
            "loss 10.294276237487793 average time 0.0025272610729137075 iter num 480\n",
            "loss 28.856351852416992 average time 0.0025324906919959177 iter num 500\n",
            "loss 34.24408721923828 average time 0.0025280007365364116 iter num 520\n",
            "loss 2.969332218170166 average time 0.00253074515184995 iter num 540\n",
            "loss 245.0133056640625 average time 0.0025279074749996813 iter num 560\n",
            "loss 0.10999158024787903 average time 0.002527660284481656 iter num 580\n",
            "loss 6.642637729644775 average time 0.0025325439649986946 iter num 600\n",
            "loss 308.3923645019531 average time 0.0025315982758057076 iter num 620\n",
            "loss 78.9751968383789 average time 0.0025220949031243833 iter num 640\n",
            "loss 100.42571258544922 average time 0.002515272828788001 iter num 660\n",
            "loss 225.7715301513672 average time 0.002517610510295085 iter num 680\n",
            "loss 1.0619350671768188 average time 0.002509377558571292 iter num 700\n",
            "loss 97.46781158447266 average time 0.0025098128402776055 iter num 720\n",
            "loss 0.05081241577863693 average time 0.0025107678148647457 iter num 740\n",
            "loss 3.3414552211761475 average time 0.0025031591526319635 iter num 760\n",
            "loss 8.652454376220703 average time 0.0025016284128203856 iter num 780\n",
            "loss 0.6797642111778259 average time 0.0025011715037499016 iter num 800\n",
            "loss 2.4553616046905518 average time 0.002502267537805415 iter num 820\n",
            "loss 149.73150634765625 average time 0.002503392355953067 iter num 840\n",
            "loss 57.02040481567383 average time 0.002496084202326254 iter num 860\n",
            "loss 4.573493003845215 average time 0.0024924461806818954 iter num 880\n",
            "loss 175.83938598632812 average time 0.002489093072222709 iter num 900\n",
            "loss 0.10111859440803528 average time 0.0024835918282613656 iter num 920\n",
            "loss 23.110395431518555 average time 0.002487672606382893 iter num 940\n",
            "loss 0.034173306077718735 average time 0.002486778502083098 iter num 960\n",
            "loss 156.29005432128906 average time 0.002486995003060396 iter num 980\n",
            "loss 25.36785888671875 average time 0.0024817085199997564 iter num 1000\n",
            "loss 161.25852966308594 average time 0.0028457546000026923 iter num 20\n",
            "loss 722.4419555664062 average time 0.002594845850001093 iter num 40\n",
            "loss 57.89254379272461 average time 0.0025587179833299464 iter num 60\n",
            "loss 443.2031555175781 average time 0.0025529583375003993 iter num 80\n",
            "loss 200.48448181152344 average time 0.00248612673999105 iter num 100\n",
            "loss 149.9088897705078 average time 0.0024818026833258954 iter num 120\n",
            "loss 280.0697021484375 average time 0.0024540625999894213 iter num 140\n",
            "loss 182.79360961914062 average time 0.0024241770624882975 iter num 160\n",
            "loss 189.70066833496094 average time 0.0024041034999880946 iter num 180\n",
            "loss 0.020700599998235703 average time 0.0023972842949899586 iter num 200\n",
            "loss 53.223426818847656 average time 0.002393149604539634 iter num 220\n",
            "loss 800.8029174804688 average time 0.002382120062496066 iter num 240\n",
            "loss 1.4412014484405518 average time 0.0023768318615339013 iter num 260\n",
            "loss 331.8074951171875 average time 0.002378663914279449 iter num 280\n",
            "loss 0.12901584804058075 average time 0.002387977656661254 iter num 300\n",
            "loss 15.022425651550293 average time 0.0023814642406215112 iter num 320\n",
            "loss 287.7202453613281 average time 0.0023779429617601477 iter num 340\n",
            "loss 49.94056701660156 average time 0.002369208588882859 iter num 360\n",
            "loss 2.6444828510284424 average time 0.0023728531842051015 iter num 380\n",
            "loss 97.84921264648438 average time 0.0023722867124945425 iter num 400\n",
            "loss 83.64673614501953 average time 0.0023804201428528618 iter num 420\n",
            "loss 189.94151306152344 average time 0.0023723234340867418 iter num 440\n",
            "loss 44.803001403808594 average time 0.002369813313039824 iter num 460\n",
            "loss 21.741666793823242 average time 0.0023772876770806308 iter num 480\n",
            "loss 0.012696021236479282 average time 0.0023790450839969706 iter num 500\n",
            "loss 3.1316757202148438 average time 0.0023781112711499367 iter num 520\n",
            "loss 10.236824035644531 average time 0.0023722713370337564 iter num 540\n",
            "loss 0.17663325369358063 average time 0.002371628087496447 iter num 560\n",
            "loss 5.341058731079102 average time 0.0023775445758584 iter num 580\n",
            "loss 3.2152020931243896 average time 0.0023726834566635564 iter num 600\n",
            "loss 84.7350082397461 average time 0.002376874808060752 iter num 620\n",
            "loss 25.15113639831543 average time 0.0023767138046824955 iter num 640\n",
            "loss 1.7924631834030151 average time 0.0023735022833279522 iter num 660\n",
            "loss 42.68404006958008 average time 0.002377937802935669 iter num 680\n",
            "loss 1.0337339639663696 average time 0.0023750343814237696 iter num 700\n",
            "loss 28.706382751464844 average time 0.002373980477773014 iter num 720\n",
            "loss 37.42021560668945 average time 0.0023731732567527516 iter num 740\n",
            "loss 0.1365966647863388 average time 0.002369832305259036 iter num 760\n",
            "loss 0.007322228513658047 average time 0.002365849506406903 iter num 780\n",
            "loss 92.07099914550781 average time 0.0023663610437463945 iter num 800\n",
            "loss 8.891518592834473 average time 0.0023635779853617995 iter num 820\n",
            "loss 0.13446994125843048 average time 0.0023631529452348417 iter num 840\n",
            "loss 75.73017883300781 average time 0.002361792737205506 iter num 860\n",
            "loss 0.013325157575309277 average time 0.00235973756590459 iter num 880\n",
            "loss 9.930826187133789 average time 0.002357130129994908 iter num 900\n",
            "loss 0.0816822424530983 average time 0.00235544258042983 iter num 920\n",
            "loss 0.0973593071103096 average time 0.0023540905265910642 iter num 940\n",
            "loss 36.685482025146484 average time 0.002351033096870244 iter num 960\n",
            "loss 0.007195044308900833 average time 0.0023488568591787675 iter num 980\n",
            "loss 61.87343215942383 average time 0.0023470915699947453 iter num 1000\n",
            "loss 50.517616271972656 average time 0.0029402261000200268 iter num 20\n",
            "loss 4.78642463684082 average time 0.0026136592000057133 iter num 40\n",
            "loss 15.47402286529541 average time 0.002543732866680178 iter num 60\n",
            "loss 4.453841209411621 average time 0.002478324700007306 iter num 80\n",
            "loss 0.00043334424844942987 average time 0.0024700995600073837 iter num 100\n",
            "loss 0.8326882719993591 average time 0.002434867150009268 iter num 120\n",
            "loss 27.90801239013672 average time 0.0024067011928683053 iter num 140\n",
            "loss 75.47086334228516 average time 0.0023850788437584924 iter num 160\n",
            "loss 217.1667938232422 average time 0.0023752686722307063 iter num 180\n",
            "loss 0.9818876385688782 average time 0.0023640881650067056 iter num 200\n",
            "loss 2.951204299926758 average time 0.002350699650004241 iter num 220\n",
            "loss 1.3890273571014404 average time 0.002370917112502487 iter num 240\n",
            "loss 10.357994079589844 average time 0.0023869488500036615 iter num 260\n",
            "loss 0.23754338920116425 average time 0.002377607728575575 iter num 280\n",
            "loss 29.206588745117188 average time 0.00237119435333625 iter num 300\n",
            "loss 105.0559310913086 average time 0.002365131440629398 iter num 320\n",
            "loss 6.787592887878418 average time 0.0023598864470627365 iter num 340\n",
            "loss 30.99358558654785 average time 0.002366635697225345 iter num 360\n",
            "loss 816.271484375 average time 0.002366808815790328 iter num 380\n",
            "loss 128.7984619140625 average time 0.0023594048950019444 iter num 400\n",
            "loss 6.744172096252441 average time 0.002388286297621432 iter num 420\n",
            "loss 255.03656005859375 average time 0.002392369447728077 iter num 440\n",
            "loss 44.90119171142578 average time 0.002387474239130528 iter num 460\n",
            "loss 19.307815551757812 average time 0.0023859274666657862 iter num 480\n",
            "loss 5.64540958404541 average time 0.0023787602279990096 iter num 500\n",
            "loss 0.11863849312067032 average time 0.0023725226788450563 iter num 520\n",
            "loss 114.40260314941406 average time 0.0023715950111089248 iter num 540\n",
            "loss 0.5043823719024658 average time 0.002370480176783677 iter num 560\n",
            "loss 395.6302795410156 average time 0.002368929234480361 iter num 580\n",
            "loss 9.201385498046875 average time 0.0023688818666634387 iter num 600\n",
            "loss 101.2640380859375 average time 0.002373804317738676 iter num 620\n",
            "loss 13.271293640136719 average time 0.00236895348124726 iter num 640\n",
            "loss 35.64592361450195 average time 0.0023640757787850844 iter num 660\n",
            "loss 29.73505401611328 average time 0.0023668223073494065 iter num 680\n",
            "loss 57.13395309448242 average time 0.002364029571426077 iter num 700\n",
            "loss 118.22579956054688 average time 0.00236409754860871 iter num 720\n",
            "loss 39.98213577270508 average time 0.0023601649716191468 iter num 740\n",
            "loss 75.5756607055664 average time 0.0023568935605241172 iter num 760\n",
            "loss 0.054462872445583344 average time 0.002363935184613498 iter num 780\n",
            "loss 267.5727233886719 average time 0.0023627793037482546 iter num 800\n",
            "loss 3.1207058429718018 average time 0.0023599622280465974 iter num 820\n",
            "loss 0.689283013343811 average time 0.0023590274226161713 iter num 840\n",
            "loss 13.024035453796387 average time 0.002356823003484869 iter num 860\n",
            "loss 62.806640625 average time 0.0023544163920415817 iter num 880\n",
            "loss 9.59117317199707 average time 0.00235611753777347 iter num 900\n",
            "loss 59.00396728515625 average time 0.002355154109778823 iter num 920\n",
            "loss 71.78752136230469 average time 0.002353425408506503 iter num 940\n",
            "loss 9.928277969360352 average time 0.002358833873953851 iter num 960\n",
            "loss 1.8959053754806519 average time 0.0023577428795872445 iter num 980\n",
            "loss 0.2917681336402893 average time 0.002355418604995748 iter num 1000\n",
            "loss 15.542562484741211 average time 0.0028394823999974505 iter num 20\n",
            "loss 51.776695251464844 average time 0.002711523699991858 iter num 40\n",
            "loss 15.77195930480957 average time 0.002581335900003978 iter num 60\n",
            "loss 445.416015625 average time 0.0024854407124990986 iter num 80\n",
            "loss 32.13313674926758 average time 0.002462678059994232 iter num 100\n",
            "loss 3.7537288665771484 average time 0.002442637183323389 iter num 120\n",
            "loss 351.3359375 average time 0.0024791028071328063 iter num 140\n",
            "loss 7.6301984786987305 average time 0.0024521472749981397 iter num 160\n",
            "loss 1.0312186479568481 average time 0.0024217564166646196 iter num 180\n",
            "loss 122.3532943725586 average time 0.0023977828549971037 iter num 200\n",
            "loss 2.9526331424713135 average time 0.0024079950999996735 iter num 220\n",
            "loss 127.35664367675781 average time 0.0023944346291661605 iter num 240\n",
            "loss 1.2092278003692627 average time 0.002391122765383303 iter num 260\n",
            "loss 0.0020503196865320206 average time 0.0023831131857126107 iter num 280\n",
            "loss 8.87396240234375 average time 0.002377851503331385 iter num 300\n",
            "loss 24.098997116088867 average time 0.0023908672843745648 iter num 320\n",
            "loss 0.029535770416259766 average time 0.002381112661761536 iter num 340\n",
            "loss 0.12254967540502548 average time 0.00237140972777436 iter num 360\n",
            "loss 41.66168212890625 average time 0.002369568776311768 iter num 380\n",
            "loss 14.63141918182373 average time 0.002366524487496804 iter num 400\n",
            "loss 58.36772537231445 average time 0.0023595224738073737 iter num 420\n",
            "loss 0.3142812252044678 average time 0.00235551373635975 iter num 440\n",
            "loss 79.43086242675781 average time 0.002356009719561746 iter num 460\n",
            "loss 14.201350212097168 average time 0.0023513458854120964 iter num 480\n",
            "loss 0.9175158739089966 average time 0.0023542725239954054 iter num 500\n",
            "loss 9.631937980651855 average time 0.002360523071149163 iter num 520\n",
            "loss 3.307277202606201 average time 0.0023565237777718184 iter num 540\n",
            "loss 2.638183116912842 average time 0.002357249487493261 iter num 560\n",
            "loss 0.2541753053665161 average time 0.002352195815511738 iter num 580\n",
            "loss 0.1578296720981598 average time 0.002351419806660715 iter num 600\n",
            "loss 43.06620788574219 average time 0.0023504232564468728 iter num 620\n",
            "loss 0.8902155160903931 average time 0.0023491713140590064 iter num 640\n",
            "loss 24.01300621032715 average time 0.002352156090905806 iter num 660\n",
            "loss 13.045024871826172 average time 0.0023640035647026574 iter num 680\n",
            "loss 117.79432678222656 average time 0.0023725286999966297 iter num 700\n",
            "loss 4.163440704345703 average time 0.0023758282541624315 iter num 720\n",
            "loss 3.4654760360717773 average time 0.0023761322108077514 iter num 740\n",
            "loss 11.528022766113281 average time 0.0023825065236801374 iter num 760\n",
            "loss 22.96786880493164 average time 0.0023835246615339528 iter num 780\n",
            "loss 32.63149642944336 average time 0.0023860203912457224 iter num 800\n",
            "loss 0.9979296922683716 average time 0.002388223459752503 iter num 820\n",
            "loss 0.740467369556427 average time 0.0023885304130920933 iter num 840\n",
            "loss 3.4948253631591797 average time 0.0023946782930204073 iter num 860\n",
            "loss 6.2678656578063965 average time 0.002399826622724597 iter num 880\n",
            "loss 3.1948986053466797 average time 0.0024016651722197618 iter num 900\n",
            "loss 12.83078384399414 average time 0.002403489088040151 iter num 920\n",
            "loss 22.828990936279297 average time 0.002405043311698398 iter num 940\n",
            "loss 125.21581268310547 average time 0.002405921135413062 iter num 960\n",
            "loss 0.3531826138496399 average time 0.002407531904078051 iter num 980\n",
            "loss 28.73655891418457 average time 0.0024126817869960177 iter num 1000\n",
            "loss 67.23844909667969 average time 0.003083596850001413 iter num 20\n",
            "loss 39.58916473388672 average time 0.0027529679250108074 iter num 40\n",
            "loss 137.31443786621094 average time 0.002680335250003433 iter num 60\n",
            "loss 0.8459790349006653 average time 0.0026307158875084726 iter num 80\n",
            "loss 64.71978759765625 average time 0.002589320070003396 iter num 100\n",
            "loss 771.84619140625 average time 0.0025803194750020947 iter num 120\n",
            "loss 289.34991455078125 average time 0.0025599228500059975 iter num 140\n",
            "loss 65.34709930419922 average time 0.0025591083000108482 iter num 160\n",
            "loss 2.0255813598632812 average time 0.002554321122235908 iter num 180\n",
            "loss 69.65133666992188 average time 0.002546663105015341 iter num 200\n",
            "loss 302.7082824707031 average time 0.0025458380545625337 iter num 220\n",
            "loss 69.58480834960938 average time 0.0025617292000144213 iter num 240\n",
            "loss 0.1751701384782791 average time 0.002550047565395849 iter num 260\n",
            "loss 195.58407592773438 average time 0.002547006339295876 iter num 280\n",
            "loss 49.47539138793945 average time 0.002545665103339161 iter num 300\n",
            "loss 30.018104553222656 average time 0.0025442852843816865 iter num 320\n",
            "loss 32.75334548950195 average time 0.0025405362705963426 iter num 340\n",
            "loss 34.38307571411133 average time 0.0025310781277855516 iter num 360\n",
            "loss 0.05463164299726486 average time 0.0025303090657955276 iter num 380\n",
            "loss 247.81982421875 average time 0.0025369590200057246 iter num 400\n",
            "loss 10.992225646972656 average time 0.0025336421428624775 iter num 420\n",
            "loss 6.829267501831055 average time 0.0025342747795512174 iter num 440\n",
            "loss 7.3478779792785645 average time 0.0025316539217452253 iter num 460\n",
            "loss 1.7531006336212158 average time 0.0025277703937552094 iter num 480\n",
            "loss 0.7254242897033691 average time 0.002524539986005948 iter num 500\n",
            "loss 4.498682498931885 average time 0.002525257151928774 iter num 520\n",
            "loss 14.877525329589844 average time 0.002522729250003724 iter num 540\n",
            "loss 0.001702971407212317 average time 0.002519220208932081 iter num 560\n",
            "loss 79.96880340576172 average time 0.0025189138879325363 iter num 580\n",
            "loss 62.64784240722656 average time 0.0025295152250017355 iter num 600\n",
            "loss 77.7909164428711 average time 0.0025270174258088035 iter num 620\n",
            "loss 2.2384085655212402 average time 0.002525175050003625 iter num 640\n",
            "loss 16.00201416015625 average time 0.0025249208863680327 iter num 660\n",
            "loss 5.764072895050049 average time 0.002524645601474664 iter num 680\n",
            "loss 0.021782996132969856 average time 0.0025244423714320873 iter num 700\n",
            "loss 3.586901584640145e-05 average time 0.0025225059208373243 iter num 720\n",
            "loss 0.09325302392244339 average time 0.0025213812702737735 iter num 740\n",
            "loss 93.11016082763672 average time 0.0025203925565829436 iter num 760\n",
            "loss 301.0196533203125 average time 0.0025178118461574228 iter num 780\n",
            "loss 1.055002212524414 average time 0.002516109571252798 iter num 800\n",
            "loss 47.38392639160156 average time 0.0025136564317108424 iter num 820\n",
            "loss 5.978209972381592 average time 0.002512511403575077 iter num 840\n",
            "loss 38.08036422729492 average time 0.0025141392453514695 iter num 860\n",
            "loss 0.059032995253801346 average time 0.0025118416852297864 iter num 880\n",
            "loss 41.26801681518555 average time 0.002510998933335284 iter num 900\n",
            "loss 1.4886857271194458 average time 0.0025098215239141987 iter num 920\n",
            "loss 8.652013778686523 average time 0.0025131767776603274 iter num 940\n",
            "loss 0.8982663750648499 average time 0.0025138446468751852 iter num 960\n",
            "loss 0.00022498168982565403 average time 0.002513131308164099 iter num 980\n",
            "loss 53.63758850097656 average time 0.0025152076470005796 iter num 1000\n",
            "loss 40.10911560058594 average time 0.0030793724499858397 iter num 20\n",
            "loss 112.8852767944336 average time 0.0027866063999908873 iter num 40\n",
            "loss 2.358649253845215 average time 0.002682905983328965 iter num 60\n",
            "loss 1.884833812713623 average time 0.0026073583499936605 iter num 80\n",
            "loss 10.125024795532227 average time 0.002587527109999428 iter num 100\n",
            "loss 133.2037811279297 average time 0.002561216324998365 iter num 120\n",
            "loss 7.441051006317139 average time 0.0025801422785743204 iter num 140\n",
            "loss 0.32515573501586914 average time 0.0025915442562535417 iter num 160\n",
            "loss 0.3190830945968628 average time 0.0025773069333341584 iter num 180\n",
            "loss 92.64451599121094 average time 0.002565608750001047 iter num 200\n",
            "loss 94.98188018798828 average time 0.002556675463637765 iter num 220\n",
            "loss 8.114686965942383 average time 0.002575019279166213 iter num 240\n",
            "loss 1256.3798828125 average time 0.002569330861538569 iter num 260\n",
            "loss 4.580413818359375 average time 0.0025651203249992703 iter num 280\n",
            "loss 34.834327697753906 average time 0.002566163943332261 iter num 300\n",
            "loss 1.1879277229309082 average time 0.002561403096871828 iter num 320\n",
            "loss 0.0007662686984986067 average time 0.0025752303294098054 iter num 340\n",
            "loss 142.83944702148438 average time 0.0025712500249956824 iter num 360\n",
            "loss 374.3970642089844 average time 0.0025692969342063217 iter num 380\n",
            "loss 2.0363616943359375 average time 0.002570815697495732 iter num 400\n",
            "loss 41.53458023071289 average time 0.002570098385708553 iter num 420\n",
            "loss 27.86267852783203 average time 0.0025733585954500625 iter num 440\n",
            "loss 0.06647121906280518 average time 0.0025746800782564566 iter num 460\n",
            "loss 6.713338375091553 average time 0.0025693851979118184 iter num 480\n",
            "loss 42.640384674072266 average time 0.0025649635779955134 iter num 500\n",
            "loss 13.580893516540527 average time 0.002565056453841827 iter num 520\n",
            "loss 5.446913242340088 average time 0.002563086253699701 iter num 540\n",
            "loss 128.16233825683594 average time 0.0025546031499966864 iter num 560\n",
            "loss 0.07238654047250748 average time 0.0025532187137902586 iter num 580\n",
            "loss 11.228011131286621 average time 0.0025416446233312703 iter num 600\n",
            "loss 70.45970153808594 average time 0.0025356477806425456 iter num 620\n",
            "loss 61.30474853515625 average time 0.0025358225249974 iter num 640\n",
            "loss 0.19633302092552185 average time 0.0025253404999971557 iter num 660\n",
            "loss 28.33341407775879 average time 0.002515719614703304 iter num 680\n",
            "loss 0.46848824620246887 average time 0.00250987098999758 iter num 700\n",
            "loss 6.695401191711426 average time 0.0025091650499979956 iter num 720\n",
            "loss 16.300039291381836 average time 0.0025039947878357257 iter num 740\n",
            "loss 3.2098963260650635 average time 0.002496916967103109 iter num 760\n",
            "loss 3.1926419734954834 average time 0.00249197153333194 iter num 780\n",
            "loss 136.87100219726562 average time 0.002487322423748992 iter num 800\n",
            "loss 24.119674682617188 average time 0.0024870946219507683 iter num 820\n",
            "loss 133.5978240966797 average time 0.002488666471428284 iter num 840\n",
            "loss 5.086285591125488 average time 0.002485227344185247 iter num 860\n",
            "loss 7.061583042144775 average time 0.002480212311362803 iter num 880\n",
            "loss 3.473139762878418 average time 0.0024812501666666422 iter num 900\n",
            "loss 0.4400806427001953 average time 0.002478251957608257 iter num 920\n",
            "loss 2.7245700359344482 average time 0.0024783449712756124 iter num 940\n",
            "loss 31.637706756591797 average time 0.002472360207290336 iter num 960\n",
            "loss 0.15523077547550201 average time 0.0024679887510195174 iter num 980\n",
            "loss 28.89333152770996 average time 0.002465752132997977 iter num 1000\n",
            "loss 9.021940231323242 average time 0.0031784668500222323 iter num 20\n",
            "loss 15.51340103149414 average time 0.0027746511750081026 iter num 40\n",
            "loss 0.8332661986351013 average time 0.002604533650007094 iter num 60\n",
            "loss 1.7966126203536987 average time 0.002549933475009425 iter num 80\n",
            "loss 403.78179931640625 average time 0.0025580268500129933 iter num 100\n",
            "loss 129.07423400878906 average time 0.002520142550008586 iter num 120\n",
            "loss 46.3222541809082 average time 0.002479174685722033 iter num 140\n",
            "loss 21.761877059936523 average time 0.0024493869500105347 iter num 160\n",
            "loss 2687.059326171875 average time 0.002476302827787751 iter num 180\n",
            "loss 279.5047912597656 average time 0.002480885740011445 iter num 200\n",
            "loss 168.56387329101562 average time 0.0024652223091028314 iter num 220\n",
            "loss 127.09246063232422 average time 0.002448489054175222 iter num 240\n",
            "loss 86.3302001953125 average time 0.0024538634115445416 iter num 260\n",
            "loss 0.5658314228057861 average time 0.0024375338928645566 iter num 280\n",
            "loss 0.36725789308547974 average time 0.0024461449566729244 iter num 300\n",
            "loss 2.6765012741088867 average time 0.0024313320281308393 iter num 320\n",
            "loss 195.33811950683594 average time 0.0024306668264751567 iter num 340\n",
            "loss 54.9859733581543 average time 0.0024213024138924136 iter num 360\n",
            "loss 333.8755798339844 average time 0.002424392334214525 iter num 380\n",
            "loss 2.633971691131592 average time 0.0024139958850025777 iter num 400\n",
            "loss 0.19504496455192566 average time 0.0024158377595259006 iter num 420\n",
            "loss 0.001543756457976997 average time 0.0024069551590938233 iter num 440\n",
            "loss 30.952144622802734 average time 0.0024141219391332885 iter num 460\n",
            "loss 70.9721908569336 average time 0.002406916308336804 iter num 480\n",
            "loss 62.02764129638672 average time 0.002403689542004031 iter num 500\n",
            "loss 7.928663730621338 average time 0.002406810167309896 iter num 520\n",
            "loss 48.7723274230957 average time 0.002400287561112304 iter num 540\n",
            "loss 12.317357063293457 average time 0.0024051546875008562 iter num 560\n",
            "loss 60.10082244873047 average time 0.002407881350001358 iter num 580\n",
            "loss 22.241119384765625 average time 0.0024197862050008706 iter num 600\n",
            "loss 7.39833927154541 average time 0.002418138648387515 iter num 620\n",
            "loss 2.2600133419036865 average time 0.002411480339061711 iter num 640\n",
            "loss 35.89223098754883 average time 0.0024146465984840964 iter num 660\n",
            "loss 4.484930515289307 average time 0.0024172671044111133 iter num 680\n",
            "loss 231.09732055664062 average time 0.0024121687799993196 iter num 700\n",
            "loss 225.13368225097656 average time 0.002414872474998712 iter num 720\n",
            "loss 3.735175132751465 average time 0.002418793428376535 iter num 740\n",
            "loss 28.066282272338867 average time 0.002413587393418706 iter num 760\n",
            "loss 173.3706817626953 average time 0.0024093761064069696 iter num 780\n",
            "loss 5.406338214874268 average time 0.00240496400874747 iter num 800\n",
            "loss 1.6526563167572021 average time 0.0024035729134117526 iter num 820\n",
            "loss 4.777170658111572 average time 0.0024082939583298046 iter num 840\n",
            "loss 2.089061975479126 average time 0.0024035952779042926 iter num 860\n",
            "loss 3.978666305541992 average time 0.0024001383477248984 iter num 880\n",
            "loss 3.8441388607025146 average time 0.0023969650722198014 iter num 900\n",
            "loss 12.388657569885254 average time 0.0023933239510847176 iter num 920\n",
            "loss 27.824031829833984 average time 0.0023951149255302144 iter num 940\n",
            "loss 10.052468299865723 average time 0.0023921374135400224 iter num 960\n",
            "loss 2.4441726207733154 average time 0.0023908911418358486 iter num 980\n",
            "loss 244.23028564453125 average time 0.0023879396539987284 iter num 1000\n",
            "loss 2.465806484222412 average time 0.0029203191000078733 iter num 20\n",
            "loss 680.58642578125 average time 0.0027263397999888637 iter num 40\n",
            "loss 264.68511962890625 average time 0.002658528783338928 iter num 60\n",
            "loss 29.550809860229492 average time 0.002546807000004492 iter num 80\n",
            "loss 30.412946701049805 average time 0.0025366594100046315 iter num 100\n",
            "loss 1505.1048583984375 average time 0.002483116250003074 iter num 120\n",
            "loss 165.3413543701172 average time 0.0025001346857168624 iter num 140\n",
            "loss 54.62364959716797 average time 0.0024868269812571954 iter num 160\n",
            "loss 0.2616140842437744 average time 0.002455451455562575 iter num 180\n",
            "loss 4.878554344177246 average time 0.0024380200000098285 iter num 200\n",
            "loss 116.504150390625 average time 0.0024274037636452953 iter num 220\n",
            "loss 1.246084451675415 average time 0.002410015770838451 iter num 240\n",
            "loss 210.23031616210938 average time 0.0024058746153899006 iter num 260\n",
            "loss 69.30144500732422 average time 0.0024040671107172784 iter num 280\n",
            "loss 62.093994140625 average time 0.00244762564666947 iter num 300\n",
            "loss 8.912763595581055 average time 0.002434573106251037 iter num 320\n",
            "loss 10.836359024047852 average time 0.0024242796676470094 iter num 340\n",
            "loss 4.471118450164795 average time 0.0024214678638878064 iter num 360\n",
            "loss 0.037105511873960495 average time 0.002426736734208876 iter num 380\n",
            "loss 78.76826477050781 average time 0.002426031919998195 iter num 400\n",
            "loss 25.396995544433594 average time 0.0024173015119009463 iter num 420\n",
            "loss 146.679931640625 average time 0.0024087660636331 iter num 440\n",
            "loss 12.17854118347168 average time 0.0024143971652135763 iter num 460\n",
            "loss 0.7086370587348938 average time 0.0024204770041616067 iter num 480\n",
            "loss 0.9549680352210999 average time 0.002421298307993311 iter num 500\n",
            "loss 121.81963348388672 average time 0.0024139155826849042 iter num 520\n",
            "loss 14.604669570922852 average time 0.002418898601845959 iter num 540\n",
            "loss 135.00372314453125 average time 0.0024214983482088266 iter num 560\n",
            "loss 1.8167428970336914 average time 0.0024234227068914466 iter num 580\n",
            "loss 7.888241767883301 average time 0.002418242663328935 iter num 600\n",
            "loss 32.25295639038086 average time 0.002414204348381997 iter num 620\n",
            "loss 32.125179290771484 average time 0.0024202648765577093 iter num 640\n",
            "loss 8.49483585357666 average time 0.0024239875818128703 iter num 660\n",
            "loss 4.872519493103027 average time 0.002418680322053286 iter num 680\n",
            "loss 32.351497650146484 average time 0.002413660951422441 iter num 700\n",
            "loss 389.75885009765625 average time 0.0024080548749948446 iter num 720\n",
            "loss 39.2428092956543 average time 0.002408480756752077 iter num 740\n",
            "loss 86.06443786621094 average time 0.0024104756473640894 iter num 760\n",
            "loss 83.72476196289062 average time 0.002411607284611177 iter num 780\n",
            "loss 0.7273042798042297 average time 0.002414149744995768 iter num 800\n",
            "loss 25.832664489746094 average time 0.002409074291459247 iter num 820\n",
            "loss 48.535926818847656 average time 0.0024110338404728367 iter num 840\n",
            "loss 6.10041618347168 average time 0.002408324438368275 iter num 860\n",
            "loss 6.640868186950684 average time 0.0024078179534050355 iter num 880\n",
            "loss 46.43718719482422 average time 0.002403241786663178 iter num 900\n",
            "loss 1.2082215547561646 average time 0.0024006912217358783 iter num 920\n",
            "loss 84.61944580078125 average time 0.0023993650021244574 iter num 940\n",
            "loss 6.2146759033203125 average time 0.0023954363708308317 iter num 960\n",
            "loss 40.52961349487305 average time 0.0023939186857115154 iter num 980\n",
            "loss 30.013025283813477 average time 0.0023909288689974344 iter num 1000\n",
            "loss 1.5383468866348267 average time 0.0030373351500088573 iter num 20\n",
            "loss 14.495595932006836 average time 0.0026286830500112045 iter num 40\n",
            "loss 557.5776977539062 average time 0.0026039445833475837 iter num 60\n",
            "loss 27.787582397460938 average time 0.0025912771875169936 iter num 80\n",
            "loss 386.5910339355469 average time 0.0025645417200075827 iter num 100\n",
            "loss 1135.8173828125 average time 0.00250252662500922 iter num 120\n",
            "loss 0.7146484851837158 average time 0.0024634853642932545 iter num 140\n",
            "loss 2504.220947265625 average time 0.002439893000003224 iter num 160\n",
            "loss 14.131305694580078 average time 0.0024310643555559685 iter num 180\n",
            "loss 59.52787780761719 average time 0.002411295659996995 iter num 200\n",
            "loss 74.79447937011719 average time 0.002397396918179724 iter num 220\n",
            "loss 36.78953170776367 average time 0.0023821594791646325 iter num 240\n",
            "loss 6.8094353675842285 average time 0.0023781423423061966 iter num 260\n",
            "loss 7.076025485992432 average time 0.0023963232464260338 iter num 280\n",
            "loss 7.134921073913574 average time 0.002384394749998743 iter num 300\n",
            "loss 64.97764587402344 average time 0.002372165746876931 iter num 320\n",
            "loss 21.383180618286133 average time 0.0023796370352964485 iter num 340\n",
            "loss 45.360965728759766 average time 0.0023698945527794676 iter num 360\n",
            "loss 0.06453143060207367 average time 0.0023635550105280487 iter num 380\n",
            "loss 7.510640621185303 average time 0.0023717225425019706 iter num 400\n",
            "loss 0.016464242711663246 average time 0.0023755957785722005 iter num 420\n",
            "loss 70.24903106689453 average time 0.002371476690909834 iter num 440\n",
            "loss 121.62620544433594 average time 0.002368023252176954 iter num 460\n",
            "loss 2.139742851257324 average time 0.0023641444187532555 iter num 480\n",
            "loss 24.808103561401367 average time 0.0023583435200034727 iter num 500\n",
            "loss 63.76388931274414 average time 0.002378195553849484 iter num 520\n",
            "loss 14.069853782653809 average time 0.0023741116240764176 iter num 540\n",
            "loss 3.5917930603027344 average time 0.0023718060625027093 iter num 560\n",
            "loss 16.230268478393555 average time 0.0023711744879333623 iter num 580\n",
            "loss 35.00899124145508 average time 0.0023755338883355154 iter num 600\n",
            "loss 7.073955535888672 average time 0.0023710104145185404 iter num 620\n",
            "loss 139.03416442871094 average time 0.0023669923375020828 iter num 640\n",
            "loss 264.8411560058594 average time 0.0023691019439414764 iter num 660\n",
            "loss 0.07920029759407043 average time 0.0023726294720608407 iter num 680\n",
            "loss 12.798965454101562 average time 0.0023678559171454645 iter num 700\n",
            "loss 25.390195846557617 average time 0.0023653570986138244 iter num 720\n",
            "loss 21.5389404296875 average time 0.0023671111662199994 iter num 740\n",
            "loss 1.3993233442306519 average time 0.002373008225004014 iter num 760\n",
            "loss 6.6496806144714355 average time 0.0023698494679531626 iter num 780\n",
            "loss 0.07132883369922638 average time 0.0023678833462540184 iter num 800\n",
            "loss 49.93291091918945 average time 0.002374102476833342 iter num 820\n",
            "loss 0.03201399743556976 average time 0.0023731340273849626 iter num 840\n",
            "loss 33.7568359375 average time 0.0023780962000035624 iter num 860\n",
            "loss 8.592101097106934 average time 0.002378937940912093 iter num 880\n",
            "loss 13.21645736694336 average time 0.0023863424266690144 iter num 900\n",
            "loss 19.091289520263672 average time 0.002387797780437248 iter num 920\n",
            "loss 10.805991172790527 average time 0.002390710336173731 iter num 940\n",
            "loss 9.072288513183594 average time 0.0023964907302113166 iter num 960\n",
            "loss 15.833504676818848 average time 0.0024030587438803696 iter num 980\n",
            "loss 4.036084175109863 average time 0.002408854276002103 iter num 1000\n",
            "loss 38.627506256103516 average time 0.0031751135999684267 iter num 20\n",
            "loss 31.232707977294922 average time 0.0028229893749767143 iter num 40\n",
            "loss 0.11809355020523071 average time 0.002716449049982354 iter num 60\n",
            "loss 32.90318298339844 average time 0.0026328439124910117 iter num 80\n",
            "loss 139.0760955810547 average time 0.002627097049992244 iter num 100\n",
            "loss 49.26222610473633 average time 0.0026030327499995563 iter num 120\n",
            "loss 20.797683715820312 average time 0.002586919978568858 iter num 140\n",
            "loss 31.608102798461914 average time 0.002586669968746946 iter num 160\n",
            "loss 8.319578170776367 average time 0.002568669177775165 iter num 180\n",
            "loss 0.8144263625144958 average time 0.0025773595500004375 iter num 200\n",
            "loss 184.26345825195312 average time 0.002572575704543438 iter num 220\n",
            "loss 19.742374420166016 average time 0.0025765903874969353 iter num 240\n",
            "loss 52.84608459472656 average time 0.002566249450000249 iter num 260\n",
            "loss 337.7219543457031 average time 0.0025621490857149964 iter num 280\n",
            "loss 44.907135009765625 average time 0.002569468950004345 iter num 300\n",
            "loss 11.645278930664062 average time 0.00257285634062967 iter num 320\n",
            "loss 111.15161895751953 average time 0.0025687331088279815 iter num 340\n",
            "loss 4.087355136871338 average time 0.002566516641671039 iter num 360\n",
            "loss 342.878173828125 average time 0.002566436363162881 iter num 380\n",
            "loss 4.665201663970947 average time 0.0025587668300045153 iter num 400\n",
            "loss 7.364329814910889 average time 0.0025565114928619032 iter num 420\n",
            "loss 4.931789398193359 average time 0.00255905166363949 iter num 440\n",
            "loss 2.912992000579834 average time 0.0025606864173927653 iter num 460\n",
            "loss 4.355210304260254 average time 0.0025581717562502605 iter num 480\n",
            "loss 23.40843391418457 average time 0.0025542994779989385 iter num 500\n",
            "loss 20.309919357299805 average time 0.002552374980768071 iter num 520\n",
            "loss 4.250099182128906 average time 0.0025504578740714757 iter num 540\n",
            "loss 0.057440612465143204 average time 0.0025468095982124138 iter num 560\n",
            "loss 8.073684692382812 average time 0.0025553325637905928 iter num 580\n",
            "loss 229.22720336914062 average time 0.0025585907616634054 iter num 600\n",
            "loss 0.21348091959953308 average time 0.0025559871096741136 iter num 620\n",
            "loss 0.1980162411928177 average time 0.0025519091781216474 iter num 640\n",
            "loss 305.67303466796875 average time 0.0025552907909059284 iter num 660\n",
            "loss 0.609091579914093 average time 0.0025603083779388086 iter num 680\n",
            "loss 19.92219352722168 average time 0.0025577567242843153 iter num 700\n",
            "loss 3.1256911754608154 average time 0.0025535401430542178 iter num 720\n",
            "loss 0.032697420567274094 average time 0.0025572832540522515 iter num 740\n",
            "loss 1.2204086780548096 average time 0.002560554109209079 iter num 760\n",
            "loss 1.6268061399459839 average time 0.0025596877025627226 iter num 780\n",
            "loss 37.24284362792969 average time 0.0025579902499987383 iter num 800\n",
            "loss 256.44671630859375 average time 0.0025566905182916166 iter num 820\n",
            "loss 3.536116361618042 average time 0.002555788470237985 iter num 840\n",
            "loss 4.486093997955322 average time 0.0025532737662790042 iter num 860\n",
            "loss 0.23052136600017548 average time 0.002551686207953904 iter num 880\n",
            "loss 4.226583957672119 average time 0.002555540833333099 iter num 900\n",
            "loss 11.731005668640137 average time 0.0025557812956519582 iter num 920\n",
            "loss 7.468547821044922 average time 0.002555114275531515 iter num 940\n",
            "loss 146.92922973632812 average time 0.0025513855572912784 iter num 960\n",
            "loss 0.30569031834602356 average time 0.00254986494387757 iter num 980\n",
            "loss 28.424821853637695 average time 0.002548129705000292 iter num 1000\n",
            "loss 1.6229145526885986 average time 0.0032457612499683817 iter num 20\n",
            "loss 35.88025665283203 average time 0.0028826428749653132 iter num 40\n",
            "loss 63.2947998046875 average time 0.002713777416643855 iter num 60\n",
            "loss 545.79443359375 average time 0.0026528119624714464 iter num 80\n",
            "loss 0.3023949861526489 average time 0.0026252172399767915 iter num 100\n",
            "loss 0.30122435092926025 average time 0.002626401516649442 iter num 120\n",
            "loss 4.946171760559082 average time 0.002643514157127161 iter num 140\n",
            "loss 57.208946228027344 average time 0.0026232928062327686 iter num 160\n",
            "loss 103.03987884521484 average time 0.002609740305543558 iter num 180\n",
            "loss 30.880918502807617 average time 0.002614933564987041 iter num 200\n",
            "loss 183.10794067382812 average time 0.0026022638454409473 iter num 220\n",
            "loss 82.24137115478516 average time 0.002594545712485304 iter num 240\n",
            "loss 12.810638427734375 average time 0.002590294534601211 iter num 260\n",
            "loss 25.066200256347656 average time 0.0025900585321307645 iter num 280\n",
            "loss 0.7936784029006958 average time 0.0025926870066534016 iter num 300\n",
            "loss 6.457380771636963 average time 0.0025851972781140375 iter num 320\n",
            "loss 25.34941864013672 average time 0.002585201399989835 iter num 340\n",
            "loss 0.1643058955669403 average time 0.0025848215194343993 iter num 360\n",
            "loss 40.13172912597656 average time 0.0025789119736762253 iter num 380\n",
            "loss 0.6140108704566956 average time 0.0025751531974918863 iter num 400\n",
            "loss 6.5463433265686035 average time 0.0025707483118973568 iter num 420\n",
            "loss 19.468795776367188 average time 0.002565579038630197 iter num 440\n",
            "loss 162.71661376953125 average time 0.002561314049994903 iter num 460\n",
            "loss 486.7786560058594 average time 0.0025561477604095256 iter num 480\n",
            "loss 16.730161666870117 average time 0.002551551769993694 iter num 500\n",
            "loss 0.3910757303237915 average time 0.0025559678980698664 iter num 520\n",
            "loss 32.97306823730469 average time 0.002559477670362648 iter num 540\n",
            "loss 6.472164630889893 average time 0.0025588748446352836 iter num 560\n",
            "loss 2.3977162837982178 average time 0.0025564172086143147 iter num 580\n",
            "loss 1.9212466478347778 average time 0.002558657169993997 iter num 600\n",
            "loss 0.4086817502975464 average time 0.002556592419348876 iter num 620\n",
            "loss 0.9969658255577087 average time 0.0025554805593674956 iter num 640\n",
            "loss 0.4074438512325287 average time 0.0025568665530224252 iter num 660\n",
            "loss 2.9454140663146973 average time 0.0025547262544050146 iter num 680\n",
            "loss 2.02827525138855 average time 0.002556464307135684 iter num 700\n",
            "loss 50.219051361083984 average time 0.002559422737493833 iter num 720\n",
            "loss 3.709103584289551 average time 0.0025626921999941186 iter num 740\n",
            "loss 5.575474262237549 average time 0.002564190744731389 iter num 760\n",
            "loss 5.109903335571289 average time 0.0025565816282006053 iter num 780\n",
            "loss 26.646034240722656 average time 0.002552690574995324 iter num 800\n",
            "loss 19.130613327026367 average time 0.002547418253654386 iter num 820\n",
            "loss 20.602949142456055 average time 0.002545228451186263 iter num 840\n",
            "loss 1.14970862865448 average time 0.00254437224185616 iter num 860\n",
            "loss 25.829910278320312 average time 0.002540426481813928 iter num 880\n",
            "loss 0.2870948910713196 average time 0.002546143936662904 iter num 900\n",
            "loss 3.4141252040863037 average time 0.002539017629343878 iter num 920\n",
            "loss 0.14394435286521912 average time 0.0025339785042515746 iter num 940\n",
            "loss 26.297971725463867 average time 0.002537085109371873 iter num 960\n",
            "loss 12.451642036437988 average time 0.0025375090020384513 iter num 980\n",
            "loss 107.53294372558594 average time 0.002537339313997336 iter num 1000\n",
            "loss 160.15846252441406 average time 0.002858700199988107 iter num 20\n",
            "loss 30.98419952392578 average time 0.0025299643249923065 iter num 40\n",
            "loss 374.59075927734375 average time 0.002514529033328472 iter num 60\n",
            "loss 0.6923688650131226 average time 0.002434236474988438 iter num 80\n",
            "loss 1.0612314939498901 average time 0.002461990399992828 iter num 100\n",
            "loss 25.087509155273438 average time 0.0024198404583254765 iter num 120\n",
            "loss 306.25347900390625 average time 0.0024303144928532155 iter num 140\n",
            "loss 98.08361053466797 average time 0.0024313534187470507 iter num 160\n",
            "loss 7.317014217376709 average time 0.0024140837499948653 iter num 180\n",
            "loss 8.928092002868652 average time 0.0024129671249966123 iter num 200\n",
            "loss 0.7155503630638123 average time 0.0024191508181822677 iter num 220\n",
            "loss 0.4106447994709015 average time 0.0024089399625021692 iter num 240\n",
            "loss 11.039983749389648 average time 0.0024140036230759364 iter num 260\n",
            "loss 0.01222930010408163 average time 0.0024031176892841746 iter num 280\n",
            "loss 28.644325256347656 average time 0.0024167330466639213 iter num 300\n",
            "loss 47.692970275878906 average time 0.0024225833937471463 iter num 320\n",
            "loss 0.5692393779754639 average time 0.002425898579408969 iter num 340\n",
            "loss 0.3041873276233673 average time 0.002414960438884843 iter num 360\n",
            "loss 28.466087341308594 average time 0.0024085511394697566 iter num 380\n",
            "loss 2.6126062870025635 average time 0.002401564742497726 iter num 400\n",
            "loss 410.1409912109375 average time 0.002393825211901826 iter num 420\n",
            "loss 24.713502883911133 average time 0.0024026259249973754 iter num 440\n",
            "loss 201.23190307617188 average time 0.002400388995649495 iter num 460\n",
            "loss 0.47836294770240784 average time 0.0023935859812477625 iter num 480\n",
            "loss 99.4626235961914 average time 0.0023863757579974843 iter num 500\n",
            "loss 0.5578028559684753 average time 0.0023915607423060253 iter num 520\n",
            "loss 0.04520518332719803 average time 0.0023973485999986394 iter num 540\n",
            "loss 240.9591064453125 average time 0.0023918667785713817 iter num 560\n",
            "loss 1.8259433507919312 average time 0.0023899340465508715 iter num 580\n",
            "loss 3.5031049251556396 average time 0.002387018746665793 iter num 600\n",
            "loss 49.301456451416016 average time 0.002387436579031219 iter num 620\n",
            "loss 27.266815185546875 average time 0.0023816533531242357 iter num 640\n",
            "loss 2.450145721435547 average time 0.0023773699348473864 iter num 660\n",
            "loss 18.25209617614746 average time 0.002373299030880523 iter num 680\n",
            "loss 3.17191219329834 average time 0.0023766914814271852 iter num 700\n",
            "loss 0.12845490872859955 average time 0.0023730565833312133 iter num 720\n",
            "loss 18.62477684020996 average time 0.002369884445944093 iter num 740\n",
            "loss 8.911413192749023 average time 0.0023763420460505514 iter num 760\n",
            "loss 0.5354359149932861 average time 0.0023792405346139043 iter num 780\n",
            "loss 0.7908273339271545 average time 0.002377840483749196 iter num 800\n",
            "loss 0.7874071002006531 average time 0.002380281458535991 iter num 820\n",
            "loss 1.7828184366226196 average time 0.0023773848845221003 iter num 840\n",
            "loss 10.458812713623047 average time 0.002380277233718315 iter num 860\n",
            "loss 51.95339584350586 average time 0.0023791691477250444 iter num 880\n",
            "loss 28.064138412475586 average time 0.0023774177922190475 iter num 900\n",
            "loss 76.80127716064453 average time 0.0023789585086917767 iter num 920\n",
            "loss 3.0110089778900146 average time 0.0023778957010604437 iter num 940\n",
            "loss 1.134750247001648 average time 0.0023813909197878996 iter num 960\n",
            "loss 120.12224578857422 average time 0.00237838758571041 iter num 980\n",
            "loss 1.6690762042999268 average time 0.002378890656995509 iter num 1000\n",
            "loss 1.6138606071472168 average time 0.003291972749980232 iter num 20\n",
            "loss 0.001449757837690413 average time 0.002779258850000588 iter num 40\n",
            "loss 3.4914908409118652 average time 0.0025958533500064127 iter num 60\n",
            "loss 47.60159683227539 average time 0.0025092245749902984 iter num 80\n",
            "loss 24.1590518951416 average time 0.0025002981099896716 iter num 100\n",
            "loss 31.774274826049805 average time 0.002470144841657884 iter num 120\n",
            "loss 0.18476209044456482 average time 0.002436127999996123 iter num 140\n",
            "loss 3.1819376945495605 average time 0.002430962143748161 iter num 160\n",
            "loss 1.6753805875778198 average time 0.0024063268444415877 iter num 180\n",
            "loss 0.8662740588188171 average time 0.0024040310799978214 iter num 200\n",
            "loss 449.8299865722656 average time 0.0023856458181801994 iter num 220\n",
            "loss 415.52001953125 average time 0.002402887179166176 iter num 240\n",
            "loss 21.801687240600586 average time 0.0024038955461553367 iter num 260\n",
            "loss 161.5830535888672 average time 0.002393906739285967 iter num 280\n",
            "loss 1.7808172702789307 average time 0.0023836813933348824 iter num 300\n",
            "loss 274.6546630859375 average time 0.0023736433375027843 iter num 320\n",
            "loss 17.78173065185547 average time 0.0023752692000016395 iter num 340\n",
            "loss 21.361417770385742 average time 0.0023722753944456247 iter num 360\n",
            "loss 9.246383666992188 average time 0.002363626515790728 iter num 380\n",
            "loss 4.857522487640381 average time 0.0023672097000013536 iter num 400\n",
            "loss 81.41058349609375 average time 0.002368631835713586 iter num 420\n",
            "loss 29.789016723632812 average time 0.0023702297113624643 iter num 440\n",
            "loss 21.699993133544922 average time 0.0023624469565205523 iter num 460\n",
            "loss 59.04640579223633 average time 0.002369382122916382 iter num 480\n",
            "loss 0.022233571857213974 average time 0.0023697990639989256 iter num 500\n",
            "loss 0.9552395939826965 average time 0.0023651650576915436 iter num 520\n",
            "loss 10.834550857543945 average time 0.0023624313518532153 iter num 540\n",
            "loss 310.07073974609375 average time 0.0023591792696436185 iter num 560\n",
            "loss 13.869646072387695 average time 0.002366276477586137 iter num 580\n",
            "loss 418.6308288574219 average time 0.002362422978334659 iter num 600\n",
            "loss 1.8287169933319092 average time 0.0023603812919367143 iter num 620\n",
            "loss 137.42532348632812 average time 0.002356016810937689 iter num 640\n",
            "loss 3.3916397094726562 average time 0.0023605612196978187 iter num 660\n",
            "loss 10.418683052062988 average time 0.002357640739706192 iter num 680\n",
            "loss 0.3681449890136719 average time 0.002356594980000344 iter num 700\n",
            "loss 0.15262816846370697 average time 0.002360925340278186 iter num 720\n",
            "loss 27.399641036987305 average time 0.002362882404054904 iter num 740\n",
            "loss 4.052541255950928 average time 0.002361980848684796 iter num 760\n",
            "loss 0.12195479869842529 average time 0.0023614449974359514 iter num 780\n",
            "loss 0.7737228870391846 average time 0.0023606938699995794 iter num 800\n",
            "loss 11.449615478515625 average time 0.002368003404878004 iter num 820\n",
            "loss 0.10718590021133423 average time 0.002364315992856735 iter num 840\n",
            "loss 14.758010864257812 average time 0.002360540908139334 iter num 860\n",
            "loss 28.422502517700195 average time 0.0023625354761357834 iter num 880\n",
            "loss 11.105788230895996 average time 0.0023634725822214223 iter num 900\n",
            "loss 111.97615814208984 average time 0.0023632688902175095 iter num 920\n",
            "loss 6.0899176597595215 average time 0.0023597317000007448 iter num 940\n",
            "loss 9.761143684387207 average time 0.002356032322917893 iter num 960\n",
            "loss 1.6064062118530273 average time 0.0023571211091846134 iter num 980\n",
            "loss 53.37919235229492 average time 0.0023575169030011695 iter num 1000\n",
            "loss 95.70330047607422 average time 0.002882554950031135 iter num 20\n",
            "loss 0.030272772535681725 average time 0.002569102300014947 iter num 40\n",
            "loss 2.5291194915771484 average time 0.0024741800333496636 iter num 60\n",
            "loss 53.81597900390625 average time 0.0024242319125164615 iter num 80\n",
            "loss 253.02020263671875 average time 0.0023975644600045596 iter num 100\n",
            "loss 15.542521476745605 average time 0.0023677029250090222 iter num 120\n",
            "loss 4.184504508972168 average time 0.0023791700714387585 iter num 140\n",
            "loss 192.99969482421875 average time 0.0023640534187592266 iter num 160\n",
            "loss 190.36785888671875 average time 0.002352929655563053 iter num 180\n",
            "loss 122.78926849365234 average time 0.002361515460005421 iter num 200\n",
            "loss 0.4002971649169922 average time 0.0023660061227319187 iter num 220\n",
            "loss 4.748251438140869 average time 0.0023688923958369895 iter num 240\n",
            "loss 67.7137451171875 average time 0.0023687431346204975 iter num 260\n",
            "loss 916.5940551757812 average time 0.0023576717607186634 iter num 280\n",
            "loss 21.064083099365234 average time 0.0023508913300042877 iter num 300\n",
            "loss 14.46687126159668 average time 0.002362847612503316 iter num 320\n",
            "loss 37.41225814819336 average time 0.00235915438823828 iter num 340\n",
            "loss 109.4663314819336 average time 0.002353306572224609 iter num 360\n",
            "loss 1.2789241075515747 average time 0.0023537813815814363 iter num 380\n",
            "loss 23.898292541503906 average time 0.002362596700002655 iter num 400\n",
            "loss 16.854747772216797 average time 0.0023650757309545563 iter num 420\n",
            "loss 48.098358154296875 average time 0.0023654337931847376 iter num 440\n",
            "loss 61.447837829589844 average time 0.0023733890021765585 iter num 460\n",
            "loss 107.80976867675781 average time 0.002385990068752619 iter num 480\n",
            "loss 12.897579193115234 average time 0.0023923988420028763 iter num 500\n",
            "loss 3.104151487350464 average time 0.0023971023730797905 iter num 520\n",
            "loss 0.2663889229297638 average time 0.0023956512425932623 iter num 540\n",
            "loss 36.320411682128906 average time 0.0023949545321439052 iter num 560\n",
            "loss 115.69084930419922 average time 0.0023999030344823732 iter num 580\n",
            "loss 149.13327026367188 average time 0.002407894724999172 iter num 600\n",
            "loss 38.60939407348633 average time 0.0024082535725802523 iter num 620\n",
            "loss 0.03355586901307106 average time 0.0024081613265611425 iter num 640\n",
            "loss 7.780976295471191 average time 0.0024180236030295447 iter num 660\n",
            "loss 0.6592792272567749 average time 0.0024182002485280706 iter num 680\n",
            "loss 13.742080688476562 average time 0.0024123443399983965 iter num 700\n",
            "loss 0.46255606412887573 average time 0.0024177031291647685 iter num 720\n",
            "loss 0.03476182743906975 average time 0.002419591510807869 iter num 740\n",
            "loss 10.11837100982666 average time 0.0024147769736816227 iter num 760\n",
            "loss 0.5401586294174194 average time 0.0024101399512792693 iter num 780\n",
            "loss 0.2522212564945221 average time 0.002407829159997448 iter num 800\n",
            "loss 1.4972504377365112 average time 0.0024055646390218877 iter num 820\n",
            "loss 22.54420280456543 average time 0.0024117758404746123 iter num 840\n",
            "loss 14.06328010559082 average time 0.0024101159976725665 iter num 860\n",
            "loss 7.3255815505981445 average time 0.00241212963977091 iter num 880\n",
            "loss 247.64859008789062 average time 0.0024127547888883783 iter num 900\n",
            "loss 0.47744011878967285 average time 0.002414799218478166 iter num 920\n",
            "loss 0.8246843814849854 average time 0.002411784047871529 iter num 940\n",
            "loss 5.279348373413086 average time 0.0024146428302079435 iter num 960\n",
            "loss 19.483366012573242 average time 0.0024162303673471554 iter num 980\n",
            "loss 41.11553955078125 average time 0.0024161149500000647 iter num 1000\n",
            "loss 0.3394179046154022 average time 0.003146441450007842 iter num 20\n",
            "loss 0.7209515571594238 average time 0.002881166174978489 iter num 40\n",
            "loss 12.530633926391602 average time 0.002789382166652861 iter num 60\n",
            "loss 0.3294389843940735 average time 0.002700121549986534 iter num 80\n",
            "loss 66.75946807861328 average time 0.0026608330999874853 iter num 100\n",
            "loss 299.7452392578125 average time 0.002655247583322762 iter num 120\n",
            "loss 4.676960468292236 average time 0.002635182971422572 iter num 140\n",
            "loss 46.32069778442383 average time 0.0026114567999954374 iter num 160\n",
            "loss 1.7806544303894043 average time 0.0025947974666627894 iter num 180\n",
            "loss 23.32051658630371 average time 0.002584340144998123 iter num 200\n",
            "loss 0.29209595918655396 average time 0.0025727310863637534 iter num 220\n",
            "loss 23.840768814086914 average time 0.0025785370374980707 iter num 240\n",
            "loss 162.07318115234375 average time 0.0025810248230754953 iter num 260\n",
            "loss 7.239195823669434 average time 0.0025783886035696925 iter num 280\n",
            "loss 8.510601997375488 average time 0.0025784110166667536 iter num 300\n",
            "loss 1.6866683959960938 average time 0.0025980695687458422 iter num 320\n",
            "loss 36.20656204223633 average time 0.002602733014701056 iter num 340\n",
            "loss 14.590124130249023 average time 0.002597412372217731 iter num 360\n",
            "loss 2.8907809257507324 average time 0.0025884773578905207 iter num 380\n",
            "loss 0.45714443922042847 average time 0.0025802975574958963 iter num 400\n",
            "loss 0.8980856537818909 average time 0.0025841289571406465 iter num 420\n",
            "loss 13.056053161621094 average time 0.002584866356814721 iter num 440\n",
            "loss 2.259072780609131 average time 0.002581548176082201 iter num 460\n",
            "loss 58.779258728027344 average time 0.0025749906749960396 iter num 480\n",
            "loss 11.00097942352295 average time 0.002576242435997756 iter num 500\n",
            "loss 44.55930709838867 average time 0.002569767634614751 iter num 520\n",
            "loss 56.16134262084961 average time 0.002569632487036752 iter num 540\n",
            "loss 8.47607135772705 average time 0.0025710569464280296 iter num 560\n",
            "loss 0.037272870540618896 average time 0.002566765627585514 iter num 580\n",
            "loss 0.08794812858104706 average time 0.0025630236849993556 iter num 600\n",
            "loss 0.010685248300433159 average time 0.002560556293549472 iter num 620\n",
            "loss 0.1286807656288147 average time 0.002562662773439328 iter num 640\n",
            "loss 0.86431884765625 average time 0.0025606086727279808 iter num 660\n",
            "loss 4.713322639465332 average time 0.0025576216617661 iter num 680\n",
            "loss 1.6882413625717163 average time 0.002555550987144345 iter num 700\n",
            "loss 52.0203971862793 average time 0.0025536119097226773 iter num 720\n",
            "loss 2.914483070373535 average time 0.0025526462594589127 iter num 740\n",
            "loss 3.750570058822632 average time 0.0025533194986843157 iter num 760\n",
            "loss 7.92058801651001 average time 0.0025515192717948713 iter num 780\n",
            "loss 91.77980041503906 average time 0.002547129932499956 iter num 800\n",
            "loss 5.0886993408203125 average time 0.002549804396341653 iter num 820\n",
            "loss 70.9387435913086 average time 0.0025496386630958936 iter num 840\n",
            "loss 1.9041502475738525 average time 0.0025501145790713053 iter num 860\n",
            "loss 95.84815216064453 average time 0.0025494827693199277 iter num 880\n",
            "loss 412.33673095703125 average time 0.0025510053477796647 iter num 900\n",
            "loss 228.92835998535156 average time 0.0025515406293503354 iter num 920\n",
            "loss 372.3487854003906 average time 0.0025516035202147267 iter num 940\n",
            "loss 5.5278730392456055 average time 0.0025505520239600325 iter num 960\n",
            "loss 0.22575776278972626 average time 0.002551811820409725 iter num 980\n",
            "loss 1.6731494665145874 average time 0.0025504311930014865 iter num 1000\n",
            "loss 192.48831176757812 average time 0.003200956699981816 iter num 20\n",
            "loss 79.15313720703125 average time 0.0028190742499930367 iter num 40\n",
            "loss 99.8715591430664 average time 0.0027066777500067475 iter num 60\n",
            "loss 280.18359375 average time 0.002653597737497648 iter num 80\n",
            "loss 5.938356876373291 average time 0.0026196986099967035 iter num 100\n",
            "loss 1141.585205078125 average time 0.0025759338916657272 iter num 120\n",
            "loss 19.80559730529785 average time 0.0025770475499971974 iter num 140\n",
            "loss 0.020108647644519806 average time 0.0025812588499960045 iter num 160\n",
            "loss 100.06722259521484 average time 0.0025727039666586884 iter num 180\n",
            "loss 80.08552551269531 average time 0.0025636288249927474 iter num 200\n",
            "loss 282.3221435546875 average time 0.002550470009088134 iter num 220\n",
            "loss 44.45285415649414 average time 0.0025493399374975447 iter num 240\n",
            "loss 1.817740559577942 average time 0.0025457300961525455 iter num 260\n",
            "loss 113.99140930175781 average time 0.0025432763464258252 iter num 280\n",
            "loss 1.157348871231079 average time 0.0025326270499999737 iter num 300\n",
            "loss 53.79482650756836 average time 0.002667501071874767 iter num 320\n",
            "loss 6.449085712432861 average time 0.002712032852941641 iter num 340\n",
            "loss 2.4388110637664795 average time 0.00270662022222344 iter num 360\n",
            "loss 0.003940657712519169 average time 0.0026976082368426095 iter num 380\n",
            "loss 14.68504810333252 average time 0.0026836624574991673 iter num 400\n",
            "loss 31.92363739013672 average time 0.002675500676189997 iter num 420\n",
            "loss 3.8009095191955566 average time 0.0026652180295457127 iter num 440\n",
            "loss 144.13644409179688 average time 0.0026602856195648635 iter num 460\n",
            "loss 82.07691955566406 average time 0.002649703641665724 iter num 480\n",
            "loss 11.12215518951416 average time 0.0026534480259997506 iter num 500\n",
            "loss 19.129812240600586 average time 0.0026480853942311674 iter num 520\n",
            "loss 282.96295166015625 average time 0.002653785346298163 iter num 540\n",
            "loss 0.006382088176906109 average time 0.0026459516571439474 iter num 560\n",
            "loss 32.4586181640625 average time 0.002640350053449201 iter num 580\n",
            "loss 457.6300048828125 average time 0.002634863561666331 iter num 600\n",
            "loss 59.35835266113281 average time 0.0026338651209672913 iter num 620\n",
            "loss 13.662679672241211 average time 0.0026301088140623817 iter num 640\n",
            "loss 0.009633897803723812 average time 0.0026249582181817875 iter num 660\n",
            "loss 0.052174072712659836 average time 0.0026196690323528776 iter num 680\n",
            "loss 126.90072631835938 average time 0.0026223765871431427 iter num 700\n",
            "loss 30.66518783569336 average time 0.0026234390569454843 iter num 720\n",
            "loss 104.37187194824219 average time 0.002618886024325437 iter num 740\n",
            "loss 18.153282165527344 average time 0.0026150881907914193 iter num 760\n",
            "loss 1.2462635040283203 average time 0.0026097117243609365 iter num 780\n",
            "loss 1.501443862915039 average time 0.002606369432502618 iter num 800\n",
            "loss 9.593947410583496 average time 0.0026021073000018677 iter num 820\n",
            "loss 0.04338476061820984 average time 0.0026000988214310012 iter num 840\n",
            "loss 8.638097763061523 average time 0.0025953920802347184 iter num 860\n",
            "loss 0.8463475108146667 average time 0.0025949851454560697 iter num 880\n",
            "loss 2.30277156829834 average time 0.002593322805556808 iter num 900\n",
            "loss 45.72517395019531 average time 0.0025889605054351746 iter num 920\n",
            "loss 8.752969741821289 average time 0.002582053945745448 iter num 940\n",
            "loss 9.502232551574707 average time 0.002584785044792426 iter num 960\n",
            "loss 43.39065170288086 average time 0.002582246507143987 iter num 980\n",
            "loss 0.013294790871441364 average time 0.0025773045150010603 iter num 1000\n",
            "loss 17.461688995361328 average time 0.003138646999991579 iter num 20\n",
            "loss 82.61318969726562 average time 0.0028450868749814616 iter num 40\n",
            "loss 0.671204149723053 average time 0.0027572526166447157 iter num 60\n",
            "loss 31.550567626953125 average time 0.002678458762486002 iter num 80\n",
            "loss 0.05973696708679199 average time 0.0026301597899919217 iter num 100\n",
            "loss 131.44290161132812 average time 0.002595270866656089 iter num 120\n",
            "loss 1.6355855464935303 average time 0.002580355307137389 iter num 140\n",
            "loss 106.75508880615234 average time 0.002553226187491475 iter num 160\n",
            "loss 76.7967300415039 average time 0.002531622116658936 iter num 180\n",
            "loss 5.281647205352783 average time 0.0025470535599936284 iter num 200\n",
            "loss 576.7518920898438 average time 0.0025440387727206307 iter num 220\n",
            "loss 0.575621485710144 average time 0.0025483425791605896 iter num 240\n",
            "loss 105.40185546875 average time 0.002533322399996835 iter num 260\n",
            "loss 4.521333694458008 average time 0.002534059539280114 iter num 280\n",
            "loss 147.73843383789062 average time 0.0025347943633263034 iter num 300\n",
            "loss 18.700716018676758 average time 0.0025314921781181 iter num 320\n",
            "loss 0.005656342953443527 average time 0.002535014182346378 iter num 340\n",
            "loss 0.36231622099876404 average time 0.002538099555547054 iter num 360\n",
            "loss 116.10970306396484 average time 0.002535918168413637 iter num 380\n",
            "loss 41.3951530456543 average time 0.0025341786824935753 iter num 400\n",
            "loss 1.5592565536499023 average time 0.0025226741476119838 iter num 420\n",
            "loss 51.001731872558594 average time 0.002530989913629934 iter num 440\n",
            "loss 35.34093475341797 average time 0.00253304679129878 iter num 460\n",
            "loss 3.9232797622680664 average time 0.0025309500541605225 iter num 480\n",
            "loss 33.79739761352539 average time 0.0025188599499938393 iter num 500\n",
            "loss 15.529030799865723 average time 0.0025235333134551534 iter num 520\n",
            "loss 2.5011680126190186 average time 0.0025270360833267794 iter num 540\n",
            "loss 0.4494244456291199 average time 0.0025342754089211082 iter num 560\n",
            "loss 14.9052152633667 average time 0.0025316859482696318 iter num 580\n",
            "loss 59.08287811279297 average time 0.002546223978328044 iter num 600\n",
            "loss 23.175579071044922 average time 0.002544782772575203 iter num 620\n",
            "loss 9.45842170715332 average time 0.0025428880062449365 iter num 640\n",
            "loss 3.829070806503296 average time 0.002541098124238212 iter num 660\n",
            "loss 1.017285943031311 average time 0.0025404444147020112 iter num 680\n",
            "loss 44.32526779174805 average time 0.002538294297140575 iter num 700\n",
            "loss 87.99937438964844 average time 0.0025359571111080313 iter num 720\n",
            "loss 0.23431621491909027 average time 0.0025355638013485767 iter num 740\n",
            "loss 41.14665603637695 average time 0.0025378507723650347 iter num 760\n",
            "loss 144.89930725097656 average time 0.0025372377128172583 iter num 780\n",
            "loss 0.3148494064807892 average time 0.002541496988746701 iter num 800\n",
            "loss 3.02406907081604 average time 0.002539915459752459 iter num 820\n",
            "loss 9.873927116394043 average time 0.002541297142853355 iter num 840\n",
            "loss 1.9360908269882202 average time 0.0025393501348802134 iter num 860\n",
            "loss 2.3894333839416504 average time 0.0025394124749952467 iter num 880\n",
            "loss 12.276823997497559 average time 0.002537755544439935 iter num 900\n",
            "loss 10.51276969909668 average time 0.0025420412956476492 iter num 920\n",
            "loss 66.65266418457031 average time 0.002550831494676594 iter num 940\n",
            "loss 30.826250076293945 average time 0.0025502424177041407 iter num 960\n",
            "loss 0.04572673141956329 average time 0.002549242480608186 iter num 980\n",
            "loss 22.883703231811523 average time 0.0025486120199957442 iter num 1000\n",
            "loss 3.8032021522521973 average time 0.0030851448500129664 iter num 20\n",
            "loss 85.45730590820312 average time 0.0027928397500090797 iter num 40\n",
            "loss 97.40550994873047 average time 0.002742211533336558 iter num 60\n",
            "loss 71.14068603515625 average time 0.002752997650000566 iter num 80\n",
            "loss 3.682234048843384 average time 0.0027566584300029715 iter num 100\n",
            "loss 232.78990173339844 average time 0.002726490425002718 iter num 120\n",
            "loss 27.386383056640625 average time 0.002700191592860587 iter num 140\n",
            "loss 6.088173866271973 average time 0.0027075892124997837 iter num 160\n",
            "loss 36.62718963623047 average time 0.002691739644445887 iter num 180\n",
            "loss 6.357614040374756 average time 0.002668466245002037 iter num 200\n",
            "loss 13.452008247375488 average time 0.002665249422729878 iter num 220\n",
            "loss 6.692735195159912 average time 0.0026589872375031593 iter num 240\n",
            "loss 51.049232482910156 average time 0.0026497804153879 iter num 260\n",
            "loss 177.01593017578125 average time 0.002636865089289943 iter num 280\n",
            "loss 142.42112731933594 average time 0.0026240243233382897 iter num 300\n",
            "loss 37.48945999145508 average time 0.002613657937506986 iter num 320\n",
            "loss 30.889144897460938 average time 0.002616052864711679 iter num 340\n",
            "loss 0.29099929332733154 average time 0.0026083614277853133 iter num 360\n",
            "loss 0.18738049268722534 average time 0.002605515236848532 iter num 380\n",
            "loss 12.500243186950684 average time 0.002607834010005945 iter num 400\n",
            "loss 37.65632247924805 average time 0.0026089657476242273 iter num 420\n",
            "loss 131.102783203125 average time 0.0026087797545518367 iter num 440\n",
            "loss 155.80970764160156 average time 0.0026011050804403686 iter num 460\n",
            "loss 1.076988935470581 average time 0.0025967960895873906 iter num 480\n",
            "loss 7.045555114746094 average time 0.00259200032000399 iter num 500\n",
            "loss 46.363182067871094 average time 0.002590100909618251 iter num 520\n",
            "loss 27.874116897583008 average time 0.002588093638892301 iter num 540\n",
            "loss 39.31218719482422 average time 0.002586202235717191 iter num 560\n",
            "loss 168.95462036132812 average time 0.0025841780241417553 iter num 580\n",
            "loss 12.496304512023926 average time 0.002583242501669929 iter num 600\n",
            "loss 36.67582702636719 average time 0.002595447788711857 iter num 620\n",
            "loss 0.30995070934295654 average time 0.002590287706252781 iter num 640\n",
            "loss 17.227516174316406 average time 0.0025903796606085603 iter num 660\n",
            "loss 45.10790252685547 average time 0.002588189458826411 iter num 680\n",
            "loss 0.19034628570079803 average time 0.0025930972314313293 iter num 700\n",
            "loss 11.046676635742188 average time 0.0025906893791696147 iter num 720\n",
            "loss 6.5130743980407715 average time 0.0025920702675703805 iter num 740\n",
            "loss 2.9980878829956055 average time 0.0025901777842127676 iter num 760\n",
            "loss 108.75109100341797 average time 0.002596492351284059 iter num 780\n",
            "loss 18.77945327758789 average time 0.0025968417175016613 iter num 800\n",
            "loss 0.0006949113449081779 average time 0.0025932423268306215 iter num 820\n",
            "loss 0.4214211106300354 average time 0.0025918919523819895 iter num 840\n",
            "loss 62.863746643066406 average time 0.0025929775255819952 iter num 860\n",
            "loss 0.7932659983634949 average time 0.0025988731409104173 iter num 880\n",
            "loss 2.642559289932251 average time 0.002595799467780277 iter num 900\n",
            "loss 9.506324768066406 average time 0.0025952841054373474 iter num 920\n",
            "loss 0.27078545093536377 average time 0.0025942838936193185 iter num 940\n",
            "loss 1.0186412334442139 average time 0.002592307867710512 iter num 960\n",
            "loss 30.581308364868164 average time 0.0025890484316354726 iter num 980\n",
            "loss 120.5280532836914 average time 0.0025884299940023537 iter num 1000\n",
            "loss 295.579345703125 average time 0.0032670202500412414 iter num 20\n",
            "loss 0.3559160530567169 average time 0.0028570791250217553 iter num 40\n",
            "loss 12.74113941192627 average time 0.0028001401000210535 iter num 60\n",
            "loss 2.0753653049468994 average time 0.002761801862516222 iter num 80\n",
            "loss 11.759407043457031 average time 0.0027162348300134908 iter num 100\n",
            "loss 15.650187492370605 average time 0.0026889891500142464 iter num 120\n",
            "loss 16.188417434692383 average time 0.0026592828214396444 iter num 140\n",
            "loss 0.030103683471679688 average time 0.002641815918758539 iter num 160\n",
            "loss 5.2132954597473145 average time 0.0026369022000027244 iter num 180\n",
            "loss 469.9087829589844 average time 0.0026231685050004216 iter num 200\n",
            "loss 20.422197341918945 average time 0.002609388440910979 iter num 220\n",
            "loss 8.644209861755371 average time 0.0026176650708341262 iter num 240\n",
            "loss 8.406392097473145 average time 0.002620494973077187 iter num 260\n",
            "loss 134.54208374023438 average time 0.0026101099107149494 iter num 280\n",
            "loss 0.09983308613300323 average time 0.0026071059566682682 iter num 300\n",
            "loss 80.2156982421875 average time 0.002599020684377962 iter num 320\n",
            "loss 35.95478820800781 average time 0.0025926184235325255 iter num 340\n",
            "loss 185.21685791015625 average time 0.0025807224583376054 iter num 360\n",
            "loss 20.29424476623535 average time 0.0025741748605289678 iter num 380\n",
            "loss 64.4406967163086 average time 0.002571821397501708 iter num 400\n",
            "loss 49.16666030883789 average time 0.0025701752714299107 iter num 420\n",
            "loss 0.07288280129432678 average time 0.002563886315911904 iter num 440\n",
            "loss 130.01165771484375 average time 0.0025548091130457103 iter num 460\n",
            "loss 5.320098400115967 average time 0.0025620696375019255 iter num 480\n",
            "loss 4.4604363441467285 average time 0.002563488478002 iter num 500\n",
            "loss 0.6981538534164429 average time 0.0025620446557708806 iter num 520\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 26.009342193603516 average time 0.0025650342055567104 iter num 540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6fa96f42ba2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_BATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m           \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m           \u001b[0;31m#T_rand = cupy.random.rand(1)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0;31m#K_rand = cupy.random.rand(1)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/_creation/from_data.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(obj, dtype, copy, order, subok, ndmin)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061f506d-9f1e-4574-9704-35b163a85bfd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60fb14d0-93ec-4e6c-86eb-e0e8291e19fd"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f54ba7e-6a9b-4689-e5d4-1d41b33a1ee1"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "### Continue to train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T"
      },
      "source": [
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import NumbaOptionDataSet\n",
        "# timer = Timer(average=True)\n",
        "# #model = Net().cuda()\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# # dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "# dataset = NumbaOptionDataSet(max_len=500, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     y_pred = model(x)\n",
        "#     loss = loss_fn(y_pred[:,0], y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 20\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs=20)\n",
        "\n",
        "# model_save_name = 'checkpoint15.pth'\n",
        "# path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885f9d9d-64e7-402b-d53d-cabd72362b1a"
      },
      "source": [
        "import torch\n",
        "# inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "# model(inputs.float())\n",
        "inputs = torch.tensor([[1, 110.0, 200.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[95.0584]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c968ca43-41ff-4c91-d3ba-c9c98ba17a09"
      },
      "source": [
        "inputs = torch.tensor([[1, 110.0, 200.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.5589, -0.9269,  0.9870, 11.2374,  0.8945, 97.3009]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwgeVDsA_Mr"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "3acb9eba-4bb0-4d4b-8f40-14366838815b"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    # inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f877abd68d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "VGk5Hw64fMdh",
        "outputId": "0f553496-d815-4c2c-ce18-7e836b408df0"
      },
      "source": [
        "## Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-48b648c657f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdeltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_delta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prices'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-48b648c657f9>\u001b[0m in \u001b[0;36mcompute_delta\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minputs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# normalize the parameter to range [0-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (18) must match the size of tensor b (6) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLStvS2qCSjm"
      },
      "source": [
        "compute_delta(110)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1I8COnUxnz"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySrey9KzB0AF"
      },
      "source": [
        "compute_delta(110).item()  # It's not 0.5!! SOS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMyME_1WCGZz"
      },
      "source": [
        "compute_delta(102).item() # Close to 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB8LPwfBMHQ"
      },
      "source": [
        "# Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoaOyCDxQy0"
      },
      "source": [
        "##Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05])]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsXgOwWZ_ru"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lca2xrBh9a"
      },
      "source": [
        "# Vega"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muozc-hzhSGA"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05]]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KATxBCAdlFt"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a timev\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05]*1]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 0.0, 110.0, sigma, 0.1, 0.05]]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEiAredqQGxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}