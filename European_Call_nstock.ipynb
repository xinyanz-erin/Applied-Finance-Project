{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "European_Call_nstock.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "TY_9g3tbdLiY",
        "u2_89jOknwjH",
        "rXT4Bg0wdL7l"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/European_Call_nstock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "4b79cc3c-c6fd-4300-d22f-b5e90e686cdc"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0  10675      0 --:--:-- --:--:-- --:--:-- 10675\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "### Train(Erin Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxT9Eida-c_"
      },
      "source": [
        "# ################################# TEST ########################################\n",
        "# %%writefile cupy_dataset.py\n",
        "\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import random\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "#         paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           X = cupy.array([])\n",
        "#           K_rand = cupy.random.rand(1)[0]\n",
        "#           B_rand = cupy.random.rand(1)[0]\n",
        "#           r_rand = cupy.random.rand(1)[0]\n",
        "#           for i in range(0,self.N_STOCKS):\n",
        "#             X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "#           X = X.reshape(self.N_STOCKS,6)\n",
        "#           X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "#           #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "#           #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "#           # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#           #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "#           # make sure the Barrier is smaller than the Strike price\n",
        "#           # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#           for i in range(self.N_STOCKS):\n",
        "#             paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "#           stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "#           rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "#           #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "#           #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "#           #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#           stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "#           cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "#           num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "#           randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                                         num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "#           b1_r = randoms_gpu[:,0]\n",
        "#           b2_r = randoms_gpu[:,1]\n",
        "#           randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "#           for i in range(interval):\n",
        "#             if i % 2 == 0:\n",
        "#                 ind = int(i/2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "#             else:\n",
        "#                 ind = int(i//2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "#           randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "#           o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "#           Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# # ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# # for i in ds:\n",
        "# #     print(i[0])\n",
        "# ################################# TEST ########################################"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dZnWTTfbf1"
      },
      "source": [
        "### Train (European Call option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeREuPw0fguQ",
        "outputId": "770a32c7-c60e-4155-d503-e3e659b5a39e"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def European_call_option(d_s, T, K, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCK): #N_BATCH is number of stocks\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    for i in range(ii, N_PATHS * N_STOCK, stride):\n",
        "        stock_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        # h = T[stock_id] / N_STEPS\n",
        "        #tmp1 = mu[stock_id]*T[stock_id]/N_STEPS\n",
        "        tmp1 = r[stock_id]*T[stock_id]/N_STEPS \n",
        "        tmp2 = math.exp(-r[stock_id]*T[stock_id]) # discount\n",
        "        tmp3 = math.sqrt(T[stock_id]/N_STEPS)\n",
        "        # running_average = 0.0\n",
        "        s_curr = S0[stock_id]\n",
        "        for n in range(N_STEPS):\n",
        "          s_curr += tmp1 * s_curr + sigma[stock_id]*s_curr*tmp3*d_normals[path_id + stock_id * N_PATHS + n * N_PATHS * N_STOCK] # stock price\n",
        "          #s_curr = s_curr * math.exp((r[batch_id] - (1/2)*sigma[batch_id]**2)*h + sigma[batch_id] * tmp3 * d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH])\n",
        "          # running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "          #if i==0 and batch_id == 2:\n",
        "          #    print(s_curr)\n",
        "          #if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "          #    break\n",
        "        # payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        # d_s[i] = tmp2 * payoff\n",
        "        d_s[i] = s_curr\n",
        "\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        #self.N_STEPS = 365\n",
        "        self.N_STEPS = 10000\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        #self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        #paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 5), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          X = cupy.array([])\n",
        "          #T_rand = cupy.random.rand(1)[0]\n",
        "          K_rand = cupy.random.rand(1)\n",
        "          #B_rand = cupy.random.rand(1)[0]\n",
        "          r_rand = cupy.random.rand(1)\n",
        "          for i in range(0, self.N_STOCKS):\n",
        "            #X =  cupy.concatenate((X, cupy.array([K_rand,B_rand]), cupy.random.rand(3), cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "            X = cupy.concatenate((X, cupy.array([1.0]), K_rand, cupy.random.rand(3), r_rand))\n",
        "          \n",
        "          X = X.reshape(self.N_STOCKS, 6)\n",
        "          #X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #[T, K, S0, sigma, mu, r]\n",
        "          X = X * ((cupy.array([1, 150.0, 150.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "            #paras[op, i*5:(i+1)*5] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          if self.N_STOCKS != 1:\n",
        "            #stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "            stocks_randoms_cov = cupy.array([0.99] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "            \n",
        "            cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "            num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "            randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                          num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "            b1_r = randoms_gpu[:,0]\n",
        "            b2_r = randoms_gpu[:,1]\n",
        "            randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "            interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "            for i in range(interval):\n",
        "              if i % 2 == 0:\n",
        "                  ind = int(i/2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "              else:\n",
        "                  ind = int(i//2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "          if self.N_STOCKS == 1:\n",
        "            randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          \n",
        "          European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "\n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "\n",
        "          K_test = K_rand * 150\n",
        "          r_test = r_rand * 0.2\n",
        "          test = o.mean(axis = 0)\n",
        "          test = (np.maximum(0, test - K_test) * np.exp(-1 * r_test)).mean()\n",
        "\n",
        "          Y[op] = test\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(2, number_path = 1000, batch = 1, seed = random.randint(0,100), stocks=3)\n",
        "# for i in ds:\n",
        "#     print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39039af-a736-4657-d726-9e67ddfd1ecf"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0, 0.1, 200.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1, 150.0, 150.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff98086-e46c-4127-ca98-a852830ed9e6"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "3458e207-d945-468f-f5a3-0d230a0ad8ce"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 1000, number_path = 1024, batch = 1, stocks = 3)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 400)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 3.7915592193603516 average time 0.4831615172000056 iter num 20\n",
            "loss 17.430482864379883 average time 0.24319467065001846 iter num 40\n",
            "loss 495.30535888671875 average time 0.16311048725002214 iter num 60\n",
            "loss 181.50157165527344 average time 0.12308183802501844 iter num 80\n",
            "loss 410.48291015625 average time 0.09909979569001962 iter num 100\n",
            "loss 343.9151916503906 average time 0.08305688894168194 iter num 120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-14d613cb6f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mnum_of_randoms_each_stock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n\u001b[0;32m--> 113\u001b[0;31m                                                           num_of_randoms_each_stock, dtype=cupy.float32)\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mb1_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandoms_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mb2_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandoms_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py\u001b[0m in \u001b[0;36mmultivariate_normal\u001b[0;34m(mean, cov, size, check_valid, tol, method, dtype)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     return rs.multivariate_normal(\n\u001b[0;32m--> 479\u001b[0;31m         mean, cov, size, check_valid, tol, method, dtype)\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py\u001b[0m in \u001b[0;36mmultivariate_normal\u001b[0;34m(self, mean, cov, size, check_valid, tol, method, dtype)\u001b[0m\n\u001b[1;32m    410\u001b[0m                                  dtype=dtype).reshape(-1, mean.shape[0])\n\u001b[1;32m    411\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecomp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "### Continue to train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T"
      },
      "source": [
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import NumbaOptionDataSet\n",
        "# timer = Timer(average=True)\n",
        "# #model = Net().cuda()\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# # dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "# dataset = NumbaOptionDataSet(max_len=500, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     y_pred = model(x)\n",
        "#     loss = loss_fn(y_pred[:,0], y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 20\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs=20)\n",
        "\n",
        "# model_save_name = 'checkpoint15.pth'\n",
        "# path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F"
      },
      "source": [
        "import torch\n",
        "# inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "# model(inputs.float())\n",
        "inputs = torch.tensor([[1, 110.0, 200.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu"
      },
      "source": [
        "inputs = torch.tensor([[1, 110.0, 200.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwgeVDsA_Mr"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    #inputs = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGk5Hw64fMdh"
      },
      "source": [
        "# ## Using Finite Difference, Change only 1 S0 at a time\n",
        "# %matplotlib inline\n",
        "# import numpy as np\n",
        "# def compute_delta(S):\n",
        "#     epsilon = 0.01\n",
        "#     inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "#     inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]]).cuda()\n",
        "#     delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "#     return delta\n",
        "# prices = np.arange(10, 200, 0.1)\n",
        "# deltas = []\n",
        "# for p in prices:\n",
        "#     deltas.append(compute_delta(p).item())\n",
        "# fig = pylab.plot(prices, deltas)\n",
        "# pylab.xlabel('prices')\n",
        "# pylab.ylabel('Delta')\n",
        "# fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLStvS2qCSjm"
      },
      "source": [
        "compute_delta(110)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1I8COnUxnz"
      },
      "source": [
        "# ##Using Finite Difference, Change 3 S0 at a time\n",
        "# %matplotlib inline\n",
        "# import numpy as np\n",
        "# def compute_delta(S):\n",
        "#     epsilon = 0.01\n",
        "#     inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "#     inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "#     delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "#     return delta\n",
        "# prices = np.arange(10, 200, 0.1)\n",
        "# deltas = []\n",
        "# for p in prices:\n",
        "#     deltas.append(compute_delta(p).item())\n",
        "# fig = pylab.plot(prices, deltas)\n",
        "# pylab.xlabel('prices')\n",
        "# pylab.ylabel('Delta')\n",
        "# fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySrey9KzB0AF"
      },
      "source": [
        "# compute_delta(110).item()  # It's not 0.5!! SOS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMyME_1WCGZz"
      },
      "source": [
        "# compute_delta(102).item() # Close to 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB8LPwfBMHQ"
      },
      "source": [
        "# Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoaOyCDxQy0"
      },
      "source": [
        "# ##Using Finite Difference, Change only 1 S0 at a time\n",
        "# %matplotlib inline\n",
        "# import numpy as np\n",
        "# def compute_gamma(S):\n",
        "#     epsilon = 0.5\n",
        "#     inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "#     inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "#     inputs3 = torch.tensor([[1, 110.02, S - epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "#     gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "#     return gamma\n",
        "# prices = np.arange(10, 200, 0.1)\n",
        "# gammas = []\n",
        "# for p in prices:\n",
        "#     gammas.append(compute_gamma(p).item())\n",
        "# fig = pylab.plot(prices, gammas)\n",
        "# pylab.xlabel('prices')\n",
        "# pylab.ylabel('Gamma')\n",
        "# fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsXgOwWZ_ru"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs3 = torch.tensor([[1, 110.0, S - epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lca2xrBh9a"
      },
      "source": [
        "# Vega"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muozc-hzhSGA"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S, 0.35 + epsilon, 0.1, 0.05]*3]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KATxBCAdlFt"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a timev\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S, 0.35 + epsilon, 0.1, 0.05]*3]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[1, 110.0, 110.0, sigma, 0.1, 0.05]*3]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEiAredqQGxf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "a.reshape((3, 3)).mean(axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV7e9vUvBSl2"
      },
      "source": [
        "a.reshape((3, 3)).mean(axis = 0) - 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRm0_CwyF-rv"
      },
      "source": [
        "(np.maximum(a.reshape((3, 3)).mean(axis = 0) - 5,0)).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZdCHkmWGRzd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}