{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Combined_v4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/Combined_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "3df751f0-fb77-4f55-c35a-d63634ed2072"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0  20519      0 --:--:-- --:--:-- --:--:-- 20519\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9MB 51kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 34.5MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. \n",
        "\n",
        "Loading all the necessary libraries:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6no5JzH-B6"
      },
      "source": [
        "# !pip install cupy-cuda101"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbkx3hXWnwi8"
      },
      "source": [
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBN3YFOnwi-"
      },
      "source": [
        "The CuPy version of batched barrier option pricing simulation is as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhj4DtLnwi-"
      },
      "source": [
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRjmX5zcnwi_"
      },
      "source": [
        "Note, the parameters (K, B, S0, sigma, mu, r) are passed in as an array with length of batch size. The output array is a two dimensional array flatten to 1-D. The first dimension is for Batch and the second dimension is for Path. \n",
        "\n",
        "Testing it out by entering two sets of option parameters:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4PMo7Inwi_"
      },
      "source": [
        "# N_PATHS = 2048000\n",
        "# N_STEPS = 365\n",
        "# N_BATCH = 2\n",
        "# T = 1.0\n",
        "\n",
        "# K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
        "# B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
        "# S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
        "# sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
        "# mu = cupy.array([0.15, 0.1], dtype=cupy.float32)\n",
        "# r =cupy.array([0.05, 0.05], dtype=cupy.float32)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpWK3wcEnwjA"
      },
      "source": [
        "Put everything into a simple function to launch this GPU kernel. The option prices for each batch is the average of the corresponding path terminal values. This can be computed easily by Cupy function `mean(axis=1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAb34NTnwjA"
      },
      "source": [
        "# def batch_run():\n",
        "#     number_of_threads = 256\n",
        "#     number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
        "#     randoms_gpu = cupy.random.normal(0, 1, N_BATCH*N_PATHS * N_STEPS, dtype=cupy.float32)\n",
        "#     output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     s = time.time()\n",
        "#     cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
        "#                        (output, np.float32(T), K, B, S0, sigma, mu, r,\n",
        "#                         randoms_gpu, N_STEPS, N_PATHS, N_BATCH))\n",
        "#     v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     e = time.time()\n",
        "#     print('time', e-s, 'v',v)\n",
        "# batch_run()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRgQCelnwjC"
      },
      "source": [
        "This produces the option prices $21.22$ and $0.848$ for these two sets of option parameters in $66ms$.\n",
        "\n",
        "It works efficiently hence we will construct an `OptionDataSet` class to wrap the above code so we can use it in Pytorch. For every `next` element, it generates uniform distributed random option parameters in the specified range, launches the GPU kernel to compute the option prices, convert the CuPy array to Pytorch tensors with zero copy via the DLPack. Note how we implemented the iterable Dataset interface:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1KUra7ZnwjC"
      },
      "source": [
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo46Vf4XnwjD"
      },
      "source": [
        "Put everything related to Pytorch dataset into a file `cupy_dataset.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQUGMBlnwjE"
      },
      "source": [
        "# #%%writefile cupy_dataset.py \n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "# cupy.cuda.set_allocator(None)\n",
        "\n",
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')\n",
        "\n",
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPAsh7JnwjF"
      },
      "source": [
        "Here is a test code to sample 10 data points with batch size 16:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLKxMF05nwjF"
      },
      "source": [
        "# from cupy_dataset import OptionDataSet\n",
        "# ds = OptionDataSet(10, number_path=100000, batch=16, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlzRTD0nwjG"
      },
      "source": [
        "We can implement the same code by using Numba to accelerate the calculation in GPU:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IsfSwVwnwjG"
      },
      "source": [
        "# import numba\n",
        "# from numba import cuda\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average)\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]:\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                               X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
        "#         o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
        "#         Y = o.mean(axis = 1) \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=1, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "597c8262-48b0-49bc-d259-7fbe90bb20ec"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "\n",
        "          X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          stocks_randoms_cov = cupy.array([0.9] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i[0])\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBOv_RiBsCWa"
      },
      "source": [
        "### PUI TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BME87CgGsFrd"
      },
      "source": [
        "# %%writefile cupy_dataset.py\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n",
        "\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp2 = math.exp(-r*T)\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)    \n",
        "\n",
        "#     for i in range(ii, N_PATHS, stride): # for each path          \n",
        "#         running_average = 0.0\n",
        "\n",
        "#         for j in range(N_STOCKS): # initialize S0\n",
        "#             s_curr[j] = S0[j]\n",
        "\n",
        "#         for n in range(N_STEPS): # for each step\n",
        "#             s_curr_avg = 0.0\n",
        "\n",
        "#             for j in range(N_STOCKS): # for each stock\n",
        "#                 tmp1 = mu[j]*T/N_STEPS  \n",
        "#                 s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n",
        "#                 s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n",
        "\n",
        "#             # add stock average to running average\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n",
        "\n",
        "#             # compare to barrier\n",
        "#             if running_average <= B:\n",
        "#                 break\n",
        "\n",
        "#         payoff = running_average - K if running_average > K else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "    \n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, number_stocks = 3, batch=1, threads=512, seed=15, T=1):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_STOCKS = number_stocks\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(T)\n",
        "#         self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "\n",
        "#         ############ <new\n",
        "#         self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n",
        "#         self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#         cupy.fill_diagonal(self.Z_cov, 1)\n",
        "#         ############ new>\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "\n",
        "#         X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "\n",
        "#         for i in range(self.N_BATCH): # for each batch\n",
        "#           self.S0 = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 200\n",
        "#           self.K = 110.0\n",
        "#           self.B = 100.0\n",
        "#           self.sigma = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.mu = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.r = 0.05\n",
        "#           self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n",
        "\n",
        "#           ############ <new - add correlation between stocks\n",
        "#           all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "          \n",
        "#           single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n",
        "#                                                                                     self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n",
        "#           Y[i] = self.output.mean()\n",
        "\n",
        "#           ############ <new - combine to get X matrix\n",
        "#           X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n",
        "#                                 self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "        \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "# ds = NumbaOptionDataSet(max_len=10, number_path=100, batch=2, seed=15)\n",
        "# for i in ds:\n",
        "#   print(i)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQt8PqinwjI"
      },
      "source": [
        "# %%writefile model.py\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6, hidden)\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0,\n",
        "#                                            198.0,\n",
        "#                                            200.0,\n",
        "#                                            0.4,\n",
        "#                                            0.2,\n",
        "#                                            0.2,]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c0adde9-7684-4dff-c84c-6a05c9c5f6ea"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm', torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2] * 3))\n",
        "        # self.register_buffer('norm',\n",
        "        #                      torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37160741-79bc-4a06-a383-0b9a556d4a00"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/c3/f472843797b5ccbb2f0e806a6927f52c7c9522bfcea8e7e881d39258368b/pytorch_ignite-0.4.5-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 32.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 34.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 33.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 34.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 35.2MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 34.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102kB 36.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 36.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 36.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133kB 36.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 36.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 36.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163kB 36.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 36.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 36.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 36.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204kB 36.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 36.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd84a6b0-8fdf-4e86-8f92-b521067763fb"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=8, stocks=3)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=8, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "        \n",
        "trainer.run(dataset, max_epochs=200)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 590.8401489257812 average time 0.004782324280058674\n",
            "loss 334.1505126953125 average time 0.004460761530062882\n",
            "loss 90.54058837890625 average time 0.004480178509984398\n",
            "loss 54.583946228027344 average time 0.004571527889929712\n",
            "loss 72.48673248291016 average time 0.004529784430033033\n",
            "loss 126.2845458984375 average time 0.004614241760045843\n",
            "loss 91.54299926757812 average time 0.004554969490109215\n",
            "loss 115.40937805175781 average time 0.004568955019894929\n",
            "loss 68.8909912109375 average time 0.004567070900102408\n",
            "loss 178.3678436279297 average time 0.0044716037999023685\n",
            "loss 202.39382934570312 average time 0.004497249119976914\n",
            "loss 164.86068725585938 average time 0.0045169095399796785\n",
            "loss 177.78305053710938 average time 0.0045126262200028576\n",
            "loss 256.6129150390625 average time 0.004481731099931494\n",
            "loss 186.39292907714844 average time 0.0044911448599123105\n",
            "loss 29.938316345214844 average time 0.004526414429965371\n",
            "loss 97.6983413696289 average time 0.004482787680008187\n",
            "loss 115.25798034667969 average time 0.004580975239878171\n",
            "loss 81.28971099853516 average time 0.004400921289998223\n",
            "loss 55.937957763671875 average time 0.004464236269905086\n",
            "loss 210.0181884765625 average time 0.0044503419300235695\n",
            "loss 79.74861145019531 average time 0.004617980929979239\n",
            "loss 192.319091796875 average time 0.0044856817299296385\n",
            "loss 78.22927856445312 average time 0.004461959289947117\n",
            "loss 90.25914764404297 average time 0.00449686407993795\n",
            "loss 318.2582702636719 average time 0.004468468310042226\n",
            "loss 125.90425109863281 average time 0.004450184379911661\n",
            "loss 312.6569519042969 average time 0.004482970370045223\n",
            "loss 156.70828247070312 average time 0.004496863540152845\n",
            "loss 37.67713165283203 average time 0.004549050780024117\n",
            "loss 101.4663314819336 average time 0.004454268629961007\n",
            "loss 73.50344848632812 average time 0.0044841518700195595\n",
            "loss 49.983558654785156 average time 0.004465988670181105\n",
            "loss 165.95269775390625 average time 0.0044935192100092535\n",
            "loss 131.94107055664062 average time 0.004524246149885585\n",
            "loss 80.17240142822266 average time 0.004473446429910837\n",
            "loss 159.6156463623047 average time 0.0044873463198746326\n",
            "loss 176.4658966064453 average time 0.004462576970126975\n",
            "loss 73.92588806152344 average time 0.0044910146300389895\n",
            "loss 158.57827758789062 average time 0.004501865320089564\n",
            "loss 147.6647491455078 average time 0.004482552699955704\n",
            "loss 123.1134033203125 average time 0.004454311599893117\n",
            "loss 63.42932891845703 average time 0.0044904728900110056\n",
            "loss 65.93623352050781 average time 0.004539332019976428\n",
            "loss 248.88677978515625 average time 0.004426373619935475\n",
            "loss 98.01756286621094 average time 0.00448347510997337\n",
            "loss 60.01118087768555 average time 0.004409096919880539\n",
            "loss 77.28255462646484 average time 0.004463318519974564\n",
            "loss 53.031707763671875 average time 0.0044777553399944735\n",
            "loss 96.02656555175781 average time 0.004517879460072436\n",
            "loss 44.36266326904297 average time 0.004457932350032934\n",
            "loss 100.084228515625 average time 0.004415381799935858\n",
            "loss 63.954612731933594 average time 0.004517003039945848\n",
            "loss 22.71973991394043 average time 0.004509897389889375\n",
            "loss 39.182945251464844 average time 0.0044559398898854855\n",
            "loss 20.281375885009766 average time 0.00446891637004228\n",
            "loss 12.101364135742188 average time 0.0044526995999149225\n",
            "loss 14.132634162902832 average time 0.004458161460042902\n",
            "loss 29.83688735961914 average time 0.004460420509985852\n",
            "loss 6.513598442077637 average time 0.0045766348400320565\n",
            "loss 8.62523078918457 average time 0.0044383320099586855\n",
            "loss 7.174614906311035 average time 0.004413991520068521\n",
            "loss 4.699916839599609 average time 0.004481907789995603\n",
            "loss 5.672478675842285 average time 0.004435973639974691\n",
            "loss 5.568362236022949 average time 0.004494415989920526\n",
            "loss 21.1680908203125 average time 0.004546928649888287\n",
            "loss 14.750200271606445 average time 0.00445025896997322\n",
            "loss 9.496002197265625 average time 0.00447859289994085\n",
            "loss 5.937029838562012 average time 0.0044269732501015825\n",
            "loss 6.618582248687744 average time 0.004423216310005956\n",
            "loss 3.6222383975982666 average time 0.0044031577400528476\n",
            "loss 5.429704189300537 average time 0.004410601269955805\n",
            "loss 3.2495718002319336 average time 0.004472480769982213\n",
            "loss 5.200623512268066 average time 0.004516217060063355\n",
            "loss 6.914062976837158 average time 0.004476604030114686\n",
            "loss 6.883407115936279 average time 0.004382893619949754\n",
            "loss 5.526799201965332 average time 0.0044228334000581526\n",
            "loss 5.752074718475342 average time 0.004410269049840281\n",
            "loss 3.5826354026794434 average time 0.004481012419946637\n",
            "loss 17.8321590423584 average time 0.004521921139912593\n",
            "loss 15.26290512084961 average time 0.004486082120001811\n",
            "loss 5.269079208374023 average time 0.004437212430093496\n",
            "loss 9.209016799926758 average time 0.004430094889903558\n",
            "loss 0.7623555660247803 average time 0.004434403129980637\n",
            "loss 4.4942426681518555 average time 0.004481698070085258\n",
            "loss 4.409201622009277 average time 0.004515855989993724\n",
            "loss 4.661934852600098 average time 0.004488554399977147\n",
            "loss 3.556830644607544 average time 0.004410899500089727\n",
            "loss 1.7687387466430664 average time 0.0044746088099964255\n",
            "loss 11.851253509521484 average time 0.004567418919887132\n",
            "loss 6.981225967407227 average time 0.0044188223000310245\n",
            "loss 6.388467788696289 average time 0.004497856209964084\n",
            "loss 9.469646453857422 average time 0.004430376829986926\n",
            "loss 3.0173678398132324 average time 0.004517701810127619\n",
            "loss 3.7650845050811768 average time 0.004411391819903656\n",
            "loss 4.877001762390137 average time 0.004549984579971351\n",
            "loss 12.527884483337402 average time 0.0044549663499674355\n",
            "loss 6.656830310821533 average time 0.004577536390006571\n",
            "loss 8.39909553527832 average time 0.0046929899600945645\n",
            "loss 5.105627059936523 average time 0.004832193349939189\n",
            "loss 10.984681129455566 average time 0.0046895836999647144\n",
            "loss 19.729637145996094 average time 0.004457744230076059\n",
            "loss 6.313597202301025 average time 0.00443207001000701\n",
            "loss 5.080733299255371 average time 0.004459447859899228\n",
            "loss 4.169828414916992 average time 0.004496282049894944\n",
            "loss 5.9753899574279785 average time 0.004502649809874129\n",
            "loss 3.9798009395599365 average time 0.004459370320018934\n",
            "loss 4.506801128387451 average time 0.004525527110126859\n",
            "loss 0.6863425970077515 average time 0.00441619045985135\n",
            "loss 3.608466386795044 average time 0.004445098049982335\n",
            "loss 8.660871505737305 average time 0.004460130689967628\n",
            "loss 2.8087995052337646 average time 0.004472713429895521\n",
            "loss 5.874791145324707 average time 0.004478425579964096\n",
            "loss 6.816969871520996 average time 0.004506068690006942\n",
            "loss 6.193079471588135 average time 0.004496269580013177\n",
            "loss 2.218540668487549 average time 0.004500981219971436\n",
            "loss 7.99827766418457 average time 0.004414642450028623\n",
            "loss 5.4102373123168945 average time 0.004450897199931205\n",
            "loss 4.108022689819336 average time 0.004404687750065932\n",
            "loss 6.374018669128418 average time 0.0044794293300583375\n",
            "loss 2.417579174041748 average time 0.0044165086700195385\n",
            "loss 2.333399772644043 average time 0.004462540909826202\n",
            "loss 0.4159998893737793 average time 0.004454025979975995\n",
            "loss 4.18670654296875 average time 0.004438548420021107\n",
            "loss 6.89142370223999 average time 0.004465298909981357\n",
            "loss 3.5222063064575195 average time 0.0044453008900200075\n",
            "loss 3.9194693565368652 average time 0.004442485910058167\n",
            "loss 3.7942068576812744 average time 0.004469325000009121\n",
            "loss 3.6235077381134033 average time 0.0044961350800258515\n",
            "loss 3.0212290287017822 average time 0.00444336331984232\n",
            "loss 3.634793758392334 average time 0.004493507660044998\n",
            "loss 4.888126373291016 average time 0.004424293720003334\n",
            "loss 5.0505170822143555 average time 0.004493595709882356\n",
            "loss 2.39052414894104 average time 0.004421350010070455\n",
            "loss 5.476199150085449 average time 0.004447265689996129\n",
            "loss 3.0933825969696045 average time 0.004477248300008796\n",
            "loss 7.816922187805176 average time 0.004454089789978752\n",
            "loss 4.275140762329102 average time 0.00445065503983642\n",
            "loss 7.685052871704102 average time 0.0045211236499199\n",
            "loss 7.084551811218262 average time 0.004489072379965364\n",
            "loss 4.0842437744140625 average time 0.0044371744798809235\n",
            "loss 2.2938232421875 average time 0.0044722071400065035\n",
            "loss 4.904199600219727 average time 0.004427112330049567\n",
            "loss 2.621089458465576 average time 0.004451870420016348\n",
            "loss 4.336874961853027 average time 0.0044408360700072085\n",
            "loss 1.9850070476531982 average time 0.004451305830043566\n",
            "loss 5.229660987854004 average time 0.004429168290153029\n",
            "loss 5.889471530914307 average time 0.004418978089943266\n",
            "loss 2.0362436771392822 average time 0.004452779779985577\n",
            "loss 5.502713203430176 average time 0.004456250589955743\n",
            "loss 3.748589277267456 average time 0.004489601200039033\n",
            "loss 2.631986141204834 average time 0.004490136179920228\n",
            "loss 8.340198516845703 average time 0.004507170490014687\n",
            "loss 6.023910045623779 average time 0.004436087280009815\n",
            "loss 7.086222171783447 average time 0.004528996019962506\n",
            "loss 2.9266090393066406 average time 0.0044718448901221565\n",
            "loss 3.7376058101654053 average time 0.004448411390076217\n",
            "loss 4.373776435852051 average time 0.004428017449918116\n",
            "loss 4.959168434143066 average time 0.004494750340018072\n",
            "loss 5.513472557067871 average time 0.004492809509847575\n",
            "loss 1.3958460092544556 average time 0.00451420681005402\n",
            "loss 9.903064727783203 average time 0.004696776870096073\n",
            "loss 3.275423526763916 average time 0.004724062760087691\n",
            "loss 5.632737159729004 average time 0.004845072859952779\n",
            "loss 2.0140461921691895 average time 0.004864810810067866\n",
            "loss 3.9463353157043457 average time 0.004897347269979946\n",
            "loss 8.533442497253418 average time 0.004850647320126882\n",
            "loss 5.547643661499023 average time 0.0047762039299595925\n",
            "loss 2.4100241661071777 average time 0.004735181250052847\n",
            "loss 2.629763126373291 average time 0.0046911887501119055\n",
            "loss 6.082484245300293 average time 0.004620896680062287\n",
            "loss 2.3399267196655273 average time 0.004780287209905509\n",
            "loss 0.9448771476745605 average time 0.004448579439995228\n",
            "loss 0.7904335856437683 average time 0.004435800660030509\n",
            "loss 1.892425775527954 average time 0.004690772649955761\n",
            "loss 6.319373607635498 average time 0.004503446550006629\n",
            "loss 5.8075056076049805 average time 0.0044751001100485155\n",
            "loss 5.710038185119629 average time 0.004555005370038998\n",
            "loss 1.0703030824661255 average time 0.004531640850054828\n",
            "loss 0.8507652878761292 average time 0.00455922440996801\n",
            "loss 4.114430904388428 average time 0.004481309880084154\n",
            "loss 4.572117805480957 average time 0.0045898942300118505\n",
            "loss 4.209930896759033 average time 0.004733196279939875\n",
            "loss 5.654213905334473 average time 0.004599350910120847\n",
            "loss 6.221920967102051 average time 0.004548466830001417\n",
            "loss 5.30130672454834 average time 0.004470222060081142\n",
            "loss 5.57475471496582 average time 0.004430994469948928\n",
            "loss 1.115776538848877 average time 0.004490507400005299\n",
            "loss 5.863517761230469 average time 0.004510714769930928\n",
            "loss 2.749934673309326 average time 0.004458428629968694\n",
            "loss 3.474310874938965 average time 0.004468433319998439\n",
            "loss 5.041614532470703 average time 0.004573374489937123\n",
            "loss 5.341259479522705 average time 0.004466212239840388\n",
            "loss 6.21877384185791 average time 0.004451097000091977\n",
            "loss 11.468637466430664 average time 0.004471263510076824\n",
            "loss 3.897812843322754 average time 0.00453002104002735\n",
            "loss 3.101701259613037 average time 0.004492063669949857\n",
            "loss 3.3870928287506104 average time 0.0045234466000692915\n",
            "loss 3.7563517093658447 average time 0.004563584029929188\n",
            "loss 3.4892146587371826 average time 0.005201803019990621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 20000\n",
              "\tepoch: 200\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 200\n",
              "\toutput: 3.4892146587371826\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d52b40f-e647-47e0-a9ff-972870b91080"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19.0088]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d7cb24-d3be-4969-ef63-c34328e995e1"
      },
      "source": [
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7027e-01,  5.3980e-03,  1.9017e-01,  2.0769e-01,  1.2806e+01,\n",
              "         -9.0995e+00, -1.9662e-01, -2.3643e-03,  2.1692e-01,  3.1043e+00,\n",
              "          2.9340e+00, -1.0467e+01, -1.9962e-01, -2.6707e-03,  2.1892e-01,\n",
              "          1.3108e+00,  1.3673e+01, -2.3803e+00]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5e168ccf-441e-4bab-d17f-9de0a2fdfedf"
      },
      "source": [
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, S, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 300, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7bec30ded0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec4e777-821e-4f60-edca-1a66eb81cf62"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-3.7895e-03,  8.2762e-05,  3.8869e-03,  7.2587e-03,  2.5312e-01,\n",
              "          -3.0191e-01,  2.4846e-04, -5.9243e-05, -2.5012e-04, -9.2791e-04,\n",
              "          -3.8586e-02, -9.8783e-03,  9.0334e-05, -2.6016e-04, -1.1755e-04,\n",
              "          -2.8810e-03,  1.4524e-02,  2.7061e-03]], device='cuda:0'),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "6607481b-52b1-44d7-a39c-1c3a62d98643"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, S, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7bd3c24350>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxc9Xnv8c+jXV5keZE3yRteMDaYzZgtkCZQtiymCQSThEJDQ9NCS9KmuVBuSUovtxfakiYNTUMCCSQkhjibQ0kctpBAsLHxgldhecGSLUuyZC3Wvjz3jzmS5bFkSVhnzkj+vl8vvXzmN+eceWYs6dFvN3dHRESkv1KiDkBERIYWJQ4RERkQJQ4RERkQJQ4RERkQJQ4RERmQtKgDSIQJEyb4zJkzow5DRGRIeeuttw65e158+SmROGbOnMm6deuiDkNEZEgxs3d7KldTlYiIDIgSh4iIDIgSh4iIDIgSh4iIDIgSh4iIDIgSh4iIDIgSh4iIDIgSh8gQ8YeiQxSVH4k6DBElDpGh4q9/tIHHfrcr6jBElDhEhoKq+hYq61toa9fGaxI9JQ6RIWB3hZqoJHkocYgMAbsr6gFQfUOSgRKHyBCwK6hxuCt1SPSUOESGgK7EEXEcIqDEITIkdDZViSQDJQ6RJNfS1sG7VQ0AqKVKkoESh0iS21fVQHtHLGMob0gyUOIQSXK7ug3FVee4JAMlDpEk15k4Jo7OjDgSkRglDpEkt7uinomjMxmVlaamKkkKoSYOM7vGzArNrMjM7unh+UwzeyZ4fo2ZzQzKx5vZK2Z2xMy+0e38EWb2P2a2w8y2mtn/CzN+kWSwq+IIs/NGYaBODkkKoSUOM0sFHgWuBRYAN5vZgrjTbgcOu/sc4KvAQ0F5E/CPwBd7uPW/uft84FzgUjO7Noz4RZKBu7NhXzWn5Y3EzKIORwQIt8axBChy993u3gIsB5bGnbMUeDI4XgFcYWbm7vXu/hqxBNLF3Rvc/ZXguAVYDxSE+B5EIvXCtjIAMtJiP6quKockgTATRz5Q3O1xSVDW4znu3gbUAOP7c3MzywU+ArzUy/N3mNk6M1tXUVExwNBFksPOYP+NP7tkFobmcUhyGJKd42aWBvwI+Lq77+7pHHd/zN0Xu/vivLy8xAYoMkiKyo8wZUwW08ePwEyJQ5JDmIljPzCt2+OCoKzHc4JkMAao7Me9HwN2uvt/DEKcIklrx8E6Tp88GgBDfRySHMJMHGuBuWY2y8wygGXAyrhzVgK3Bsc3AC97HzOczOz/EEswnx/keEWSSlt7B7vKj3D6pNFdZerjkGSQFtaN3b3NzO4CVgGpwBPuvtXMHgDWuftK4HHg+2ZWBFQRSy4AmNleIAfIMLPrgauAWuA+YAewPhhl8g13/05Y70MkKnsr62lp7zha41BTlSSJ0BIHgLs/DzwfV3Z/t+Mm4MZerp3Zy21VX5dTwo6DdQDM61bjEEkGQ7JzXORU8M7BOlJTjDkTRwFgZmqokqSgxCGSpHYcrGPm+BFkpad2lampSpKBEodIknqn7OiIKuhso1XmkOgpcYgkoYaWNt6tauD0STldZVpxRJKFEodIEtpZdgR3OH3yqGPK1VQlyUCJQyQJFZbFRlSdPvnYGofyhiQDJQ6RJFR4sI6s9BSmjxvRVaaZ45IslDhEklDhwTrmThxNasrRZBGbAKg6h0RPiUMkCRXGjajqpLQhyUCJQyTJVNW3UFHXfMwaVYCWVZekocQhkmQKD3Z2jMfVODQeV5KEEodIkik8WAvAfDVVSZJS4hBJMttL6xg7Ip280ZnHlMeaqpQ6JHpKHCJJZvvBWhZMzcHimqbUUiXJQolDJIm0tXdQeLCOM7pN/OukvCHJQolDJInsOVRPc1sHC6YenzhAo6okOShxiCSRbaWxjvEzpvRQ4zDT1rGSFJQ4RJLI9tI6MlJTmJ036rjn1FQlyUKJQySJbCutZc7EUWSk9fyjqaYqSQZKHCJJZHtpbY/NVNC5VlWCAxLpQaiJw8yuMbNCMysys3t6eD7TzJ4Jnl9jZjOD8vFm9oqZHTGzb8Rdc76ZbQ6u+brFj1kUGaIq6pqpqGvutWPcONrHsWZ3JXPve55lj71BRV1zIsMUCS9xmFkq8ChwLbAAuNnMFsSddjtw2N3nAF8FHgrKm4B/BL7Yw62/CXwWmBt8XTP40Ysk3vaujvHjZ4wDx3Ry/OMvttDa7qzeXcUvNu5PQHQiR4VZ41gCFLn7bndvAZYDS+POWQo8GRyvAK4wM3P3end/jVgC6WJmU4Acd1/tsSm0TwHXh/geRBKmM3Es6KWpCo42Vb1TdqSrrGDsiF7OFglHmIkjHyju9rgkKOvxHHdvA2qA8X3cs6SPewJgZneY2TozW1dRUTHA0EUSb/P+GvJzs8kdkdHj80ZsrSotOyJRG7ad4+7+mLsvdvfFeXl5UYcj0qdNJdWcMy231+c7e/OKqxqPKW/vUCKRxAozcewHpnV7XBCU9XiOmaUBY4DKPu5Z0Mc9RYacyiPNFFc1cva0MSc+0WFjSfUxRW0dHSFGJnK8MBPHWmCumc0yswxgGbAy7pyVwK3B8Q3Ay36Ceri7lwK1ZnZRMJrqT4FfDH7oIom1KUgGZxecoMYRjKp6u7iazLQUvrbsHADuXr6RbQdqExKnCISYOII+i7uAVcB24Fl332pmD5jZR4PTHgfGm1kR8LdA15BdM9sLPALcZmYl3UZk/RXwHaAI2AX8Kqz3IJIoG4trSDE4q6D3GkfnPI5NJdUsnJrDedPHdj33ny/vTESYIgCkhXlzd38eeD6u7P5ux03Ajb1cO7OX8nXAmYMXpUj0NhVXM2/SaEZk9P4jaQZtHc6Og7XcvGQ6aalHx+eemd9HE5fIIBq2neMiQ4W799kx3mlnWR1NrR2cXZBLasrRxJE7Ij3MEEWOocQhErF9VQ1UN7Rydh+JwzDqW9oBOHtaLqndFk3QCF1JpFCbqkSkbxuL++4Yh6PDcXOy0pg5fgTNbUdHUylvSCKpxiESsY3F1WSnpzJv0vFLqffk7Gm5mBlZ6amsvveKkKMTOZ4Sh0jENhVXc1b+GNJST/zj2Nwaq2F0r5l09XOorUoSSIlDJEKt7R1sOVDb98Q/YkuSACzqNmS3s/lKaUMSSYlDJEKFB+toaevos2McoLE11jF+4WlHl3PTngISBSUOkQhtCDrG+zMU92Pn5TNn4ijGZB8/9FYtVZJIGlUlEqFNxdVMGJVBfm52n+c+8olzjivTPmYSBdU4RCK0qbiaswtyTzoBaKl1SSQlDpGI1DW1UlRxpF/9G73pTDdKG5JIShwiEdlcUoM7J5c41FIlEVDiEInIxq6l1E9+gUK1VEkiKXGIRGRTcTWzJozsdavY/jANyJUIKHGIRGRTcc2g1DZAfRySWEocIhE4UN3Iwdqmk+rfALp6xzWqShJJiUMkAm/uqQJgyaxxJ3UfdY5LFJQ4RCKwZk8Vo7PSmD85J+pQRAZMiUMkAm/uqeSCmeOO2cXvveiax6GWKkkgJQ6RBDt0pJldFfUn3UwFWnJEohFq4jCza8ys0MyKzOyeHp7PNLNngufXmNnMbs/dG5QXmtnV3cq/YGZbzWyLmf3IzLLCfA8ig23tIPVvdOcaVyUJFFriMLNU4FHgWmABcLOZLYg77XbgsLvPAb4KPBRcuwBYBiwErgH+y8xSzSwf+BtgsbufCaQG54kMGWv2VJGdnsqZU09+KK7qGxKFMGscS4Aid9/t7i3AcmBp3DlLgSeD4xXAFRarey8Flrt7s7vvAYqC+0FsRd9sM0sDRgAHQnwPIoPuzT1VnDcjl4y0wfvxUx+HJFKYiSMfKO72uCQo6/Ecd28DaoDxvV3r7vuBfwP2AaVAjbv/JpToRUJQ09jK9oO1LJk5vu+T+0E7AEoUhlTnuJmNJVYbmQVMBUaa2ad7OfcOM1tnZusqKioSGaZIr956twr3wevf0JIjEoUwE8d+YFq3xwVBWY/nBE1PY4DKE1x7JbDH3SvcvRX4KXBJTy/u7o+5+2J3X5yXlzcIb0fk5K3ZU0V6qnHu9JOcMR5HTVWSSGEmjrXAXDObZWYZxDqxV8adsxK4NTi+AXjZY2snrASWBaOuZgFzgTeJNVFdZGYjgr6QK4DtIb4HkUH15p4qzi7IJSs9dVDud7SpSplDEie0xBH0WdwFrCL2y/1Zd99qZg+Y2UeD0x4HxptZEfC3wD3BtVuBZ4FtwK+BO9293d3XEOtEXw9sDuJ/LKz3IDKYGlra2FxSM6jDcEWiEOqe4+7+PPB8XNn93Y6bgBt7ufZB4MEeyr8MfHlwIxUJ34Z91bR1eCiJQ01VkkhDqnNcZChbs7uSFIPzZ4wdtHtq4rhEQYlDJEHW7Kli4dQxjM5KjzoUkZOixCGSAI0t7WworubCQW6m6hyOq/04JJGUOEQSYM2eSlraOrhs3uAODVdTlUSh353jZnYmsTWnuhYVdPenwghKZLj5/c5DZKSlDHqNo5MqHJJI/UocZvZl4I+IJY7niS1c+BqgxCHSD797p4ILZ40btPkbnVThkCj0t6nqBmKT7Q66+58BZxOb5S0ifSitaWRn+REumzshtNdQhUMSqb+Jo9HdO4A2M8sByjl2SRAR6cXvdx4C4PJB7t+Aoxs5qalKEqm/fRzrzCwX+DbwFnAEeCO0qESGkd/vPETe6ExOnzR60O+tpiqJQr8Sh7v/VXD432b2ayDH3d8OLyyR4aG9w3ltZwUfmD8x1G1etVaVJNJARlUtAmZ2XmNmc9z9pyHFJTIsbD1Qw+GGVi6fG84KzV2LHCpvSAL1d1TVE8AiYCvQERQ7sWXNRaQXnf0b7wupYzzMWoxIb/pb47jI3eP3CxeRPrz6TgULp+YwYVRmqK+jCockUn9HVb1hZkocIgNwpLmN9e8e5rKQmqlEotLfGsdTxJLHQaCZ2GAOd/dFoUUmMsSt3lVJW4dz+bzw5m90USeHJFB/E8fjwC3ENk/q6ONcEQFeLixnZEbqoC6j3hMzNVVJYvU3cVS4e/y2ryLSi44O56XtZVw+L4/MtMFdZiSeuscl0fqbODaY2Q+BXxJrqgJAw3FFerblQA1ltc1cecakhLyeWqokkfqbOLKJJYyrupVpOK5IL17cXk6KwQfmTwz9tcxMEwAlofo7c/zPwg5EZDh5cVsZ588Yy7iRGaG/lpqqJNH6OwFwFvDXdJs5DuDuHw0nLJGh60B1I9tKa7nn2vkJe001VUki9Xcex8+BvcB/Av/e7euEzOwaMys0syIzu6eH5zPN7Jng+TVmNrPbc/cG5YVmdnW38lwzW2FmO8xsu5ld3M/3IJIQL20vA0hY/4Ymj0ui9bePo8ndvz6QG5tZKvAo8MdACbDWzFa6+7Zup90OHHb3OWa2DHgIuCmYbLgMWAhMBV40s3nu3g58Dfi1u99gZhnAiIHEJRK2F7aXM2vCSGbnjUzYa6rCIYnU3xrH18zsy2Z2sZmd1/nVxzVLgCJ33+3uLcByYGncOUuBJ4PjFcAVFlt8Zymw3N2b3X0PUAQsMbMxwOXE5pXg7i3uXt3P9yASuiPNbazeVckVIa+G251haqqShOpvjeMsYhMAP8ixixx+8ATX5APF3R6XABf2do67t5lZDTA+KF8dd20+0AhUAN81s7OJ7Q1yt7vXx7+4md0B3AEwffr0vt+hyCB4tbCClvYOrkhQMxVwXO/4VV99lTPzx/DIJ85JXAxySulvjeNG4DR3f7+7fyD4OlHSCEsacB7wTXc/F6gHjus7AXD3x9x9sbsvzsvTWkGSGM+9fYAJozJZMmtcQl+3czhuyeEG3ik7wk/X70/o68uppb+JYwuQO8B77+fY7WULgrIezzGzNGL7mFee4NoSoMTd1wTlK4glEpHIHWlu4+Ud5Vx31mRSUxLXY939lX6x8QAA08ep60/C09/EkQvsMLNVZray86uPa9YCc81sVtCJvQyIv2YlcGtwfAPwsrt7UL4sGHU1C5gLvOnuB4FiMzs9uOYKYBsiSeCl7WU0t3Xw4UVTE//iDk2t7fxg9bsA5OdmJz4GOWX0t4/jywO9cdBncRewCkgFnnD3rWb2ALAuWPvqceD7ZlYEVBFLLgTnPUssKbQBdwYjqiA2n+TpIBntBjQ5UZLCLzeVMjkni8UhL2oYr3ORwxVvlVBa0wRAh3rLJUT9nTn+6nu5ubs/DzwfV3Z/t+MmYv0nPV37IPBgD+UbgcXvJR6RsNQ0tvK7dyq45eIZpCSwmQpio6oAfryumPmTRzMmO13DcyVU/WqqMrOLzGytmR0xsxYzazez2rCDExkqXthWRkt7Bx9eNCWS1y88WMemkhpuOL+AFDNcNQ4JUX/7OL4B3AzsJLbg4Z8Tm9wnIsAvNx2gYGw250wb6BiSk2cW26I2LcW4/tz8WNOV8oaEqL+JA3cvAlLdvd3dvwtcE15YIkPH4foWXi86xIcXTU3YpL/uOl/x/BljmTAqkxQz9XFIqPrbOd4QdEZvMrOHgVIGkHREhrNVWw/S1uGRNVPVt8TGjZw7PdYprx0BJWz9/eV/S3DuncQm3RUAHw8rKJGh5LWiQ0wZk8XCqTmRxnHD+flAbH+ODmUOCdEJaxxmthQocPdHg8evAhOJ/UHzBrE1pEROWa3tHbyxq5JL50yIpJkKYOyIdMaOyGDOxNFA0HSlpioJUV9NVV8imFsRyATOB0YB3yU2c1vklPXbwgoq61tYek4Ek/4CG+6/6pjHKYZqHBKqvhJHhrt3X6jwNXevAqrMLHFrRoskqRVvFTNhVAaXz0ue9dC0layEra8+jmOmwLr7Xd0eJs9PikgE3q2s54VtZVx/Tj7pqckzViTFoKOj7/NE3qu+vtvXmNln4wvN7C+AN8MJSWRoeOqNd0kx47ZLZ0YdShxTfUNC1VdT1ReAn5vZJ4H1Qdn5xPo6rg8zMJFk1tLWwc827OeqhZMoGJtcK9GmGJo5LqE6YeJw93LgEjP7ILFtXAH+x91fDj0ykST24vYyqupbuHHxtL5PTjDNHJew9XeRw5cBJQuRwDNri5kyJovL5yZfV59mjkvYkqdHT2SIOFDdyO92VnDj+QUJ3bCpvzRzXMKmxCEyQCveKsGdpGymgs6Z40dTx9Nr3uXnG7SVrAye/q5VJSJAR4fz7LpiLp0znmlJuj1rbOZ47HhfZQP3/WwLANefmx9ZTDK8qMYhMgCv7zpEyeFGbrpgetSh9Kqzj8PdueWJNQCclqf5ujJ4lDhEBuDJP+xl/MgMrl44KepQemXBkiPPvV3Ku5UNAOSNyow4KhlOlDhE+undynpe2lHOpy6cTmZaatTh9CrFjJa2Dh769Q4y01K4YOZYDc+VQaXEIdJPT/7hXVLN+NRFM6IO5YQMOFjbRMnhRr51y/lkpKVoeK4MqlATh5ldY2aFZlZkZvf08HymmT0TPL/GzGZ2e+7eoLzQzK6Ouy7VzDaY2XNhxi/S6UhzGz9eV8yHFk1hUk5W1OGcUOfy7udOz+X98/IwTm5ex30/28yvNpcOVngyDISWOMwsldi+5NcCC4CbzWxB3Gm3A4fdfQ7wVeCh4NoFxJZzX0hsi9r/Cu7X6W5ge1ixi8Rbsa6YuuY2brtkZtSh9KlzW5DPXzkPMzupeR1rdlfy9Jp93PfzLYMWnwx9YdY4lgBF7r7b3VuA5cDSuHOWAk8GxyuAKyz259JSYLm7N7v7HmIbRi0BMLMC4EPAd0KMXaRLa3sH3/79Hs6dntu1PWsye9+cCXxicQGXz50AdI6yGvh96ppa+fLKrV33FOkU5jyOfKD7Xh4lwIW9nePubWZWA4wPylfHXds5CP0/iG0wNfpEL25mdwB3AEyfnrxDJyX5/XzDfvZXN/LA0oV9n5wErj83/5g5GwNZ9LCptZ27l29gyazx/PNz27rKs9OTdzCAJN6Q6hw3sw8D5e7+Vl/nuvtj7r7Y3Rfn5SXfekIyNLR3ON/87S7OmJLDB+dPjDqc92Qga1f9/Yq3WbW1rCtpZKalkDc6UxtDyTHCTBz7ge5rMhQEZT2eY2ZpwBig8gTXXgp81Mz2Emv6+qCZ/SCM4EUAfrWllN2H6rnzA7Mj21P8ZJlZvzZ2+t07Ffxy04Gux/dddwZb/+lq0lJMw3nlGGEmjrXAXDObZWYZxDq7V8adsxK4NTi+AXjZY3XqlcCyYNTVLGAu8Ka73+vuBe4+M7jfy+7+6RDfg5zC3J1HX9nFaXkjufbMKVGH857FJgSe+Dd/TWMr9/8i1gH+5++bxeIZY/nzy2aRlpqCoUUT5Vih9XEEfRZ3AauAVOAJd99qZg8A69x9JfA48H0zKwKqiCUDgvOeBbYBbcCd7t4eVqwiPXl5RznbS2v51xsWJeUquP3Vn9C/+ONN7K1s4MnPLOH9cfunm6nGIccKdZFDd38eeD6u7P5ux03Ajb1c+yDw4Anu/Vvgt4MRp0g8d+frLxeRn5s95BcHPFEfx5b9NTzx2h5e2FbGnR+YfVzS6KQ+DulOq+OK9GDV1oNsKq7moY+fRXrqkBpDcpzehuO2dzgf/s/XAJgwKoO7PjC3x+tNbVUSZ2j/RIiEoK29g4dXFTJn4ig+fl5B1OGctN76OH6w+t2u468vO5fsjJ6H3GpjKImnGodInB+/VcLuinq+dcv5pA3x2gbEahzxeWPrgRoefH4775+Xx39/+vxekwaAYf2eByKnhqH/UyEyiBpb2vnqC+9w3vRcrlqQvEunD0R8jaOtvYO/fWYTudnpPPKJs0+YNDqvV9qQ7pQ4RLr57h/2UF7XzD3XnjFk523Ei69x/GhtMYVldTywdCHj+7FPh4FGVckx1FQlEjh0pJlv/nYXV8yfyJJZ46IOZ9B01jjaO5yXd5TzyG8KuXDWOK5eOLmf15tqHHIMJQ6RwL+tKqSxpZ17r5sfdSiDKsWM5rYOZv/D0ZHxDyw9s981qliNQ6lDjlJTlQiwYd9hnllXzG2XzGTOxBOunznkVDe0UlHX3PX44Y8v4vTJA3iP6uOQOKpxyCmvqbWdL/54E1Nysrj7yp7nMgxlsyeO5MXt8Jd/NJv/dc3Aa1MGyhxyDCUOOeU98sI77Kqo5/u3L2F0VnrU4Qy6v7/qdO76wJz3/N5ifRzKHHKUEoec0tbtreLbv9/Npy6czmVzh+fy+2mpKYw+ifkoGlUl8dTHIaesxpZYE1V+bjb3XndG1OEkrWEyKlkGkWoccsp6eNUO9lY28MPPXsioTP0onIhqHNKdahxySlq9u5Lvvr6XWy+ewSWztZ/2iRjq45BjKXHIKae+uY0vrXibGeNH8L+uHV5zNsJgphqHHEv1czmluDv3/WwzJYcbWH7HxYzI0I9AfyhvSHeqccgp5Zm1xfx84wE+f+W8YbWsSJi0A6DEU+KQU8b20lq+vHIrl82dwJ0fmBN1OENGbFCVMoccpcQhp4QjzW3c+fR6xmSn89WbzhnSe4gnmvo4JJ4aeGXYc3fu+cnb7K2s54efvYgJ/VhKXI7SfhwSL9Qah5ldY2aFZlZkZvf08HymmT0TPL/GzGZ2e+7eoLzQzK4OyqaZ2Stmts3MtprZ3WHGL8PDN14u4rm3S/ni1adz0Wnjow5nyNEOgBIvtMRhZqnAo8C1wALgZjNbEHfa7cBhd58DfBV4KLh2AbAMWAhcA/xXcL824O/cfQFwEXBnD/cU6fLc2wf49xfe4WPn5vOX758ddThDkmocEi/MGscSoMjdd7t7C7AcWBp3zlLgyeB4BXCFxTYJWAosd/dmd98DFAFL3L3U3dcDuHsdsB3ID/E9yBC2Yd9h/u7ZTSyeMZZ/+fhZw2ZHv0TTWlUSL8zEkQ8Ud3tcwvG/5LvOcfc2oAYY359rg2atc4E1gxizDBP7Khv47FPrmJSTxbduOZ/MtBPvqy0noB0AJc6QHFVlZqOAnwCfd/faXs65w8zWmdm6ioqKxAYokdpf3cjN315Na7vzxG2L+7WvtvROOwBKvDATx35gWrfHBUFZj+eYWRowBqg80bVmlk4saTzt7j/t7cXd/TF3X+zui/Pyhudy2XK8stomPvnt1dQ2tfKD2y8cdrv5RUEtfBIvzMSxFphrZrPMLINYZ/fKuHNWArcGxzcAL3vsT5uVwLJg1NUsYC7wZtD/8Tiw3d0fCTF2GYIq6pr55LdXc6iumSc/s4SzCsZEHdKwoD4OiRfaPA53bzOzu4BVQCrwhLtvNbMHgHXuvpJYEvi+mRUBVcSSC8F5zwLbiI2kutPd283sfcAtwGYz2xi81D+4+/NhvQ8ZGsprm7jl8Tc5UN3Ek59ZwnnTx0Yd0rChHQAlXqgTAINf6M/Hld3f7bgJuLGXax8EHowre43OFRBEAjvL6rjtu2s53NDC47cu1hpUg0w1DomnmeMypK3eXckdT60jMz2VZ//iYs7MV/PUYFMfh8RT4pAh6+cb9vOlFW8zffwIvnvbBUwbNyLqkIYt1TikOyUOGXLaO5yHV+3gW6/u5sJZ43jslsWMGZEedVjDVm87ADa0tJGdnqqJlacgJQ4ZUqrqW7h7+QZ+v/MQn75oOl/+yELSU4fkdKSho9vquLVNrby4rYzn3i7l5R3l/O8PncGfX3ZatPFJwilxyJCxqbiav3p6PRV1zfzLx87i5iXTow7plGDE1qqqqGvmpsfeYHdFPSMzYjPxSw439nhNeV0TG/ZVc7CmiT+9eIZqJcOMEockPXfn6TX7eOCX28gbncmKv7yYRQW5UYd1yjhQ00hxVSMXPPgiAPdcO5/bLpnJZQ+/QnNbO3B0ZvlrRYf42Yb9/HT90bm+588Yq0ELw4wShyS1yiPN3PPTzbywrYz3z8vjP246h7EjM6IO65SyZOZ4iqtKOCt/DP98/ZmcMy2WtDNSU/jRm8VMHzeSHQdr+cXGA13XLJ4xlvqWdraX1nKkuS2q0CUkShyStF4pLOdLK96mpqGV//2hM/jMpbNI0c59CfevNyzi325cdFxzU2dCeOjXO7rKrj9nKl/443nMGD+SjcXVXP/o6zS0KHEMN0ocknTK65r4P89tZ+WmA5w+aTRPfWYJZ0zJiTqsU1Zvyfq/PnUeKzceoKaxlVsvmcnFs4/dJGt0VuzXy0HE8uQAAA/ySURBVG8LK/jg/EmhxymJo8QhSaO9w/nhmnd5eFUhza0dfP7KuXzu/bPJSteS6Mno0jkTuHTOhF6fP23CSPJGZ/K7dypwd3WQDyNKHBI5d+e1okP8y/M72FZay/vmTOCBpQs5LW9U1KHJSTAzUs3YW9nAG7squeQESUaGFiUOidSGfYd5+NeFvLG7koKx2Xz95nP5yKIp+ut0mPjTS2bw8K8LqWpoiToUGURKHBKJTcXVfOOVIl7YVsb4kRl85SMLuPnC6dqpb5i5ZuFkHv51IW3tWrNkOFHikIRxd14vquSbrxbxelElOVlpfOHKedx+2SxGZepbcTjqnNXf0t4RcSQymPTTKqFram3nl5sO8OQbe9myv5aJozP5h+vmc/OS6YzO0hpTw1lGWixxtCpxDCtKHBKafZUN/GjtPpa/uY/DDa3MnTiK//snZ/Gx8/I1UuoU0VnjaG3roL3DKSo/wumTY9v5dnQ475TXkZ2eyriRGfojYghR4pBBdbCmiec3l/L85lLWvXuYFIM/XjCJWy+OjfNXp/epJT019v/9lV9uY19VI0+8vodRmWlcOmc8f9hVSV3T0cmBe/7lOn1/DBFKHHLSSmsa+dXmg13JAmD+5NF88ap5fOy8AqbmZkccoUSlszZRVd/CE6/vAWIzzn+/8xBXnDGJ2XkjeXrNPirqmvnJ+v3ccH5BxBFLf5ifAju0LF682NetWxd1GENeS1sHew7VM25kBvuq6nllRwWvFJaz9UAtEEsWHzprCtctmsJszcGQQEeHc+cP1/NOWR33XHsGZ0wZzbiRGYzIiP3d2tTazqKv/IaW9o7jlml3d7YeqKWtw7vWyJLEMbO33H3xceVKHNKThpY2tpfWsfVADVv217Blfy07y+to7TasMjXFOH/6WP5ofh5XL5ysZCHv2eaSGj7yjdc4b3ouiwpyaevooPJIS2xp9tomAFbfewWTx2RFHOmppbfEoaYqofJIM9tKa9l6oJZtB2rZVlrL7oojdAQ5YtzIDBZOzeHyeadR29RKe7tz2bwJXDYnTzvvyaA4q2AMi2eMZd27h1m/rxqAnKw0Lpk9gdb2Dl7aUU51Y4sSR5IINXGY2TXA14BU4Dvu/v/ins8EngLOByqBm9x9b/DcvcDtQDvwN+6+qj/3lGO1dzhltU3sr27kQHUjJYcbu47rmtooOdxAWW1z1/n5udksmJrDh86awpn5YzgzP4fJOVnqtJTQPXTDIspqmpg/JYe0VCMnGGX1yo5yXtpRTmNLe8QRSqfQEoeZpQKPAn8MlABrzWylu2/rdtrtwGF3n2Nmy4CHgJvMbAGwDFgITAVeNLN5wTV93fOU0d7hVNY3U1bTTFltE2V1TRysaWL/4UZKguRwsKaJto5jmyPHjcxg/MgMMtNTuHT2BBZMzYl9Tckhd4T2upBozM4b1WNzZ+fQbSWOnjW3tfO91/cyKiuNjNQU/umX2+hwJ9UMM3jzvisHffh7mDWOJUCRu+8GMLPlwFKg+y/5pcBXguMVwDcs9qftUmC5uzcDe8ysKLgf/bjnkOHuNLd1UN/cRk1jK7VNsX87v2qDr9hzrbhDhzsHa5spr22ivK6Z9rikkGIwOSeL/LHZLJ4xlqm52eSPzSY/N5uCsdlMzc3u6pQUGQo6l2f/5HfWcGZ+DjddMJ1bLpoRcVSJ09beQWu7s7+6gY3FNbz6TgVb9tdQ39xGY0s7dT1slHXBzLGclZ8bSyAh7GET5m+QfKC42+MS4MLeznH3NjOrAcYH5avjrs0Pjvu6JwBmdgdwB8D06e9tb+r1+w5T29hKc1tH7Ku1/ehxWzvNrd2O2zqCx+39Pr+lre/ZtBlpKYzJTic7PZWGlnbGjUxnUk4WcydOYFJOJpNysrq+JudkMWFUBmnBpCuR4WDh1Bzyc7PZX93Ilv21bNm/hQVTRjMpJ4t9lQ3srWxg6TlTGTkMlq1ZuekAj/ymkImjs0hJgVGZaazeXXXcLopnT8vlwlnjyM5IJTs9lcljsnjfnAk4kJ6SwrRx2aE2Lw/9T7oX7v4Y8BjERlW9l3t8acXbFJUfOeE5GWkpZKalkJmWGvs3vdtxWgo52eldx5lpqcHzR8/JSEthVGYaY7LTyckO/s1KDx6na4a1nPLMjNfv+SCH61vYUHyYz3xvHR//5hvHnDMqK42Pnj21X/frrOkn28/W1gM1/M2PNgAwMSeLxpZ2qhtauWT2eA7UxJLmPy9dyJULJjFlTLRzo8JMHPuBad0eFwRlPZ1TYmZpwBhineQnuravew6aRz5xNm0d3mtiyEhN0VamIgkydmQGH5w/iQeWLqS6oZXMtBQm5mTyhWc2cSSYgV7X1EqKGSMz02jvcA5UN7L1QC3bS2vZcbCWgzVNbD9YR0tbB4984mzK65r5w65KSqsbmZSTxYHqRqaNG0HJ4QZ2H6rnn5eeyZ+cm8+IjNTj/oJ3d0oON1JV38KCqTldy6sMVFt7B//ws808u64EgHuunc/n3j/75D6skIU2jyNIBO8AVxD75b4W+KS7b+12zp3AWe7+uaBz/GPu/gkzWwj8kFi/xlTgJWAuYH3dsyeaxyEyPNU0tnL2P/2Gz185l6LyIzz3dmmP55nBrPEjmTwmi/qWdjYVV/d6z0k5mSwqyOWFbWXHlGempfChRVPISk+luKqBHQfrqKiLjUi8YOZYlp6Tz6cunN5nE9G6vVWs33eYPYcaaG5rZ83uKvZXNwLwxG2L+cDpE5NmFGPC53EEfRZ3AauIDZ19wt23mtkDwDp3Xwk8Dnw/6PyuIjaSiuC8Z4l1ercBd7p7e/BGjrtnWO9BRJJbdtDc9B8v7jzuuT+7dCaz80ZxZv4Y5k0a1TUoxN35n82ljM5KZ/7k0YzJTmflxgPkj83mzPwxjMmODQN+cVsZrxSWU9/cxpHmdl59p5wXt5WRnprC1NxsLp+bx9nTxnD/L7aydu9h1u49TEtbB5++aEbXqsDxfrFxP3cv3wjE5qmMyEjjYG0T507P5Sefu2TItGBo5riIDGlLH32dyiPNnJU/hq8tO5ddFUfYXVHPhxZNScjrl9U2sav8CJ/8zhoACsZm88WrTuecabmU1zWzautBqupb2LK/hp1Bn+kf7vlg1xpuTa3tSdff0klLjihxiEiINpfUcNNjb9DQw3yTgrHZjBuZwTnTcrlm4eQhs/+6lhwREQnRWQVj2PKVq9lbWc/vdx5i/b7D5GSlc/v7ZjFzwsiowxtUShwiIoMkJcU4LW8Up+WN4tZLZkYdTmg0U0xERAZEiUNERAZEiUNERAZEiUNERAZEiUNERAZEiUNERAZEiUNERAZEiUNERAbklFhyxMwqgHejjqMXE4BDUQdxAorv5Ci+k6P4Ts7JxjfD3fPiC0+JxJHMzGxdT2vBJAvFd3IU38lRfCcnrPjUVCUiIgOixCEiIgOixBG9x6IOoA+K7+QovpOj+E5OKPGpj0NERAZENQ4RERkQJQ4RERkQJY4EMbNpZvaKmW0zs61mdndQ/hUz229mG4Ov6yKOc6+ZbQ5iWReUjTOzF8xsZ/Dv2IhiO73b57TRzGrN7PNRfoZm9oSZlZvZlm5lPX5eFvN1Mysys7fN7LyI4vtXM9sRxPAzM8sNymeaWWO3z/G/I4qv1/9PM7s3+PwKzezqiOJ7pltse81sY1AexefX2++VcL8H3V1fCfgCpgDnBcejgXeABcBXgC9GHV+3OPcCE+LKHgbuCY7vAR5KgjhTgYPAjCg/Q+By4DxgS1+fF3Ad8CvAgIuANRHFdxWQFhw/1C2+md3Pi/Dz6/H/M/h52QRkArOAXUBqouOLe/7fgfsj/Px6+70S6vegahwJ4u6l7r4+OK4DtgP50UbVb0uBJ4PjJ4HrI4yl0xXALnePdEUAd/8dUBVX3NvntRR4ymNWA7lmNiXR8bn7b9y9LXi4GigIM4YT6eXz681SYLm7N7v7HqAIWBJacJw4PjMz4BPAj8KM4URO8Hsl1O9BJY4ImNlM4FxgTVB0V1BtfCKqZqBuHPiNmb1lZncEZZPcvTQ4PghMiia0Yyzj2B/YZPoMe/u88oHibueVEP0fD58h9hdop1lmtsHMXjWzy6IKip7/P5Pt87sMKHP3nd3KIvv84n6vhPo9qMSRYGY2CvgJ8Hl3rwW+CcwGzgFKiVV9o/Q+dz8PuBa408wu7/6kx+q7kY7hNrMM4KPAj4OiZPsMuyTD59UbM7sPaAOeDopKgenufi7wt8APzSwngtCS9v8zzs0c+8dLZJ9fD79XuoTxPajEkUBmlk7sP/dpd/8pgLuXuXu7u3cA3ybkqndf3H1/8G858LMgnrLO6mzwb3l0EQKxpLbe3csg+T5Dev+89gPTup1XEJQlnJndBnwY+FTwi4WgCagyOH6LWB/CvETHdoL/z2T6/NKAjwHPdJZF9fn19HuFkL8HlTgSJGgPfRzY7u6PdCvv3r74J8CW+GsTxcxGmtnozmNinahbgJXArcFptwK/iCbCLsf8pZdMn2Ggt89rJfCnwciWi4Cabs0JCWNm1wBfAj7q7g3dyvPMLDU4Pg2YC+yOIL7e/j9XAsvMLNPMZgXxvZno+AJXAjvcvaSzIIrPr7ffK4T9PZjIEQCn8hfwPmLVxbeBjcHXdcD3gc1B+UpgSoQxnkZs1MomYCtwX1A+HngJ2Am8CIyLMMaRQCUwpltZZJ8hsQRWCrQSay++vbfPi9hIlkeJ/SW6GVgcUXxFxNq5O78P/zs49+PB//tGYD3wkYji6/X/E7gv+PwKgWujiC8o/x7wubhzo/j8evu9Eur3oJYcERGRAVFTlYiIDIgSh4iIDIgSh4iIDIgSh4iIDIgSh4iIDIgSh0gCmdkDZnZl1HGInAwNxxVJEDNLdff2qOMQOVmqcYgMgmAvhh1m9rSZbTezFWY2Itiv4SEzWw/caGbfM7MbgmsuMLM/mNkmM3vTzEabWarF9stYGyzy9xfBuVPM7HfBPg9bIl6AUE5xaVEHIDKMnE5sZvHrZvYE8FdBeaXHFo7sXO6jc6HGZ4Cb3H1tsBheI7GZ0zXufoGZZQKvm9lviK2LtMrdHwyWtRiR2LcmcpQSh8jgKXb314PjHwB/Exw/08O5pwOl7r4WwIMVTc3sKmBRZ60EGENszaO1wBPBgnY/d/eNIb0HkT4pcYgMnvgOw87H9QO4hwF/7e6rjnsitsT9h4Dvmdkj7v7UewtT5OSoj0Nk8Ew3s4uD408Cr53g3EJgipldABD0b6QBq4C/DGoWmNm8YNXiGcQ2Dfo28B1i25mKREKJQ2TwFBLb/Go7MJbYhkQ9cvcW4CbgP81sE/ACkEUsKWwD1pvZFuBbxFoG/gjYZGYbguu+FuL7EDkhDccVGQTBtp3PufuZEYciEjrVOEREZEBU4xARkQFRjUNERAZEiUNERAZEiUNERAZEiUNERAZEiUNERAbk/wNKQIXuL6h0wQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 100.0, 120.0, sigma, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}