{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "check_European_Call_jax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/trained_1stock_European_Call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qBAHU7BwBp5"
      },
      "source": [
        "nstock = 1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "037936a3-b6b3-42ab-e009-149326a35279"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** (1/2)\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = nstock\n",
        "numsteps = 50\n",
        "numpaths = 100000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([100.]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 110.0 \n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.14519\n",
            "100 loops, best of 5: 5.69 ms per loop\n",
            "[0.3981949]\n",
            "100 loops, best of 5: 19.1 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd7952c-11bc-4a3b-c988-d1ec1fe4ed6c"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        # Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        Y = cupy.zeros((self.N_BATCH), dtype = cupy.float32)\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 1.0)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "          drift = jnp.array(np.random.random(self.N_STOCKS) * 0.1)\n",
        "\n",
        "          T = self.T\n",
        "          K = np.random.random(1) * 1.0\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          # European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          # gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          # Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          # Y[op, 0] = European_Call_price\n",
        "          # Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "          Y[op] = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys)\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 10000, batch = 2, seed = 15, stocks=3) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "9b5475cf-329c-4b48-b09f-067ad8a610a7"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024, nstock = 1):\n",
        "        super(Net, self).__init__()\n",
        "        self.nstock = nstock\n",
        "        self.fc1 = nn.Linear(6*self.nstock, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1) # 4 outputs: price, delta1, delta2, delta3\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1, 1.0, 1.0, 0.3, 0.1, 0.1] * self.nstock)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "7b35da86-9283-4768-98ef-2e1b4065c82c"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 33.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3CyULkENYKb",
        "outputId": "239c68d3-a566-492c-dda8-35f36a2239bf"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net(nstock = nstock).cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = nstock) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    y_pred = model(x)\n",
        "    # print(y_pred)\n",
        "\n",
        "    # def compute_deltas(x):\n",
        "    #   inputs = x\n",
        "    #   inputs.requires_grad = True\n",
        "    #   first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "    #   return first_order_gradient[0][[2,8,14]]\n",
        "\n",
        "    # # print(torch.unbind(x))\n",
        "    # # print([compute_deltas(x) for x in torch.unbind(x)])\n",
        "    # # print(torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0))\n",
        "\n",
        "    # deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    # y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # # print(y_pred)\n",
        "\n",
        "    # loss_weight = 1/(y.mean(axis=0)**2)\n",
        "    # # print(y.mean(axis=0))\n",
        "    # # print((y.mean(axis=0)**2))\n",
        "    # # print(1/(y.mean(axis=0)**2))\n",
        "    \n",
        "    # loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    # # print(loss_weight)\n",
        "    # # print(loss_weight_normalized)\n",
        "    # loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean() # compute weighted MSE between the 2 arrays\n",
        "    # # print((y_pred - y) ** 2)\n",
        "    # # print((y_pred - y) ** 2 * loss_weight_normalized)\n",
        "    # # print(loss)\n",
        "\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 95.3509472310543 average time 0.1257410344499988 iter num 20\n",
            "loss 131.7405980080366 average time 0.06436657852499791 iter num 40\n",
            "loss 211.66328340768814 average time 0.04388873076666423 iter num 60\n",
            "loss 153.05710956454277 average time 0.033630716974998975 iter num 80\n",
            "loss 104.06767949461937 average time 0.027468077459999733 iter num 100\n",
            "loss 81.88100531697273 average time 0.02724962774999966 iter num 20\n",
            "loss 122.46387079358101 average time 0.015111069350000861 iter num 40\n",
            "loss 88.61375972628593 average time 0.011011187083333596 iter num 60\n",
            "loss 72.46330380439758 average time 0.008972172125000455 iter num 80\n",
            "loss 80.75384423136711 average time 0.007742385020000313 iter num 100\n",
            "loss 23.51046074181795 average time 0.028969684650003557 iter num 20\n",
            "loss 18.407772295176983 average time 0.015912761374999463 iter num 40\n",
            "loss 3.335975343361497 average time 0.011569071699999257 iter num 60\n",
            "loss 4.007378011010587 average time 0.009436933087500776 iter num 80\n",
            "loss 3.87809966923669 average time 0.008114738019999948 iter num 100\n",
            "loss 5.7609123177826405 average time 0.026903188699992597 iter num 20\n",
            "loss 1.9509428238961846 average time 0.014874378974998593 iter num 40\n",
            "loss 3.7024199264124036 average time 0.010982638116666976 iter num 60\n",
            "loss 2.659037127159536 average time 0.00896046762499907 iter num 80\n",
            "loss 2.3710151435807347 average time 0.007733450529998436 iter num 100\n",
            "loss 4.3163239024579525 average time 0.028103750849993503 iter num 20\n",
            "loss 3.2894290052354336 average time 0.015494674324992274 iter num 40\n",
            "loss 2.308899274794385 average time 0.011278226949996414 iter num 60\n",
            "loss 3.3405874273739755 average time 0.009185061337497302 iter num 80\n",
            "loss 2.72750505246222 average time 0.007944887569996695 iter num 100\n",
            "loss 3.72476177290082 average time 0.02812120090000292 iter num 20\n",
            "loss 2.186133642680943 average time 0.015477801950000014 iter num 40\n",
            "loss 1.831979607231915 average time 0.011261956833333404 iter num 60\n",
            "loss 3.441676963120699 average time 0.009159123625001797 iter num 80\n",
            "loss 3.511110262479633 average time 0.007901610880001044 iter num 100\n",
            "loss 7.417227607220411 average time 0.02902108940000687 iter num 20\n",
            "loss 3.641837975010276 average time 0.01594206370000819 iter num 40\n",
            "loss 1.8941269081551582 average time 0.011558899383340796 iter num 60\n",
            "loss 3.836851683445275 average time 0.009387997150005845 iter num 80\n",
            "loss 2.8209612355567515 average time 0.008090863570002967 iter num 100\n",
            "loss 1.6002134361770004 average time 0.02756896870000105 iter num 20\n",
            "loss 2.629287773743272 average time 0.01517981674999902 iter num 40\n",
            "loss 2.9452270246110857 average time 0.011076776116666073 iter num 60\n",
            "loss 1.906316028907895 average time 0.009007947425000395 iter num 80\n",
            "loss 3.4927678643725812 average time 0.00776507453000022 iter num 100\n",
            "loss 4.315756959840655 average time 0.02722337850000258 iter num 20\n",
            "loss 1.3395427959039807 average time 0.015023199774989848 iter num 40\n",
            "loss 2.872701152227819 average time 0.011017428149989428 iter num 60\n",
            "loss 1.8349906895309687 average time 0.008958432037492515 iter num 80\n",
            "loss 2.5005993666127324 average time 0.007719314729991993 iter num 100\n",
            "loss 1.9145620171912014 average time 0.027454140399999005 iter num 20\n",
            "loss 1.8904852913692594 average time 0.01512826510000025 iter num 40\n",
            "loss 1.5394488582387567 average time 0.011041900366668264 iter num 60\n",
            "loss 3.67179251043126 average time 0.008981939037502685 iter num 80\n",
            "loss 2.6259737205691636 average time 0.00774222201999919 iter num 100\n",
            "loss 1.9901045016013086 average time 0.026915732550008897 iter num 20\n",
            "loss 4.317808197811246 average time 0.0148535796500056 iter num 40\n",
            "loss 1.860436168499291 average time 0.010827175016669344 iter num 60\n",
            "loss 2.059732796624303 average time 0.00883408302500186 iter num 80\n",
            "loss 1.0528304846957326 average time 0.007627724609999404 iter num 100\n",
            "loss 12.033109087496996 average time 0.027828021900000978 iter num 20\n",
            "loss 5.437289364635944 average time 0.015322794024996256 iter num 40\n",
            "loss 3.4851371310651302 average time 0.011135456949995159 iter num 60\n",
            "loss 3.5984732676297426 average time 0.009063225949998355 iter num 80\n",
            "loss 1.8159320461563766 average time 0.00780180593999944 iter num 100\n",
            "loss 1.9999290816485882 average time 0.027068917849987884 iter num 20\n",
            "loss 3.5021943040192127 average time 0.01500435107499527 iter num 40\n",
            "loss 1.5312281902879477 average time 0.010936318366666834 iter num 60\n",
            "loss 0.8346364484168589 average time 0.00891100401249787 iter num 80\n",
            "loss 2.2178597282618284 average time 0.007715532759997359 iter num 100\n",
            "loss 1.4352446305565536 average time 0.027215866900002085 iter num 20\n",
            "loss 2.364152605878189 average time 0.014999149449997163 iter num 40\n",
            "loss 1.5105375496204942 average time 0.010921637083331841 iter num 60\n",
            "loss 2.30648263823241 average time 0.008884032337499547 iter num 80\n",
            "loss 1.018986731651239 average time 0.007664270750001379 iter num 100\n",
            "loss 2.7103611500933766 average time 0.027410659550014316 iter num 20\n",
            "loss 1.7788888362701982 average time 0.01512652030000936 iter num 40\n",
            "loss 2.261940680909902 average time 0.011009457900001962 iter num 60\n",
            "loss 1.6733951633796096 average time 0.008961804700001607 iter num 80\n",
            "loss 1.8856236420106143 average time 0.007730949319998217 iter num 100\n",
            "loss 3.3078872365877032 average time 0.027500493099989854 iter num 20\n",
            "loss 1.1889964662259445 average time 0.015154146449992823 iter num 40\n",
            "loss 3.071289975196123 average time 0.011027010699996254 iter num 60\n",
            "loss 1.2262376549188048 average time 0.008969437337495379 iter num 80\n",
            "loss 1.8525798805058002 average time 0.007732762759999332 iter num 100\n",
            "loss 1.5879716374911368 average time 0.027476704550008436 iter num 20\n",
            "loss 1.6598661022726446 average time 0.015162684725009968 iter num 40\n",
            "loss 2.232758852187544 average time 0.011056954100005593 iter num 60\n",
            "loss 2.1921600273344666 average time 0.008982983625004692 iter num 80\n",
            "loss 0.6589190161321312 average time 0.007741970010006299 iter num 100\n",
            "loss 4.046579706482589 average time 0.02699229164999224 iter num 20\n",
            "loss 2.0021459204144776 average time 0.014897315975002812 iter num 40\n",
            "loss 1.0095546895172447 average time 0.01085411861666709 iter num 60\n",
            "loss 0.6322780245682225 average time 0.008835207500000308 iter num 80\n",
            "loss 0.7886157254688442 average time 0.00762673066000616 iter num 100\n",
            "loss 2.4234892043750733 average time 0.02730008200001066 iter num 20\n",
            "loss 1.743743196129799 average time 0.01503479772501919 iter num 40\n",
            "loss 1.4084579015616328 average time 0.010944068050029425 iter num 60\n",
            "loss 1.2256577610969543 average time 0.008902567612526013 iter num 80\n",
            "loss 1.0113044845638797 average time 0.007675902880027934 iter num 100\n",
            "loss 1.6038486501201987 average time 0.027151999400007298 iter num 20\n",
            "loss 0.9235899051418528 average time 0.01497909597501348 iter num 40\n",
            "loss 1.193069329019636 average time 0.010907040783346625 iter num 60\n",
            "loss 1.142544424510561 average time 0.00887020696251568 iter num 80\n",
            "loss 0.7676595851080492 average time 0.007643779980010095 iter num 100\n",
            "loss 8.52056429721415 average time 0.026975870949991078 iter num 20\n",
            "loss 6.676792982034385 average time 0.014870347699985586 iter num 40\n",
            "loss 1.7841483349911869 average time 0.01083155798334398 iter num 60\n",
            "loss 0.8420141239184886 average time 0.008820248575005962 iter num 80\n",
            "loss 1.5087233623489738 average time 0.007607888570005343 iter num 100\n",
            "loss 1.9395584240555763 average time 0.026839227099981145 iter num 20\n",
            "loss 0.9074853733181953 average time 0.014846779224990314 iter num 40\n",
            "loss 1.2908638746012002 average time 0.010807432716668093 iter num 60\n",
            "loss 0.5094143125461414 average time 0.008795209500001988 iter num 80\n",
            "loss 0.6237950583454221 average time 0.007593036080006641 iter num 100\n",
            "loss 1.3977251364849508 average time 0.027293617899999845 iter num 20\n",
            "loss 0.9760347893461585 average time 0.015015885249999883 iter num 40\n",
            "loss 0.6198796472745016 average time 0.01092868968332823 iter num 60\n",
            "loss 0.9788208990357816 average time 0.008885936812501428 iter num 80\n",
            "loss 0.8611634257249534 average time 0.007658862720006709 iter num 100\n",
            "loss 4.413971910253167 average time 0.02688545385000225 iter num 20\n",
            "loss 4.524890391621739 average time 0.014814685024998654 iter num 40\n",
            "loss 1.1923772399313748 average time 0.010798329783335702 iter num 60\n",
            "loss 0.5423605034593493 average time 0.008797502637509069 iter num 80\n",
            "loss 0.5217519355937839 average time 0.0075816696600145405 iter num 100\n",
            "loss 0.9693064930615947 average time 0.027239562200009004 iter num 20\n",
            "loss 2.109165216097608 average time 0.015004367275003006 iter num 40\n",
            "loss 0.6332677003229037 average time 0.01092056476666509 iter num 60\n",
            "loss 0.5155372855369933 average time 0.008877098149997664 iter num 80\n",
            "loss 0.36301349609857425 average time 0.0076500176099989405 iter num 100\n",
            "loss 1.575031055836007 average time 0.026894513450031354 iter num 20\n",
            "loss 0.5808087735204026 average time 0.014836236775005318 iter num 40\n",
            "loss 0.38221784052439034 average time 0.010831008749998242 iter num 60\n",
            "loss 0.8621339657111093 average time 0.008823226024998122 iter num 80\n",
            "loss 0.4011700366390869 average time 0.007609782099993936 iter num 100\n",
            "loss 1.3856861914973706 average time 0.028146567199985383 iter num 20\n",
            "loss 1.4275057765189558 average time 0.015474531199998864 iter num 40\n",
            "loss 0.8052914927247912 average time 0.011261113850006646 iter num 60\n",
            "loss 0.6056168058421463 average time 0.009141532687502263 iter num 80\n",
            "loss 0.348196153936442 average time 0.007885745290000159 iter num 100\n",
            "loss 3.320852993056178 average time 0.027255234550011664 iter num 20\n",
            "loss 1.6032755956985056 average time 0.015024147799994125 iter num 40\n",
            "loss 1.190917391795665 average time 0.010950260799999494 iter num 60\n",
            "loss 0.5221216270001605 average time 0.008932368637496779 iter num 80\n",
            "loss 0.870487856445834 average time 0.007704467099997601 iter num 100\n",
            "loss 0.9036145638674498 average time 0.027426632899994273 iter num 20\n",
            "loss 0.26097513909917325 average time 0.015092046600017284 iter num 40\n",
            "loss 0.23456965209334157 average time 0.010994921566687783 iter num 60\n",
            "loss 0.484586889797356 average time 0.008939473812509391 iter num 80\n",
            "loss 0.6210975698195398 average time 0.0077033161600138554 iter num 100\n",
            "loss 0.8261127368314192 average time 0.026928246100010256 iter num 20\n",
            "loss 0.7229534821817651 average time 0.014951481499997499 iter num 40\n",
            "loss 0.43966509110759944 average time 0.010881795566660912 iter num 60\n",
            "loss 0.3520185418892652 average time 0.008853046612500748 iter num 80\n",
            "loss 0.23194019377115183 average time 0.007634946770003807 iter num 100\n",
            "loss 0.7952781015774235 average time 0.027244278849991588 iter num 20\n",
            "loss 0.5044907811679877 average time 0.01500063375000309 iter num 40\n",
            "loss 0.35556047805584967 average time 0.010926062233340871 iter num 60\n",
            "loss 0.31942137866280973 average time 0.008884266462510481 iter num 80\n",
            "loss 0.6232567830011249 average time 0.007668627570001263 iter num 100\n",
            "loss 2.9567067394964397 average time 0.02757840885001315 iter num 20\n",
            "loss 1.220907288370654 average time 0.015172623850014588 iter num 40\n",
            "loss 0.5433666228782386 average time 0.011034209599999182 iter num 60\n",
            "loss 0.35988363379146904 average time 0.008975893499999188 iter num 80\n",
            "loss 0.30743794923182577 average time 0.007731285039997146 iter num 100\n",
            "loss 1.2730200251098722 average time 0.02677343509997172 iter num 20\n",
            "loss 0.2726737147895619 average time 0.014763688599992974 iter num 40\n",
            "loss 0.8952321513788775 average time 0.010760654000000614 iter num 60\n",
            "loss 0.26719513698481023 average time 0.008761007762498708 iter num 80\n",
            "loss 0.2967734144476708 average time 0.007563237920001029 iter num 100\n",
            "loss 11.743351351469755 average time 0.027341081499992016 iter num 20\n",
            "loss 1.6555203183088452 average time 0.015046925299998293 iter num 40\n",
            "loss 0.78845456300769 average time 0.010953202183331238 iter num 60\n",
            "loss 0.29164897568989545 average time 0.008907924287510128 iter num 80\n",
            "loss 0.21973555703880265 average time 0.007701329270007591 iter num 100\n",
            "loss 0.4095339318155311 average time 0.027364112649979688 iter num 20\n",
            "loss 1.229029439855367 average time 0.0150804969499859 iter num 40\n",
            "loss 0.436394038842991 average time 0.010972452549989005 iter num 60\n",
            "loss 0.5212757605477236 average time 0.008930073937492011 iter num 80\n",
            "loss 0.20195890101604164 average time 0.007701934579990848 iter num 100\n",
            "loss 0.6253982428461313 average time 0.02689582184999608 iter num 20\n",
            "loss 0.9782217966858298 average time 0.01484397627498879 iter num 40\n",
            "loss 0.26680358132580295 average time 0.010815545766661216 iter num 60\n",
            "loss 0.2998251511598937 average time 0.008814845137501947 iter num 80\n",
            "loss 0.20825869796681218 average time 0.007608291629996984 iter num 100\n",
            "loss 0.5010300810681656 average time 0.02758591960000558 iter num 20\n",
            "loss 0.4210064798826352 average time 0.015209023524982967 iter num 40\n",
            "loss 0.1339742993877735 average time 0.011065806433324117 iter num 60\n",
            "loss 0.29756363801425323 average time 0.008989780774999189 iter num 80\n",
            "loss 0.2982297883136198 average time 0.007744511440000679 iter num 100\n",
            "loss 3.5378968459554017 average time 0.02717272200001162 iter num 20\n",
            "loss 0.9523052722215652 average time 0.01497954487502966 iter num 40\n",
            "loss 0.2649710586410947 average time 0.010912132483368472 iter num 60\n",
            "loss 0.36856748920399696 average time 0.008873194000017293 iter num 80\n",
            "loss 0.33914802770596 average time 0.007658683140016364 iter num 100\n",
            "loss 3.161352942697704 average time 0.026951015999986793 iter num 20\n",
            "loss 1.5093901311047375 average time 0.014857926724994286 iter num 40\n",
            "loss 0.33816209906945005 average time 0.01085386740001013 iter num 60\n",
            "loss 0.3015552101715002 average time 0.008835420550008166 iter num 80\n",
            "loss 0.4360664388514124 average time 0.007617677260004712 iter num 100\n",
            "loss 0.33117892598966137 average time 0.02671891869998717 iter num 20\n",
            "loss 0.8534564403817058 average time 0.01481909885001187 iter num 40\n",
            "loss 0.21647821995429695 average time 0.010821822150023763 iter num 60\n",
            "loss 0.4003516733064316 average time 0.008812844212525307 iter num 80\n",
            "loss 0.30900962883606553 average time 0.007623614920016735 iter num 100\n",
            "loss 0.24041813958319835 average time 0.027418490050058607 iter num 20\n",
            "loss 0.1726046139083337 average time 0.015090906600016751 iter num 40\n",
            "loss 0.14155662938719615 average time 0.011015557250016173 iter num 60\n",
            "loss 0.13978219612909015 average time 0.008952512362520792 iter num 80\n",
            "loss 0.15691735825384967 average time 0.0077134484500084 iter num 100\n",
            "loss 0.373451184714213 average time 0.027342175950002455 iter num 20\n",
            "loss 0.27535861590877175 average time 0.015062147499986623 iter num 40\n",
            "loss 0.134174115373753 average time 0.010962287233307203 iter num 60\n",
            "loss 0.18157199519919232 average time 0.008938407037476281 iter num 80\n",
            "loss 0.1797560980776325 average time 0.007705098869973881 iter num 100\n",
            "loss 0.3876737901009619 average time 0.02680918509993262 iter num 20\n",
            "loss 0.35112592740915716 average time 0.014839502699953756 iter num 40\n",
            "loss 0.4054462624480948 average time 0.010830939266626653 iter num 60\n",
            "loss 0.19251854610047303 average time 0.008822375524965764 iter num 80\n",
            "loss 0.16977630366454832 average time 0.0076206836999563165 iter num 100\n",
            "loss 0.6389080226654187 average time 0.02673455834994911 iter num 20\n",
            "loss 0.25313762307632715 average time 0.014745185274932737 iter num 40\n",
            "loss 0.28864988053101115 average time 0.010740489116642493 iter num 60\n",
            "loss 0.2234919520560652 average time 0.008741237162467996 iter num 80\n",
            "loss 0.30469574994640425 average time 0.007537302099981389 iter num 100\n",
            "loss 0.9269538713851944 average time 0.026913410549946094 iter num 20\n",
            "loss 0.24854234652593732 average time 0.014960070974950668 iter num 40\n",
            "loss 0.18203620129497722 average time 0.010895746783292755 iter num 60\n",
            "loss 0.2009374657063745 average time 0.008863616462463142 iter num 80\n",
            "loss 0.31540123018203303 average time 0.007642503069964732 iter num 100\n",
            "loss 3.138912725262344 average time 0.02686359459992218 iter num 20\n",
            "loss 3.0792065081186593 average time 0.014797504399939498 iter num 40\n",
            "loss 0.6311823381111026 average time 0.01078902541662501 iter num 60\n",
            "loss 0.4126240673940629 average time 0.008779389762457868 iter num 80\n",
            "loss 0.2746073550952133 average time 0.007572092309951586 iter num 100\n",
            "loss 0.4292810626793653 average time 0.02706998130006468 iter num 20\n",
            "loss 0.2153593595721759 average time 0.014917033425069804 iter num 40\n",
            "loss 0.21766067220596597 average time 0.010858521466699737 iter num 60\n",
            "loss 0.21209529222687706 average time 0.008847905600026707 iter num 80\n",
            "loss 0.11223544788663276 average time 0.007634507760012638 iter num 100\n",
            "loss 0.32009909773478284 average time 0.027855929299994385 iter num 20\n",
            "loss 0.5510035771294497 average time 0.015308792375014946 iter num 40\n",
            "loss 0.20627358026104048 average time 0.011125173399993098 iter num 60\n",
            "loss 0.1586031066835858 average time 0.009029939574986656 iter num 80\n",
            "loss 0.18290093066752888 average time 0.007776443869984178 iter num 100\n",
            "loss 0.15933921531541273 average time 0.02678246175003096 iter num 20\n",
            "loss 0.35818226024275646 average time 0.01476576607504967 iter num 40\n",
            "loss 0.1616630470380187 average time 0.010754421833363873 iter num 60\n",
            "loss 0.1822743797674775 average time 0.008754574450034625 iter num 80\n",
            "loss 0.22005820937920362 average time 0.007559358800031077 iter num 100\n",
            "loss 1.0176059731747955 average time 0.027368376049980724 iter num 20\n",
            "loss 6.550122052431107 average time 0.015053832400008104 iter num 40\n",
            "loss 1.118247164413333 average time 0.010969083066659854 iter num 60\n",
            "loss 0.3923966141883284 average time 0.00890802792500267 iter num 80\n",
            "loss 0.2895330180763267 average time 0.007687901629992666 iter num 100\n",
            "loss 0.44684009480988607 average time 0.02727725795002698 iter num 20\n",
            "loss 0.14065113646211103 average time 0.015024845400012055 iter num 40\n",
            "loss 0.1870739652076736 average time 0.01092555726665978 iter num 60\n",
            "loss 0.20774226868525147 average time 0.008892042562501956 iter num 80\n",
            "loss 0.16288508049910888 average time 0.007661013530018863 iter num 100\n",
            "loss 0.9158576722256839 average time 0.027620078449990614 iter num 20\n",
            "loss 1.8706652917899191 average time 0.015196723624978859 iter num 40\n",
            "loss 0.20905816199956462 average time 0.011057217533334551 iter num 60\n",
            "loss 0.15655208699172363 average time 0.008988503925002079 iter num 80\n",
            "loss 0.17150679923361167 average time 0.007752364509997278 iter num 100\n",
            "loss 0.5355102257453837 average time 0.026859014250021573 iter num 20\n",
            "loss 0.44559557863976806 average time 0.014796438024961844 iter num 40\n",
            "loss 0.21000851120334119 average time 0.01077880018331901 iter num 60\n",
            "loss 0.14818942872807384 average time 0.00876338979997513 iter num 80\n",
            "loss 0.182168951141648 average time 0.0075645472399855865 iter num 100\n",
            "loss 0.25453608031966724 average time 0.026750201799950447 iter num 20\n",
            "loss 0.22333546439767815 average time 0.014751306849996126 iter num 40\n",
            "loss 0.17424414181732573 average time 0.01075348579998566 iter num 60\n",
            "loss 0.09981535185943358 average time 0.008742664424988788 iter num 80\n",
            "loss 0.0821848698251415 average time 0.00754318909999256 iter num 100\n",
            "loss 0.2857346407836303 average time 0.02666720125005213 iter num 20\n",
            "loss 0.3074917185585946 average time 0.014715870800034735 iter num 40\n",
            "loss 0.1503206840425264 average time 0.010727745933339368 iter num 60\n",
            "loss 0.18221793652628548 average time 0.008739589212518694 iter num 80\n",
            "loss 0.24588236556155607 average time 0.007547378320018652 iter num 100\n",
            "loss 1.3870872498955578 average time 0.027062329650038918 iter num 20\n",
            "loss 2.2640370298177004 average time 0.014915848824989553 iter num 40\n",
            "loss 0.3702771937241778 average time 0.010850594749990705 iter num 60\n",
            "loss 0.3144076617900282 average time 0.008828367912485646 iter num 80\n",
            "loss 0.10090799150930252 average time 0.00761524209000072 iter num 100\n",
            "loss 0.1480687569710426 average time 0.02796840570003951 iter num 20\n",
            "loss 0.2858926382032223 average time 0.015351636725006302 iter num 40\n",
            "loss 0.10451556590851396 average time 0.01114237613332231 iter num 60\n",
            "loss 0.08194934707717039 average time 0.009037739724988114 iter num 80\n",
            "loss 0.08592296580900438 average time 0.007775349519988595 iter num 100\n",
            "loss 0.6099815800553188 average time 0.026637309900092988 iter num 20\n",
            "loss 0.622655134066008 average time 0.014678722850010217 iter num 40\n",
            "loss 1.1530512711033225 average time 0.010687657249998967 iter num 60\n",
            "loss 0.3804314837907441 average time 0.008692923374997007 iter num 80\n",
            "loss 0.2221274553448893 average time 0.007501432359999853 iter num 100\n",
            "loss 0.4612656630342826 average time 0.027224952650021804 iter num 20\n",
            "loss 0.2793535895762034 average time 0.014982316549992446 iter num 40\n",
            "loss 0.16008905731723644 average time 0.010898404816657603 iter num 60\n",
            "loss 0.10801118150993716 average time 0.008848961862486248 iter num 80\n",
            "loss 0.18507604181650095 average time 0.0076213466399894965 iter num 100\n",
            "loss 0.11361109500285238 average time 0.0268066427000349 iter num 20\n",
            "loss 0.31664436392020434 average time 0.014763352425052289 iter num 40\n",
            "loss 0.10845882570720278 average time 0.010770219033361172 iter num 60\n",
            "loss 0.09722212780616246 average time 0.008762410562519562 iter num 80\n",
            "loss 0.1139122286986094 average time 0.007574799700028052 iter num 100\n",
            "loss 0.11902075129910372 average time 0.028395193000005748 iter num 20\n",
            "loss 0.14505567378364503 average time 0.015578231249992313 iter num 40\n",
            "loss 0.13261140338727273 average time 0.01130723650001073 iter num 60\n",
            "loss 0.17766324162948877 average time 0.00917437281251523 iter num 80\n",
            "loss 0.10227475286228582 average time 0.007889301380027974 iter num 100\n",
            "loss 0.7577463838970289 average time 0.02745496435002224 iter num 20\n",
            "loss 0.22780304789193906 average time 0.015129451150028218 iter num 40\n",
            "loss 0.08861240530677605 average time 0.01099320151665779 iter num 60\n",
            "loss 0.08091654308373109 average time 0.008928454387495321 iter num 80\n",
            "loss 0.10339127584302332 average time 0.007714649669987921 iter num 100\n",
            "loss 0.16422940461779945 average time 0.027183548500011055 iter num 20\n",
            "loss 0.29220325814094394 average time 0.014994033025004683 iter num 40\n",
            "loss 0.07844685569580179 average time 0.010916169983359699 iter num 60\n",
            "loss 0.12972599506611004 average time 0.008902116562529728 iter num 80\n",
            "loss 0.07012694368313532 average time 0.007683283060014218 iter num 100\n",
            "loss 0.25257175366277806 average time 0.02755914765000398 iter num 20\n",
            "loss 0.11843428183055948 average time 0.015180628250004703 iter num 40\n",
            "loss 0.12852206054958515 average time 0.011052794199993816 iter num 60\n",
            "loss 0.11767120668082498 average time 0.00897624362499414 iter num 80\n",
            "loss 0.060587944972212426 average time 0.00773424999999861 iter num 100\n",
            "loss 2.1810628823004663 average time 0.027314148950040362 iter num 20\n",
            "loss 0.30813975172350183 average time 0.015071158175032906 iter num 40\n",
            "loss 0.2703079007915221 average time 0.010973455983344139 iter num 60\n",
            "loss 0.09443496310268529 average time 0.00894193927503011 iter num 80\n",
            "loss 0.07041730896162335 average time 0.007711843380020582 iter num 100\n",
            "loss 0.8442325633950531 average time 0.027110326050024013 iter num 20\n",
            "loss 0.5334572051651776 average time 0.014963919075034937 iter num 40\n",
            "loss 0.06610466698475648 average time 0.010929766483347219 iter num 60\n",
            "loss 0.0779060155764455 average time 0.0089044317500111 iter num 80\n",
            "loss 0.05971330210741144 average time 0.007683268520013371 iter num 100\n",
            "loss 0.28196838684380054 average time 0.028075625349947585 iter num 20\n",
            "loss 0.32777847081888467 average time 0.015459627774953332 iter num 40\n",
            "loss 0.1382236041536089 average time 0.01129147738329266 iter num 60\n",
            "loss 0.059522258197830524 average time 0.009176430237477006 iter num 80\n",
            "loss 0.14264420315157622 average time 0.00791016208999281 iter num 100\n",
            "loss 0.9310628229286522 average time 0.027794881550016727 iter num 20\n",
            "loss 0.33975749829551205 average time 0.015311593674994128 iter num 40\n",
            "loss 0.2892127668019384 average time 0.011149469383334084 iter num 60\n",
            "loss 0.07582527359772939 average time 0.009055716837485761 iter num 80\n",
            "loss 0.07906542123237159 average time 0.007810347289987476 iter num 100\n",
            "loss 0.22600910597248003 average time 0.028642975300022043 iter num 20\n",
            "loss 0.8066673035500571 average time 0.015746520725008394 iter num 40\n",
            "loss 0.15468169294763356 average time 0.011451110433328419 iter num 60\n",
            "loss 0.07661388735868968 average time 0.009289740512491563 iter num 80\n",
            "loss 0.07762188943161163 average time 0.00800084643999071 iter num 100\n",
            "loss 0.2979017335746903 average time 0.028371526349974373 iter num 20\n",
            "loss 0.1481124127167277 average time 0.01563461217498343 iter num 40\n",
            "loss 0.14174870557326358 average time 0.01136476654995325 iter num 60\n",
            "loss 0.1086475640477147 average time 0.009240220949959622 iter num 80\n",
            "loss 0.13519741514755879 average time 0.007963309379952078 iter num 100\n",
            "loss 2.9327603988349438 average time 0.027804792599954452 iter num 20\n",
            "loss 0.45633070840267465 average time 0.015325415449967749 iter num 40\n",
            "loss 0.1609395803825464 average time 0.011170135799981532 iter num 60\n",
            "loss 0.09633209629100747 average time 0.009088206324980775 iter num 80\n",
            "loss 0.0855846064951038 average time 0.007842238179987361 iter num 100\n",
            "loss 0.44318323489278555 average time 0.02769778264998877 iter num 20\n",
            "loss 1.345633208984509 average time 0.015257874699989316 iter num 40\n",
            "loss 0.46173332520993426 average time 0.011143302916677082 iter num 60\n",
            "loss 0.1920086651807651 average time 0.009083555825026225 iter num 80\n",
            "loss 0.09017170668812469 average time 0.00783044585002699 iter num 100\n",
            "loss 1.2593183782882988 average time 0.02726439704999848 iter num 20\n",
            "loss 0.13411195141088683 average time 0.015050764724969667 iter num 40\n",
            "loss 0.10588595614535734 average time 0.010974605983309023 iter num 60\n",
            "loss 0.11822372471215203 average time 0.00893992656247633 iter num 80\n",
            "loss 0.0952962273004232 average time 0.007734285979986453 iter num 100\n",
            "loss 0.2951907845272217 average time 0.02745590674999221 iter num 20\n",
            "loss 0.4429486580193043 average time 0.015136387074994673 iter num 40\n",
            "loss 0.22625637939199805 average time 0.011036460116656599 iter num 60\n",
            "loss 0.17998230759985745 average time 0.008992289887504511 iter num 80\n",
            "loss 0.20084291463717818 average time 0.007762441029999536 iter num 100\n",
            "loss 0.14138642654870637 average time 0.02788875119995282 iter num 20\n",
            "loss 0.13416150977718644 average time 0.015344961474966112 iter num 40\n",
            "loss 0.13341646081244107 average time 0.011173361399967992 iter num 60\n",
            "loss 0.062105241340759676 average time 0.009087576512467877 iter num 80\n",
            "loss 0.03249160499763093 average time 0.007858995969977514 iter num 100\n",
            "loss 0.13330076399142854 average time 0.02744544930005759 iter num 20\n",
            "loss 0.0944324074225733 average time 0.015120911325027464 iter num 40\n",
            "loss 0.08377327503694687 average time 0.011009032150028967 iter num 60\n",
            "loss 0.0745529086998431 average time 0.008949951450011895 iter num 80\n",
            "loss 0.1096923733712174 average time 0.007718109730008109 iter num 100\n",
            "loss 0.3406215910217725 average time 0.027050397800030622 iter num 20\n",
            "loss 0.242548412643373 average time 0.014922244875015167 iter num 40\n",
            "loss 0.035830753404297866 average time 0.010891962600006385 iter num 60\n",
            "loss 0.06807383215345908 average time 0.008870560474997546 iter num 80\n",
            "loss 0.039469227886002045 average time 0.0076538096599961135 iter num 100\n",
            "loss 2.356064796913415 average time 0.027140878749992225 iter num 20\n",
            "loss 6.330477772280574 average time 0.014957099124978867 iter num 40\n",
            "loss 0.4260107016307302 average time 0.010906304216655371 iter num 60\n",
            "loss 0.28479254979174584 average time 0.008869333925002821 iter num 80\n",
            "loss 0.09223053893947508 average time 0.007655120850004095 iter num 100\n",
            "loss 0.19539711502147838 average time 0.02784890785001153 iter num 20\n",
            "loss 0.07025279046501964 average time 0.015317264000032083 iter num 40\n",
            "loss 0.7356433343375102 average time 0.01114239435004644 iter num 60\n",
            "loss 0.0630596696282737 average time 0.009067904800019733 iter num 80\n",
            "loss 0.09920496268023271 average time 0.00784743724001146 iter num 100\n",
            "loss 0.17251659301109612 average time 0.027583910099974674 iter num 20\n",
            "loss 0.23202583179227076 average time 0.015203780800038657 iter num 40\n",
            "loss 0.14927114534657449 average time 0.01109860081670225 iter num 60\n",
            "loss 0.08553693078283686 average time 0.009023073400021531 iter num 80\n",
            "loss 0.10313515304005705 average time 0.0077913935200240305 iter num 100\n",
            "loss 0.47508830903097987 average time 0.02732634924998365 iter num 20\n",
            "loss 0.10077485057990998 average time 0.01506159809998735 iter num 40\n",
            "loss 0.1903644806589 average time 0.010979696383333248 iter num 60\n",
            "loss 0.07121535418264102 average time 0.00892171542501501 iter num 80\n",
            "loss 0.05096454970043851 average time 0.007693603960015025 iter num 100\n",
            "loss 0.30199968023225665 average time 0.02730800125004862 iter num 20\n",
            "loss 0.13771847079624422 average time 0.015040660950046459 iter num 40\n",
            "loss 0.047367275328724645 average time 0.010957395083369192 iter num 60\n",
            "loss 0.07774053301545791 average time 0.008938601237525745 iter num 80\n",
            "loss 0.10939544154098257 average time 0.007705421680038853 iter num 100\n",
            "loss 0.1616267218196299 average time 0.027259865150017503 iter num 20\n",
            "loss 0.10304580428055488 average time 0.015034226949978802 iter num 40\n",
            "loss 0.046054979065957014 average time 0.010965579066654149 iter num 60\n",
            "loss 0.04845056537305936 average time 0.008919483562431196 iter num 80\n",
            "loss 0.11137514775327872 average time 0.007691442489949622 iter num 100\n",
            "loss 2.3734127171337605 average time 0.02764700524990076 iter num 20\n",
            "loss 0.5103363946545869 average time 0.01522250702500969 iter num 40\n",
            "loss 0.4494729364523664 average time 0.01112976828335377 iter num 60\n",
            "loss 0.11806364454969298 average time 0.009050147487494086 iter num 80\n",
            "loss 0.08093014912446961 average time 0.007797765859986611 iter num 100\n",
            "loss 1.8423396977595985 average time 0.027521542750173468 iter num 20\n",
            "loss 2.149334322893992 average time 0.015181533100189881 iter num 40\n",
            "loss 0.2516956737963483 average time 0.011053844016729878 iter num 60\n",
            "loss 0.09579574907547794 average time 0.009024970312555069 iter num 80\n",
            "loss 0.058039449868374504 average time 0.00778044358007719 iter num 100\n",
            "loss 1.042020448949188 average time 0.027418582799964498 iter num 20\n",
            "loss 0.18911543520516716 average time 0.015110749324935568 iter num 40\n",
            "loss 0.24575276256655343 average time 0.011033570550004394 iter num 60\n",
            "loss 0.0562665036341059 average time 0.008973573162529646 iter num 80\n",
            "loss 0.03788700951190549 average time 0.0077381099200465545 iter num 100\n",
            "loss 0.12796321243513376 average time 0.02760392979989774 iter num 20\n",
            "loss 0.09987489647755865 average time 0.015200644899891813 iter num 40\n",
            "loss 0.05935678473178996 average time 0.0110964604666151 iter num 60\n",
            "loss 0.03973019374825526 average time 0.009019078399944647 iter num 80\n",
            "loss 0.04210732186038513 average time 0.007779787149947879 iter num 100\n",
            "loss 0.08473626621707808 average time 0.027226918950009348 iter num 20\n",
            "loss 0.19345010514371097 average time 0.015025633275013207 iter num 40\n",
            "loss 0.060346947066136636 average time 0.010945348783328276 iter num 60\n",
            "loss 0.08935930964071304 average time 0.008917230400049903 iter num 80\n",
            "loss 0.037416980376292486 average time 0.0076929039400693 iter num 100\n",
            "loss 0.1263327430933714 average time 0.02738705739998295 iter num 20\n",
            "loss 0.27374419005354866 average time 0.015100817125016874 iter num 40\n",
            "loss 0.10199568350799382 average time 0.011020746300027895 iter num 60\n",
            "loss 0.11783755326177925 average time 0.008964185425008963 iter num 80\n",
            "loss 0.14701095096825156 average time 0.007731300320028822 iter num 100\n",
            "loss 0.36534856917569414 average time 0.028203361949999818 iter num 20\n",
            "loss 0.16403655536123551 average time 0.01548910232502294 iter num 40\n",
            "loss 0.04937136509397533 average time 0.011273128249983225 iter num 60\n",
            "loss 0.09408870937477332 average time 0.009185770262524784 iter num 80\n",
            "loss 0.05994940693199169 average time 0.007908985889998802 iter num 100\n",
            "loss 0.43612475565169007 average time 0.027013993950049553 iter num 20\n",
            "loss 0.11476169675006531 average time 0.014912844549940019 iter num 40\n",
            "loss 0.08795204848865978 average time 0.010870463349995892 iter num 60\n",
            "loss 0.042629899326129816 average time 0.008866384887483036 iter num 80\n",
            "loss 0.05885085556656122 average time 0.00765225380998345 iter num 100\n",
            "loss 1.0717577242758125 average time 0.027159538150044682 iter num 20\n",
            "loss 0.3753216515178792 average time 0.014962687275033203 iter num 40\n",
            "loss 0.1261180113942828 average time 0.010904093983344864 iter num 60\n",
            "loss 0.21253297745715827 average time 0.008881545112524236 iter num 80\n",
            "loss 0.03534540610417025 average time 0.007662226369993732 iter num 100\n",
            "loss 0.37253372283885255 average time 0.02723065039995163 iter num 20\n",
            "loss 0.09046007107826881 average time 0.015009115799944083 iter num 40\n",
            "loss 0.13325659892871045 average time 0.0109392983666415 iter num 60\n",
            "loss 0.05395274911279557 average time 0.008925832112481658 iter num 80\n",
            "loss 0.05013456302549457 average time 0.007701841760008392 iter num 100\n",
            "loss 0.3252411261200905 average time 0.02811632460011424 iter num 20\n",
            "loss 0.24167395167751238 average time 0.015433567300078721 iter num 40\n",
            "loss 0.14097673556534573 average time 0.011211338800088318 iter num 60\n",
            "loss 0.10000625479733571 average time 0.00911269570008244 iter num 80\n",
            "loss 0.04965063908457523 average time 0.007853984590055915 iter num 100\n",
            "loss 0.17144275261671282 average time 0.027103325149892044 iter num 20\n",
            "loss 0.0945364354265621 average time 0.014944708099983472 iter num 40\n",
            "loss 0.15538760635536164 average time 0.010924232416634065 iter num 60\n",
            "loss 0.030740286547370488 average time 0.008893969424957505 iter num 80\n",
            "loss 0.08413437171839178 average time 0.00767177291997541 iter num 100\n",
            "loss 11.77897909656167 average time 0.02684166489998461 iter num 20\n",
            "loss 2.7385802241042256 average time 0.0148116164749581 iter num 40\n",
            "loss 0.6000035864417441 average time 0.010797201533311334 iter num 60\n",
            "loss 0.24765289708739147 average time 0.008784629374963515 iter num 80\n",
            "loss 0.21556765204877593 average time 0.007581762029994934 iter num 100\n",
            "loss 0.24370136088691652 average time 0.026988079799912155 iter num 20\n",
            "loss 0.182361327460967 average time 0.014869556924941208 iter num 40\n",
            "loss 0.18024966266239062 average time 0.010849508799947217 iter num 60\n",
            "loss 0.27582620532484725 average time 0.008829192424957454 iter num 80\n",
            "loss 0.05989144028717419 average time 0.007618128879939832 iter num 100\n",
            "loss 0.122948858916061 average time 0.02709813490009765 iter num 20\n",
            "loss 0.10162578291783575 average time 0.014921722675057935 iter num 40\n",
            "loss 0.13494511222233996 average time 0.010865234566684498 iter num 60\n",
            "loss 0.20375924577820115 average time 0.008835764600053152 iter num 80\n",
            "loss 0.11351308785378933 average time 0.007627067450011964 iter num 100\n",
            "loss 0.269656338787172 average time 0.027980154500073694 iter num 20\n",
            "loss 0.17030019080266356 average time 0.015406503200028964 iter num 40\n",
            "loss 0.13333354218048044 average time 0.01121971529998215 iter num 60\n",
            "loss 0.15607798559358343 average time 0.009112965349982006 iter num 80\n",
            "loss 0.12539077943074517 average time 0.007838359600027616 iter num 100\n",
            "loss 0.2780646173050627 average time 0.027808052950013006 iter num 20\n",
            "loss 0.1533131035102997 average time 0.015298506475119211 iter num 40\n",
            "loss 0.11561378414626233 average time 0.011135440883435876 iter num 60\n",
            "loss 0.04715219802164938 average time 0.009036642900059632 iter num 80\n",
            "loss 0.03892255335813388 average time 0.007785106230039673 iter num 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 10000\n",
              "\tepoch: 100\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 100\n",
              "\toutput: 3.892255335813388e-06\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.OptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d9c5c4-409b-4209-adc1-90c9c8b20e52"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'european_call_1_v2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce3f277-b0bd-449e-964e-07a2ac00a761"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31dbdec-04b5-4220-d4f0-66638c0a4efb"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'european_call_1_v2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f4b87d-fe4d-44e6-99d7-f7eb1dabda0b"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net(nstock = nstock).cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57"
      },
      "source": [
        "# # If memory is not enough, try changing parameters and restarting session\n",
        "# # loss will converge\n",
        "\n",
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import OptionDataSet\n",
        "# import numpy as np\n",
        "# timer = Timer(average=True)\n",
        "# #model = Net().cuda()\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# #dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "# dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 8, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     # print(x)\n",
        "#     y = batch[1]\n",
        "#     # print(y)\n",
        "#     y_pred = model(x)\n",
        "#     # print(y_pred)\n",
        "\n",
        "#     def compute_deltas(x):\n",
        "#       inputs = x\n",
        "#       inputs.requires_grad = True\n",
        "#       first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "#       return first_order_gradient[0][[2,8,14]]\n",
        "\n",
        "#     # print(torch.unbind(x))\n",
        "#     # print([compute_deltas(x) for x in torch.unbind(x)])\n",
        "#     # print(torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0))\n",
        "\n",
        "#     deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "#     y_pred = torch.cat((y_pred, deltas), 1)\n",
        "#     # print(y_pred)\n",
        "\n",
        "#     loss_weight = 1/(y.mean(axis=0)**2)\n",
        "#     # print(y.mean(axis=0))\n",
        "#     # print((y.mean(axis=0)**2))\n",
        "#     # print(1/(y.mean(axis=0)**2))\n",
        "    \n",
        "#     loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "#     # print(loss_weight)\n",
        "#     # print(loss_weight_normalized)\n",
        "#     loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean() # compute weighted MSE between the 2 arrays\n",
        "#     # print((y_pred - y) ** 2)\n",
        "#     # print((y_pred - y) ** 2 * loss_weight_normalized)\n",
        "#     # print(loss)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 10\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs = 50)\n",
        "\n",
        "# model_save_name = 'jax_european_test_3.pth'\n",
        "# path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d55d63f-4b22-48de-f4d7-c75e7123c9c9"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 0.8, 0.8, 0.25, 0.05, 0.05]*nstock]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "# first_order_gradient[0][[2,8,14]]\n",
        "first_order_gradient[0][2]\n",
        "\n",
        "# price, delta1, delta2, delta3\n",
        "# should be around (0.067710705, 0.22125466 , 0.22136934 , 0.22104672)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.1065]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6972, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_2AXrPt7bNj"
      },
      "source": [
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "\n",
        "# numstocks = nstock\n",
        "# numsteps = 50\n",
        "# numpaths = 100000\n",
        "\n",
        "# rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "# rng, key = jax.random.split(rng)\n",
        "\n",
        "# drift = jnp.array([0.05]*numstocks)\n",
        "# r = drift\n",
        "# cov = jnp.identity(numstocks)*0.25*0.25\n",
        "# initial_stocks = jnp.array([0.8]*numstocks) # must be float\n",
        "# T = 1.0\n",
        "# K = 0.8\n",
        "\n",
        "# # option price\n",
        "# print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# # delta test\n",
        "# goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "# print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "b16b1674-f8a9-4b15-a9be-2def3ad3c2bc"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.8, S, 0.25, 0.05, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(0, 1, 0.01)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f14b2681590>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NspNEBFzf1mM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}