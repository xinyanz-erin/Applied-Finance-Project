{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knock_Out_Call_jax_v3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/Knock_Out_Call_jax_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUAoeddX9Kr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8fc0cc-6320-4cf1-e0a2-cb2a235061f2"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   4647      0 --:--:-- --:--:-- --:--:--  4633\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9 MB 31 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 51.0 MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZcdKRMY5rO1"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GluQSli5uSr",
        "outputId": "81526e5b-b554-48d6-8103-1d62e6996997"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 3\n",
        "numsteps = 50\n",
        "numpaths = 100000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.5*0.5\n",
        "initial_stocks = jnp.array([100.]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 60.0\n",
        "B = 78.4 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, 100000)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T))\n",
        "%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T))\n",
        "%timeit goptionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32.85719\n",
            "[0.22481976 0.22449198 0.22490814]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls2j6MLZBkMk"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNMPwm--Bk10",
        "outputId": "d5f328be-56ed-4e68-dcde-46c7081793c3"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T, keys): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 7), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 200.0)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.4)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "          drift = jnp.array(np.random.random(self.N_STOCKS) * 0.2)\n",
        "\n",
        "          T = self.T\n",
        "          K = np.random.random(1) * 200.0\n",
        "          B = np.random.random(1) * 200.0 * 0.9 # B can't be too large\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, B, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, B, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "          # T, B, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(B), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 10000, batch = 3, seed = 15, stocks=3) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_HfTbZ7DCVk",
        "outputId": "9f6ef497-56a6-4e60-c186-1575bf290773"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(7*3, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 4) # 4 outputs: price, delta1, delta2, delta3\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1, 200.0*0.9, 200.0, 200.0, 0.4, 0.2, 0.1]*3)) # don't use numpy here - will give error later\n",
        "                                                                                          # T, B, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvvkcm_cJDdX"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY4HxqfrJElx",
        "outputId": "f7e3493d-7332-4fc5-e89f-4e72c3b7e159"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 15.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6x6aYDRuJGhV",
        "outputId": "19d26961-9506-4ab5-8f11-d02f8f1c43c0"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    y_pred = model(x)\n",
        "    # print(y_pred)\n",
        "    # print(y.mean(axis=0))\n",
        "    loss_weight = y.mean(axis=0)[0]/y.mean(axis=0)\n",
        "    # loss_weight = torch.tensor([1, 50, 50, 50]).cuda() # switch to this so that more weight is assigned to price\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    # print(loss_weight)\n",
        "    # print(loss_weight_normalized)\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean() * 100 # compute weighted MSE between the 2 arrays\n",
        "    # print((y_pred - y) ** 2)\n",
        "    # print((y_pred - y) ** 2 * loss_weight_normalized)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.21573415398597717 average time 0.5524972275499976 iter num 20\n",
            "loss 0.5041149854660034 average time 0.27771978017501053 iter num 40\n",
            "loss 0.4529551863670349 average time 0.18610052670000338 iter num 60\n",
            "loss 0.47228682041168213 average time 0.14032825667500076 iter num 80\n",
            "loss 0.2646041214466095 average time 0.11285470184999895 iter num 100\n",
            "loss 0.15927483141422272 average time 0.07082559605000824 iter num 20\n",
            "loss 0.12746113538742065 average time 0.036819739824994716 iter num 40\n",
            "loss 0.13059252500534058 average time 0.0255166225333312 iter num 60\n",
            "loss 0.04682614654302597 average time 0.019840630324998186 iter num 80\n",
            "loss 0.10658369958400726 average time 0.016440676769996117 iter num 100\n",
            "loss 0.0429360494017601 average time 0.06938878285001807 iter num 20\n",
            "loss 0.044093430042266846 average time 0.036122561250010676 iter num 40\n",
            "loss 0.09273669123649597 average time 0.025020603600008448 iter num 60\n",
            "loss 0.12930044531822205 average time 0.01947083567500556 iter num 80\n",
            "loss 0.037821464240550995 average time 0.016147946860002094 iter num 100\n",
            "loss 0.11053621768951416 average time 0.06857368250000491 iter num 20\n",
            "loss 0.16798856854438782 average time 0.03570148227499317 iter num 40\n",
            "loss 0.052394554018974304 average time 0.024754929299994424 iter num 60\n",
            "loss 0.07073800265789032 average time 0.019288326487495056 iter num 80\n",
            "loss 0.1133776307106018 average time 0.015998294099997565 iter num 100\n",
            "loss 0.02487357333302498 average time 0.07070028205001791 iter num 20\n",
            "loss 0.10516931861639023 average time 0.03678663545000518 iter num 40\n",
            "loss 0.050176672637462616 average time 0.0254715433166704 iter num 60\n",
            "loss 0.07810159027576447 average time 0.019809991862499033 iter num 80\n",
            "loss 0.12448913604021072 average time 0.0164089252799954 iter num 100\n",
            "loss 0.07942688465118408 average time 0.07109679569999798 iter num 20\n",
            "loss 0.03566494584083557 average time 0.036962070199984966 iter num 40\n",
            "loss 0.046085577458143234 average time 0.025621889066667337 iter num 60\n",
            "loss 0.05307469144463539 average time 0.019919669200001523 iter num 80\n",
            "loss 0.04507233947515488 average time 0.016495146400004615 iter num 100\n",
            "loss 0.06507661938667297 average time 0.06935429110001223 iter num 20\n",
            "loss 0.13071374595165253 average time 0.036069643600012566 iter num 40\n",
            "loss 0.09234511852264404 average time 0.025015599383345945 iter num 60\n",
            "loss 0.0700041800737381 average time 0.019481852337511894 iter num 80\n",
            "loss 0.06930217891931534 average time 0.01615535872000919 iter num 100\n",
            "loss 0.08241866528987885 average time 0.07241045814996597 iter num 20\n",
            "loss 0.11927248537540436 average time 0.037601869474991646 iter num 40\n",
            "loss 0.061065927147865295 average time 0.026009916616658302 iter num 60\n",
            "loss 0.09731219708919525 average time 0.020209705462494298 iter num 80\n",
            "loss 0.07463806867599487 average time 0.016730440739997902 iter num 100\n",
            "loss 0.01191734429448843 average time 0.0699141842999893 iter num 20\n",
            "loss 0.04624790698289871 average time 0.036451627824999375 iter num 40\n",
            "loss 0.08126257359981537 average time 0.02530344826666351 iter num 60\n",
            "loss 0.1136660948395729 average time 0.01968738246250439 iter num 80\n",
            "loss 0.028675610199570656 average time 0.016313988330005032 iter num 100\n",
            "loss 0.3026381731033325 average time 0.06901867485003095 iter num 20\n",
            "loss 0.12211461365222931 average time 0.03593643015001362 iter num 40\n",
            "loss 0.03543121740221977 average time 0.02492468473333247 iter num 60\n",
            "loss 0.09110869467258453 average time 0.01943379588750247 iter num 80\n",
            "loss 0.10616134107112885 average time 0.016162622720000856 iter num 100\n",
            "loss 0.07055500894784927 average time 0.07033904600003779 iter num 20\n",
            "loss 0.06510347872972488 average time 0.03663339855000913 iter num 40\n",
            "loss 0.03233679011464119 average time 0.02539384591667613 iter num 60\n",
            "loss 0.0844101533293724 average time 0.019746367362506588 iter num 80\n",
            "loss 0.03817935287952423 average time 0.016353985090006518 iter num 100\n",
            "loss 0.09732281416654587 average time 0.06872244915000465 iter num 20\n",
            "loss 0.08861741423606873 average time 0.03575405602499586 iter num 40\n",
            "loss 0.06158049777150154 average time 0.024933595799992266 iter num 60\n",
            "loss 0.06537245959043503 average time 0.019436618537503135 iter num 80\n",
            "loss 0.050444528460502625 average time 0.016110207350002383 iter num 100\n",
            "loss 0.04090193659067154 average time 0.06782710325003335 iter num 20\n",
            "loss 0.04418609291315079 average time 0.035320620200002394 iter num 40\n",
            "loss 0.06042616814374924 average time 0.024479116266684288 iter num 60\n",
            "loss 0.062483325600624084 average time 0.019060849750007946 iter num 80\n",
            "loss 0.04158792644739151 average time 0.015812717430026168 iter num 100\n",
            "loss 0.025693446397781372 average time 0.06932018635000077 iter num 20\n",
            "loss 0.016311820596456528 average time 0.03606967039999063 iter num 40\n",
            "loss 0.016059288755059242 average time 0.024983534366666997 iter num 60\n",
            "loss 0.02089770883321762 average time 0.019441599712484957 iter num 80\n",
            "loss 0.04880029708147049 average time 0.01611766656998043 iter num 100\n",
            "loss 0.025548215955495834 average time 0.07245952474997921 iter num 20\n",
            "loss 0.03239908814430237 average time 0.03763784567497623 iter num 40\n",
            "loss 0.028878148645162582 average time 0.026026664916639675 iter num 60\n",
            "loss 0.021208342164754868 average time 0.020227956199983056 iter num 80\n",
            "loss 0.04352305456995964 average time 0.016738321089974306 iter num 100\n",
            "loss 0.021486006677150726 average time 0.06993689305002135 iter num 20\n",
            "loss 0.007208248600363731 average time 0.03637401744999806 iter num 40\n",
            "loss 0.006855427287518978 average time 0.02520924361666251 iter num 60\n",
            "loss 0.003361192997545004 average time 0.019605261200001677 iter num 80\n",
            "loss 0.0037679653614759445 average time 0.01625053996000588 iter num 100\n",
            "loss 0.009062353521585464 average time 0.0692886091000446 iter num 20\n",
            "loss 0.004923459142446518 average time 0.03608928552501993 iter num 40\n",
            "loss 0.01527983695268631 average time 0.025016212149997347 iter num 60\n",
            "loss 0.0026162154972553253 average time 0.01946131917498519 iter num 80\n",
            "loss 0.019305389374494553 average time 0.016173614809990796 iter num 100\n",
            "loss 0.010927819646894932 average time 0.06825045634996059 iter num 20\n",
            "loss 0.02381376177072525 average time 0.03554191159998936 iter num 40\n",
            "loss 0.002995694987475872 average time 0.024645714883316336 iter num 60\n",
            "loss 0.0017461010720580816 average time 0.01919857807497465 iter num 80\n",
            "loss 0.003702745772898197 average time 0.015916689429982398 iter num 100\n",
            "loss 0.007166354916989803 average time 0.07014287524996235 iter num 20\n",
            "loss 0.008623143658041954 average time 0.03647343654998849 iter num 40\n",
            "loss 0.006875575985759497 average time 0.025290056150023096 iter num 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f2cb6f5ae6f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m           \u001b[0;31m# T, B, K, S, sigma, mu, r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m           \u001b[0mparas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m           \u001b[0mparas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m           \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   3567\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected input type for array: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0;32m--> 461\u001b[0;31m                                        weak_type=new_weak_type)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbitcast_convert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    266\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSHNyXROJiBl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtO_Tgt4Jju3",
        "outputId": "c3643063-519f-4318-8d3e-27966824a4e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwnE4eMAJksV"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knockout_test_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dNzYb0YJnsc"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33tWx0fkJpJn"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jljJh3IJqHy"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knockout_test_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeXKJSRkJr2p"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvAMGITqJskd"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pBlIEZ2JuNC"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 10000, batch = 8, seed = np.random.randint(10000), stocks = 3) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    y_pred = model(x)\n",
        "    # print(y_pred)\n",
        "    # print(y.mean(axis=0))\n",
        "    # loss_weight = y.mean(axis=0)[0]/y.mean(axis=0)\n",
        "    loss_weight = torch.tensor([1, 50, 50, 50]).cuda() # switch to this so that more weight is assigned to price\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    # print(loss_weight)\n",
        "    # print(loss_weight_normalized)\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean() * 100 # compute weighted MSE between the 2 arrays\n",
        "    # print((y_pred - y) ** 2)\n",
        "    # print((y_pred - y) ** 2 * loss_weight_normalized)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 10)\n",
        "\n",
        "model_save_name = 'jax_knockout_test_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_9wBJH9JyeD"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udvXGGe_JyAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85082528-2737-4d7c-f1bc-1f4629cccf08"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 90, 110.0, 100.0, 0.25, 0., 0.]*3]).cuda() # T, B, K, S, sigma, mu, r\n",
        "model(inputs.float())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.0940, 0.0369, 0.0377, 0.0383]], device='cuda:0',\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0lMfvTyyEoC"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "8MjaHg6ZyFb5",
        "outputId": "2c93b3c4-eef8-4a18-85ef-93b90bf9d536"
      },
      "source": [
        "import pylab\n",
        "prices = np.arange(0, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    inputs = torch.tensor([[1, 90, 110.0, p, 0.25, 0., 0.]*3]).cuda() # T, B, K, S, sigma, mu, r\n",
        "    deltas.append(model(inputs.float())[0][1]) # delta for S1\n",
        "\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5cba1cc8d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}