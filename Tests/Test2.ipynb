{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/main/Tests/Test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "cb1ccb8d-ecae-4ea2-ac66-2e95f172a1ba"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0   9186      0 --:--:-- --:--:-- --:--:--  9186\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9MB 89kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 45.3MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "65b2d5c6-a950-46e9-e784-01861991b43d"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "\n",
        "          X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          stocks_randoms_cov = cupy.array([0.3] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i[0])\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343dd6c5-0eee-440e-c4e1-832cc5982401"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(60, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        # self.register_buffer('norm',\n",
        "        #                      torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2]*10)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf13803a-b4cf-49f7-f97e-4f94c9472ccd"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/c3/f472843797b5ccbb2f0e806a6927f52c7c9522bfcea8e7e881d39258368b/pytorch_ignite-0.4.5-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 27.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 32.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 24.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 14.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 14.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102kB 13.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 13.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 13.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133kB 13.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 13.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 13.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163kB 13.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 13.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 13.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 13.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204kB 13.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 13.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeadf063-cb9d-4115-ca8b-80bfa6aa45a0"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=16, stocks=10)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=300)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 303.23748779296875 average time 0.008034856130016123 iter num 100\n",
            "loss 234.1334991455078 average time 0.008100629409964312 iter num 100\n",
            "loss 53.61421585083008 average time 0.008170115699977032 iter num 100\n",
            "loss 81.87948608398438 average time 0.008177746760011359 iter num 100\n",
            "loss 18.34607696533203 average time 0.008145604119890777 iter num 100\n",
            "loss 53.31464385986328 average time 0.008111063150081463 iter num 100\n",
            "loss 51.216434478759766 average time 0.00818383220999749 iter num 100\n",
            "loss 36.55934143066406 average time 0.008294617319825193 iter num 100\n",
            "loss 40.44537353515625 average time 0.00845836643995426 iter num 100\n",
            "loss 44.841575622558594 average time 0.008696041659950424 iter num 100\n",
            "loss 34.50199890136719 average time 0.008125760250113672 iter num 100\n",
            "loss 47.69036102294922 average time 0.008234533630020451 iter num 100\n",
            "loss 60.41330337524414 average time 0.008038866259939824 iter num 100\n",
            "loss 23.74372100830078 average time 0.00804449150004075 iter num 100\n",
            "loss 76.9830093383789 average time 0.008123254109996196 iter num 100\n",
            "loss 57.077728271484375 average time 0.008019701000011991 iter num 100\n",
            "loss 46.31510925292969 average time 0.00823452874987197 iter num 100\n",
            "loss 35.33603286743164 average time 0.008115759920037817 iter num 100\n",
            "loss 44.92732238769531 average time 0.008078957529942272 iter num 100\n",
            "loss 76.98106384277344 average time 0.00813405919005163 iter num 100\n",
            "loss 34.507781982421875 average time 0.008110632639836695 iter num 100\n",
            "loss 54.651283264160156 average time 0.008118523439934506 iter num 100\n",
            "loss 61.78301239013672 average time 0.008230409610023344 iter num 100\n",
            "loss 41.49278259277344 average time 0.008117333780028275 iter num 100\n",
            "loss 60.056907653808594 average time 0.008225054549966445 iter num 100\n",
            "loss 39.674354553222656 average time 0.008551799710003251 iter num 100\n",
            "loss 40.92711639404297 average time 0.00807319568017192 iter num 100\n",
            "loss 74.32640075683594 average time 0.008186401990042214 iter num 100\n",
            "loss 60.659461975097656 average time 0.008353660879984091 iter num 100\n",
            "loss 51.559593200683594 average time 0.008096015390110551 iter num 100\n",
            "loss 72.03429412841797 average time 0.008088610309951037 iter num 100\n",
            "loss 38.450599670410156 average time 0.007962367880027158 iter num 100\n",
            "loss 38.96019744873047 average time 0.008407956049941276 iter num 100\n",
            "loss 28.109088897705078 average time 0.00809278094993715 iter num 100\n",
            "loss 40.80097961425781 average time 0.008123084459875828 iter num 100\n",
            "loss 33.66151809692383 average time 0.00814823893004359 iter num 100\n",
            "loss 21.398292541503906 average time 0.008146728509764216 iter num 100\n",
            "loss 29.52263069152832 average time 0.008184349770108384 iter num 100\n",
            "loss 32.7337532043457 average time 0.008074608049973904 iter num 100\n",
            "loss 34.8250846862793 average time 0.00799032440994779 iter num 100\n",
            "loss 55.66707992553711 average time 0.008101652510031272 iter num 100\n",
            "loss 62.8337516784668 average time 0.008175629979978111 iter num 100\n",
            "loss 29.39590072631836 average time 0.008120008309997501 iter num 100\n",
            "loss 49.686161041259766 average time 0.00815441225997347 iter num 100\n",
            "loss 27.43294906616211 average time 0.008121234120080771 iter num 100\n",
            "loss 34.3974609375 average time 0.008452883620029751 iter num 100\n",
            "loss 51.44890594482422 average time 0.008140824239944777 iter num 100\n",
            "loss 49.81181716918945 average time 0.008158172609946633 iter num 100\n",
            "loss 45.020992279052734 average time 0.00821126277991425 iter num 100\n",
            "loss 48.9969482421875 average time 0.0082028417899528 iter num 100\n",
            "loss 44.39314270019531 average time 0.00836300430997653 iter num 100\n",
            "loss 40.23860168457031 average time 0.008088027920002787 iter num 100\n",
            "loss 27.216598510742188 average time 0.007956573510036834 iter num 100\n",
            "loss 38.744232177734375 average time 0.008150688769965199 iter num 100\n",
            "loss 41.861270904541016 average time 0.008079493330078548 iter num 100\n",
            "loss 58.04634094238281 average time 0.008063752890066098 iter num 100\n",
            "loss 33.538116455078125 average time 0.008140246750008373 iter num 100\n",
            "loss 34.06836700439453 average time 0.008077860249977675 iter num 100\n",
            "loss 25.320222854614258 average time 0.00825650737993783 iter num 100\n",
            "loss 54.408111572265625 average time 0.008024499400016793 iter num 100\n",
            "loss 52.585487365722656 average time 0.00805308238012003 iter num 100\n",
            "loss 43.552528381347656 average time 0.008592701619982109 iter num 100\n",
            "loss 30.566936492919922 average time 0.008066868349978905 iter num 100\n",
            "loss 54.92920684814453 average time 0.007947379980014374 iter num 100\n",
            "loss 47.936344146728516 average time 0.008039471350039094 iter num 100\n",
            "loss 39.6456184387207 average time 0.008054919949900067 iter num 100\n",
            "loss 34.194786071777344 average time 0.008065632660036499 iter num 100\n",
            "loss 39.495521545410156 average time 0.008041985930067313 iter num 100\n",
            "loss 31.740964889526367 average time 0.008073361429887882 iter num 100\n",
            "loss 67.83158111572266 average time 0.008222484980014997 iter num 100\n",
            "loss 74.56192016601562 average time 0.008030769869856158 iter num 100\n",
            "loss 12.600029945373535 average time 0.008093376630131387 iter num 100\n",
            "loss 43.14063262939453 average time 0.007989442590023828 iter num 100\n",
            "loss 69.75379943847656 average time 0.008025503540065983 iter num 100\n",
            "loss 32.26152038574219 average time 0.008046539300030417 iter num 100\n",
            "loss 31.545724868774414 average time 0.008210215409908414 iter num 100\n",
            "loss 30.99832534790039 average time 0.008206885070067074 iter num 100\n",
            "loss 23.154064178466797 average time 0.008060487819966511 iter num 100\n",
            "loss 34.1868782043457 average time 0.008130650140046783 iter num 100\n",
            "loss 36.04488754272461 average time 0.00842297454995787 iter num 100\n",
            "loss 45.12902069091797 average time 0.008236781029991107 iter num 100\n",
            "loss 40.31682205200195 average time 0.008142699159852782 iter num 100\n",
            "loss 49.49067306518555 average time 0.008214922819915956 iter num 100\n",
            "loss 36.100311279296875 average time 0.008066827389884566 iter num 100\n",
            "loss 77.27843475341797 average time 0.008172299340058089 iter num 100\n",
            "loss 25.880477905273438 average time 0.008102074039943546 iter num 100\n",
            "loss 29.35432243347168 average time 0.008300668619904172 iter num 100\n",
            "loss 70.31553649902344 average time 0.008103823740020744 iter num 100\n",
            "loss 20.492076873779297 average time 0.00814845286007767 iter num 100\n",
            "loss 28.00055694580078 average time 0.008182770719959081 iter num 100\n",
            "loss 66.62348175048828 average time 0.008596603749901987 iter num 100\n",
            "loss 36.04169464111328 average time 0.00813513627006614 iter num 100\n",
            "loss 73.00625610351562 average time 0.008085198020035022 iter num 100\n",
            "loss 29.976459503173828 average time 0.008146668540048268 iter num 100\n",
            "loss 64.54158782958984 average time 0.008058684609986813 iter num 100\n",
            "loss 47.40500259399414 average time 0.008605389000022114 iter num 100\n",
            "loss 30.631582260131836 average time 0.008086998509988917 iter num 100\n",
            "loss 55.598243713378906 average time 0.008167357520014774 iter num 100\n",
            "loss 34.34745788574219 average time 0.008458227350056404 iter num 100\n",
            "loss 65.44390106201172 average time 0.00833675294994464 iter num 100\n",
            "loss 58.02904510498047 average time 0.00826273292008409 iter num 100\n",
            "loss 29.73779296875 average time 0.008531283679931221 iter num 100\n",
            "loss 35.449649810791016 average time 0.008124639770085196 iter num 100\n",
            "loss 20.468242645263672 average time 0.008419653980163276 iter num 100\n",
            "loss 32.131103515625 average time 0.008114233500073169 iter num 100\n",
            "loss 55.258888244628906 average time 0.007973610540084337 iter num 100\n",
            "loss 41.138668060302734 average time 0.00820482611994521 iter num 100\n",
            "loss 30.811748504638672 average time 0.00804330493996531 iter num 100\n",
            "loss 29.51436996459961 average time 0.008249514220005949 iter num 100\n",
            "loss 21.547042846679688 average time 0.0081751150801756 iter num 100\n",
            "loss 42.803062438964844 average time 0.007928357090113422 iter num 100\n",
            "loss 38.529781341552734 average time 0.008042415600175445 iter num 100\n",
            "loss 44.9981689453125 average time 0.008074893450084346 iter num 100\n",
            "loss 35.63672637939453 average time 0.008052626410080848 iter num 100\n",
            "loss 44.63733673095703 average time 0.008217378860099416 iter num 100\n",
            "loss 63.740440368652344 average time 0.008074770179810002 iter num 100\n",
            "loss 58.19991683959961 average time 0.008183495400026004 iter num 100\n",
            "loss 30.153186798095703 average time 0.008127288360119564 iter num 100\n",
            "loss 18.876934051513672 average time 0.008293147000003956 iter num 100\n",
            "loss 67.94409942626953 average time 0.008041578389911591 iter num 100\n",
            "loss 27.802194595336914 average time 0.008128262880018156 iter num 100\n",
            "loss 51.65989685058594 average time 0.007904183779974119 iter num 100\n",
            "loss 53.39796447753906 average time 0.00819313381005486 iter num 100\n",
            "loss 53.68716812133789 average time 0.008217578430067079 iter num 100\n",
            "loss 29.008636474609375 average time 0.00813908796008036 iter num 100\n",
            "loss 49.193885803222656 average time 0.008138186100040912 iter num 100\n",
            "loss 57.477333068847656 average time 0.00796874817009666 iter num 100\n",
            "loss 61.84587097167969 average time 0.007982459789873246 iter num 100\n",
            "loss 34.4809684753418 average time 0.008047378720148117 iter num 100\n",
            "loss 27.523250579833984 average time 0.008276021030123957 iter num 100\n",
            "loss 58.258758544921875 average time 0.008090881339958287 iter num 100\n",
            "loss 48.633914947509766 average time 0.008084679980020154 iter num 100\n",
            "loss 49.873260498046875 average time 0.008034862559979957 iter num 100\n",
            "loss 39.00084686279297 average time 0.008706948339986411 iter num 100\n",
            "loss 29.51613998413086 average time 0.008233977250038152 iter num 100\n",
            "loss 53.10211181640625 average time 0.008089956880176032 iter num 100\n",
            "loss 54.16815185546875 average time 0.008723831120078103 iter num 100\n",
            "loss 35.167076110839844 average time 0.008176021550134464 iter num 100\n",
            "loss 54.454532623291016 average time 0.008091821310044907 iter num 100\n",
            "loss 35.122642517089844 average time 0.00808705971994641 iter num 100\n",
            "loss 62.94386291503906 average time 0.008309140809906239 iter num 100\n",
            "loss 57.63331985473633 average time 0.008064178839922533 iter num 100\n",
            "loss 20.172653198242188 average time 0.008170088740043865 iter num 100\n",
            "loss 50.45927810668945 average time 0.008022925499972189 iter num 100\n",
            "loss 43.08631896972656 average time 0.008020845859828114 iter num 100\n",
            "loss 86.70353698730469 average time 0.008360343799758994 iter num 100\n",
            "loss 50.65079116821289 average time 0.0081037383500734 iter num 100\n",
            "loss 24.90582275390625 average time 0.0081454854200274 iter num 100\n",
            "loss 40.27150344848633 average time 0.008108341889965232 iter num 100\n",
            "loss 35.304969787597656 average time 0.007993995160068152 iter num 100\n",
            "loss 48.80284118652344 average time 0.008019157870112394 iter num 100\n",
            "loss 33.9094123840332 average time 0.008050106540104025 iter num 100\n",
            "loss 43.808258056640625 average time 0.008065901039917662 iter num 100\n",
            "loss 53.614891052246094 average time 0.008066301960025158 iter num 100\n",
            "loss 45.39091491699219 average time 0.0080306970599122 iter num 100\n",
            "loss 32.29217529296875 average time 0.008107955869818398 iter num 100\n",
            "loss 33.95355987548828 average time 0.00807761844000197 iter num 100\n",
            "loss 34.10855484008789 average time 0.008129984079860151 iter num 100\n",
            "loss 21.7734317779541 average time 0.008158084610222431 iter num 100\n",
            "loss 43.35725402832031 average time 0.008341178100126854 iter num 100\n",
            "loss 44.57796096801758 average time 0.008065364450012567 iter num 100\n",
            "loss 28.842304229736328 average time 0.00800104244008253 iter num 100\n",
            "loss 20.169525146484375 average time 0.008099124709988246 iter num 100\n",
            "loss 38.280738830566406 average time 0.008553011610165412 iter num 100\n",
            "loss 26.006561279296875 average time 0.008030819660161797 iter num 100\n",
            "loss 57.91952133178711 average time 0.008112917049984389 iter num 100\n",
            "loss 67.6416244506836 average time 0.008500722910139303 iter num 100\n",
            "loss 21.51764678955078 average time 0.008082442150043789 iter num 100\n",
            "loss 33.71759033203125 average time 0.00799823902991193 iter num 100\n",
            "loss 44.02230453491211 average time 0.008092045890152803 iter num 100\n",
            "loss 54.13186264038086 average time 0.008067964469846629 iter num 100\n",
            "loss 65.80049133300781 average time 0.007983058459831226 iter num 100\n",
            "loss 48.20000457763672 average time 0.008320918699682807 iter num 100\n",
            "loss 41.14781951904297 average time 0.008022391209924535 iter num 100\n",
            "loss 41.99833297729492 average time 0.00805491659026302 iter num 100\n",
            "loss 19.914592742919922 average time 0.007944528429907222 iter num 100\n",
            "loss 30.82138442993164 average time 0.008206083920013043 iter num 100\n",
            "loss 40.34733581542969 average time 0.007981674440088682 iter num 100\n",
            "loss 32.326866149902344 average time 0.00812461286001053 iter num 100\n",
            "loss 24.131519317626953 average time 0.00813093202013988 iter num 100\n",
            "loss 27.69122314453125 average time 0.008612547209886544 iter num 100\n",
            "loss 48.570411682128906 average time 0.008292103339881578 iter num 100\n",
            "loss 32.714141845703125 average time 0.00818913445011276 iter num 100\n",
            "loss 58.282020568847656 average time 0.00815637929008517 iter num 100\n",
            "loss 26.339427947998047 average time 0.008108986969891702 iter num 100\n",
            "loss 48.91944885253906 average time 0.008141269370062218 iter num 100\n",
            "loss 22.580244064331055 average time 0.008120326180178382 iter num 100\n",
            "loss 18.323402404785156 average time 0.008396599180086924 iter num 100\n",
            "loss 37.30451965332031 average time 0.008701438250136561 iter num 100\n",
            "loss 26.978130340576172 average time 0.00812041267985478 iter num 100\n",
            "loss 17.9988956451416 average time 0.008102547710113867 iter num 100\n",
            "loss 16.60284423828125 average time 0.0081351352400452 iter num 100\n",
            "loss 35.43721008300781 average time 0.008003014380010427 iter num 100\n",
            "loss 25.184358596801758 average time 0.00808072372990864 iter num 100\n",
            "loss 13.837749481201172 average time 0.008080269309903087 iter num 100\n",
            "loss 36.758644104003906 average time 0.008698827179941873 iter num 100\n",
            "loss 15.231847763061523 average time 0.00805972848971578 iter num 100\n",
            "loss 11.247307777404785 average time 0.00818257899984019 iter num 100\n",
            "loss 15.668456077575684 average time 0.008100525680056307 iter num 100\n",
            "loss 11.695932388305664 average time 0.008049619949924819 iter num 100\n",
            "loss 12.397597312927246 average time 0.008128383799958101 iter num 100\n",
            "loss 11.666473388671875 average time 0.008063351580094604 iter num 100\n",
            "loss 9.329017639160156 average time 0.008122776979980699 iter num 100\n",
            "loss 11.326508522033691 average time 0.008087745519915188 iter num 100\n",
            "loss 5.838667869567871 average time 0.008137589679936354 iter num 100\n",
            "loss 6.337447166442871 average time 0.008129045710011268 iter num 100\n",
            "loss 5.639461994171143 average time 0.00806519137990108 iter num 100\n",
            "loss 7.448593616485596 average time 0.008315453590148536 iter num 100\n",
            "loss 10.022026062011719 average time 0.008087886080029421 iter num 100\n",
            "loss 2.8999218940734863 average time 0.008044175709910633 iter num 100\n",
            "loss 2.6525967121124268 average time 0.008093656230121269 iter num 100\n",
            "loss 5.5394463539123535 average time 0.008209233639863669 iter num 100\n",
            "loss 3.970593214035034 average time 0.00810265231964877 iter num 100\n",
            "loss 1.9262067079544067 average time 0.008165931230141723 iter num 100\n",
            "loss 7.605729103088379 average time 0.008215323739859741 iter num 100\n",
            "loss 2.8805670738220215 average time 0.008110693209928285 iter num 100\n",
            "loss 4.740337371826172 average time 0.00836744711992651 iter num 100\n",
            "loss 5.385188102722168 average time 0.00813898036005412 iter num 100\n",
            "loss 4.011033058166504 average time 0.008120815500078607 iter num 100\n",
            "loss 6.58607292175293 average time 0.007934615129961457 iter num 100\n",
            "loss 3.224271535873413 average time 0.007941859499951534 iter num 100\n",
            "loss 5.106851100921631 average time 0.008109103719980339 iter num 100\n",
            "loss 1.9729996919631958 average time 0.007975267149959109 iter num 100\n",
            "loss 3.437804937362671 average time 0.008154496420065698 iter num 100\n",
            "loss 3.8632447719573975 average time 0.008043169359843887 iter num 100\n",
            "loss 2.8766889572143555 average time 0.008475498419866199 iter num 100\n",
            "loss 5.789254665374756 average time 0.008012760629862897 iter num 100\n",
            "loss 4.57478666305542 average time 0.008073813910232274 iter num 100\n",
            "loss 5.790346145629883 average time 0.008027988920039206 iter num 100\n",
            "loss 3.6131410598754883 average time 0.007948294109737616 iter num 100\n",
            "loss 5.5741400718688965 average time 0.008054434719888377 iter num 100\n",
            "loss 7.319239616394043 average time 0.008069272579923563 iter num 100\n",
            "loss 6.506725311279297 average time 0.008041901960132236 iter num 100\n",
            "loss 1.5814783573150635 average time 0.008120589729951461 iter num 100\n",
            "loss 2.8303275108337402 average time 0.008013166359705792 iter num 100\n",
            "loss 4.0884246826171875 average time 0.008116123340223567 iter num 100\n",
            "loss 3.970391273498535 average time 0.008216483699688979 iter num 100\n",
            "loss 2.644136428833008 average time 0.008017054869997082 iter num 100\n",
            "loss 7.1147003173828125 average time 0.008575289340042218 iter num 100\n",
            "loss 3.580031633377075 average time 0.008027369740193534 iter num 100\n",
            "loss 3.9507853984832764 average time 0.008249342429771787 iter num 100\n",
            "loss 1.9342085123062134 average time 0.008103711620460672 iter num 100\n",
            "loss 4.34352445602417 average time 0.00805248948992812 iter num 100\n",
            "loss 6.192610263824463 average time 0.008034877779828094 iter num 100\n",
            "loss 3.637965440750122 average time 0.00804950676014414 iter num 100\n",
            "loss 5.489809036254883 average time 0.008059569529905275 iter num 100\n",
            "loss 3.916172504425049 average time 0.007995219269942027 iter num 100\n",
            "loss 6.003042221069336 average time 0.008086769879882922 iter num 100\n",
            "loss 3.91078519821167 average time 0.007996353849848674 iter num 100\n",
            "loss 8.696883201599121 average time 0.008038509259640704 iter num 100\n",
            "loss 3.9906492233276367 average time 0.008096477839862927 iter num 100\n",
            "loss 4.812880516052246 average time 0.008016943020083999 iter num 100\n",
            "loss 4.47882080078125 average time 0.008031760430167196 iter num 100\n",
            "loss 3.8693904876708984 average time 0.008087996360045509 iter num 100\n",
            "loss 4.1992411613464355 average time 0.008187973180320114 iter num 100\n",
            "loss 3.9287798404693604 average time 0.008288944999912928 iter num 100\n",
            "loss 2.1681222915649414 average time 0.00869577412999206 iter num 100\n",
            "loss 5.697703838348389 average time 0.008312403290074145 iter num 100\n",
            "loss 1.0040444135665894 average time 0.007994422449919512 iter num 100\n",
            "loss 3.455465078353882 average time 0.008032465940050314 iter num 100\n",
            "loss 2.70385479927063 average time 0.008052627790057158 iter num 100\n",
            "loss 1.6960241794586182 average time 0.008124870439969527 iter num 100\n",
            "loss 7.930771350860596 average time 0.008021847479649296 iter num 100\n",
            "loss 3.810340404510498 average time 0.008049686710037349 iter num 100\n",
            "loss 3.7799315452575684 average time 0.008682121499914501 iter num 100\n",
            "loss 3.4893271923065186 average time 0.008166151140358124 iter num 100\n",
            "loss 4.0772881507873535 average time 0.008191073979942303 iter num 100\n",
            "loss 4.217813968658447 average time 0.008118647419942136 iter num 100\n",
            "loss 5.159431457519531 average time 0.007988822510233149 iter num 100\n",
            "loss 3.729581356048584 average time 0.008067594399835797 iter num 100\n",
            "loss 3.0069055557250977 average time 0.008183100660280615 iter num 100\n",
            "loss 2.3411662578582764 average time 0.008147854099952383 iter num 100\n",
            "loss 4.0428266525268555 average time 0.008193122620286885 iter num 100\n",
            "loss 5.628401756286621 average time 0.007984597489703447 iter num 100\n",
            "loss 6.672317981719971 average time 0.008023785299665178 iter num 100\n",
            "loss 6.538495063781738 average time 0.00803022529991722 iter num 100\n",
            "loss 3.5418996810913086 average time 0.008548250510139042 iter num 100\n",
            "loss 6.879951000213623 average time 0.007994256910023978 iter num 100\n",
            "loss 2.5149717330932617 average time 0.007981439280192716 iter num 100\n",
            "loss 2.9581000804901123 average time 0.007999117900180864 iter num 100\n",
            "loss 3.2292842864990234 average time 0.00800420392992237 iter num 100\n",
            "loss 4.757018089294434 average time 0.007955149779991188 iter num 100\n",
            "loss 3.8004496097564697 average time 0.007966182160016615 iter num 100\n",
            "loss 4.306015968322754 average time 0.00803952609992848 iter num 100\n",
            "loss 4.309551239013672 average time 0.008060509570022988 iter num 100\n",
            "loss 3.218831777572632 average time 0.007955943970155204 iter num 100\n",
            "loss 4.165575981140137 average time 0.008011253500189924 iter num 100\n",
            "loss 3.384459972381592 average time 0.007979250349926588 iter num 100\n",
            "loss 8.088719367980957 average time 0.007979557299950101 iter num 100\n",
            "loss 2.7097830772399902 average time 0.008020804869884159 iter num 100\n",
            "loss 4.251108169555664 average time 0.008278223309898749 iter num 100\n",
            "loss 3.34539794921875 average time 0.008196766929904698 iter num 100\n",
            "loss 1.9988689422607422 average time 0.008030827280017548 iter num 100\n",
            "loss 2.2264766693115234 average time 0.008047883050167001 iter num 100\n",
            "loss 4.358580589294434 average time 0.008136837989841297 iter num 100\n",
            "loss 3.882735252380371 average time 0.008057488699851092 iter num 100\n",
            "loss 2.034511089324951 average time 0.007994051279965789 iter num 100\n",
            "loss 2.4062609672546387 average time 0.008050297160043556 iter num 100\n",
            "loss 3.562394142150879 average time 0.00811065067005984 iter num 100\n",
            "loss 4.460245132446289 average time 0.008502115039991622 iter num 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 30000\n",
              "\tepoch: 300\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 300\n",
              "\toutput: 4.460245132446289\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c70eef1-04e4-40a6-d827-f2c8846e7f49"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05]*10]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.8888]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d355b707-76cf-4ae1-fef1-990a1cfaced0"
      },
      "source": [
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05]*10]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.3190e-02, -6.8024e-03,  4.0406e-02,  7.1551e-01,  3.6934e+00,\n",
              "         -4.2241e+00, -2.4204e-02,  1.0757e-03,  3.2375e-02,  1.5809e+00,\n",
              "         -3.6054e-01, -2.0697e+00, -2.2368e-02, -1.2811e-02,  2.9612e-02,\n",
              "          1.3492e+00,  3.7432e+00, -3.5720e+00, -4.5685e-02, -1.5914e-02,\n",
              "          5.4077e-02,  1.1930e+00,  5.8875e+00, -6.2974e+00, -3.8035e-02,\n",
              "         -1.5457e-02,  4.5165e-02,  2.1995e+00,  2.5406e+00,  4.9683e-01,\n",
              "         -2.9481e-02, -6.5766e-03,  3.9756e-02,  1.4767e+00,  6.2216e+00,\n",
              "         -2.9301e+00, -3.7061e-02, -6.9420e-03,  4.0077e-02,  1.3633e+00,\n",
              "          3.2187e+00, -6.3199e+00, -3.3045e-02, -7.0926e-03,  4.1046e-02,\n",
              "          1.2168e+00, -3.5831e-01, -1.4971e+00, -3.8251e-02, -2.8914e-03,\n",
              "          4.1947e-02,  1.0327e+00, -5.8363e-01, -2.0951e+00, -4.3308e-02,\n",
              "         -1.6717e-02,  4.8837e-02,  8.8746e-01,  4.5664e+00, -7.4679e+00]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "82990993-aded-4ce8-a69f-5477169a7aed"
      },
      "source": [
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, S, 0.35, 0.1, 0.05] + ([110.0, 100.0, 120.0, 0.35, 0.1, 0.05]*9)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 300, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7252d27cd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2610c6-e8e4-4d3b-9611-1dbd360afb13"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05]*10]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-7.4465e-04, -2.8638e-05,  7.6823e-04, -5.3477e-04,  6.2361e-02,\n",
              "          -8.0012e-02, -5.9937e-05, -3.7046e-05,  5.7979e-05,  6.5147e-03,\n",
              "          -9.4844e-04, -5.9900e-03, -5.4921e-05, -5.1203e-05,  6.7252e-05,\n",
              "           1.0131e-03,  1.1272e-02, -1.3352e-02, -7.1744e-05, -7.1100e-05,\n",
              "           7.4695e-05,  4.8369e-03,  1.0527e-02, -1.1261e-02, -4.7681e-05,\n",
              "          -2.8765e-05,  4.6975e-05,  5.1215e-03,  6.1947e-03,  8.5363e-03,\n",
              "          -5.1383e-05,  4.6382e-06,  6.0869e-05,  3.7714e-04,  9.8458e-03,\n",
              "          -4.3109e-03, -6.8546e-05,  3.0225e-05,  5.1435e-05,  5.3562e-03,\n",
              "           8.3908e-03, -1.8440e-02, -3.8319e-05, -1.7639e-05,  4.1045e-05,\n",
              "           7.1630e-03, -3.8234e-03, -3.3764e-03, -2.5716e-05, -1.5380e-05,\n",
              "           2.7715e-05,  1.7978e-03,  1.8710e-03, -2.2327e-03, -5.2896e-05,\n",
              "          -3.2788e-05,  6.4109e-05,  8.7442e-05,  4.0136e-03, -1.4433e-02]],\n",
              "        device='cuda:0'),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "73260606-f944-41ab-f4e2-cf65e33f19e0"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, S, 0.35, 0.1, 0.05] + ([110.0, 100.0, 120.0, 0.35, 0.1, 0.05]*9)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 250, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7252c7d990>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "ddd6f478-7d5a-4bc5-d51b-899c8508aa01"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 100.0, 120.0, sigma, 0.1, 0.05] + ([110.0, 100.0, 120.0, 0.35, 0.1, 0.05]*9)]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7252bb0790>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "98963f21-9fdc-43cd-e214-a56278b4471b"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "upper bound is too small\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ade689496c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mquoted_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbisection_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoted_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'implied volativity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    }
  ]
}