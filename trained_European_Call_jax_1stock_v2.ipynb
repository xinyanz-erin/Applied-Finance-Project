{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/trained_European_Call_jax_1stock_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "1656f342-0bff-439e-adf3-3ceca9ae73fc"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0807]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.2597*0.2597\n",
        "initial_stocks = jnp.array([0.7178]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.2106\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5236568\n",
            "[1.0001798]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb900e8-5b98-4c0b-a37d-c7f10983dee2"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "# version 1, 2, 6\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 1.0)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          K = np.random.random(1) * 1.0\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32) # remember to change this!\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = 15, stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mgw7IYgtcvx"
      },
      "source": [
        "# %%writefile cupy_dataset.py\n",
        "# # version 3, 4, 5, 7\n",
        "# import cupy\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "# from jax import random\n",
        "# from jax import jit\n",
        "# import numpy as np\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "#     stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "#     stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "#                             jax.ops.index[0],         # initialization of stock prices\n",
        "#                             initial_stocks)\n",
        "#     noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "#     sigma = jnp.diag(cov) ** 0.5\n",
        "#     dt = T / numsteps\n",
        "#     def time_step(t, val):\n",
        "#         #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "#         dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "#         val = jax.ops.index_update(val,\n",
        "#                             jax.ops.index[t],\n",
        "#                             val[t-1] * dx2)\n",
        "#         return val\n",
        "#     return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "# def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "#     return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "#     # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "# ###################################################################################################\n",
        "# # these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "# fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "# batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "# ###################################################################################################\n",
        "\n",
        "# class OptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 50\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = 1.0 # assume T = 1, use float here\n",
        "#         self.seed = seed\n",
        "#         np.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num >= self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "#         X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           rng = jax.random.PRNGKey(self.seed)\n",
        "#           rng, key = jax.random.split(rng)\n",
        "\n",
        "#           ################################################################################################### generate random input numbers\n",
        "\n",
        "#           initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 200.0)\n",
        "\n",
        "#           corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "#           sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.4)\n",
        "#           cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "#           r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "#           drift = r # To match BS, use drift = r\n",
        "\n",
        "#           T = self.T\n",
        "#           K = np.random.random(1) * 200.0\n",
        "\n",
        "#           ###################################################################################################\n",
        "#           ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "#           keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "#           European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "#           gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "#           Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "#           ###################################################################################################\n",
        "#           ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "#           Y[op, 0] = European_Call_price\n",
        "#           Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "#           paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "#           paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "#           X[op,] = cupy.array(paras)\n",
        "\n",
        "#           ###################################################################################################\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# # ds = OptionDataSet(max_len = 2, number_path = 10000, batch = 2, seed = 15, stocks=3) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# # for i in ds:\n",
        "# #     print(i)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efnASCRJfH_R"
      },
      "source": [
        "# #%%writefile cupy_dataset.py\n",
        "# import cupy\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "# from jax import random\n",
        "# from jax import jit\n",
        "# import numpy as np\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "#     stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "#     stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "#                             jax.ops.index[0],         # initialization of stock prices\n",
        "#                             initial_stocks)\n",
        "#     noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "#     sigma = jnp.diag(cov) ** 0.5\n",
        "#     dt = T / numsteps\n",
        "#     def time_step(t, val):\n",
        "#         dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "#         val = jax.ops.index_update(val,\n",
        "#                             jax.ops.index[t],\n",
        "#                             val[t-1] * dx)\n",
        "#         return val\n",
        "#     return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "# def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "#     return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "# ###################################################################################################\n",
        "# # these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "# fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "# batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "# ###################################################################################################\n",
        "\n",
        "# class OptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len, number_path, batch, stocks):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 50\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = 1.0 # assume T = 1, use float here\n",
        "#         # self.seed = seed\n",
        "#         # np.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num >= self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "#         X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           np.random.seed(np.random.randint(10000))\n",
        "#           rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "#           rng, key = jax.random.split(rng)\n",
        "\n",
        "#           ################################################################################################### generate random input numbers\n",
        "\n",
        "#           initial_stocks = jnp.array(np.random.random(self.N_STOCKS) * 1.0)\n",
        "\n",
        "#           corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "#           sigma = jnp.array(np.random.random(self.N_STOCKS) * 0.3)\n",
        "#           cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "#           r = jnp.repeat(jnp.array(np.random.random(1) * 0.1), self.N_STOCKS)\n",
        "#           drift = jnp.array(np.random.random(self.N_STOCKS) * 0.1)\n",
        "\n",
        "#           T = self.T\n",
        "#           K = np.random.random(1) * 1.0\n",
        "\n",
        "#           ###################################################################################################\n",
        "#           ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "#           keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "#           European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "#           gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "#           Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "#           ###################################################################################################\n",
        "#           ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "#           Y[op, 0] = European_Call_price\n",
        "#           Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "#           # T, K, S, sigma, mu, r\n",
        "#           paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "#           paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "#           X[op,] = cupy.array(paras)\n",
        "\n",
        "#           ###################################################################################################\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 10000, batch = 2, stocks=3) # When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "db1edbf2-59ac-434d-f015-c8c0896c1349"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 1.0, 1.0, 0.3, 0.1, 0.1]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O0MfhTadgkO"
      },
      "source": [
        "# %%writefile model.py\n",
        "# # version 3,7\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6*1, hidden) # remember to change this!\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1) # 1 outputs: price\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([1, 200.0, 200.0, 0.4, 0.1, 0.1]*1)) # don't use numpy here - will give error later\n",
        "#                                                                                # T, K, S, sigma, mu, r\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y6naGih16DQ"
      },
      "source": [
        "# # version 4, 5\n",
        "# %%writefile model.py\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6*1, hidden) # remember to change this!\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 2) # 2 outputs: price, delta\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([1, 200.0, 200.0, 0.4, 0.1, 0.1]*1)) # don't use numpy here - will give error later\n",
        "#                                                                                # T, K, S, sigma, mu, r\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyUoWsGTDQmN"
      },
      "source": [
        "# # version 6\n",
        "# %%writefile model.py\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6*1, hidden) # remember to change this!\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 2) # 2 outputs: price, delta\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([1.0, 1.0, 1.0, 0.3, 0.1, 0.1]*1)) # don't use numpy here - will give error later\n",
        "#                                                                                # T, K, S, sigma, mu, r\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "0ad4ed86-7713-48ca-9c00-1efb225b5008"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.7)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBEzHyv7n-z7"
      },
      "source": [
        "# # version 1 & version 3 (200)\n",
        "# # If memory is not enough, try changing parameters and restarting session\n",
        "# # loss will converge\n",
        "\n",
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import OptionDataSet\n",
        "# import numpy as np\n",
        "# timer = Timer(average=True)\n",
        "# model = Net().cuda()\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = OptionDataSet(max_len = 100, number_path = int(1e6), batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     y = y[:,0].reshape(-1,1)\n",
        "#     #print(y)\n",
        "#     y_pred = model(x)\n",
        "#     #print(y_pred)\n",
        "#     loss = loss_fn(y_pred[:,:], y[:,:]) # compute MSE between the 2 arrays\n",
        "#     #print(loss)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 20\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "087e5e27-01ff-44be-afa7-03aa70bc1d09"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.013804377056658268 average time 0.20097316244996363 iter num 20\n",
            "loss 0.013547497801482677 average time 0.10784878504999824 iter num 40\n",
            "loss 0.015108984895050526 average time 0.0768151311666467 iter num 60\n",
            "loss 0.008190256543457508 average time 0.06126157638748282 iter num 80\n",
            "loss 0.01769290119409561 average time 0.05192215447998933 iter num 100\n",
            "loss 0.02364984340965748 average time 0.03646207305002917 iter num 20\n",
            "loss 0.01126737892627716 average time 0.025683993675067997 iter num 40\n",
            "loss 0.015724733471870422 average time 0.02225810866671812 iter num 60\n",
            "loss 0.005341275129467249 average time 0.020356744412532636 iter num 80\n",
            "loss 0.0055426666513085365 average time 0.01933542493002278 iter num 100\n",
            "loss 0.016154490411281586 average time 0.0367463811999869 iter num 20\n",
            "loss 0.002646077424287796 average time 0.025774725599967498 iter num 40\n",
            "loss 0.004602628760039806 average time 0.022021366599968437 iter num 60\n",
            "loss 0.004555930383503437 average time 0.02020826411246617 iter num 80\n",
            "loss 0.0032389662228524685 average time 0.019163010759971257 iter num 100\n",
            "loss 0.001039105118252337 average time 0.03711476189998848 iter num 20\n",
            "loss 0.0009425661992281675 average time 0.026068012800021734 iter num 40\n",
            "loss 0.0005286865634843707 average time 0.022773172933360303 iter num 60\n",
            "loss 7.315122638829052e-05 average time 0.020799667262525644 iter num 80\n",
            "loss 0.0002579714637249708 average time 0.019592502380023687 iter num 100\n",
            "loss 0.0006481822929345071 average time 0.03639458315005868 iter num 20\n",
            "loss 0.0006201652577146888 average time 0.025531160675052435 iter num 40\n",
            "loss 0.0005881967954337597 average time 0.022311794250049387 iter num 60\n",
            "loss 0.0002345465763937682 average time 0.02053366223753983 iter num 80\n",
            "loss 0.00014648449723608792 average time 0.01944031873002132 iter num 100\n",
            "loss 0.0002345891116419807 average time 0.03680906565004989 iter num 20\n",
            "loss 0.0009123375057242811 average time 0.025758635100044103 iter num 40\n",
            "loss 0.0003416985855437815 average time 0.022205919516689696 iter num 60\n",
            "loss 0.0003016524133272469 average time 0.02031940752499963 iter num 80\n",
            "loss 0.00016347301425412297 average time 0.019447513410004832 iter num 100\n",
            "loss 0.0009443722665309906 average time 0.03664548025001295 iter num 20\n",
            "loss 0.0006564826471731067 average time 0.025744909800016556 iter num 40\n",
            "loss 0.00019740447169169784 average time 0.022186796216654633 iter num 60\n",
            "loss 0.0004023844376206398 average time 0.02048888596249867 iter num 80\n",
            "loss 0.0003811195492744446 average time 0.019396286959986356 iter num 100\n",
            "loss 0.0007367903599515557 average time 0.03735712599996077 iter num 20\n",
            "loss 0.0002797560300678015 average time 0.026394137849968045 iter num 40\n",
            "loss 0.0003209576243534684 average time 0.02264383701665338 iter num 60\n",
            "loss 0.0002202265168307349 average time 0.02086502906248029 iter num 80\n",
            "loss 0.00018789657042361796 average time 0.019845717859975593 iter num 100\n",
            "loss 0.0003562472411431372 average time 0.03753237294988594 iter num 20\n",
            "loss 0.0005352333537302911 average time 0.026358530224990774 iter num 40\n",
            "loss 0.00048499787226319313 average time 0.02247647933332549 iter num 60\n",
            "loss 0.00037289492320269346 average time 0.02054927669999529 iter num 80\n",
            "loss 0.00014382934023160487 average time 0.019372877270006937 iter num 100\n",
            "loss 0.0005472198245115578 average time 0.036875363799958906 iter num 20\n",
            "loss 0.0002389481960562989 average time 0.025822086149969437 iter num 40\n",
            "loss 0.0003529091482050717 average time 0.022313207249991744 iter num 60\n",
            "loss 0.0004383274063002318 average time 0.020440587537490274 iter num 80\n",
            "loss 0.000170099112438038 average time 0.019401310359985473 iter num 100\n",
            "loss 0.0005890073953196406 average time 0.0376821795999831 iter num 20\n",
            "loss 0.00043226132402196527 average time 0.026232533500024146 iter num 40\n",
            "loss 0.00011922120756935328 average time 0.022537448950023038 iter num 60\n",
            "loss 0.0006656981422565877 average time 0.020706015362526385 iter num 80\n",
            "loss 0.0002974683011416346 average time 0.0195656357200005 iter num 100\n",
            "loss 0.0005226711509749293 average time 0.03742283720002888 iter num 20\n",
            "loss 0.00021838568500243127 average time 0.026488622950023456 iter num 40\n",
            "loss 0.00021080096485093236 average time 0.022764231683337736 iter num 60\n",
            "loss 0.00014624130562879145 average time 0.021018697412506525 iter num 80\n",
            "loss 9.150971891358495e-05 average time 0.019777935490001255 iter num 100\n",
            "loss 0.00031937300809659064 average time 0.03726123549995464 iter num 20\n",
            "loss 0.0003655211185105145 average time 0.02621261107497048 iter num 40\n",
            "loss 0.00013804396439809352 average time 0.022523841299963956 iter num 60\n",
            "loss 6.570070399902761e-05 average time 0.02057314646247619 iter num 80\n",
            "loss 0.00046255410416051745 average time 0.019416349399984937 iter num 100\n",
            "loss 0.0006494401022791862 average time 0.036565209400009735 iter num 20\n",
            "loss 0.0006408192566595972 average time 0.025707575899969016 iter num 40\n",
            "loss 0.0008337196195498109 average time 0.021994149366643494 iter num 60\n",
            "loss 0.00013848673552274704 average time 0.020149753424971096 iter num 80\n",
            "loss 0.00037037715082988143 average time 0.019193115349976322 iter num 100\n",
            "loss 0.00015444001473952085 average time 0.03728206825003326 iter num 20\n",
            "loss 0.0012687405105680227 average time 0.026302495725042264 iter num 40\n",
            "loss 0.0015859021805226803 average time 0.02253164286670047 iter num 60\n",
            "loss 0.00024374053464271128 average time 0.020592667575044743 iter num 80\n",
            "loss 0.0001948061544680968 average time 0.01953446540002915 iter num 100\n",
            "loss 0.00025476637529209256 average time 0.036182199299992134 iter num 20\n",
            "loss 0.00030473709921352565 average time 0.025598160175013616 iter num 40\n",
            "loss 0.0009486984345130622 average time 0.021985263750002558 iter num 60\n",
            "loss 0.0001649654150241986 average time 0.020185160874990515 iter num 80\n",
            "loss 0.000185439464985393 average time 0.019258297909991597 iter num 100\n",
            "loss 0.00029752060072496533 average time 0.03696898035000231 iter num 20\n",
            "loss 0.00039490338531322777 average time 0.026014950124977076 iter num 40\n",
            "loss 0.00037688505835831165 average time 0.02225451506664437 iter num 60\n",
            "loss 0.0002662859915290028 average time 0.020528881762464833 iter num 80\n",
            "loss 0.00041160083492286503 average time 0.01943209127996397 iter num 100\n",
            "loss 0.0017471604514867067 average time 0.037157740250017925 iter num 20\n",
            "loss 0.0010123888496309519 average time 0.025915273774978688 iter num 40\n",
            "loss 0.0005347809637896717 average time 0.022203482066637057 iter num 60\n",
            "loss 0.00036026304587721825 average time 0.020337772399966526 iter num 80\n",
            "loss 3.9804261177778244e-05 average time 0.019358145049977793 iter num 100\n",
            "loss 0.0002069300098810345 average time 0.037408060150050916 iter num 20\n",
            "loss 0.00014218840806279331 average time 0.026567267175028064 iter num 40\n",
            "loss 0.00013744289753958583 average time 0.02274653471668747 iter num 60\n",
            "loss 0.00011333554721204564 average time 0.020807878400000847 iter num 80\n",
            "loss 0.00038921667146496475 average time 0.019655523460010045 iter num 100\n",
            "loss 0.0010756086558103561 average time 0.03699818680004228 iter num 20\n",
            "loss 0.0016886130906641483 average time 0.02629493022503766 iter num 40\n",
            "loss 0.0006390145281329751 average time 0.022448840183339296 iter num 60\n",
            "loss 0.0003634376625996083 average time 0.020600039375011647 iter num 80\n",
            "loss 0.00012265463010407984 average time 0.0194867719100057 iter num 100\n",
            "loss 0.0006674084579572082 average time 0.036280234049991124 iter num 20\n",
            "loss 0.0003298697411082685 average time 0.025558873624970602 iter num 40\n",
            "loss 0.0003791491035372019 average time 0.022284007166619327 iter num 60\n",
            "loss 0.0001673164078965783 average time 0.020452024237454226 iter num 80\n",
            "loss 7.121720409486443e-05 average time 0.01931292002996088 iter num 100\n",
            "loss 7.900062337284908e-05 average time 0.03679771040001469 iter num 20\n",
            "loss 0.0004677753313444555 average time 0.026200717775043358 iter num 40\n",
            "loss 0.0005341048818081617 average time 0.022491329316691615 iter num 60\n",
            "loss 0.00019132794113829732 average time 0.02062117128753016 iter num 80\n",
            "loss 0.0001909018901642412 average time 0.01952088304000881 iter num 100\n",
            "loss 0.0003724343841895461 average time 0.03769048295005177 iter num 20\n",
            "loss 0.00019842945039272308 average time 0.026577565225022682 iter num 40\n",
            "loss 0.0001947805576492101 average time 0.022604676033332303 iter num 60\n",
            "loss 5.474652425618842e-05 average time 0.020687939499987353 iter num 80\n",
            "loss 0.00012444399180822074 average time 0.019507141169983696 iter num 100\n",
            "loss 0.0006541108014062047 average time 0.03684251399995446 iter num 20\n",
            "loss 0.0003203862579539418 average time 0.025837850224979773 iter num 40\n",
            "loss 0.00046293274499475956 average time 0.022411141016626366 iter num 60\n",
            "loss 9.456886618863791e-05 average time 0.02049268418745669 iter num 80\n",
            "loss 0.00011533511860761791 average time 0.019290834849985002 iter num 100\n",
            "loss 0.0001601705007487908 average time 0.03656130255001244 iter num 20\n",
            "loss 0.00010008456592913717 average time 0.026108867799985093 iter num 40\n",
            "loss 5.881971446797252e-05 average time 0.02264033558332509 iter num 60\n",
            "loss 0.00019878448802046478 average time 0.020928837774994234 iter num 80\n",
            "loss 9.778267849469557e-05 average time 0.01981738479998967 iter num 100\n",
            "loss 0.0007382757030427456 average time 0.036843515950045 iter num 20\n",
            "loss 0.000834595353808254 average time 0.02616123295000534 iter num 40\n",
            "loss 0.00023304502246901393 average time 0.022709372116613242 iter num 60\n",
            "loss 3.849132190225646e-05 average time 0.02080176529998994 iter num 80\n",
            "loss 0.00010996127821272239 average time 0.01972776770002838 iter num 100\n",
            "loss 9.69398461165838e-05 average time 0.037761520199956065 iter num 20\n",
            "loss 6.358577229548246e-05 average time 0.026670643675038262 iter num 40\n",
            "loss 0.00024199552717618644 average time 0.022684627016648543 iter num 60\n",
            "loss 0.00016547154518775642 average time 0.020711756999958197 iter num 80\n",
            "loss 0.00012413313379511237 average time 0.01953316778995031 iter num 100\n",
            "loss 0.0002297821338288486 average time 0.03645429889998013 iter num 20\n",
            "loss 0.0003031915402971208 average time 0.02571612949993778 iter num 40\n",
            "loss 9.759904787642881e-05 average time 0.022197170833275475 iter num 60\n",
            "loss 0.00020791520364582539 average time 0.020423416062430987 iter num 80\n",
            "loss 0.0002024216955760494 average time 0.019267570119964147 iter num 100\n",
            "loss 0.0006171127315610647 average time 0.037815132099967744 iter num 20\n",
            "loss 0.00011065872968174517 average time 0.026234686425027576 iter num 40\n",
            "loss 0.00013794764527119696 average time 0.02243886440002522 iter num 60\n",
            "loss 0.00010479705088073388 average time 0.020512180462480954 iter num 80\n",
            "loss 0.00018004373123403639 average time 0.019319896699980747 iter num 100\n",
            "loss 0.0006919465376995504 average time 0.03709600160009359 iter num 20\n",
            "loss 0.00023177840921562165 average time 0.025861973000019134 iter num 40\n",
            "loss 3.81754944100976e-05 average time 0.022250453116703282 iter num 60\n",
            "loss 8.568789053242654e-05 average time 0.020557443499990315 iter num 80\n",
            "loss 0.0001514241739641875 average time 0.01949926832998244 iter num 100\n",
            "loss 0.00016660340770613402 average time 0.03705395279980621 iter num 20\n",
            "loss 8.166950283339247e-05 average time 0.026219558174852865 iter num 40\n",
            "loss 0.0001294993853662163 average time 0.022671831433232606 iter num 60\n",
            "loss 0.00011138750414829701 average time 0.020806939624912958 iter num 80\n",
            "loss 9.65311992331408e-05 average time 0.0197422335999363 iter num 100\n",
            "loss 0.0002694461145438254 average time 0.03681287870003871 iter num 20\n",
            "loss 0.0004021896456833929 average time 0.02607748907507812 iter num 40\n",
            "loss 0.0007854708237573504 average time 0.022322324483351016 iter num 60\n",
            "loss 0.0001014575973385945 average time 0.020442738725034815 iter num 80\n",
            "loss 0.0001414081925759092 average time 0.01933402579003996 iter num 100\n",
            "loss 0.0009126925142481923 average time 0.037237479100031126 iter num 20\n",
            "loss 0.002038855804130435 average time 0.026283391624974683 iter num 40\n",
            "loss 0.0002580590662546456 average time 0.022554890666651773 iter num 60\n",
            "loss 0.00016033365682233125 average time 0.020657763874987724 iter num 80\n",
            "loss 0.0001622053823666647 average time 0.019517346669981634 iter num 100\n",
            "loss 0.00018241829820908606 average time 0.03643956320001962 iter num 20\n",
            "loss 9.45087886066176e-05 average time 0.025608587375018033 iter num 40\n",
            "loss 6.857729749754071e-05 average time 0.021968138933364873 iter num 60\n",
            "loss 0.00012891645019408315 average time 0.020205648437513445 iter num 80\n",
            "loss 2.722083081607707e-05 average time 0.01911267598999984 iter num 100\n",
            "loss 8.668929513078183e-05 average time 0.03738880859996243 iter num 20\n",
            "loss 0.00010667841706890613 average time 0.026646713224999986 iter num 40\n",
            "loss 0.0001675804378464818 average time 0.022814576666708792 iter num 60\n",
            "loss 9.119629248743877e-05 average time 0.020791901562529348 iter num 80\n",
            "loss 1.1306765372864902e-05 average time 0.019564913990025162 iter num 100\n",
            "loss 0.0010310247307643294 average time 0.036771043599947005 iter num 20\n",
            "loss 0.00046746496809646487 average time 0.02580087729998013 iter num 40\n",
            "loss 0.0016706434544175863 average time 0.022331575500008208 iter num 60\n",
            "loss 9.521660103928298e-05 average time 0.02040325686247115 iter num 80\n",
            "loss 2.5780096621019766e-05 average time 0.019279225979962577 iter num 100\n",
            "loss 0.00010390088573331013 average time 0.03654139949999262 iter num 20\n",
            "loss 0.00019388506188988686 average time 0.025769725824966373 iter num 40\n",
            "loss 6.275906343944371e-05 average time 0.02231517391669513 iter num 60\n",
            "loss 7.075766916386783e-05 average time 0.02053676947502936 iter num 80\n",
            "loss 3.729658419615589e-05 average time 0.01952037921001647 iter num 100\n",
            "loss 0.0002483996213413775 average time 0.036362519749945935 iter num 20\n",
            "loss 0.00018051083316095173 average time 0.02571983735001595 iter num 40\n",
            "loss 4.395210271468386e-05 average time 0.02212656669998978 iter num 60\n",
            "loss 3.9934577216627076e-05 average time 0.02023685954999337 iter num 80\n",
            "loss 3.9173253753688186e-05 average time 0.019248979379981393 iter num 100\n",
            "loss 0.00010241415293421596 average time 0.037012165749956694 iter num 20\n",
            "loss 9.667188714956865e-05 average time 0.025983702050007197 iter num 40\n",
            "loss 0.0001580561074661091 average time 0.02222918835001716 iter num 60\n",
            "loss 3.4768447221722454e-05 average time 0.020384611087501982 iter num 80\n",
            "loss 0.00021341224783100188 average time 0.019259145970017925 iter num 100\n",
            "loss 6.0061560361646116e-05 average time 0.037151648200097045 iter num 20\n",
            "loss 0.0003166651586070657 average time 0.026099608300023648 iter num 40\n",
            "loss 0.00024158087035175413 average time 0.022237923116729993 iter num 60\n",
            "loss 1.5674955648137257e-05 average time 0.020569485837563663 iter num 80\n",
            "loss 3.6347195418784395e-05 average time 0.019400400340018678 iter num 100\n",
            "loss 4.251822974765673e-05 average time 0.037018610350060044 iter num 20\n",
            "loss 2.5191615350195207e-05 average time 0.026077228650024153 iter num 40\n",
            "loss 5.700120163965039e-05 average time 0.022270791499992507 iter num 60\n",
            "loss 6.605700764339417e-05 average time 0.020420908937455805 iter num 80\n",
            "loss 7.280157296918333e-05 average time 0.019406630709981984 iter num 100\n",
            "loss 0.00030182432965375483 average time 0.03691018520003127 iter num 20\n",
            "loss 0.0014778405893594027 average time 0.02613084680006068 iter num 40\n",
            "loss 0.0001545224222354591 average time 0.022409842066690545 iter num 60\n",
            "loss 0.0001151021133409813 average time 0.020569844312501574 iter num 80\n",
            "loss 5.7257366279372945e-05 average time 0.01939645768004084 iter num 100\n",
            "loss 0.00074429449159652 average time 0.037456296450091034 iter num 20\n",
            "loss 0.00011084479046985507 average time 0.02603046697499849 iter num 40\n",
            "loss 2.9688526410609484e-05 average time 0.022362459700025282 iter num 60\n",
            "loss 5.947516910964623e-05 average time 0.02047313885003632 iter num 80\n",
            "loss 3.65039668395184e-05 average time 0.019295571380016554 iter num 100\n",
            "loss 0.00017687960644252598 average time 0.03672321635017397 iter num 20\n",
            "loss 0.00023627090558875352 average time 0.025857075225098926 iter num 40\n",
            "loss 3.4168439015047625e-05 average time 0.022135093300100075 iter num 60\n",
            "loss 3.3698299375828356e-05 average time 0.020337349175053986 iter num 80\n",
            "loss 9.447001502849162e-05 average time 0.019224012020049485 iter num 100\n",
            "loss 0.00011940058175241575 average time 0.03619463855002323 iter num 20\n",
            "loss 0.00011536318925209343 average time 0.025436620974960533 iter num 40\n",
            "loss 0.00011682467447826639 average time 0.022005629266626178 iter num 60\n",
            "loss 8.286282536573708e-05 average time 0.020321218999924895 iter num 80\n",
            "loss 0.00012480231816880405 average time 0.019297906179945132 iter num 100\n",
            "loss 6.650124123552814e-05 average time 0.036916715749885046 iter num 20\n",
            "loss 0.00021255557658150792 average time 0.025802660999966066 iter num 40\n",
            "loss 5.2206480177119374e-05 average time 0.022176955566601465 iter num 60\n",
            "loss 1.4567778634955175e-05 average time 0.020390491774958264 iter num 80\n",
            "loss 2.2728789190296084e-05 average time 0.019232697419929536 iter num 100\n",
            "loss 0.00019738060655072331 average time 0.03728163509990736 iter num 20\n",
            "loss 0.00012565172801259905 average time 0.025918882900009522 iter num 40\n",
            "loss 4.7995170461945236e-05 average time 0.02214944714990755 iter num 60\n",
            "loss 1.3255306839710101e-05 average time 0.020633418037402863 iter num 80\n",
            "loss 9.847366527537815e-06 average time 0.01949840384992058 iter num 100\n",
            "loss 7.097351044649258e-05 average time 0.037950119750030355 iter num 20\n",
            "loss 0.00015619397163391113 average time 0.026578922550038443 iter num 40\n",
            "loss 0.00016200265963561833 average time 0.02267347491667048 iter num 60\n",
            "loss 4.433610229170881e-05 average time 0.02066954588749468 iter num 80\n",
            "loss 3.985613875556737e-05 average time 0.01952387707998241 iter num 100\n",
            "loss 0.0001552706235088408 average time 0.037301491149901264 iter num 20\n",
            "loss 2.3764896468492225e-05 average time 0.02607934389991442 iter num 40\n",
            "loss 9.742409019963816e-05 average time 0.022342284016561582 iter num 60\n",
            "loss 3.3225409424630925e-05 average time 0.02047664741241988 iter num 80\n",
            "loss 2.2174775949679315e-05 average time 0.019307095689919153 iter num 100\n",
            "loss 0.00016350505757145584 average time 0.03794644139989032 iter num 20\n",
            "loss 0.00010573364852461964 average time 0.02666188232485638 iter num 40\n",
            "loss 7.23019620636478e-05 average time 0.023000044099959874 iter num 60\n",
            "loss 1.271906694455538e-05 average time 0.021075458874963714 iter num 80\n",
            "loss 1.171179610537365e-05 average time 0.019944429419983863 iter num 100\n",
            "loss 0.0002633089607115835 average time 0.03747656729992741 iter num 20\n",
            "loss 0.0004048535192850977 average time 0.026242886349996296 iter num 40\n",
            "loss 0.00015320998500101268 average time 0.02260628738337497 iter num 60\n",
            "loss 3.708955046022311e-05 average time 0.020637376562513056 iter num 80\n",
            "loss 3.9288450352614745e-05 average time 0.019540429999979098 iter num 100\n",
            "loss 2.478607348166406e-05 average time 0.036890778550014144 iter num 20\n",
            "loss 1.5055114999995567e-05 average time 0.025748969099981877 iter num 40\n",
            "loss 5.555291136261076e-05 average time 0.02207846658332831 iter num 60\n",
            "loss 4.09986860177014e-05 average time 0.020405232849986986 iter num 80\n",
            "loss 5.759384293924086e-05 average time 0.019349927249995746 iter num 100\n",
            "loss 6.996878073550761e-05 average time 0.037468851299945524 iter num 20\n",
            "loss 6.450803630286828e-05 average time 0.02612519967494791 iter num 40\n",
            "loss 0.00017733112326823175 average time 0.022331229200002176 iter num 60\n",
            "loss 2.4214081349782646e-05 average time 0.020434483437509244 iter num 80\n",
            "loss 0.00010762643069028854 average time 0.019311268470028155 iter num 100\n",
            "loss 4.649635229725391e-05 average time 0.03669456385005106 iter num 20\n",
            "loss 5.4531617934117094e-05 average time 0.026032494675018825 iter num 40\n",
            "loss 2.258578570035752e-05 average time 0.02231030133331539 iter num 60\n",
            "loss 8.33899830467999e-05 average time 0.02042204647498238 iter num 80\n",
            "loss 1.538661672384478e-05 average time 0.01932622423000794 iter num 100\n",
            "loss 0.0001156332582468167 average time 0.037418884200042156 iter num 20\n",
            "loss 0.00023319781757891178 average time 0.02622306085004311 iter num 40\n",
            "loss 6.96125061949715e-05 average time 0.02235978573336676 iter num 60\n",
            "loss 4.345014167483896e-05 average time 0.020593282700042438 iter num 80\n",
            "loss 4.589666787069291e-05 average time 0.01953683084005206 iter num 100\n",
            "loss 0.0001620662515051663 average time 0.03693005604991413 iter num 20\n",
            "loss 6.510037201223895e-05 average time 0.025822731599942018 iter num 40\n",
            "loss 4.271707803127356e-05 average time 0.02239158336657662 iter num 60\n",
            "loss 1.3881245649827179e-05 average time 0.02057017976244424 iter num 80\n",
            "loss 2.855853563232813e-05 average time 0.019393987779931194 iter num 100\n",
            "loss 6.699524965370074e-05 average time 0.036619836150066476 iter num 20\n",
            "loss 4.895870733889751e-05 average time 0.025892376350043378 iter num 40\n",
            "loss 4.343823457020335e-05 average time 0.0221604312333208 iter num 60\n",
            "loss 3.872231172863394e-05 average time 0.02034788761246773 iter num 80\n",
            "loss 3.1090221455087885e-05 average time 0.01924100676998023 iter num 100\n",
            "loss 0.003874773159623146 average time 0.038586175949967584 iter num 20\n",
            "loss 0.0001052227962645702 average time 0.026819871125053397 iter num 40\n",
            "loss 3.437589111854322e-05 average time 0.022984491750063778 iter num 60\n",
            "loss 7.73496285546571e-05 average time 0.020995313812534278 iter num 80\n",
            "loss 2.226072501798626e-05 average time 0.019784512590022132 iter num 100\n",
            "loss 0.00016628409503027797 average time 0.036899388150095545 iter num 20\n",
            "loss 0.000307997950585559 average time 0.025859387625041565 iter num 40\n",
            "loss 4.600991451297887e-05 average time 0.02203774538338621 iter num 60\n",
            "loss 5.603309546131641e-05 average time 0.020298627662521086 iter num 80\n",
            "loss 8.242232433985919e-05 average time 0.019292458560012164 iter num 100\n",
            "loss 4.974117109668441e-05 average time 0.03601835105005193 iter num 20\n",
            "loss 0.00014259085583034903 average time 0.025336982825069753 iter num 40\n",
            "loss 3.5704731999430805e-05 average time 0.02195546928337535 iter num 60\n",
            "loss 2.874538768082857e-05 average time 0.020099293737541758 iter num 80\n",
            "loss 5.035172307543689e-06 average time 0.019005985900003 iter num 100\n",
            "loss 0.0002456728252582252 average time 0.037813895299996146 iter num 20\n",
            "loss 4.376091601443477e-05 average time 0.026601169475065946 iter num 40\n",
            "loss 2.6463727408554405e-05 average time 0.022772288500088205 iter num 60\n",
            "loss 1.548834734421689e-05 average time 0.02074661657507022 iter num 80\n",
            "loss 3.690645098686218e-05 average time 0.01954110492003565 iter num 100\n",
            "loss 5.528596375370398e-05 average time 0.03621986280004421 iter num 20\n",
            "loss 0.000108565844129771 average time 0.025746116000095753 iter num 40\n",
            "loss 2.828735887305811e-05 average time 0.02237193205000949 iter num 60\n",
            "loss 2.837627471308224e-05 average time 0.02054263941249701 iter num 80\n",
            "loss 9.05169054021826e-06 average time 0.019435017370014976 iter num 100\n",
            "loss 0.00022002262994647026 average time 0.036560103449892266 iter num 20\n",
            "loss 6.960428436286747e-05 average time 0.02568722674991477 iter num 40\n",
            "loss 5.5922257161000744e-05 average time 0.022194848899926 iter num 60\n",
            "loss 1.6061640053521842e-05 average time 0.020372520287446606 iter num 80\n",
            "loss 5.879567834199406e-05 average time 0.019402865609945367 iter num 100\n",
            "loss 4.0187635022448376e-05 average time 0.036310092700023236 iter num 20\n",
            "loss 5.3460920753423125e-05 average time 0.02582659307506674 iter num 40\n",
            "loss 0.00013777471031062305 average time 0.022130353850070606 iter num 60\n",
            "loss 1.05120916487067e-05 average time 0.020271063950053757 iter num 80\n",
            "loss 4.6967368689365685e-05 average time 0.019301784120034426 iter num 100\n",
            "loss 3.9539132558275014e-05 average time 0.036745580100068766 iter num 20\n",
            "loss 6.042989116394892e-05 average time 0.025851112124951215 iter num 40\n",
            "loss 1.932697523443494e-05 average time 0.022303212766640476 iter num 60\n",
            "loss 8.881850590114482e-06 average time 0.02041095351248714 iter num 80\n",
            "loss 5.6774581025820225e-05 average time 0.019239062629994804 iter num 100\n",
            "loss 7.462123176082969e-05 average time 0.037144527550071874 iter num 20\n",
            "loss 2.6648551283869892e-05 average time 0.026152032925006098 iter num 40\n",
            "loss 4.0633331082062796e-06 average time 0.022320754333334965 iter num 60\n",
            "loss 7.823907071724534e-05 average time 0.02064844834998212 iter num 80\n",
            "loss 2.4144015696947463e-05 average time 0.01956494642996404 iter num 100\n",
            "loss 7.544174150098115e-05 average time 0.037123615550081014 iter num 20\n",
            "loss 6.698002107441425e-05 average time 0.026112256150054237 iter num 40\n",
            "loss 3.49227266269736e-05 average time 0.022337473783409223 iter num 60\n",
            "loss 7.462983921868727e-05 average time 0.02046953311253219 iter num 80\n",
            "loss 4.86544968225644e-06 average time 0.01926741760000368 iter num 100\n",
            "loss 3.477206701063551e-05 average time 0.03606759700001021 iter num 20\n",
            "loss 5.3469262638827786e-05 average time 0.02540734522499406 iter num 40\n",
            "loss 4.0689559682505205e-05 average time 0.021863748766706218 iter num 60\n",
            "loss 7.44920689612627e-05 average time 0.02015149101252973 iter num 80\n",
            "loss 4.826865915674716e-05 average time 0.019059403820037915 iter num 100\n",
            "loss 0.00021959545847494155 average time 0.036654653499999766 iter num 20\n",
            "loss 0.00010018973989645019 average time 0.02555833690005329 iter num 40\n",
            "loss 8.004057599464431e-05 average time 0.021890484550067413 iter num 60\n",
            "loss 1.877850081655197e-05 average time 0.020209592962589795 iter num 80\n",
            "loss 1.311441337747965e-05 average time 0.019246262690076038 iter num 100\n",
            "loss 8.594070823164657e-05 average time 0.03651907144999313 iter num 20\n",
            "loss 0.00011629282380454242 average time 0.02562244277503396 iter num 40\n",
            "loss 6.908139766892418e-05 average time 0.021946226966671627 iter num 60\n",
            "loss 1.3210556062404066e-05 average time 0.020101761975001863 iter num 80\n",
            "loss 1.4959075997467153e-05 average time 0.019157392619990787 iter num 100\n",
            "loss 5.48011958017014e-05 average time 0.0359192197000084 iter num 20\n",
            "loss 4.876687307842076e-05 average time 0.025186368424942884 iter num 40\n",
            "loss 5.103109288029373e-05 average time 0.02167602018330399 iter num 60\n",
            "loss 3.206573455827311e-05 average time 0.019928373812490463 iter num 80\n",
            "loss 1.7745946024660952e-05 average time 0.01889319573998364 iter num 100\n",
            "loss 2.579549436632078e-05 average time 0.037136008300058164 iter num 20\n",
            "loss 4.442859426490031e-05 average time 0.025970537450075427 iter num 40\n",
            "loss 2.787805351545103e-05 average time 0.02219195195004128 iter num 60\n",
            "loss 2.555267565185204e-05 average time 0.020327220800015765 iter num 80\n",
            "loss 3.8276884879451245e-06 average time 0.019224234540024553 iter num 100\n",
            "loss 1.1209648619114887e-05 average time 0.03656672785009505 iter num 20\n",
            "loss 1.3960394426248968e-05 average time 0.025532065625066026 iter num 40\n",
            "loss 5.785669418401085e-05 average time 0.022038983716735554 iter num 60\n",
            "loss 3.56224445567932e-05 average time 0.020215548500038948 iter num 80\n",
            "loss 2.7267326004221104e-05 average time 0.019151241600038702 iter num 100\n",
            "loss 3.20487524732016e-05 average time 0.036739311449946396 iter num 20\n",
            "loss 4.715660907095298e-05 average time 0.02580506959993727 iter num 40\n",
            "loss 0.0001783746265573427 average time 0.022056689016638605 iter num 60\n",
            "loss 1.796176002244465e-05 average time 0.020298389662491444 iter num 80\n",
            "loss 7.462358098564437e-06 average time 0.019326922979989832 iter num 100\n",
            "loss 3.903394826920703e-05 average time 0.03814918295015559 iter num 20\n",
            "loss 6.0389236750779673e-05 average time 0.02628950517505473 iter num 40\n",
            "loss 5.477605009218678e-05 average time 0.02249532818335259 iter num 60\n",
            "loss 4.281655128579587e-05 average time 0.02050500592501976 iter num 80\n",
            "loss 3.8647776818834245e-06 average time 0.019345807910021905 iter num 100\n",
            "loss 1.825637446017936e-05 average time 0.03648112305008908 iter num 20\n",
            "loss 0.0002577352279331535 average time 0.02567880500018873 iter num 40\n",
            "loss 5.695839354302734e-05 average time 0.022079170383464467 iter num 60\n",
            "loss 1.343468375125667e-05 average time 0.020333676250152165 iter num 80\n",
            "loss 3.374132575117983e-05 average time 0.01918126432011377 iter num 100\n",
            "loss 0.000346692802850157 average time 0.03664546720006001 iter num 20\n",
            "loss 0.0001508015557192266 average time 0.025513354775034712 iter num 40\n",
            "loss 5.068521568318829e-05 average time 0.021927804333396732 iter num 60\n",
            "loss 8.12063371995464e-05 average time 0.020056721412561273 iter num 80\n",
            "loss 2.293006400577724e-05 average time 0.01909338749004746 iter num 100\n",
            "loss 0.0005018403753638268 average time 0.03683391244994709 iter num 20\n",
            "loss 2.734072768362239e-05 average time 0.02575114209989806 iter num 40\n",
            "loss 1.0251647836412303e-05 average time 0.022017921316592037 iter num 60\n",
            "loss 2.9702743631787598e-05 average time 0.02035201528746029 iter num 80\n",
            "loss 0.00010060430213343352 average time 0.019297549669954606 iter num 100\n",
            "loss 0.00019210370373912156 average time 0.03555687270004455 iter num 20\n",
            "loss 0.00019733929366338998 average time 0.024991392550077762 iter num 40\n",
            "loss 6.16261240793392e-05 average time 0.021738457466729717 iter num 60\n",
            "loss 1.6200861864490435e-05 average time 0.019975725262554533 iter num 80\n",
            "loss 6.1874616221757606e-06 average time 0.018907130720026543 iter num 100\n",
            "loss 3.761858170037158e-05 average time 0.03724309289996199 iter num 20\n",
            "loss 4.708680717158131e-05 average time 0.025886041074977585 iter num 40\n",
            "loss 5.220852472120896e-06 average time 0.022188249599958 iter num 60\n",
            "loss 6.550159923790488e-06 average time 0.020447785512487826 iter num 80\n",
            "loss 5.63571984457667e-06 average time 0.019367905639974196 iter num 100\n",
            "loss 2.8574471798492596e-05 average time 0.03669682584991278 iter num 20\n",
            "loss 2.8301685233600438e-05 average time 0.025790128474977792 iter num 40\n",
            "loss 2.057719939330127e-05 average time 0.02208314144995711 iter num 60\n",
            "loss 2.8865957574453205e-05 average time 0.02030441277495356 iter num 80\n",
            "loss 1.5700969015597366e-05 average time 0.019215125909950073 iter num 100\n",
            "loss 3.4344015148235485e-05 average time 0.03692890030001763 iter num 20\n",
            "loss 9.003550803754479e-05 average time 0.025942737449963716 iter num 40\n",
            "loss 5.285109727992676e-05 average time 0.022403002283272144 iter num 60\n",
            "loss 1.2901723493996542e-05 average time 0.020487344062462397 iter num 80\n",
            "loss 2.0283719095459674e-06 average time 0.01932472563996271 iter num 100\n",
            "loss 3.5380275221541524e-05 average time 0.03651024615001006 iter num 20\n",
            "loss 6.06044995947741e-05 average time 0.025716647950025616 iter num 40\n",
            "loss 1.5488560165977105e-05 average time 0.022082686683355255 iter num 60\n",
            "loss 9.660627256380394e-06 average time 0.02023722577504259 iter num 80\n",
            "loss 8.679147867951542e-06 average time 0.019291414070057727 iter num 100\n",
            "loss 3.8034635508665815e-05 average time 0.037234356600038154 iter num 20\n",
            "loss 2.618991675262805e-05 average time 0.02610861722498612 iter num 40\n",
            "loss 1.0477122486918233e-05 average time 0.022491795883297527 iter num 60\n",
            "loss 2.306523674633354e-05 average time 0.02060636439997552 iter num 80\n",
            "loss 4.210478891764069e-06 average time 0.01958376139999018 iter num 100\n",
            "loss 8.515227818861604e-05 average time 0.03747820599996885 iter num 20\n",
            "loss 2.019675957853906e-05 average time 0.02610699237498011 iter num 40\n",
            "loss 2.579254942247644e-05 average time 0.022410410166685324 iter num 60\n",
            "loss 1.6839821910252795e-05 average time 0.020506066849998207 iter num 80\n",
            "loss 7.982166607689578e-06 average time 0.01962590263000493 iter num 100\n",
            "loss 8.856772183207795e-05 average time 0.037573966499985546 iter num 20\n",
            "loss 0.0006460681324824691 average time 0.026121180224981798 iter num 40\n",
            "loss 0.00043729832395911217 average time 0.022348069116666616 iter num 60\n",
            "loss 6.158900941954926e-05 average time 0.02043055607500719 iter num 80\n",
            "loss 3.402133734198287e-05 average time 0.019286407060017153 iter num 100\n",
            "loss 0.0002129117929143831 average time 0.0368108517499877 iter num 20\n",
            "loss 8.223616168834269e-05 average time 0.025762205424939566 iter num 40\n",
            "loss 4.6937409933889285e-05 average time 0.022184155766687277 iter num 60\n",
            "loss 1.7419244613847695e-05 average time 0.020328825100000357 iter num 80\n",
            "loss 1.0288469638908282e-05 average time 0.019147130009996544 iter num 100\n",
            "loss 0.00010315116378478706 average time 0.037413352599969585 iter num 20\n",
            "loss 3.3247237297473475e-05 average time 0.02627827637502378 iter num 40\n",
            "loss 2.5602459572837688e-05 average time 0.02233007918336322 iter num 60\n",
            "loss 9.996609151130542e-06 average time 0.020461182237499997 iter num 80\n",
            "loss 1.1828160495497286e-05 average time 0.019348684739979943 iter num 100\n",
            "loss 0.00010296269465470687 average time 0.03720349739996891 iter num 20\n",
            "loss 1.758385587891098e-05 average time 0.026009663899958468 iter num 40\n",
            "loss 8.479252755932976e-06 average time 0.022519071449930077 iter num 60\n",
            "loss 6.981166279729223e-06 average time 0.020518334612438592 iter num 80\n",
            "loss 6.460261829488445e-06 average time 0.019372528849935407 iter num 100\n",
            "loss 9.62380290729925e-05 average time 0.036414886799911984 iter num 20\n",
            "loss 7.308203203137964e-05 average time 0.025850699174998226 iter num 40\n",
            "loss 7.258557161549106e-05 average time 0.022383842966655722 iter num 60\n",
            "loss 2.9018601708230563e-05 average time 0.02056231387499565 iter num 80\n",
            "loss 8.947469723352697e-06 average time 0.019359543599966854 iter num 100\n",
            "loss 2.164129909942858e-05 average time 0.03618949369993061 iter num 20\n",
            "loss 7.906822429504246e-05 average time 0.02536484857494088 iter num 40\n",
            "loss 0.0003700831439346075 average time 0.021940427699958794 iter num 60\n",
            "loss 3.910624946001917e-05 average time 0.020174625987499438 iter num 80\n",
            "loss 1.2144249012635555e-05 average time 0.0191266518300381 iter num 100\n",
            "loss 2.836155726981815e-05 average time 0.03664934869998433 iter num 20\n",
            "loss 0.0002185660705436021 average time 0.025817108100045515 iter num 40\n",
            "loss 3.796639430220239e-05 average time 0.02219749461670896 iter num 60\n",
            "loss 1.911335129989311e-05 average time 0.02041503082506324 iter num 80\n",
            "loss 7.752680176054128e-06 average time 0.019320615500037094 iter num 100\n",
            "loss 0.00021114877017680556 average time 0.036653455650002796 iter num 20\n",
            "loss 1.3297841178427916e-05 average time 0.026182633900020846 iter num 40\n",
            "loss 9.320231583842542e-06 average time 0.022452272899984867 iter num 60\n",
            "loss 4.0119894038070925e-06 average time 0.020534574087514558 iter num 80\n",
            "loss 5.155031431058887e-06 average time 0.0193276981100189 iter num 100\n",
            "loss 0.00029934619669802487 average time 0.03698620079999273 iter num 20\n",
            "loss 0.0001830945984693244 average time 0.026335516799963442 iter num 40\n",
            "loss 4.131465539103374e-05 average time 0.02255567754993232 iter num 60\n",
            "loss 3.435262988205068e-05 average time 0.02059130794992825 iter num 80\n",
            "loss 5.134889579494484e-05 average time 0.019542896389948508 iter num 100\n",
            "loss 0.0004015513404738158 average time 0.03768393215009382 iter num 20\n",
            "loss 0.0015554383862763643 average time 0.026233292699976117 iter num 40\n",
            "loss 6.908322393428534e-05 average time 0.022828923816647754 iter num 60\n",
            "loss 7.03934274497442e-05 average time 0.021004711625005258 iter num 80\n",
            "loss 5.0571103201946244e-05 average time 0.01977278975000445 iter num 100\n",
            "loss 0.00047745995107106864 average time 0.03741144700011319 iter num 20\n",
            "loss 0.00010412612755317241 average time 0.026115221800046128 iter num 40\n",
            "loss 8.936744416132569e-05 average time 0.022233984033330975 iter num 60\n",
            "loss 5.468065137392841e-05 average time 0.02047098345000222 iter num 80\n",
            "loss 0.00013018568279221654 average time 0.01935548225000275 iter num 100\n",
            "loss 6.12316798651591e-05 average time 0.03690372630007914 iter num 20\n",
            "loss 4.8606172640575096e-05 average time 0.02594704262508003 iter num 40\n",
            "loss 6.0383117670426145e-06 average time 0.022264305266708105 iter num 60\n",
            "loss 2.009483432630077e-05 average time 0.020465089712570262 iter num 80\n",
            "loss 3.454795660218224e-05 average time 0.01926675389009688 iter num 100\n",
            "loss 9.810682240640745e-05 average time 0.036693635199981145 iter num 20\n",
            "loss 1.505676846136339e-05 average time 0.025715300724959887 iter num 40\n",
            "loss 3.569909677025862e-05 average time 0.022149792566642644 iter num 60\n",
            "loss 1.2863455594924744e-05 average time 0.02025688797497196 iter num 80\n",
            "loss 1.0103758540935814e-05 average time 0.019196867129994643 iter num 100\n",
            "loss 5.761676584370434e-05 average time 0.03627530455005399 iter num 20\n",
            "loss 3.211283910786733e-05 average time 0.025706132875006914 iter num 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-50f4c3cfa7be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0minitial_stocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m           \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# assume no correlation between stocks here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m           \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   3567\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected input type for array: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0;32m--> 461\u001b[0;31m                                        weak_type=new_weak_type)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbitcast_convert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    264\u001b[0m     top_trace = find_top_trace(\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36msafe_map\u001b[0;34m(f, *args)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'length mismatch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munzip2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z6vCaMA2JS3"
      },
      "source": [
        "# # version 4\n",
        "# # If memory is not enough, try changing parameters and restarting session\n",
        "# # loss will converge\n",
        "\n",
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import OptionDataSet\n",
        "# import numpy as np\n",
        "# timer = Timer(average=True)\n",
        "# model = Net().cuda()\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     #print(y)\n",
        "#     y_pred = model(x)\n",
        "#     #print(y_pred)\n",
        "#     loss = loss_fn(y_pred[:,:], y[:,:]) # compute MSE between the 2 arrays\n",
        "#     #print(loss)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 20\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzWwehfQrGSm"
      },
      "source": [
        "# # version 5, 6\n",
        "# # If memory is not enough, try changing parameters and restarting session\n",
        "# # loss will converge\n",
        "\n",
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import OptionDataSet\n",
        "# import numpy as np\n",
        "# timer = Timer(average=True)\n",
        "# model = Net().cuda()\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = OptionDataSet(max_len = 100, number_path = 100000, batch =32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     #print(y)\n",
        "#     y_pred = model(x)\n",
        "#     #print(y_pred)\n",
        "#     loss = ((y_pred - y) ** 2).mean(axis=0).sum()\n",
        "#     #print(loss)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 20\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # * 10000 for version 6\n",
        "        \n",
        "# trainer.run(dataset, max_epochs = 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7d256d-4837-4656-e25f-31ba4beaedc0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'new_european_1stock_v2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ba1d2c-112f-4873-fc66-c06f1cb788fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829fac04-80e0-4726-c0f3-e8693421a059"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'new_european_1stock_v2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\"\n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae956071-ddc3-44c7-c188-83e639ec515c"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1000000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output * 10000, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 30)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_test_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFjWBtGk7ej5"
      },
      "source": [
        "# # version 4\n",
        "# # If memory is not enough, try changing parameters and restarting session\n",
        "# # loss will converge\n",
        "\n",
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import OptionDataSet\n",
        "# import numpy as np\n",
        "# timer = Timer(average=True)\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = OptionDataSet(max_len = 100, number_path = 1000000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     #print(y)\n",
        "#     y_pred = model(x)\n",
        "#     #print(y_pred)\n",
        "#     loss = loss_fn(y_pred[:,:], y[:,:]) # compute MSE between the 2 arrays\n",
        "#     #print(loss)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 20\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs = 50)\n",
        "\n",
        "# model_save_name = 'jax_european_1stock_version4_2.pth'\n",
        "# path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Lkc1HtpW7S"
      },
      "source": [
        "# # version 5\n",
        "# # If memory is not enough, try changing parameters and restarting session\n",
        "# # loss will converge\n",
        "\n",
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import OptionDataSet\n",
        "# import numpy as np\n",
        "# timer = Timer(average=True)\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = OptionDataSet(max_len = 100, number_path = 1000000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     #print(y)\n",
        "#     y_pred = model(x)\n",
        "#     #print(y_pred)\n",
        "#     loss = ((y_pred - y) ** 2).mean(axis=0).sum()\n",
        "#     #print(loss)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 20\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs = 20)\n",
        "\n",
        "# model_save_name = 'jax_european_1stock_version5_1.pth'\n",
        "# path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsadwOuJph79"
      },
      "source": [
        "a = np.array([[2.5661e+01, 9.5063e-01],\n",
        "        [6.5737e-07, 3.5066e-06]])\n",
        "b = np.array([[32.7189,  0.9209],\n",
        "        [ 1.5464,  0.3001]])\n",
        "((a-b)**2).mean(axis=0).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec562b8-3254-422a-b1d3-f2c207d23522"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 0.8, 0.8, 0.25, 0.05, 0.05]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.098688, 0.627409)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.0914]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6244], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2AXrPt7bNj",
        "outputId": "a8ffea36-b5e6-4add-d40f-87f9ece5f0f0"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "›\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([0.8]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.8\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09876674\n",
            "[0.62784874]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMq_9z4e1OiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2c0ac2-7089-4747-b31f-6a43036c9bf7"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 110.0, 100.0, 0.25, 0.05, 0.05]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price, delta: ' + str(model(inputs.float())))\n",
        "\n",
        "# inputs.requires_grad = True\n",
        "# x = model(inputs.float())\n",
        "# x.backward()\n",
        "# first_order_gradient = inputs.grad\n",
        "# first_order_gradient[0][[2]]\n",
        "\n",
        "# should be 8.058, 0.478"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price, delta: tensor([[4.5221]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKx_UH-k1o0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e06ea5-90fd-4490-9d21-12f2e1c905cf"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.05]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([100.0]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 110.0\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.029676\n",
            "[0.47848195]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "lwApH0GT9bBK",
        "outputId": "523c1860-435b-49d6-eb1e-b1e1ad808b9d"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.5, S, 0.25, 0.05, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(0, 1, 0.01)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f229f2a6750>]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPT31tpTOibD"
      },
      "source": [
        "# import pylab\n",
        "# import numpy as np\n",
        "# prices = np.arange(0, 200, 1)\n",
        "# deltas = []\n",
        "# for p in prices:\n",
        "#     inputs = torch.tensor([[1, 110.0, p, 0.25, 0.05, 0.05]]).cuda() # T, K, S, sigma, mu, r\n",
        "#     deltas.append(model(inputs.float())[0][1])\n",
        "# fig = pylab.plot(prices, deltas)\n",
        "# pylab.xlabel('prices')\n",
        "# pylab.ylabel('Delta')\n",
        "# fig"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UaG5KUBaaRE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}