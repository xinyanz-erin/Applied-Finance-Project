{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knock_Out_Test",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/Knock_Out_Random_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV142iHY0Xmr"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z08iNuoQ19oX"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "        dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx2)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0349]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.identity(numstocks)*0.1995*0.1995\n",
        "initial_stocks = jnp.array([0.8222]*numstocks) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 0.9723\n",
        "B = 0.5088 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhCjomZ80Wwq",
        "outputId": "a4e9e695-12bc-49ad-d60d-aaa46b7aeaf9"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "        dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx2)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 3\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0124]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.array([[0.2625**2, 0, 0], [0, 0.2446**2, 0], [0, 0, 0.3074**2]])\n",
        "initial_stocks = jnp.array([1.1262, 1.0031, 1.0572]) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 1.2482\n",
        "B = 0.9522 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# test 3 stocks\n",
        "# 1.0000, 1.2482, 0.9522, 1.1262, 0.2625, 0.0124, 0.0124, \n",
        "# 1.0000, 1.2482, 0.9522, 1.0031, 0.2446, 0.0124, 0.0124, \n",
        "# 1.0000, 1.2482, 0.9522, 1.0572, 0.3074, 0.0124, 0.0124"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.016996417\n",
            "[0.06254641 0.05926677 0.06709909]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49af690e-1952-4b25-d60b-ea325b14bc6e"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "# version 1, 2, 6\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T, keys): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "#keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 7), dtype = cupy.float32)  # Add Barrier\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(0.75 + np.random.random(self.N_STOCKS) * 0.5)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(0.15 + np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(0.01 + np.random.random(1) * 0.03), self.N_STOCKS)\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          K = 0.75 + np.random.random(1) * 0.5\n",
        "          B = 0.6 + np.random.random(1) * 0.5\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          Knock_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, B, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, B, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = Knock_Call_price\n",
        "          Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32) # remember to change this!\n",
        "\n",
        "          # T, K, S, sigma, mu, r,B\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), jnp.repeat(jnp.array(B), self.N_STOCKS), initial_stocks, sigma, drift, r) # T,K,B,S,sigma,drift,r\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=3) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "f6c486c6-252c-45bb-ec1a-ba31108933bb"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(7*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.5, 0.3, 0.03, 0.03]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, B, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.6, 0.75, 0.15, 0.01, 0.01]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "2922a282-3c87-4a35-c102-9d72ce2e27f8"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 37.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 34.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndNUBU8Js17G",
        "outputId": "0f5cac74-596c-430a-de94-e93e306b9ed9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Rb9fZ1rteqM",
        "outputId": "28fdde77-dec8-4ec1-effe-dab8ff8dec86"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_random_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.11786102503538132 average time 0.31416455010004257 iter num 20\n",
            "loss 0.12945109605789185 average time 0.18376528347502016 iter num 40\n",
            "loss 0.07896321266889572 average time 0.14012204586668986 iter num 60\n",
            "loss 0.040061239153146744 average time 0.11831732990002024 iter num 80\n",
            "loss 0.08334900438785553 average time 0.10532477916001198 iter num 100\n",
            "loss 0.07101080566644669 average time 0.15905570544996409 iter num 20\n",
            "loss 0.05922384560108185 average time 0.10507131212496575 iter num 40\n",
            "loss 0.054796941578388214 average time 0.0874057409166312 iter num 60\n",
            "loss 0.05620301887392998 average time 0.07904698602497717 iter num 80\n",
            "loss 0.05846242234110832 average time 0.07365907519998245 iter num 100\n",
            "loss 0.059219468384981155 average time 0.1670618715499586 iter num 20\n",
            "loss 0.08881227672100067 average time 0.11034810604998029 iter num 40\n",
            "loss 0.0697491243481636 average time 0.09150639898332429 iter num 60\n",
            "loss 0.0776045024394989 average time 0.08179250416248465 iter num 80\n",
            "loss 0.06903309375047684 average time 0.07603921248998631 iter num 100\n",
            "loss 0.0774436965584755 average time 0.15963675909995345 iter num 20\n",
            "loss 0.08044370263814926 average time 0.10635875522497144 iter num 40\n",
            "loss 0.07506337016820908 average time 0.08884537754997078 iter num 60\n",
            "loss 0.08684900403022766 average time 0.07992557113747693 iter num 80\n",
            "loss 0.05294780060648918 average time 0.07491654000995823 iter num 100\n",
            "loss 0.09542058408260345 average time 0.1630005113000152 iter num 20\n",
            "loss 0.07865580171346664 average time 0.10890589609999779 iter num 40\n",
            "loss 0.06331626325845718 average time 0.09103123891666959 iter num 60\n",
            "loss 0.08461683988571167 average time 0.0817025926624865 iter num 80\n",
            "loss 0.06591475754976273 average time 0.07595548527999654 iter num 100\n",
            "loss 0.06829880177974701 average time 0.16141750390008838 iter num 20\n",
            "loss 0.09573382139205933 average time 0.10712134360001073 iter num 40\n",
            "loss 0.05670086666941643 average time 0.08911967048332675 iter num 60\n",
            "loss 0.08933469653129578 average time 0.08014567182499946 iter num 80\n",
            "loss 0.06120898574590683 average time 0.07483767467000234 iter num 100\n",
            "loss 0.07603680342435837 average time 0.16283745059997728 iter num 20\n",
            "loss 0.06161625683307648 average time 0.10875678889999563 iter num 40\n",
            "loss 0.07912486791610718 average time 0.0906595740999819 iter num 60\n",
            "loss 0.06530056893825531 average time 0.08183301712497837 iter num 80\n",
            "loss 0.04687902331352234 average time 0.07652207393997741 iter num 100\n",
            "loss 0.06447646766901016 average time 0.1615095549000216 iter num 20\n",
            "loss 0.0883132815361023 average time 0.10850870077498484 iter num 40\n",
            "loss 0.07488054782152176 average time 0.09007949866663693 iter num 60\n",
            "loss 0.08003417402505875 average time 0.08102470271247739 iter num 80\n",
            "loss 0.047617923468351364 average time 0.07573067339998943 iter num 100\n",
            "loss 0.06985502690076828 average time 0.1644007992500292 iter num 20\n",
            "loss 0.058721620589494705 average time 0.10952507365004749 iter num 40\n",
            "loss 0.049181923270225525 average time 0.09136172761670877 iter num 60\n",
            "loss 0.07365337759256363 average time 0.08200945823754183 iter num 80\n",
            "loss 0.06567583978176117 average time 0.07660005186002763 iter num 100\n",
            "loss 0.07647490501403809 average time 0.16247590794994266 iter num 20\n",
            "loss 0.061949510127305984 average time 0.10820379347497919 iter num 40\n",
            "loss 0.0668390691280365 average time 0.08985643486665443 iter num 60\n",
            "loss 0.06494760513305664 average time 0.08079580778750142 iter num 80\n",
            "loss 0.060615722090005875 average time 0.0751556186599828 iter num 100\n",
            "loss 0.07268784195184708 average time 0.16068368984997505 iter num 20\n",
            "loss 0.053923945873975754 average time 0.10681071775001101 iter num 40\n",
            "loss 0.06600789725780487 average time 0.08912357909998718 iter num 60\n",
            "loss 0.0688495934009552 average time 0.0799984863499958 iter num 80\n",
            "loss 0.06637027114629745 average time 0.07470220049999625 iter num 100\n",
            "loss 0.07464268058538437 average time 0.15720001609993234 iter num 20\n",
            "loss 0.0736021101474762 average time 0.10467822912498832 iter num 40\n",
            "loss 0.07592873275279999 average time 0.08825449970002713 iter num 60\n",
            "loss 0.06438352912664413 average time 0.07934052511251366 iter num 80\n",
            "loss 0.05828769505023956 average time 0.07411029283000971 iter num 100\n",
            "loss 0.048566218465566635 average time 0.16073525929991775 iter num 20\n",
            "loss 0.05683979019522667 average time 0.10695811549987866 iter num 40\n",
            "loss 0.04859307408332825 average time 0.088908831949963 iter num 60\n",
            "loss 0.07058202475309372 average time 0.0801611031625157 iter num 80\n",
            "loss 0.05642465129494667 average time 0.07478582764001658 iter num 100\n",
            "loss 0.07863201946020126 average time 0.16126073305008504 iter num 20\n",
            "loss 0.0630190521478653 average time 0.10713147957501405 iter num 40\n",
            "loss 0.05446263402700424 average time 0.08909391021666124 iter num 60\n",
            "loss 0.04086208716034889 average time 0.08029045000000679 iter num 80\n",
            "loss 0.05155274644494057 average time 0.07493799652999769 iter num 100\n",
            "loss 0.06290005147457123 average time 0.16298670859996492 iter num 20\n",
            "loss 0.06610970199108124 average time 0.10859968360000494 iter num 40\n",
            "loss 0.05814959853887558 average time 0.090135588450039 iter num 60\n",
            "loss 0.0639805868268013 average time 0.08161689280001383 iter num 80\n",
            "loss 0.05272315442562103 average time 0.07616013531002863 iter num 100\n",
            "loss 0.04272477328777313 average time 0.16245185289999425 iter num 20\n",
            "loss 0.05774006247520447 average time 0.10807247587501934 iter num 40\n",
            "loss 0.04325219243764877 average time 0.09009305725006318 iter num 60\n",
            "loss 0.054544128477573395 average time 0.08140600811256035 iter num 80\n",
            "loss 0.06184643507003784 average time 0.07605594435003696 iter num 100\n",
            "loss 0.04132738336920738 average time 0.16361542955010008 iter num 20\n",
            "loss 0.06668085604906082 average time 0.10917658529997425 iter num 40\n",
            "loss 0.06713872402906418 average time 0.09074070973333619 iter num 60\n",
            "loss 0.04957197234034538 average time 0.08161996520001366 iter num 80\n",
            "loss 0.055179402232170105 average time 0.07651129431002118 iter num 100\n",
            "loss 0.04354117438197136 average time 0.1675238344999798 iter num 20\n",
            "loss 0.060438044369220734 average time 0.11120906269998158 iter num 40\n",
            "loss 0.048770032823085785 average time 0.0926693429666102 iter num 60\n",
            "loss 0.05520804598927498 average time 0.08331193481247964 iter num 80\n",
            "loss 0.06608221679925919 average time 0.0775773233599648 iter num 100\n",
            "loss 0.048151645809412 average time 0.1623617943499994 iter num 20\n",
            "loss 0.019472509622573853 average time 0.10775979650006776 iter num 40\n",
            "loss 0.04392970725893974 average time 0.08975672361669543 iter num 60\n",
            "loss 0.06897632032632828 average time 0.08122822668747176 iter num 80\n",
            "loss 0.057425860315561295 average time 0.07603242942996985 iter num 100\n",
            "loss 0.052157871425151825 average time 0.16293843674993697 iter num 20\n",
            "loss 0.038596346974372864 average time 0.10851713212503 iter num 40\n",
            "loss 0.03805280849337578 average time 0.09029608394998831 iter num 60\n",
            "loss 0.05356913432478905 average time 0.08146816830003445 iter num 80\n",
            "loss 0.05583515390753746 average time 0.07610592617000293 iter num 100\n",
            "loss 0.03352456912398338 average time 0.16302521190000335 iter num 20\n",
            "loss 0.03152697533369064 average time 0.10901082432505974 iter num 40\n",
            "loss 0.04785831645131111 average time 0.0905885576833801 iter num 60\n",
            "loss 0.05043701082468033 average time 0.0819215796375147 iter num 80\n",
            "loss 0.04102659225463867 average time 0.07647374399999535 iter num 100\n",
            "loss 0.031601935625076294 average time 0.16353027450008994 iter num 20\n",
            "loss 0.034867409616708755 average time 0.10973499557505875 iter num 40\n",
            "loss 0.0329325832426548 average time 0.09135218706672579 iter num 60\n",
            "loss 0.04137955978512764 average time 0.08199568198751876 iter num 80\n",
            "loss 0.03130355104804039 average time 0.0764973776500392 iter num 100\n",
            "loss 0.040794435888528824 average time 0.1643629717999829 iter num 20\n",
            "loss 0.03923996537923813 average time 0.10933491477508142 iter num 40\n",
            "loss 0.060599762946367264 average time 0.09149034660005478 iter num 60\n",
            "loss 0.0441514253616333 average time 0.08287723621252781 iter num 80\n",
            "loss 0.03632436692714691 average time 0.07725430282003799 iter num 100\n",
            "loss 0.037264447659254074 average time 0.1645705026000087 iter num 20\n",
            "loss 0.061249490827322006 average time 0.11022803760001806 iter num 40\n",
            "loss 0.04871465638279915 average time 0.09206868733338827 iter num 60\n",
            "loss 0.03511570766568184 average time 0.08307527862501729 iter num 80\n",
            "loss 0.029504001140594482 average time 0.07737804480000704 iter num 100\n",
            "loss 0.05417117476463318 average time 0.16441923884999596 iter num 20\n",
            "loss 0.0390038825571537 average time 0.10930367002499679 iter num 40\n",
            "loss 0.033555153757333755 average time 0.09094770288335591 iter num 60\n",
            "loss 0.03854663670063019 average time 0.08169955457501601 iter num 80\n",
            "loss 0.048887573182582855 average time 0.07666519125000378 iter num 100\n",
            "loss 0.045713141560554504 average time 0.16319497989998127 iter num 20\n",
            "loss 0.05096045508980751 average time 0.10901072457495502 iter num 40\n",
            "loss 0.02893763594329357 average time 0.09162094189996424 iter num 60\n",
            "loss 0.031389087438583374 average time 0.0824482290249648 iter num 80\n",
            "loss 0.02975696139037609 average time 0.07703387296000073 iter num 100\n",
            "loss 0.02273666113615036 average time 0.16434980809995067 iter num 20\n",
            "loss 0.047320324927568436 average time 0.10990509962493888 iter num 40\n",
            "loss 0.03164101019501686 average time 0.09191684363328628 iter num 60\n",
            "loss 0.038003645837306976 average time 0.08231831672495674 iter num 80\n",
            "loss 0.06882800161838531 average time 0.07685219646997211 iter num 100\n",
            "loss 0.02720576710999012 average time 0.1698120512501191 iter num 20\n",
            "loss 0.03546920046210289 average time 0.11211720655003318 iter num 40\n",
            "loss 0.04489774629473686 average time 0.09291786525003166 iter num 60\n",
            "loss 0.027013059705495834 average time 0.08340540501253599 iter num 80\n",
            "loss 0.050004743039608 average time 0.07751605582003322 iter num 100\n",
            "loss 0.04582778736948967 average time 0.163340254600098 iter num 20\n",
            "loss 0.050245944410562515 average time 0.10866095389999372 iter num 40\n",
            "loss 0.04051024466753006 average time 0.0908484314666263 iter num 60\n",
            "loss 0.04184864088892937 average time 0.08198244888744739 iter num 80\n",
            "loss 0.05612441524863243 average time 0.07633057835996623 iter num 100\n",
            "loss 0.04573197662830353 average time 0.16896846494987586 iter num 20\n",
            "loss 0.0424601249396801 average time 0.11160397377491335 iter num 40\n",
            "loss 0.03887821361422539 average time 0.09214165379997515 iter num 60\n",
            "loss 0.03484957292675972 average time 0.08298965984998859 iter num 80\n",
            "loss 0.04992566630244255 average time 0.07755488727004377 iter num 100\n",
            "loss 0.02357492968440056 average time 0.17175755505013512 iter num 20\n",
            "loss 0.02500651217997074 average time 0.11270276720024412 iter num 40\n",
            "loss 0.0426296629011631 average time 0.09342555478345578 iter num 60\n",
            "loss 0.03345346823334694 average time 0.08360801423759767 iter num 80\n",
            "loss 0.029066728428006172 average time 0.07779115202010871 iter num 100\n",
            "loss 0.04993631690740585 average time 0.17164909325001645 iter num 20\n",
            "loss 0.029736822471022606 average time 0.11323866437510333 iter num 40\n",
            "loss 0.048993777483701706 average time 0.09315281355008362 iter num 60\n",
            "loss 0.03715578839182854 average time 0.08340051983752801 iter num 80\n",
            "loss 0.02123512700200081 average time 0.07747560035006246 iter num 100\n",
            "loss 0.04780018702149391 average time 0.16255428235044747 iter num 20\n",
            "loss 0.04897713661193848 average time 0.10932804417525403 iter num 40\n",
            "loss 0.05110679194331169 average time 0.09127136186677186 iter num 60\n",
            "loss 0.031196048483252525 average time 0.08196719403761107 iter num 80\n",
            "loss 0.03425927832722664 average time 0.0764268058700145 iter num 100\n",
            "loss 0.05197913572192192 average time 0.16190584365003816 iter num 20\n",
            "loss 0.059082042425870895 average time 0.10854369534995385 iter num 40\n",
            "loss 0.02529580146074295 average time 0.09082415188325589 iter num 60\n",
            "loss 0.029971882700920105 average time 0.0819050347249913 iter num 80\n",
            "loss 0.02773156575858593 average time 0.0766922336099924 iter num 100\n",
            "loss 0.028914274647831917 average time 0.16693683199982842 iter num 20\n",
            "loss 0.040179893374443054 average time 0.11073300509988257 iter num 40\n",
            "loss 0.05432218685746193 average time 0.09295458538323752 iter num 60\n",
            "loss 0.053432997316122055 average time 0.08366275479991145 iter num 80\n",
            "loss 0.044900450855493546 average time 0.07798119089991815 iter num 100\n",
            "loss 0.045176029205322266 average time 0.16898689895015195 iter num 20\n",
            "loss 0.05388040840625763 average time 0.11175207712503835 iter num 40\n",
            "loss 0.0440463162958622 average time 0.09250265848331765 iter num 60\n",
            "loss 0.04756616801023483 average time 0.08296460586252578 iter num 80\n",
            "loss 0.04023614153265953 average time 0.07737795241997446 iter num 100\n",
            "loss 0.032911546528339386 average time 0.1623040555001353 iter num 20\n",
            "loss 0.036759793758392334 average time 0.10886900022510418 iter num 40\n",
            "loss 0.0539381206035614 average time 0.09097867235007774 iter num 60\n",
            "loss 0.045408155769109726 average time 0.08221448573747239 iter num 80\n",
            "loss 0.03588895499706268 average time 0.07681159309993746 iter num 100\n",
            "loss 0.033667221665382385 average time 0.1636824203999822 iter num 20\n",
            "loss 0.05649430677294731 average time 0.10875306725001792 iter num 40\n",
            "loss 0.03696141764521599 average time 0.09073918291672574 iter num 60\n",
            "loss 0.05398699268698692 average time 0.08173023496253791 iter num 80\n",
            "loss 0.08230248093605042 average time 0.07658479736997834 iter num 100\n",
            "loss 0.02697119116783142 average time 0.16453202095008237 iter num 20\n",
            "loss 0.02787250466644764 average time 0.10978040109998802 iter num 40\n",
            "loss 0.03770400211215019 average time 0.09120632139996208 iter num 60\n",
            "loss 0.033160172402858734 average time 0.08214079963745462 iter num 80\n",
            "loss 0.038302499800920486 average time 0.07632131234999179 iter num 100\n",
            "loss 0.03753511607646942 average time 0.1621706459998677 iter num 20\n",
            "loss 0.03458341211080551 average time 0.10847691327485336 iter num 40\n",
            "loss 0.03298243135213852 average time 0.09043544406658839 iter num 60\n",
            "loss 0.023236799985170364 average time 0.08153269369988721 iter num 80\n",
            "loss 0.047968920320272446 average time 0.07580771883987836 iter num 100\n",
            "loss 0.0370694100856781 average time 0.16064778119998663 iter num 20\n",
            "loss 0.04940409958362579 average time 0.10697847904998525 iter num 40\n",
            "loss 0.03965797647833824 average time 0.08873382953330898 iter num 60\n",
            "loss 0.03902854025363922 average time 0.07964821203745487 iter num 80\n",
            "loss 0.04446692392230034 average time 0.07407756664997578 iter num 100\n",
            "loss 0.027354687452316284 average time 0.16138472989987349 iter num 20\n",
            "loss 0.024428827688097954 average time 0.10684033995007666 iter num 40\n",
            "loss 0.05700802803039551 average time 0.08847136886661247 iter num 60\n",
            "loss 0.04239311441779137 average time 0.07915570267500698 iter num 80\n",
            "loss 0.044891998171806335 average time 0.07394156385997121 iter num 100\n",
            "loss 0.04024113714694977 average time 0.15402891799994906 iter num 20\n",
            "loss 0.03659406676888466 average time 0.10248202104999109 iter num 40\n",
            "loss 0.034623343497514725 average time 0.08521839266671426 iter num 60\n",
            "loss 0.062203168869018555 average time 0.0766427098250233 iter num 80\n",
            "loss 0.05326205864548683 average time 0.07147459920002802 iter num 100\n",
            "loss 0.035433679819107056 average time 0.15712923090013647 iter num 20\n",
            "loss 0.028083927929401398 average time 0.10432830755012219 iter num 40\n",
            "loss 0.04209638386964798 average time 0.08624032378335565 iter num 60\n",
            "loss 0.024149417877197266 average time 0.078324617650037 iter num 80\n",
            "loss 0.04004049673676491 average time 0.07356403014004172 iter num 100\n",
            "loss 0.02557281218469143 average time 0.16573537029989893 iter num 20\n",
            "loss 0.05946909263730049 average time 0.11053811964989109 iter num 40\n",
            "loss 0.04795188456773758 average time 0.09209286831658876 iter num 60\n",
            "loss 0.030830947682261467 average time 0.0828181521874285 iter num 80\n",
            "loss 0.04427115246653557 average time 0.07740427409993572 iter num 100\n",
            "loss 0.03429084271192551 average time 0.1683592967997356 iter num 20\n",
            "loss 0.06151258572936058 average time 0.11165229754988104 iter num 40\n",
            "loss 0.047611791640520096 average time 0.09184500329990139 iter num 60\n",
            "loss 0.05349363014101982 average time 0.08211125537493444 iter num 80\n",
            "loss 0.05455237999558449 average time 0.07608578381994448 iter num 100\n",
            "loss 0.05289314314723015 average time 0.1579252776999965 iter num 20\n",
            "loss 0.029414381831884384 average time 0.10559701454999412 iter num 40\n",
            "loss 0.05574207007884979 average time 0.08796968185009367 iter num 60\n",
            "loss 0.045254990458488464 average time 0.07895943383757639 iter num 80\n",
            "loss 0.025834297761321068 average time 0.07365771073011274 iter num 100\n",
            "loss 0.048098839819431305 average time 0.16048328119977667 iter num 20\n",
            "loss 0.032772913575172424 average time 0.10710508292481791 iter num 40\n",
            "loss 0.05700620263814926 average time 0.08887549166662817 iter num 60\n",
            "loss 0.06022302061319351 average time 0.07982655551254539 iter num 80\n",
            "loss 0.039602652192115784 average time 0.07417457581001145 iter num 100\n",
            "loss 0.041531410068273544 average time 0.15925083394995454 iter num 20\n",
            "loss 0.023163700476288795 average time 0.1064986513999429 iter num 40\n",
            "loss 0.040623582899570465 average time 0.08922384981669892 iter num 60\n",
            "loss 0.038904234766960144 average time 0.08023720208757368 iter num 80\n",
            "loss 0.0410785973072052 average time 0.07474608754006112 iter num 100\n",
            "loss 0.04092434421181679 average time 0.16169975454986343 iter num 20\n",
            "loss 0.06819828599691391 average time 0.10806961769990267 iter num 40\n",
            "loss 0.04717163369059563 average time 0.090025602483153 iter num 60\n",
            "loss 0.029414460062980652 average time 0.08131578623736005 iter num 80\n",
            "loss 0.05078953877091408 average time 0.0757506351498887 iter num 100\n",
            "loss 0.03618149086833 average time 0.16184094975005792 iter num 20\n",
            "loss 0.053394805639982224 average time 0.1085403959249561 iter num 40\n",
            "loss 0.05674850940704346 average time 0.09045471828333879 iter num 60\n",
            "loss 0.03677546605467796 average time 0.08154284612498941 iter num 80\n",
            "loss 0.024224361404776573 average time 0.07609982045001744 iter num 100\n",
            "loss 0.047317516058683395 average time 0.16525062319988137 iter num 20\n",
            "loss 0.07213572412729263 average time 0.11012051292495925 iter num 40\n",
            "loss 0.037235062569379807 average time 0.09170623484988027 iter num 60\n",
            "loss 0.046405330300331116 average time 0.08249185924996709 iter num 80\n",
            "loss 0.04073699936270714 average time 0.07697043382000629 iter num 100\n",
            "loss 0.022347453981637955 average time 0.1632293612500689 iter num 20\n",
            "loss 0.035631757229566574 average time 0.10833778662504301 iter num 40\n",
            "loss 0.03956008702516556 average time 0.09066461528342794 iter num 60\n",
            "loss 0.02899731881916523 average time 0.08165160022506371 iter num 80\n",
            "loss 0.046844951808452606 average time 0.07651164869004788 iter num 100\n",
            "loss 0.04891426861286163 average time 0.16509723919989483 iter num 20\n",
            "loss 0.06155724450945854 average time 0.11032135412497154 iter num 40\n",
            "loss 0.03094075806438923 average time 0.09200250856656567 iter num 60\n",
            "loss 0.032723113894462585 average time 0.0826189988999431 iter num 80\n",
            "loss 0.0171219389885664 average time 0.07736275879995447 iter num 100\n",
            "loss 0.05176975205540657 average time 0.1647998841500339 iter num 20\n",
            "loss 0.027760010212659836 average time 0.10914203330012243 iter num 40\n",
            "loss 0.06271462887525558 average time 0.09056873926683692 iter num 60\n",
            "loss 0.04023600369691849 average time 0.08168553668756431 iter num 80\n",
            "loss 0.027256513014435768 average time 0.07666410650002944 iter num 100\n",
            "loss 0.06321864575147629 average time 0.16345680760014147 iter num 20\n",
            "loss 0.056568849831819534 average time 0.10902015482502066 iter num 40\n",
            "loss 0.04367692768573761 average time 0.09035017316670443 iter num 60\n",
            "loss 0.029382767155766487 average time 0.08158268362506078 iter num 80\n",
            "loss 0.05368465185165405 average time 0.07625851833001435 iter num 100\n",
            "loss 0.046949002891778946 average time 0.1658713793500283 iter num 20\n",
            "loss 0.05653311312198639 average time 0.11053446535001968 iter num 40\n",
            "loss 0.03820414841175079 average time 0.09167596856668751 iter num 60\n",
            "loss 0.050060614943504333 average time 0.082918745387542 iter num 80\n",
            "loss 0.024494538083672523 average time 0.07738192485012405 iter num 100\n",
            "loss 0.039597898721694946 average time 0.1629658847498831 iter num 20\n",
            "loss 0.03333242982625961 average time 0.1088725835000787 iter num 40\n",
            "loss 0.0890563353896141 average time 0.09074740015000013 iter num 60\n",
            "loss 0.02849338762462139 average time 0.08163956151252023 iter num 80\n",
            "loss 0.0516129732131958 average time 0.07612855108001895 iter num 100\n",
            "loss 0.056607022881507874 average time 0.16771748020000815 iter num 20\n",
            "loss 0.04255686700344086 average time 0.11166036487488781 iter num 40\n",
            "loss 0.03860008716583252 average time 0.09312584306656693 iter num 60\n",
            "loss 0.04648665711283684 average time 0.08388051449996965 iter num 80\n",
            "loss 0.055851276963949203 average time 0.07852299463000235 iter num 100\n",
            "loss 0.054106198251247406 average time 0.16156456949993298 iter num 20\n",
            "loss 0.05036700889468193 average time 0.10833118995001315 iter num 40\n",
            "loss 0.042323414236307144 average time 0.09026733373339084 iter num 60\n",
            "loss 0.04656397923827171 average time 0.0814461797499689 iter num 80\n",
            "loss 0.03956369310617447 average time 0.0761968605799484 iter num 100\n",
            "loss 0.05668152868747711 average time 0.16537662549972082 iter num 20\n",
            "loss 0.05355628579854965 average time 0.11033044034975319 iter num 40\n",
            "loss 0.04219576716423035 average time 0.09173138889988573 iter num 60\n",
            "loss 0.061472583562135696 average time 0.08267935736237178 iter num 80\n",
            "loss 0.03969711437821388 average time 0.07720095586988464 iter num 100\n",
            "loss 0.03174121677875519 average time 0.16437121054987075 iter num 20\n",
            "loss 0.038007188588380814 average time 0.10985063892489962 iter num 40\n",
            "loss 0.048650193959474564 average time 0.09164198536661085 iter num 60\n",
            "loss 0.05716465786099434 average time 0.08238701202492393 iter num 80\n",
            "loss 0.03957134857773781 average time 0.07675559427996631 iter num 100\n",
            "loss 0.0724051371216774 average time 0.16478409314986492 iter num 20\n",
            "loss 0.065931037068367 average time 0.10910510127487213 iter num 40\n",
            "loss 0.02791961282491684 average time 0.09154428176664926 iter num 60\n",
            "loss 0.031848713755607605 average time 0.08287457937501586 iter num 80\n",
            "loss 0.02575734630227089 average time 0.0772441689200059 iter num 100\n",
            "loss 0.05957142263650894 average time 0.16869493389976925 iter num 20\n",
            "loss 0.05201524496078491 average time 0.11196605727477618 iter num 40\n",
            "loss 0.026117762550711632 average time 0.09318220934986433 iter num 60\n",
            "loss 0.0248374342918396 average time 0.08369610502495561 iter num 80\n",
            "loss 0.03978877142071724 average time 0.0777179867999439 iter num 100\n",
            "loss 0.044609203934669495 average time 0.1624068296999212 iter num 20\n",
            "loss 0.0649968609213829 average time 0.10856940807484534 iter num 40\n",
            "loss 0.016856161877512932 average time 0.09094450068317504 iter num 60\n",
            "loss 0.05859941244125366 average time 0.08214238821242362 iter num 80\n",
            "loss 0.09350410848855972 average time 0.07656518272991888 iter num 100\n",
            "loss 0.05684652924537659 average time 0.1629991240499294 iter num 20\n",
            "loss 0.055746980011463165 average time 0.10850615875006042 iter num 40\n",
            "loss 0.0275709331035614 average time 0.09084618461665742 iter num 60\n",
            "loss 0.06427699327468872 average time 0.08182041778752591 iter num 80\n",
            "loss 0.051764242351055145 average time 0.07631710383002428 iter num 100\n",
            "loss 0.03477458655834198 average time 0.1630091369001093 iter num 20\n",
            "loss 0.08301707357168198 average time 0.10820739705013693 iter num 40\n",
            "loss 0.027736548334360123 average time 0.08964138073351932 iter num 60\n",
            "loss 0.04333998262882233 average time 0.0806456496876308 iter num 80\n",
            "loss 0.03738453611731529 average time 0.07564353057019617 iter num 100\n",
            "loss 0.06065566837787628 average time 0.16460437484984142 iter num 20\n",
            "loss 0.03500416502356529 average time 0.10900227189995348 iter num 40\n",
            "loss 0.04660259932279587 average time 0.09083307376671049 iter num 60\n",
            "loss 0.06261332333087921 average time 0.08165062435000436 iter num 80\n",
            "loss 0.06262987852096558 average time 0.07580717591004942 iter num 100\n",
            "loss 0.05375313386321068 average time 0.16059466330025315 iter num 20\n",
            "loss 0.038170214742422104 average time 0.10693829092533633 iter num 40\n",
            "loss 0.047226157039403915 average time 0.08956723626688472 iter num 60\n",
            "loss 0.05704321339726448 average time 0.08039488448780503 iter num 80\n",
            "loss 0.033049117773771286 average time 0.07497276235018945 iter num 100\n",
            "loss 0.04669077321887016 average time 0.16267312095005765 iter num 20\n",
            "loss 0.044666703790426254 average time 0.10824126245015578 iter num 40\n",
            "loss 0.021271303296089172 average time 0.08983157791675088 iter num 60\n",
            "loss 0.03606278449296951 average time 0.08081383445014581 iter num 80\n",
            "loss 0.04853660240769386 average time 0.0754642980101562 iter num 100\n",
            "loss 0.048936158418655396 average time 0.1629422477508342 iter num 20\n",
            "loss 0.07285848259925842 average time 0.1078496900004211 iter num 40\n",
            "loss 0.047282032668590546 average time 0.08931154770022355 iter num 60\n",
            "loss 0.04415101185441017 average time 0.08025352211257086 iter num 80\n",
            "loss 0.03629017621278763 average time 0.07488104255011421 iter num 100\n",
            "loss 0.03439575433731079 average time 0.16283685770031298 iter num 20\n",
            "loss 0.027268061414361 average time 0.10811094062537449 iter num 40\n",
            "loss 0.059475988149642944 average time 0.09066251055040388 iter num 60\n",
            "loss 0.03479793295264244 average time 0.08149718227546146 iter num 80\n",
            "loss 0.03970368206501007 average time 0.07589579588049673 iter num 100\n",
            "loss 0.036526042968034744 average time 0.15929235965031693 iter num 20\n",
            "loss 0.06455458700656891 average time 0.10613658547526938 iter num 40\n",
            "loss 0.07884887605905533 average time 0.08815393800020198 iter num 60\n",
            "loss 0.04206950217485428 average time 0.07935524581266691 iter num 80\n",
            "loss 0.041032709181308746 average time 0.07403944350004167 iter num 100\n",
            "loss 0.05446088686585426 average time 0.1602845416007767 iter num 20\n",
            "loss 0.05381779000163078 average time 0.10693937775058657 iter num 40\n",
            "loss 0.02684831991791725 average time 0.0890856701670297 iter num 60\n",
            "loss 0.02630700170993805 average time 0.07997985403762868 iter num 80\n",
            "loss 0.0706392228603363 average time 0.07492212008011848 iter num 100\n",
            "loss 0.042926616966724396 average time 0.16068683524972585 iter num 20\n",
            "loss 0.05510518327355385 average time 0.10761423157491663 iter num 40\n",
            "loss 0.05335747078061104 average time 0.0894866801166548 iter num 60\n",
            "loss 0.031910255551338196 average time 0.08092655426244164 iter num 80\n",
            "loss 0.05706006661057472 average time 0.07523684967000008 iter num 100\n",
            "loss 0.03522104769945145 average time 0.16314237649967253 iter num 20\n",
            "loss 0.04608744755387306 average time 0.10768636029979461 iter num 40\n",
            "loss 0.04914990812540054 average time 0.08945604809984313 iter num 60\n",
            "loss 0.029204033315181732 average time 0.08055126447493421 iter num 80\n",
            "loss 0.0533306822180748 average time 0.07517167627996969 iter num 100\n",
            "loss 0.059943635016679764 average time 0.16255848324999533 iter num 20\n",
            "loss 0.0603225976228714 average time 0.10766612990000794 iter num 40\n",
            "loss 0.03370935469865799 average time 0.0895683230000941 iter num 60\n",
            "loss 0.04002581909298897 average time 0.08070009811262935 iter num 80\n",
            "loss 0.04854225739836693 average time 0.0751136330900772 iter num 100\n",
            "loss 0.04101398587226868 average time 0.1625454514502053 iter num 20\n",
            "loss 0.027318768203258514 average time 0.1079464303250461 iter num 40\n",
            "loss 0.05638980120420456 average time 0.0900808861835685 iter num 60\n",
            "loss 0.042129818350076675 average time 0.08114641180022772 iter num 80\n",
            "loss 0.021474501118063927 average time 0.0757609299401156 iter num 100\n",
            "loss 0.04622418060898781 average time 0.15714877634982258 iter num 20\n",
            "loss 0.06905093789100647 average time 0.10511539697499757 iter num 40\n",
            "loss 0.03884519264101982 average time 0.08782412948324539 iter num 60\n",
            "loss 0.030886335298419 average time 0.07954148721232741 iter num 80\n",
            "loss 0.025543829426169395 average time 0.07428179631988314 iter num 100\n",
            "loss 0.042294375598430634 average time 0.15988189640011113 iter num 20\n",
            "loss 0.057666581124067307 average time 0.10632637582502866 iter num 40\n",
            "loss 0.09139364212751389 average time 0.08866913484986677 iter num 60\n",
            "loss 0.04739784449338913 average time 0.07999548898760622 iter num 80\n",
            "loss 0.05599204823374748 average time 0.07491445562009176 iter num 100\n",
            "loss 0.0312797836959362 average time 0.16246867834961448 iter num 20\n",
            "loss 0.03411071375012398 average time 0.10766906584976824 iter num 40\n",
            "loss 0.03709967061877251 average time 0.08939713233315463 iter num 60\n",
            "loss 0.04864158481359482 average time 0.08011245441234678 iter num 80\n",
            "loss 0.04222816973924637 average time 0.0747784463098651 iter num 100\n",
            "loss 0.08845458179712296 average time 0.1621165270997153 iter num 20\n",
            "loss 0.05734127759933472 average time 0.10757704667485087 iter num 40\n",
            "loss 0.05167880281805992 average time 0.08915622839977004 iter num 60\n",
            "loss 0.06341943889856339 average time 0.08002715922489187 iter num 80\n",
            "loss 0.07307461649179459 average time 0.07479874180982733 iter num 100\n",
            "loss 0.019563516601920128 average time 0.1626108958504119 iter num 20\n",
            "loss 0.0197451151907444 average time 0.10784537460040156 iter num 40\n",
            "loss 0.058827947825193405 average time 0.0898441390336302 iter num 60\n",
            "loss 0.048688143491744995 average time 0.08110928285018418 iter num 80\n",
            "loss 0.03103712387382984 average time 0.07581332912021026 iter num 100\n",
            "loss 0.05004431679844856 average time 0.1622214097998949 iter num 20\n",
            "loss 0.06339845806360245 average time 0.11011115032470116 iter num 40\n",
            "loss 0.0326809287071228 average time 0.09169005383325081 iter num 60\n",
            "loss 0.060606591403484344 average time 0.08241699402497034 iter num 80\n",
            "loss 0.04088534414768219 average time 0.07686390546990879 iter num 100\n",
            "loss 0.045405931770801544 average time 0.16733860865006137 iter num 20\n",
            "loss 0.04234053194522858 average time 0.1111280862502099 iter num 40\n",
            "loss 0.03886737674474716 average time 0.09254499068350318 iter num 60\n",
            "loss 0.06787755340337753 average time 0.08295465680021152 iter num 80\n",
            "loss 0.039346933364868164 average time 0.07737152445017273 iter num 100\n",
            "loss 0.039979323744773865 average time 0.16841039065020597 iter num 20\n",
            "loss 0.05695536360144615 average time 0.11166637322530733 iter num 40\n",
            "loss 0.07609301060438156 average time 0.09276713656690845 iter num 60\n",
            "loss 0.03869205340743065 average time 0.08336279751265466 iter num 80\n",
            "loss 0.0271319393068552 average time 0.07780754982013605 iter num 100\n",
            "loss 0.04790697991847992 average time 0.17012885494987132 iter num 20\n",
            "loss 0.032305337488651276 average time 0.11216643159996238 iter num 40\n",
            "loss 0.057855281978845596 average time 0.09306722016657053 iter num 60\n",
            "loss 0.029147323220968246 average time 0.08337763933741371 iter num 80\n",
            "loss 0.05734198912978172 average time 0.07781761380996613 iter num 100\n",
            "loss 0.032023895531892776 average time 0.17027107360008814 iter num 20\n",
            "loss 0.048487789928913116 average time 0.11206682340025509 iter num 40\n",
            "loss 0.03593463450670242 average time 0.09317933463353256 iter num 60\n",
            "loss 0.05797082558274269 average time 0.08360748365007567 iter num 80\n",
            "loss 0.05181851238012314 average time 0.07786882764001347 iter num 100\n",
            "loss 0.04924200102686882 average time 0.17034885614975792 iter num 20\n",
            "loss 0.05319966748356819 average time 0.11295886854986747 iter num 40\n",
            "loss 0.03144533932209015 average time 0.09415586924981957 iter num 60\n",
            "loss 0.046225324273109436 average time 0.08449400016238542 iter num 80\n",
            "loss 0.05443089082837105 average time 0.07835285392986407 iter num 100\n",
            "loss 0.045197583734989166 average time 0.16941901735062856 iter num 20\n",
            "loss 0.0460335910320282 average time 0.11207648962545136 iter num 40\n",
            "loss 0.07602125406265259 average time 0.09294525386685563 iter num 60\n",
            "loss 0.061231426894664764 average time 0.0833718658626367 iter num 80\n",
            "loss 0.04460139200091362 average time 0.07793157679014256 iter num 100\n",
            "loss 0.04123612865805626 average time 0.16678597005047777 iter num 20\n",
            "loss 0.06837982684373856 average time 0.11054261147537545 iter num 40\n",
            "loss 0.05629226192831993 average time 0.09238392770027228 iter num 60\n",
            "loss 0.04880218952894211 average time 0.08307871447523212 iter num 80\n",
            "loss 0.03049592673778534 average time 0.07720899048024876 iter num 100\n",
            "loss 0.0526139922440052 average time 0.169057207500191 iter num 20\n",
            "loss 0.094300776720047 average time 0.11166219295027986 iter num 40\n",
            "loss 0.06835154443979263 average time 0.09252904411702427 iter num 60\n",
            "loss 0.05315346643328667 average time 0.0830253326377715 iter num 80\n",
            "loss 0.0592833049595356 average time 0.07733472481020726 iter num 100\n",
            "loss 0.02452744171023369 average time 0.1697767997997289 iter num 20\n",
            "loss 0.07183786481618881 average time 0.1124721751247307 iter num 40\n",
            "loss 0.04213511198759079 average time 0.09278490223332483 iter num 60\n",
            "loss 0.0526156984269619 average time 0.08307159817486535 iter num 80\n",
            "loss 0.04432815685868263 average time 0.07749887150988798 iter num 100\n",
            "loss 0.017940396443009377 average time 0.17024655574987263 iter num 20\n",
            "loss 0.0491146594285965 average time 0.11287814547476956 iter num 40\n",
            "loss 0.048356909304857254 average time 0.09342606388314986 iter num 60\n",
            "loss 0.04571905359625816 average time 0.08412938296251013 iter num 80\n",
            "loss 0.03286505863070488 average time 0.07832601957001316 iter num 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-efbdda1db380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_random_1.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0mKnock_Call_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m           \u001b[0mgooptionvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptionvalueavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m           \u001b[0mDeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooptionvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0;31m###################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mgrad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m       \u001b[0m_check_input_dtype_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholomorphic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m       ans, vjp_py, aux = _vjp(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, reduce_axes, *primals)\u001b[0m\n\u001b[1;32m   2311\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun_nokwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     out_primal, out_vjp = ad.vjp(\n\u001b[0;32m-> 2313\u001b[0;31m         flat_fun, primals_flat, reduce_axes=reduce_axes)\n\u001b[0m\u001b[1;32m   2314\u001b[0m     \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux, reduce_axes)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m   \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_primal_pval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout_primal_pval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    511\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJaxprTrace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36moptionvalueavg\u001b[0;34m(key, initial_stocks, numsteps, drift, r, cov, K, B, T, keys)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# down-and-out call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n\u001b[0m\u001b[1;32m     28\u001b[0m                                 (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n\u001b[1;32m     29\u001b[0m                     jnp.exp(-r[0] * T))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    509\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__lt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__le__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__gt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   6583\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6584\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6585\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6586\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeferring_binary_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    753\u001b[0m   \u001b[0;31m# jit, after fixing a surprising interaction with remat(..., concrete=True).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0m_promote_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;31m# Comparison on complex types are defined as a lexicographic ordering on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;31m# the (real, imag) pair.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_promote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    579\u001b[0m   \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m   \u001b[0m_check_no_float0s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_promote_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_promote_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_promote_args_inexact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_promote_dtypes\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0mto_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lattice_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0mto_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_promote_dtypes_inexact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0mto_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lattice_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0mto_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_promote_dtypes_inexact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0;32m--> 481\u001b[0;31m                                        weak_type=new_weak_type)\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbitcast_convert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    270\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    271\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    416\u001b[0m   compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n\u001b[1;32m    417\u001b[0m                                         **params)\n\u001b[0;32m--> 418\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    440\u001b[0m                                     prim.name, donated_invars, *arg_specs)\n\u001b[1;32m    441\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(name, compiled, output_buffer_counts, handlers, kept_var_idx, *args)\u001b[0m\n\u001b[1;32m   1098\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m           if x is not token and i in kept_var_idx))\n\u001b[0;32m-> 1100\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moutput_buffer_counts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fngypgoW_J1T"
      },
      "source": [
        "# 20:15\n",
        "# 2:08"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5144650e-3b2a-4089-e776-1be0d7eedaad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_random_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4fb03eb-73b3-4747-bc94-938e21961238"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_random_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 5000000, batch = 16, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    adjusted_y2 = (1000 * y)**2\n",
        "    loss_weight = 1/adjusted_y2.mean(axis=0)\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    \n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 1000)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_random_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 0.8, 1, 0.25, 0.02, 0.02]]).cuda() # T, K, B, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[3]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.10632345, 0.5543747)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovJwXo3-YEu"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "        dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx2)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.02]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.]*numstocks) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "B = 0.8 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5p-DEWf49yu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "correct_call_prices = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    correct_call_prices.append(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, correct_call_prices, label = \"correct_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(correct_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.0, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeZWsZQxfTnz"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.775, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 0.775, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwZ1fc-afcDx"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.225, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.225, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}