{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid_Test_Knock_Out_Call_1stock_Final_Version",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Final/Grid_Test_Knock_Out_Call_1stock_Final_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYigDkiy0HU9",
        "outputId": "1c3f7bcc-263d-4ad4-967b-ef28796c3191"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 10)\n",
        "K_range = jnp.linspace(0.75, 1.25, 8)\n",
        "B_range = jnp.linspace(0.5, 1.0, 8)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "\n",
        "print(S_range)\n",
        "print(K_range)\n",
        "print(B_range)\n",
        "print(sigma_range)\n",
        "print(r_range)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75       0.8055556  0.86111116 0.9166666  0.97222227 1.0277778\n",
            " 1.0833334  1.138889   1.1944445  1.25      ]\n",
            "[0.75       0.82142854 0.89285713 0.9642857  1.0357143  1.1071429\n",
            " 1.1785713  1.25      ]\n",
            "[0.5        0.5714286  0.6428572  0.71428573 0.78571427 0.85714287\n",
            " 0.92857146 1.        ]\n",
            "[0.15       0.25       0.35000002 0.45      ]\n",
            "[0.01  0.025 0.04 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQxpJqK6OZr"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "\n",
        "goptionvalueavg = jax.grad(optionvalueavg, argnums=1)\n",
        "\n",
        "#################################################################### Adjust all parameters here (not inside class)\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 10)\n",
        "K_range = jnp.linspace(0.75, 1.25, 8)\n",
        "B_range = jnp.linspace(0.5, 1.0, 8)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "T = 1.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "####################################################################\n",
        "\n",
        "call = []\n",
        "count = 0\n",
        "\n",
        "for S in S_range:\n",
        "  for K in K_range:\n",
        "    for B in B_range:\n",
        "      for r in r_range:\n",
        "        for sigma in sigma_range:    \n",
        "\n",
        "          initial_stocks = jnp.array([S]*numstocks) # must be float\n",
        "          r_tmp = jnp.array([r]*numstocks)\n",
        "          drift = r_tmp\n",
        "          cov = jnp.identity(numstocks)*sigma*sigma\n",
        "\n",
        "          Knock_Out_Call_price = optionvalueavg(key, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "          Deltas = goptionvalueavg(keys, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "          call.append([T, K, B, S, sigma, r, r, Knock_Out_Call_price] + list(Deltas)) #T, K, B, S, sigma, mu, r, price, delta\n",
        "          \n",
        "          count += 1\n",
        "          print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "e_OUtP8GUwj5",
        "outputId": "a6ef55df-7269-41e7-b14b-c7fdfbd9ef92"
      },
      "source": [
        "Thedataset = pd.DataFrame(call)\n",
        "Thedataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.048392188</td>\n",
              "      <td>0.556177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.07790985</td>\n",
              "      <td>0.564888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.10672783</td>\n",
              "      <td>0.573440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.1332959</td>\n",
              "      <td>0.571065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.054101802</td>\n",
              "      <td>0.595272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7675</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.20006092</td>\n",
              "      <td>0.472318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7676</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.10021045</td>\n",
              "      <td>0.632276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7677</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.14452648</td>\n",
              "      <td>0.584406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7678</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.180067</td>\n",
              "      <td>0.530899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7679</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.207353</td>\n",
              "      <td>0.483880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7680 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6            7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010  0.048392188  0.556177\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010   0.07790985  0.564888\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010   0.10672783  0.573440\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010    0.1332959  0.571065\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.054101802  0.595272\n",
              "...   ...   ...  ...   ...   ...    ...    ...          ...       ...\n",
              "7675  1.0  1.25  1.0  1.25  0.45  0.025  0.025   0.20006092  0.472318\n",
              "7676  1.0  1.25  1.0  1.25  0.15  0.040  0.040   0.10021045  0.632276\n",
              "7677  1.0  1.25  1.0  1.25  0.25  0.040  0.040   0.14452648  0.584406\n",
              "7678  1.0  1.25  1.0  1.25  0.35  0.040  0.040     0.180067  0.530899\n",
              "7679  1.0  1.25  1.0  1.25  0.45  0.040  0.040     0.207353  0.483880\n",
              "\n",
              "[7680 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQKnflf6peX"
      },
      "source": [
        "# save to csv\n",
        "Thedataset.to_csv('Knock_Out_Call_1stock_MC_Datset_v3.csv', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "skGWSSsG8TGG",
        "outputId": "1f767d10-b4a9-40e8-e75c-0ae4725d0ca5"
      },
      "source": [
        "# read csv\n",
        "import pandas as pd\n",
        "\n",
        "Thedataset = pd.read_csv('Knock_Out_Call_1stock_MC_Datset_v3.csv', header=None)\n",
        "Thedataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.048392</td>\n",
              "      <td>0.556177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.077910</td>\n",
              "      <td>0.564888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.106728</td>\n",
              "      <td>0.573440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.133296</td>\n",
              "      <td>0.571065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.054102</td>\n",
              "      <td>0.595272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7675</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.200061</td>\n",
              "      <td>0.472318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7676</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.100210</td>\n",
              "      <td>0.632276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7677</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.144526</td>\n",
              "      <td>0.584406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7678</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.180067</td>\n",
              "      <td>0.530899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7679</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.207353</td>\n",
              "      <td>0.483880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7680 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6         7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010  0.048392  0.556177\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010  0.077910  0.564888\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010  0.106728  0.573440\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010  0.133296  0.571065\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.054102  0.595272\n",
              "...   ...   ...  ...   ...   ...    ...    ...       ...       ...\n",
              "7675  1.0  1.25  1.0  1.25  0.45  0.025  0.025  0.200061  0.472318\n",
              "7676  1.0  1.25  1.0  1.25  0.15  0.040  0.040  0.100210  0.632276\n",
              "7677  1.0  1.25  1.0  1.25  0.25  0.040  0.040  0.144526  0.584406\n",
              "7678  1.0  1.25  1.0  1.25  0.35  0.040  0.040  0.180067  0.530899\n",
              "7679  1.0  1.25  1.0  1.25  0.45  0.040  0.040  0.207353  0.483880\n",
              "\n",
              "[7680 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4HV-G44th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d9723f-a764-47f9-ee1f-92eb4b7833b0"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch\n",
        "torch.set_printoptions(precision=6)\n",
        "\n",
        "Thedataset_X = Thedataset.iloc[:,:7]\n",
        "Thedataset_Y = Thedataset.iloc[:,7:]\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.X = cupy.array(Thedataset_X)\n",
        "        self.Y = cupy.array(Thedataset_Y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(self.X.toDlpack()), from_dlpack(self.Y.toDlpack()))\n",
        "\n",
        "# print\n",
        "ds = OptionDataSet(max_len = 1)\n",
        "for i in ds:\n",
        "    print(i[0])\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    print(i[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.000000, 0.750000, 0.500000,  ..., 0.150000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.500000,  ..., 0.250000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.500000,  ..., 0.350000, 0.010000, 0.010000],\n",
            "        ...,\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.250000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.350000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.450000, 0.040000, 0.040000]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([7680, 7])\n",
            "tensor([[0.048392, 0.556177],\n",
            "        [0.077910, 0.564888],\n",
            "        [0.106728, 0.573440],\n",
            "        ...,\n",
            "        [0.144526, 0.584406],\n",
            "        [0.180067, 0.530899],\n",
            "        [0.207353, 0.483880]], device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([7680, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "55f68e62-fb2b-42db-cada-f716b1d91c9f"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(7*1, 64) # remember to change this!\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 256)\n",
        "        self.fc4 = nn.Linear(256, 128)\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "        self.fc6 = nn.Linear(64, 2) # 2 outputs: price, delta\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.5, 0.3, 0.03, 0.03]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, B, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.5, 0.75, 0.15, 0.01, 0.01]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "a5e603b7-42fd-4092-9dfc-433e201c8b3e"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 36.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeLVZiiaDS4y",
        "outputId": "89c08b28-c1fc-42a4-eaa6-422c505a37f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CyULkENYKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fb7d0e-f881-4b65-b604-7f3400d2611e"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    # print(x.shape)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "    y_pred = model(x.float())\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    # def compute_deltas(x):\n",
        "    #   inputs = x.float()\n",
        "    #   inputs.requires_grad = True\n",
        "    #   first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "    #   return first_order_gradient[0][[3]]  # Now index 3 is stock price, not 2\n",
        "\n",
        "    # deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    # y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # # print(y_pred)\n",
        "    # # print(y_pred.shape)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 50\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 500)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_oldmethod_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.04958564573772794 average time 0.002956771420040241 iter num 50\n",
            "loss 0.04386825480778129 average time 0.0026878790000228036 iter num 100\n",
            "loss 0.01510279569442875 average time 0.002418064380008218 iter num 50\n",
            "loss 0.011590710847733582 average time 0.0025680611900043004 iter num 100\n",
            "loss 0.010161176612043951 average time 0.0026181361799899605 iter num 50\n",
            "loss 0.010085095634810094 average time 0.00255959339000583 iter num 100\n",
            "loss 0.009837692004544324 average time 0.002885395180019259 iter num 50\n",
            "loss 0.009785370374371802 average time 0.002729049430035957 iter num 100\n",
            "loss 0.009578871012534738 average time 0.002473291880050965 iter num 50\n",
            "loss 0.009526192344242632 average time 0.0024777331000268533 iter num 100\n",
            "loss 0.00930151415699861 average time 0.0024274592400342953 iter num 50\n",
            "loss 0.00924173935476411 average time 0.0024027553800306124 iter num 100\n",
            "loss 0.008984471731951878 average time 0.0024870215599912624 iter num 50\n",
            "loss 0.008915228280291794 average time 0.0025229165699920484 iter num 100\n",
            "loss 0.008615239472370537 average time 0.0025384387400117702 iter num 50\n",
            "loss 0.008533867969267176 average time 0.0024267290300076638 iter num 100\n",
            "loss 0.008181340820475506 average time 0.0023380516799807083 iter num 50\n",
            "loss 0.008085734604890133 average time 0.002487808749965552 iter num 100\n",
            "loss 0.007673225908998655 average time 0.0024443633200280602 iter num 50\n",
            "loss 0.0075613364145461805 average time 0.0024175317699882725 iter num 100\n",
            "loss 0.007077708695054445 average time 0.0023781451399827345 iter num 50\n",
            "loss 0.006945244778042573 average time 0.002408424219988774 iter num 100\n",
            "loss 0.006364934973581284 average time 0.0025997631599602757 iter num 50\n",
            "loss 0.006201983694277115 average time 0.0027146898700038946 iter num 100\n",
            "loss 0.00546986507177183 average time 0.0025516464799784444 iter num 50\n",
            "loss 0.005259248586566063 average time 0.0024957614499908232 iter num 100\n",
            "loss 0.004341598792031379 average time 0.0023835685799986094 iter num 50\n",
            "loss 0.004103669644277937 average time 0.002349295539988816 iter num 100\n",
            "loss 0.0033090485207044828 average time 0.002446688959989842 iter num 50\n",
            "loss 0.003178626540751263 average time 0.002588829089982028 iter num 100\n",
            "loss 0.00287528576088166 average time 0.002402757959989685 iter num 50\n",
            "loss 0.0028311646351982757 average time 0.002402602349998233 iter num 100\n",
            "loss 0.002680274855861328 average time 0.002360842899997806 iter num 50\n",
            "loss 0.0026446430332480067 average time 0.002361113650013067 iter num 100\n",
            "loss 0.0025003986633236348 average time 0.0025153615399631235 iter num 50\n",
            "loss 0.002462810741909909 average time 0.002462353029945916 iter num 100\n",
            "loss 0.0023104435535972767 average time 0.002436899499989522 iter num 50\n",
            "loss 0.002270244846730019 average time 0.002395273119986996 iter num 100\n",
            "loss 0.0021023986677834003 average time 0.002399520300059521 iter num 50\n",
            "loss 0.0020590411543416763 average time 0.0024068285600424132 iter num 100\n",
            "loss 0.0018875985021940475 average time 0.002692891079987021 iter num 50\n",
            "loss 0.0018465601815338556 average time 0.0027683037199994942 iter num 100\n",
            "loss 0.0016964893703984997 average time 0.0025418935600464466 iter num 50\n",
            "loss 0.0016634752078580337 average time 0.0025435593100019104 iter num 100\n",
            "loss 0.001547999018402903 average time 0.002599913980002384 iter num 50\n",
            "loss 0.001522954341776846 average time 0.0024880734499947723 iter num 100\n",
            "loss 0.001432326782240954 average time 0.002543562379969444 iter num 50\n",
            "loss 0.0014114407939702053 average time 0.0025252815099884173 iter num 100\n",
            "loss 0.0013365382395073631 average time 0.002673335780036723 iter num 50\n",
            "loss 0.0013193174559472222 average time 0.0025076042100135964 iter num 100\n",
            "loss 0.0012509624305265218 average time 0.002419097400006649 iter num 50\n",
            "loss 0.0012337831210108856 average time 0.0024389289500049926 iter num 100\n",
            "loss 0.0011644997894802145 average time 0.0024005415399915363 iter num 50\n",
            "loss 0.001146915425487144 average time 0.0023488918899829516 iter num 100\n",
            "loss 0.0010755920539344283 average time 0.0024019744400175114 iter num 50\n",
            "loss 0.0010574320395310586 average time 0.002372478290021718 iter num 100\n",
            "loss 0.000983892986129512 average time 0.0023506752199955374 iter num 50\n",
            "loss 0.0009652106577087352 average time 0.002368861310005741 iter num 100\n",
            "loss 0.0008900371744879385 average time 0.0024096757599727427 iter num 50\n",
            "loss 0.0008710943132000475 average time 0.002385340739979256 iter num 100\n",
            "loss 0.0007978812222952232 average time 0.002375413820000176 iter num 50\n",
            "loss 0.000780319217822864 average time 0.00245363089998591 iter num 100\n",
            "loss 0.0007144647681148594 average time 0.002497478779996527 iter num 50\n",
            "loss 0.0006984821881941085 average time 0.0024887955399799464 iter num 100\n",
            "loss 0.0006358738910013242 average time 0.003002207539975643 iter num 50\n",
            "loss 0.000620530085370376 average time 0.0027815502899875355 iter num 100\n",
            "loss 0.000560836272610858 average time 0.0023887675800142462 iter num 50\n",
            "loss 0.0005463871907273757 average time 0.0023704339600226377 iter num 100\n",
            "loss 0.000497347400726336 average time 0.002569042579998495 iter num 50\n",
            "loss 0.0004849176361819067 average time 0.002583903779996035 iter num 100\n",
            "loss 0.00044146070233161774 average time 0.0024764640600278653 iter num 50\n",
            "loss 0.0004309499985458382 average time 0.002446426300029998 iter num 100\n",
            "loss 0.000390437601622155 average time 0.0024004518000310783 iter num 50\n",
            "loss 0.0003807377100025009 average time 0.002366429470002913 iter num 100\n",
            "loss 0.00034383811896029 average time 0.0024180532799982757 iter num 50\n",
            "loss 0.000335149735769005 average time 0.0024715100499770413 iter num 100\n",
            "loss 0.0003026388067719984 average time 0.002462842179975269 iter num 50\n",
            "loss 0.0002951266269145939 average time 0.002424578280001697 iter num 100\n",
            "loss 0.0002679920851857158 average time 0.002553555200020128 iter num 50\n",
            "loss 0.000261937761161494 average time 0.0024982274700141717 iter num 100\n",
            "loss 0.00024326448460196753 average time 0.0025056359999780396 iter num 50\n",
            "loss 0.0002384959853072372 average time 0.0024903995300155656 iter num 100\n",
            "loss 0.00022279188213915373 average time 0.0024008938199858677 iter num 50\n",
            "loss 0.00021910014649537409 average time 0.0024136521999980685 iter num 100\n",
            "loss 0.00020507029590460514 average time 0.002446985320020758 iter num 50\n",
            "loss 0.00020173621238751355 average time 0.002368935380009134 iter num 100\n",
            "loss 0.00018895592483489994 average time 0.00246272832005161 iter num 50\n",
            "loss 0.00018589097113992333 average time 0.002451614240017079 iter num 100\n",
            "loss 0.00017404810543960824 average time 0.0024554179800088606 iter num 50\n",
            "loss 0.00017118450491946497 average time 0.0024983694200091124 iter num 100\n",
            "loss 0.00016005181406048816 average time 0.0023985612399610545 iter num 50\n",
            "loss 0.00015734336417789859 average time 0.002415686189988264 iter num 100\n",
            "loss 0.00014682050051740197 average time 0.002499345779979194 iter num 50\n",
            "loss 0.0001442694373530807 average time 0.0025039679299879935 iter num 100\n",
            "loss 0.00014145428216388913 average time 0.002711676539993277 iter num 50\n",
            "loss 0.00013569190959460088 average time 0.0025548271000252498 iter num 100\n",
            "loss 0.0001295574721248973 average time 0.0023760708199733927 iter num 50\n",
            "loss 0.00012806013621902737 average time 0.0023819230499975675 iter num 100\n",
            "loss 0.0001222144739668712 average time 0.0028088786399439415 iter num 50\n",
            "loss 0.00012079036140092724 average time 0.002707288649985458 iter num 100\n",
            "loss 0.00011524381855211522 average time 0.002561758699994243 iter num 50\n",
            "loss 0.00011389598504831483 average time 0.002457608540012188 iter num 100\n",
            "loss 0.00010865821677390727 average time 0.0024935445000210167 iter num 50\n",
            "loss 0.00010738858182200332 average time 0.002568319390011311 iter num 100\n",
            "loss 0.00010246488607462823 average time 0.00253842747994895 iter num 50\n",
            "loss 0.00010127349826942491 average time 0.002436059319961714 iter num 100\n",
            "loss 9.666317111635392e-05 average time 0.0024968276799972955 iter num 50\n",
            "loss 9.555125821994034e-05 average time 0.0025001998100015045 iter num 100\n",
            "loss 9.125667738921577e-05 average time 0.0025638190600056985 iter num 50\n",
            "loss 9.022189058774712e-05 average time 0.0025320864600098503 iter num 100\n",
            "loss 8.622799663535852e-05 average time 0.0025179921600101806 iter num 50\n",
            "loss 8.526617554885755e-05 average time 0.0025490840399788797 iter num 100\n",
            "loss 8.155728155977356e-05 average time 0.0025214878999668144 iter num 50\n",
            "loss 8.066580685706333e-05 average time 0.002454721119997885 iter num 100\n",
            "loss 7.723604879910173e-05 average time 0.0024815356399722076 iter num 50\n",
            "loss 7.641361541001718e-05 average time 0.002406440579984519 iter num 100\n",
            "loss 7.523006839564738e-05 average time 0.002580113559970414 iter num 50\n",
            "loss 7.304428877967433e-05 average time 0.0024680474399838203 iter num 100\n",
            "loss 7.062878995219748e-05 average time 0.0024644703200010555 iter num 50\n",
            "loss 7.004490050662067e-05 average time 0.002491100970005391 iter num 100\n",
            "loss 6.77803134915602e-05 average time 0.002424086679920947 iter num 50\n",
            "loss 6.723159909902101e-05 average time 0.0024671605499815997 iter num 100\n",
            "loss 6.510037185897375e-05 average time 0.002552272240018283 iter num 50\n",
            "loss 6.458327627795065e-05 average time 0.0026291784400109463 iter num 100\n",
            "loss 6.257318812993853e-05 average time 0.0024305720600114 iter num 50\n",
            "loss 6.208445091724863e-05 average time 0.002474067580019437 iter num 100\n",
            "loss 6.108465099844471e-05 average time 0.0024059022199708125 iter num 50\n",
            "loss 6.040660437336892e-05 average time 0.0024821980699925917 iter num 100\n",
            "loss 5.9051675175988416e-05 average time 0.0023422276600012994 iter num 50\n",
            "loss 5.872095616494677e-05 average time 0.0023742121699979178 iter num 100\n",
            "loss 5.742405655548805e-05 average time 0.002518765299973893 iter num 50\n",
            "loss 5.710389552310086e-05 average time 0.0024871420399995257 iter num 100\n",
            "loss 5.583844229208641e-05 average time 0.0024919204599973457 iter num 50\n",
            "loss 5.552530850843636e-05 average time 0.0024543565900194153 iter num 100\n",
            "loss 5.4286060523637705e-05 average time 0.0023854758800189303 iter num 50\n",
            "loss 5.397902339272096e-05 average time 0.002492550760011909 iter num 100\n",
            "loss 5.2762255140085065e-05 average time 0.0025807693000569998 iter num 50\n",
            "loss 5.2460357706572624e-05 average time 0.00252806094004427 iter num 100\n",
            "loss 5.126266115487875e-05 average time 0.0025355902600313128 iter num 50\n",
            "loss 5.0965238247698606e-05 average time 0.0026033538200272233 iter num 100\n",
            "loss 4.9783737531253735e-05 average time 0.0024218210800336236 iter num 50\n",
            "loss 4.9489704451673996e-05 average time 0.0024667993500270312 iter num 100\n",
            "loss 4.862784159664694e-05 average time 0.002401555759988696 iter num 50\n",
            "loss 4.825380705466209e-05 average time 0.002446501050003462 iter num 100\n",
            "loss 4.725733081473069e-05 average time 0.002446549979995325 iter num 50\n",
            "loss 4.700715853184026e-05 average time 0.0024749024199900303 iter num 100\n",
            "loss 4.6012297152504784e-05 average time 0.002603355000001102 iter num 50\n",
            "loss 4.576298123502735e-05 average time 0.0025065932700090343 iter num 100\n",
            "loss 4.476496821813431e-05 average time 0.0024620406400390495 iter num 50\n",
            "loss 4.451463541802535e-05 average time 0.0025115992600240134 iter num 100\n",
            "loss 4.403427331114726e-05 average time 0.0028532376200018916 iter num 50\n",
            "loss 4.343336617560426e-05 average time 0.0027484952399890973 iter num 100\n",
            "loss 4.254743585175663e-05 average time 0.002419008359993313 iter num 50\n",
            "loss 4.232509546028328e-05 average time 0.0025357652600041546 iter num 100\n",
            "loss 4.14341990898073e-05 average time 0.002439335199978814 iter num 50\n",
            "loss 4.1210029480520517e-05 average time 0.002424846649973915 iter num 100\n",
            "loss 4.129478251476034e-05 average time 0.0023808407400156286 iter num 50\n",
            "loss 4.019217506058113e-05 average time 0.002405083540029409 iter num 100\n",
            "loss 3.941048923399238e-05 average time 0.0024330988400015486 iter num 50\n",
            "loss 3.916962442894467e-05 average time 0.0023911014199984493 iter num 100\n",
            "loss 3.8929501098933946e-05 average time 0.0024035421399730694 iter num 50\n",
            "loss 3.822289183854357e-05 average time 0.0025123194299794703 iter num 100\n",
            "loss 3.744185952524641e-05 average time 0.0026411527200525596 iter num 50\n",
            "loss 3.724519971828476e-05 average time 0.0026905880000549585 iter num 100\n",
            "loss 3.744892124893211e-05 average time 0.002544562960001713 iter num 50\n",
            "loss 3.6373399705148664e-05 average time 0.002452188609991026 iter num 100\n",
            "loss 3.5644936988141894e-05 average time 0.0024865290399975495 iter num 50\n",
            "loss 3.546076242636263e-05 average time 0.002518347440004618 iter num 100\n",
            "loss 3.4806446520310607e-05 average time 0.0023778703199604932 iter num 50\n",
            "loss 3.455170505508256e-05 average time 0.002423816699965755 iter num 100\n",
            "loss 3.5640038036349445e-05 average time 0.0026808955199976482 iter num 50\n",
            "loss 3.4088119972980704e-05 average time 0.002565861859989127 iter num 100\n",
            "loss 3.3496397064584465e-05 average time 0.0025291288399876067 iter num 50\n",
            "loss 3.335435076268288e-05 average time 0.002634203939996951 iter num 100\n",
            "loss 3.278853580139282e-05 average time 0.0024938814799679676 iter num 50\n",
            "loss 3.2646428321070986e-05 average time 0.002649099709979055 iter num 100\n",
            "loss 3.20755832028342e-05 average time 0.0029076261200225418 iter num 50\n",
            "loss 3.193182952015127e-05 average time 0.002626928479999151 iter num 100\n",
            "loss 3.135412067964728e-05 average time 0.002386197559972061 iter num 50\n",
            "loss 3.120863365266486e-05 average time 0.002468448899976465 iter num 100\n",
            "loss 3.062396716275683e-05 average time 0.0024918927999988227 iter num 50\n",
            "loss 3.047677960007265e-05 average time 0.0024677008800108526 iter num 100\n",
            "loss 2.988556028127876e-05 average time 0.002424456540065876 iter num 50\n",
            "loss 2.9736813060898472e-05 average time 0.002415352810007789 iter num 100\n",
            "loss 2.9139619696686276e-05 average time 0.00252245207996566 iter num 50\n",
            "loss 2.898943658508517e-05 average time 0.0025009965899789677 iter num 100\n",
            "loss 3.02000468130373e-05 average time 0.0023794358800023475 iter num 50\n",
            "loss 2.852499037262898e-05 average time 0.0024657966399809082 iter num 100\n",
            "loss 2.8027596372102825e-05 average time 0.0026231224599905546 iter num 50\n",
            "loss 2.790648139115564e-05 average time 0.002601448039990828 iter num 100\n",
            "loss 2.7424080686474477e-05 average time 0.0026738740000109827 iter num 50\n",
            "loss 2.7302693153404998e-05 average time 0.0025271440700089442 iter num 100\n",
            "loss 2.681474115335481e-05 average time 0.0024385706599969127 iter num 50\n",
            "loss 2.6691789316139158e-05 average time 0.0023944872400034002 iter num 100\n",
            "loss 2.619748017772394e-05 average time 0.002416699619998326 iter num 50\n",
            "loss 2.6072964157960478e-05 average time 0.002418757959994764 iter num 100\n",
            "loss 2.5575902827799698e-05 average time 0.002417912660012007 iter num 50\n",
            "loss 2.5446704385768687e-05 average time 0.002475460899972859 iter num 100\n",
            "loss 2.914650340477281e-05 average time 0.0024035897599969758 iter num 50\n",
            "loss 2.5389262145738348e-05 average time 0.0023870159300213344 iter num 100\n",
            "loss 2.4955626191993668e-05 average time 0.0024115996199725485 iter num 50\n",
            "loss 2.4863618792462978e-05 average time 0.0023953342099821383 iter num 100\n",
            "loss 2.450347747390274e-05 average time 0.0024622797000392895 iter num 50\n",
            "loss 2.441376716706407e-05 average time 0.0024236088600355287 iter num 100\n",
            "loss 2.4053660693299212e-05 average time 0.0026665033799963566 iter num 50\n",
            "loss 2.396295531205881e-05 average time 0.0027468406499883712 iter num 100\n",
            "loss 2.3597742852975538e-05 average time 0.002637612599992281 iter num 50\n",
            "loss 2.3505550348060828e-05 average time 0.002500702999996065 iter num 100\n",
            "loss 2.313408694468877e-05 average time 0.002592489899980137 iter num 50\n",
            "loss 2.30402836727453e-05 average time 0.0024792834599929848 iter num 100\n",
            "loss 2.2662349244778653e-05 average time 0.0026177767400258744 iter num 50\n",
            "loss 2.2566921949116118e-05 average time 0.0026063726400207088 iter num 100\n",
            "loss 2.2182521712025163e-05 average time 0.002571040700058802 iter num 50\n",
            "loss 2.208550729836254e-05 average time 0.002585605480026061 iter num 100\n",
            "loss 2.1694924867896904e-05 average time 0.0026353328399727617 iter num 50\n",
            "loss 2.1596430947695953e-05 average time 0.0026800717499781967 iter num 100\n",
            "loss 2.120017227838418e-05 average time 0.002498426539959837 iter num 50\n",
            "loss 2.1100303866855138e-05 average time 0.002469013329982772 iter num 100\n",
            "loss 2.320978847644632e-05 average time 0.0024637094199806597 iter num 50\n",
            "loss 2.0829860203605232e-05 average time 0.0024557522500117555 iter num 100\n",
            "loss 2.0491189146707775e-05 average time 0.0026330032000078064 iter num 50\n",
            "loss 2.0404094667249825e-05 average time 0.002589752020026026 iter num 100\n",
            "loss 2.007673836067287e-05 average time 0.0023384896400330036 iter num 50\n",
            "loss 1.9994351932637016e-05 average time 0.0023604456600332924 iter num 100\n",
            "loss 1.966296241577354e-05 average time 0.002752804779984217 iter num 50\n",
            "loss 1.9579376770781204e-05 average time 0.002977364509997642 iter num 100\n",
            "loss 1.9243012685357395e-05 average time 0.0027209253999899373 iter num 50\n",
            "loss 1.915821312584591e-05 average time 0.0025771156499740755 iter num 100\n",
            "loss 1.896440408296131e-05 average time 0.00236795726003038 iter num 50\n",
            "loss 1.877729678856092e-05 average time 0.002453892469989114 iter num 100\n",
            "loss 2.0427565995715464e-05 average time 0.0025255286399442412 iter num 50\n",
            "loss 1.868458686394055e-05 average time 0.0025853049499619373 iter num 100\n",
            "loss 1.8387005123676337e-05 average time 0.002413512380035172 iter num 50\n",
            "loss 1.831890256130243e-05 average time 0.0023782381200226155 iter num 100\n",
            "loss 1.8048579837791463e-05 average time 0.0023742612799924246 iter num 50\n",
            "loss 1.7980844071057136e-05 average time 0.002394019019998268 iter num 100\n",
            "loss 1.770868565612976e-05 average time 0.002747848679982781 iter num 50\n",
            "loss 1.7640097012454856e-05 average time 0.0025572763799982566 iter num 100\n",
            "loss 1.736396572807632e-05 average time 0.00250208064004255 iter num 50\n",
            "loss 1.7294260552936523e-05 average time 0.00242350091001299 iter num 100\n",
            "loss 1.701368630864094e-05 average time 0.0024589783600094963 iter num 50\n",
            "loss 1.6942872201026472e-05 average time 0.002521989559995745 iter num 100\n",
            "loss 1.6658899407787317e-05 average time 0.00252455809996718 iter num 50\n",
            "loss 1.6586662900064522e-05 average time 0.0025652612100111583 iter num 100\n",
            "loss 1.929474377754522e-05 average time 0.002540421300000162 iter num 50\n",
            "loss 1.6453763611926563e-05 average time 0.0024606070400113823 iter num 100\n",
            "loss 1.6220960015237926e-05 average time 0.002700767959959194 iter num 50\n",
            "loss 1.616642440752879e-05 average time 0.0025656881899703875 iter num 100\n",
            "loss 1.5948557766648546e-05 average time 0.0024713935000090716 iter num 50\n",
            "loss 1.589368369508462e-05 average time 0.002418246529996395 iter num 100\n",
            "loss 1.567254031187687e-05 average time 0.0024077797799782275 iter num 50\n",
            "loss 1.5616707131176084e-05 average time 0.002468320659982055 iter num 100\n",
            "loss 1.539148905759348e-05 average time 0.0024706085200068627 iter num 50\n",
            "loss 1.533457745239978e-05 average time 0.00242115953999928 iter num 100\n",
            "loss 1.510713696456057e-05 average time 0.0024319938399912644 iter num 50\n",
            "loss 1.504746470828731e-05 average time 0.002386406169998736 iter num 100\n",
            "loss 2.1162855087508502e-05 average time 0.0025010090999694513 iter num 50\n",
            "loss 1.50130797256474e-05 average time 0.0024487607499804652 iter num 100\n",
            "loss 1.4785088304256968e-05 average time 0.0026684989799741743 iter num 50\n",
            "loss 1.4736205694842435e-05 average time 0.002615695439949377 iter num 100\n",
            "loss 1.4547212237591937e-05 average time 0.002488129519942959 iter num 50\n",
            "loss 1.4499868054038285e-05 average time 0.0025166610699579907 iter num 100\n",
            "loss 1.4309609782195822e-05 average time 0.002535255259981568 iter num 50\n",
            "loss 1.426161823523459e-05 average time 0.002485702329981905 iter num 100\n",
            "loss 1.4068265314619192e-05 average time 0.0024267066600168617 iter num 50\n",
            "loss 1.4019471097980093e-05 average time 0.0024162860900105443 iter num 100\n",
            "loss 1.382284805621261e-05 average time 0.002503261339998062 iter num 50\n",
            "loss 1.377323575838856e-05 average time 0.0024661503499964965 iter num 100\n",
            "loss 1.3573421275701603e-05 average time 0.002362463879962888 iter num 50\n",
            "loss 1.352300506735989e-05 average time 0.00239783868998984 iter num 100\n",
            "loss 1.7132397266855332e-05 average time 0.002463881180037788 iter num 50\n",
            "loss 1.3523690048937867e-05 average time 0.002438412470037292 iter num 100\n",
            "loss 1.3320166649139458e-05 average time 0.0026439370599473477 iter num 50\n",
            "loss 1.3278187697246484e-05 average time 0.00263629509997827 iter num 100\n",
            "loss 1.311264941349454e-05 average time 0.0025390914399667963 iter num 50\n",
            "loss 1.3071311407279518e-05 average time 0.0026555664999978034 iter num 100\n",
            "loss 1.2905399403605515e-05 average time 0.0025897860600070997 iter num 50\n",
            "loss 1.2863587560000128e-05 average time 0.0025119848500162333 iter num 100\n",
            "loss 1.2695485575516817e-05 average time 0.002717539960003705 iter num 50\n",
            "loss 1.2653091759333027e-05 average time 0.0025466956500076778 iter num 100\n",
            "loss 1.2482445037865086e-05 average time 0.00242369226003575 iter num 50\n",
            "loss 1.2439389208081012e-05 average time 0.0024854236000328455 iter num 100\n",
            "loss 1.2266206497898005e-05 average time 0.002400386299996171 iter num 50\n",
            "loss 1.222243723636289e-05 average time 0.002400997100003224 iter num 100\n",
            "loss 1.6373318166620695e-05 average time 0.0026010875400061194 iter num 50\n",
            "loss 1.2190601689892661e-05 average time 0.002516744230006225 iter num 100\n",
            "loss 1.202433068848386e-05 average time 0.0024007873599566666 iter num 50\n",
            "loss 1.1983705484932629e-05 average time 0.002426822559973516 iter num 100\n",
            "loss 1.183566836011943e-05 average time 0.0026488467400395165 iter num 50\n",
            "loss 1.1798618614703038e-05 average time 0.0026071742800240828 iter num 100\n",
            "loss 1.1649694713336821e-05 average time 0.0024784319600166783 iter num 50\n",
            "loss 1.1612176219542273e-05 average time 0.00266340483001386 iter num 100\n",
            "loss 1.1461163854518074e-05 average time 0.0029256855599760455 iter num 50\n",
            "loss 1.1423094880570755e-05 average time 0.002614013379975404 iter num 100\n",
            "loss 1.127272941496345e-05 average time 0.002446729500024958 iter num 50\n",
            "loss 1.1231895632278097e-05 average time 0.002464053500020782 iter num 100\n",
            "loss 1.3725016927969824e-05 average time 0.002398194719989988 iter num 50\n",
            "loss 1.115503316479603e-05 average time 0.0024497924399747716 iter num 100\n",
            "loss 1.1026401324990844e-05 average time 0.0024157100200136485 iter num 50\n",
            "loss 1.0996264153213706e-05 average time 0.0025810656699695755 iter num 100\n",
            "loss 1.0875732144553663e-05 average time 0.002560778820043197 iter num 50\n",
            "loss 1.084534561857492e-05 average time 0.002522220390005714 iter num 100\n",
            "loss 1.0722913433643597e-05 average time 0.002561487840011978 iter num 50\n",
            "loss 1.0691872187522171e-05 average time 0.002493678480000199 iter num 100\n",
            "loss 1.088991901628159e-05 average time 0.00275823943999967 iter num 50\n",
            "loss 1.0659828171187113e-05 average time 0.0026772289199971057 iter num 100\n",
            "loss 1.0536206292896621e-05 average time 0.0023292442999809284 iter num 50\n",
            "loss 1.0506302589854334e-05 average time 0.0023758062100068854 iter num 100\n",
            "loss 1.0395814954640546e-05 average time 0.002477649520014893 iter num 50\n",
            "loss 1.0368069509170106e-05 average time 0.0023964289500190716 iter num 100\n",
            "loss 1.0256360414631823e-05 average time 0.002911507579992758 iter num 50\n",
            "loss 1.0228161577421952e-05 average time 0.0027906407000182296 iter num 100\n",
            "loss 1.011444037779657e-05 average time 0.00238559203998193 iter num 50\n",
            "loss 1.0085717496951903e-05 average time 0.002464453609991324 iter num 100\n",
            "loss 1.002311716573362e-05 average time 0.0025346588399861505 iter num 50\n",
            "loss 9.943862379282391e-06 average time 0.0024070635500038407 iter num 100\n",
            "loss 1.3598010669332823e-05 average time 0.0024626908599566376 iter num 50\n",
            "loss 1.0097715653202536e-05 average time 0.0024646451999706187 iter num 100\n",
            "loss 9.938066987809294e-06 average time 0.002839553540043198 iter num 50\n",
            "loss 9.90847584387298e-06 average time 0.002816420100025425 iter num 100\n",
            "loss 9.812570987429657e-06 average time 0.0027108271199813316 iter num 50\n",
            "loss 9.789291971492328e-06 average time 0.0025856390799981455 iter num 100\n",
            "loss 9.696686606052678e-06 average time 0.002385154519961361 iter num 50\n",
            "loss 9.673494365727915e-06 average time 0.002430938690004041 iter num 100\n",
            "loss 9.580450855606016e-06 average time 0.0027138715800174395 iter num 50\n",
            "loss 9.557009981125959e-06 average time 0.002688215550010682 iter num 100\n",
            "loss 9.462636772567274e-06 average time 0.002572348619996774 iter num 50\n",
            "loss 9.438806100187336e-06 average time 0.0026355812199881255 iter num 100\n",
            "loss 9.342776457190308e-06 average time 0.0025727396000092993 iter num 50\n",
            "loss 9.318510361372144e-06 average time 0.002615511149992926 iter num 100\n",
            "loss 9.220688317108432e-06 average time 0.0025445583200144027 iter num 50\n",
            "loss 9.195973164363114e-06 average time 0.0025101974899871494 iter num 100\n",
            "loss 9.096264110308873e-06 average time 0.0027065456999753224 iter num 50\n",
            "loss 9.071078255933113e-06 average time 0.0025417333399855124 iter num 100\n",
            "loss 9.166777784396428e-06 average time 0.002483567720037172 iter num 50\n",
            "loss 8.95005080861335e-06 average time 0.0024809446100061906 iter num 100\n",
            "loss 1.0650414149362285e-05 average time 0.0025118043399925227 iter num 50\n",
            "loss 9.190963273183407e-06 average time 0.0024508212299906517 iter num 100\n",
            "loss 9.021223926312714e-06 average time 0.002379775999988851 iter num 50\n",
            "loss 8.998357648753159e-06 average time 0.0024073324199753187 iter num 100\n",
            "loss 8.916653997723915e-06 average time 0.0025401795799643878 iter num 50\n",
            "loss 8.897347865420896e-06 average time 0.002526891009974861 iter num 100\n",
            "loss 8.82145433601966e-06 average time 0.0025338273599936657 iter num 50\n",
            "loss 8.80263027327146e-06 average time 0.0025003037400028916 iter num 100\n",
            "loss 8.727318145328071e-06 average time 0.0025786218399389328 iter num 50\n",
            "loss 8.708416177017414e-06 average time 0.00254485313999794 iter num 100\n",
            "loss 8.632387771755032e-06 average time 0.002873752980012796 iter num 50\n",
            "loss 8.613212083165315e-06 average time 0.002745319940004265 iter num 100\n",
            "loss 8.536002452885508e-06 average time 0.0025475767399984763 iter num 50\n",
            "loss 8.516496389231344e-06 average time 0.002473482830009743 iter num 100\n",
            "loss 8.43784290667285e-06 average time 0.0027013720200193348 iter num 50\n",
            "loss 8.41798254836856e-06 average time 0.0025533779400302593 iter num 100\n",
            "loss 8.337751054445747e-06 average time 0.0023617096400175798 iter num 50\n",
            "loss 8.317470260496917e-06 average time 0.0023539031200016324 iter num 100\n",
            "loss 8.246994152614049e-06 average time 0.002415557699996498 iter num 50\n",
            "loss 8.216167550917087e-06 average time 0.00237300434998815 iter num 100\n",
            "loss 8.541634849141398e-06 average time 0.0025131305400009295 iter num 50\n",
            "loss 8.179774714462687e-06 average time 0.0024227327400194554 iter num 100\n",
            "loss 8.107455667280328e-06 average time 0.00259787393996703 iter num 50\n",
            "loss 8.089093280503547e-06 average time 0.00249816947999534 iter num 100\n",
            "loss 8.01875889816639e-06 average time 0.0024176266400263557 iter num 50\n",
            "loss 8.001020883842676e-06 average time 0.002400594500036277 iter num 100\n",
            "loss 7.9295711745836e-06 average time 0.0028071148399612866 iter num 50\n",
            "loss 7.911217599426726e-06 average time 0.0026756797599637137 iter num 100\n",
            "loss 9.566232921310604e-06 average time 0.002561090300068827 iter num 50\n",
            "loss 7.912819029839251e-06 average time 0.0025385809800309287 iter num 100\n",
            "loss 7.83309543531947e-06 average time 0.0029472596000050546 iter num 50\n",
            "loss 7.8162049985963e-06 average time 0.0026725805499881973 iter num 100\n",
            "loss 7.749531417850075e-06 average time 0.0023802757799967367 iter num 50\n",
            "loss 7.732816955641994e-06 average time 0.0024729858700129627 iter num 100\n",
            "loss 7.665595159448953e-06 average time 0.003001104699987991 iter num 50\n",
            "loss 7.648620028311823e-06 average time 0.002897515539980304 iter num 100\n",
            "loss 7.580150702796108e-06 average time 0.002445511419973627 iter num 50\n",
            "loss 7.562827398285508e-06 average time 0.00259773386998404 iter num 100\n",
            "loss 7.5747047264940905e-06 average time 0.002619493739985046 iter num 50\n",
            "loss 7.477461736254096e-06 average time 0.0025026451199619258 iter num 100\n",
            "loss 9.461872845427588e-06 average time 0.002430635720038481 iter num 50\n",
            "loss 7.682136798115035e-06 average time 0.002594390670019493 iter num 100\n",
            "loss 7.546052514283453e-06 average time 0.0026851250000254367 iter num 50\n",
            "loss 7.526635246951574e-06 average time 0.002754433310024069 iter num 100\n",
            "loss 7.462964252701668e-06 average time 0.00278217113998835 iter num 50\n",
            "loss 7.448054308017273e-06 average time 0.002735833899987483 iter num 100\n",
            "loss 7.3896991158405295e-06 average time 0.002477466699965589 iter num 50\n",
            "loss 7.375275755359451e-06 average time 0.0024199834699811617 iter num 100\n",
            "loss 7.317729497040643e-06 average time 0.0024411810400124523 iter num 50\n",
            "loss 7.303292295302402e-06 average time 0.0024540158000127123 iter num 100\n",
            "loss 7.24535177809672e-06 average time 0.003114041179978813 iter num 50\n",
            "loss 7.2307745730041355e-06 average time 0.002918123729996296 iter num 100\n",
            "loss 7.1720017228332985e-06 average time 0.0025983277799878123 iter num 50\n",
            "loss 7.157166929086396e-06 average time 0.0025897993099943052 iter num 100\n",
            "loss 7.0973466077665555e-06 average time 0.0028120902600403497 iter num 50\n",
            "loss 7.082245673663908e-06 average time 0.0026148999599990928 iter num 100\n",
            "loss 7.02123462231474e-06 average time 0.00253640998002993 iter num 50\n",
            "loss 7.005819390866168e-06 average time 0.002484055200034163 iter num 100\n",
            "loss 6.945052013622112e-06 average time 0.002349680259985689 iter num 50\n",
            "loss 6.928037915711869e-06 average time 0.002372103199995763 iter num 100\n",
            "loss 7.184644088202624e-06 average time 0.0025549327800217726 iter num 50\n",
            "loss 6.874765301405124e-06 average time 0.0024932998200119984 iter num 100\n",
            "loss 6.903442602504957e-06 average time 0.002929436500016891 iter num 50\n",
            "loss 6.842768701954748e-06 average time 0.0028448364700216187 iter num 100\n",
            "loss 6.786925047901905e-06 average time 0.0027961474599942446 iter num 50\n",
            "loss 6.773506085726907e-06 average time 0.0025714120799875673 iter num 100\n",
            "loss 6.719627433563683e-06 average time 0.0023444097399988096 iter num 50\n",
            "loss 6.706040009182621e-06 average time 0.0023589860200081605 iter num 100\n",
            "loss 6.656708786797017e-06 average time 0.0029146813599800225 iter num 50\n",
            "loss 6.6380034787787485e-06 average time 0.002747915859981731 iter num 100\n",
            "loss 9.18701170991115e-06 average time 0.0025593952599683688 iter num 50\n",
            "loss 6.769393466752565e-06 average time 0.002526164969972342 iter num 100\n",
            "loss 6.67128392786808e-06 average time 0.0023667794599987247 iter num 50\n",
            "loss 6.654992432280356e-06 average time 0.002351007089982886 iter num 100\n",
            "loss 6.60378534182494e-06 average time 0.0024658944599923417 iter num 50\n",
            "loss 6.591597235318342e-06 average time 0.0024456576199918343 iter num 100\n",
            "loss 6.543514589774241e-06 average time 0.0024127791200317006 iter num 50\n",
            "loss 6.53157010867284e-06 average time 0.002428853200026424 iter num 100\n",
            "loss 6.483734061482049e-06 average time 0.0025533273800010647 iter num 50\n",
            "loss 6.471707823636295e-06 average time 0.0025025342400022054 iter num 100\n",
            "loss 6.423330090711923e-06 average time 0.0024652566399618082 iter num 50\n",
            "loss 6.41114045225808e-06 average time 0.0024304496899640073 iter num 100\n",
            "loss 6.361922818395669e-06 average time 0.0026594114599902243 iter num 50\n",
            "loss 6.349487573217665e-06 average time 0.0026191522599901874 iter num 100\n",
            "loss 6.299297479281478e-06 average time 0.0024156068799766218 iter num 50\n",
            "loss 6.286594387483911e-06 average time 0.002610128829996938 iter num 100\n",
            "loss 6.235305561454684e-06 average time 0.0025438047999614354 iter num 50\n",
            "loss 6.222311139608751e-06 average time 0.002574558229966897 iter num 100\n",
            "loss 6.19207142046972e-06 average time 0.0026494656000159013 iter num 50\n",
            "loss 6.157716845518145e-06 average time 0.0025827294399914536 iter num 100\n",
            "loss 9.611444281543453e-06 average time 0.002399210059966208 iter num 50\n",
            "loss 6.27665638710157e-06 average time 0.0024347737700145446 iter num 100\n",
            "loss 6.182168667450258e-06 average time 0.002903180940011225 iter num 50\n",
            "loss 6.170291215465866e-06 average time 0.002738983620020008 iter num 100\n",
            "loss 6.127983780657697e-06 average time 0.002798860240027352 iter num 50\n",
            "loss 6.117891324317165e-06 average time 0.0027283304300271993 iter num 100\n",
            "loss 6.078013126528177e-06 average time 0.002777525120000064 iter num 50\n",
            "loss 6.068086141080489e-06 average time 0.0026233956499800114 iter num 100\n",
            "loss 6.028241206590507e-06 average time 0.002871358379989033 iter num 50\n",
            "loss 6.018222412528049e-06 average time 0.002639324939996186 iter num 100\n",
            "loss 5.977779378569787e-06 average time 0.0023811404199295793 iter num 50\n",
            "loss 5.9675712457963855e-06 average time 0.0024080853499663137 iter num 100\n",
            "loss 5.926317708308062e-06 average time 0.0023783894799908013 iter num 50\n",
            "loss 5.915876832074397e-06 average time 0.002386167639974701 iter num 100\n",
            "loss 5.873682593967586e-06 average time 0.0027688084399960645 iter num 50\n",
            "loss 5.862990097933854e-06 average time 0.00266937032999067 iter num 100\n",
            "loss 5.820286367994102e-06 average time 0.0026466068199715665 iter num 50\n",
            "loss 5.809081386817645e-06 average time 0.0025540530999842302 iter num 100\n",
            "loss 8.140036090642636e-06 average time 0.002937546140019549 iter num 50\n",
            "loss 5.865733774204901e-06 average time 0.002745424580039071 iter num 100\n",
            "loss 5.8006435617496e-06 average time 0.0026593528199464343 iter num 50\n",
            "loss 5.789919846173862e-06 average time 0.002632752359963888 iter num 100\n",
            "loss 5.749145184129498e-06 average time 0.00276547252004093 iter num 50\n",
            "loss 5.739200417281716e-06 average time 0.0027416891300117642 iter num 100\n",
            "loss 5.699490468737009e-06 average time 0.0026074669999889013 iter num 50\n",
            "loss 5.689542337891077e-06 average time 0.0025603369400050723 iter num 100\n",
            "loss 5.64952563287451e-06 average time 0.002528615079972951 iter num 50\n",
            "loss 5.639428584490183e-06 average time 0.0024397941599909245 iter num 100\n",
            "loss 5.598710758968307e-06 average time 0.0026119231400025455 iter num 50\n",
            "loss 5.5884083837959545e-06 average time 0.0025863396799832117 iter num 100\n",
            "loss 5.546853106275165e-06 average time 0.002500983720037766 iter num 50\n",
            "loss 5.5362969438559525e-06 average time 0.0025886286400282187 iter num 100\n",
            "loss 5.51912847003479e-06 average time 0.00299532354002622 iter num 50\n",
            "loss 5.502341619690691e-06 average time 0.002722363810016759 iter num 100\n",
            "loss 5.518940322478298e-06 average time 0.0025227527399692916 iter num 50\n",
            "loss 5.4773756864041384e-06 average time 0.0025893388599706668 iter num 100\n",
            "loss 5.434649262156751e-06 average time 0.002505970839947622 iter num 50\n",
            "loss 5.424336186570293e-06 average time 0.0025774159399725247 iter num 100\n",
            "loss 5.383126386380177e-06 average time 0.0026259991800088754 iter num 50\n",
            "loss 5.3727125547363815e-06 average time 0.002522106519995759 iter num 100\n",
            "loss 5.3849753770725975e-06 average time 0.0024496990400075446 iter num 50\n",
            "loss 5.324707207816407e-06 average time 0.0024555023200127837 iter num 100\n",
            "loss 8.490415117403876e-06 average time 0.0027502814000581566 iter num 50\n",
            "loss 5.43877276350697e-06 average time 0.0026915882600314946 iter num 100\n",
            "loss 5.361891446163652e-06 average time 0.002496046340038447 iter num 50\n",
            "loss 5.351524762023517e-06 average time 0.002507685490018048 iter num 100\n",
            "loss 5.31551235804353e-06 average time 0.0024094678400069826 iter num 50\n",
            "loss 5.307055342258708e-06 average time 0.0024168037600293246 iter num 100\n",
            "loss 5.273960331971817e-06 average time 0.002707410979983251 iter num 50\n",
            "loss 5.2657775192562104e-06 average time 0.0025585848099854047 iter num 100\n",
            "loss 5.233125344544764e-06 average time 0.0026856863600369252 iter num 50\n",
            "loss 5.224938529866155e-06 average time 0.0025394996400063975 iter num 100\n",
            "loss 5.192034771572842e-06 average time 0.0028089455199824444 iter num 50\n",
            "loss 5.183744112006369e-06 average time 0.0026750629499701973 iter num 100\n",
            "loss 5.150286225880638e-06 average time 0.0025411043999974937 iter num 50\n",
            "loss 5.141842504911686e-06 average time 0.002630616310007099 iter num 100\n",
            "loss 5.107700105215963e-06 average time 0.0025156540399893855 iter num 50\n",
            "loss 5.0990559295721205e-06 average time 0.002435660680007459 iter num 100\n",
            "loss 5.065389672328828e-06 average time 0.0026195260800068354 iter num 50\n",
            "loss 5.055316913780884e-06 average time 0.0026912901899913775 iter num 100\n",
            "loss 6.592290989811689e-06 average time 0.0024997658800202773 iter num 50\n",
            "loss 5.115279324492838e-06 average time 0.0025265933600257997 iter num 100\n",
            "loss 5.057174309770522e-06 average time 0.002401974419990438 iter num 50\n",
            "loss 5.047664774690776e-06 average time 0.0024066647899962847 iter num 100\n",
            "loss 5.013556489069331e-06 average time 0.002512848659989686 iter num 50\n",
            "loss 5.005295441772075e-06 average time 0.0024483590499903584 iter num 100\n",
            "loss 4.972448290660278e-06 average time 0.0025220943000022087 iter num 50\n",
            "loss 4.964251638579257e-06 average time 0.002554369080003198 iter num 100\n",
            "loss 4.9312999015629255e-06 average time 0.0024145137399955276 iter num 50\n",
            "loss 4.922994125208878e-06 average time 0.002719860430029257 iter num 100\n",
            "loss 4.889550007034461e-06 average time 0.002611674919999132 iter num 50\n",
            "loss 4.8810899721804004e-06 average time 0.0025401167699828876 iter num 100\n",
            "loss 4.846978813836116e-06 average time 0.0025406337199729025 iter num 50\n",
            "loss 4.838344036703797e-06 average time 0.002487675549996311 iter num 100\n",
            "loss 4.815565915625065e-06 average time 0.003075713159978477 iter num 50\n",
            "loss 4.795146686592015e-06 average time 0.0030207618399708735 iter num 100\n",
            "loss 1.2049984099966813e-05 average time 0.0026144295599806354 iter num 50\n",
            "loss 4.907594210012345e-06 average time 0.0027200568100033706 iter num 100\n",
            "loss 4.827565566690443e-06 average time 0.002475733539986322 iter num 50\n",
            "loss 4.8176339051898445e-06 average time 0.0024901749799983008 iter num 100\n",
            "loss 4.783627511535443e-06 average time 0.0024176576400077466 iter num 50\n",
            "loss 4.775712444972527e-06 average time 0.002507736759994259 iter num 100\n",
            "loss 4.744803698405081e-06 average time 0.0024106442000083918 iter num 50\n",
            "loss 4.737183434589848e-06 average time 0.0024895776099901922 iter num 100\n",
            "loss 4.706818843976072e-06 average time 0.00269949849999648 iter num 50\n",
            "loss 4.6992250787366455e-06 average time 0.002589736360018833 iter num 100\n",
            "loss 4.668685569649577e-06 average time 0.0025612382599956617 iter num 50\n",
            "loss 4.661002217766405e-06 average time 0.002608380500009844 iter num 100\n",
            "loss 4.630049155238977e-06 average time 0.002577035300018906 iter num 50\n",
            "loss 4.622233020445139e-06 average time 0.002482166260001577 iter num 100\n",
            "loss 4.590694780120323e-06 average time 0.002445945940016827 iter num 50\n",
            "loss 4.582725817231065e-06 average time 0.002496838359998037 iter num 100\n",
            "loss 4.647464175971246e-06 average time 0.002509150719997706 iter num 50\n",
            "loss 4.547658779460632e-06 average time 0.0025216323500217184 iter num 100\n",
            "loss 6.403819645207832e-06 average time 0.0025248517400086714 iter num 50\n",
            "loss 4.594756726887511e-06 average time 0.0025179129000116518 iter num 100\n",
            "loss 4.5479780524194345e-06 average time 0.002499701999986428 iter num 50\n",
            "loss 4.540168634208685e-06 average time 0.0024169581999922227 iter num 100\n",
            "loss 4.510552008654611e-06 average time 0.002583950999969602 iter num 50\n",
            "loss 4.503336001371616e-06 average time 0.0026009491399781835 iter num 100\n",
            "loss 4.474651026461937e-06 average time 0.0025721011600398926 iter num 50\n",
            "loss 4.467486721098666e-06 average time 0.002499054289992273 iter num 100\n",
            "loss 4.438655018537929e-06 average time 0.002620051820013032 iter num 50\n",
            "loss 4.431396823343537e-06 average time 0.00261245805001181 iter num 100\n",
            "loss 4.4021298872787105e-06 average time 0.0028321142000095278 iter num 50\n",
            "loss 4.394721878366237e-06 average time 0.002709798659993794 iter num 100\n",
            "loss 4.365413395171019e-06 average time 0.0026184031599768786 iter num 50\n",
            "loss 4.357342110710843e-06 average time 0.0025397361399791406 iter num 100\n",
            "loss 7.837868604035899e-06 average time 0.0025656248399991456 iter num 50\n",
            "loss 4.4274934900927555e-06 average time 0.0026345983800001704 iter num 100\n",
            "loss 4.370399673421462e-06 average time 0.002619996479998008 iter num 50\n",
            "loss 4.3621581710124365e-06 average time 0.0025967947399976763 iter num 100\n",
            "loss 4.332250637919843e-06 average time 0.002447209859992654 iter num 50\n",
            "loss 4.325107669066844e-06 average time 0.0025814254099987013 iter num 100\n",
            "loss 4.296938049307737e-06 average time 0.0027365018599630276 iter num 50\n",
            "loss 4.289938956016768e-06 average time 0.0025945640999816533 iter num 100\n",
            "loss 4.261924877037287e-06 average time 0.002361939500005974 iter num 50\n",
            "loss 4.254880120158582e-06 average time 0.0023735525499796496 iter num 100\n",
            "loss 4.226854743509131e-06 average time 0.0024716947599790727 iter num 50\n",
            "loss 4.219483566196155e-06 average time 0.0024459085699572824 iter num 100\n",
            "loss 4.444308160673125e-06 average time 0.002538908020023882 iter num 50\n",
            "loss 4.191095932652322e-06 average time 0.0024863025400190964 iter num 100\n",
            "loss 4.167776743721092e-06 average time 0.0024308778799695574 iter num 50\n",
            "loss 4.160950611984346e-06 average time 0.002489542859998437 iter num 100\n",
            "loss 5.257811321231731e-06 average time 0.0025009667599624665 iter num 50\n",
            "loss 4.1887231893554e-06 average time 0.0024767034199840053 iter num 100\n",
            "loss 4.152397207534535e-06 average time 0.0023776592999911372 iter num 50\n",
            "loss 4.144814037759243e-06 average time 0.002369337339964659 iter num 100\n",
            "loss 4.118877663043827e-06 average time 0.0024277409799833547 iter num 50\n",
            "loss 4.1125107500310745e-06 average time 0.002476426799989895 iter num 100\n",
            "loss 4.08705780400174e-06 average time 0.0024537697400501203 iter num 50\n",
            "loss 4.0806768373476896e-06 average time 0.0024201969400155575 iter num 100\n",
            "loss 4.054991257348568e-06 average time 0.00256712029996379 iter num 50\n",
            "loss 4.048512207646164e-06 average time 0.0026827685799935353 iter num 100\n",
            "loss 4.022864452570915e-06 average time 0.0028209326399974088 iter num 50\n",
            "loss 4.015772291900175e-06 average time 0.0026637975699850356 iter num 100\n",
            "loss 5.786190953669721e-06 average time 0.0025528761999885317 iter num 50\n",
            "loss 4.066331741686262e-06 average time 0.002484158669985845 iter num 100\n",
            "loss 4.020921808343326e-06 average time 0.0025560333800331136 iter num 50\n",
            "loss 4.013767130054951e-06 average time 0.002478187540032195 iter num 100\n",
            "loss 3.987465717732453e-06 average time 0.002445528660000491 iter num 50\n",
            "loss 3.981119530377449e-06 average time 0.0024113317199908123 iter num 100\n",
            "loss 3.955991493156663e-06 average time 0.0024164055999972333 iter num 50\n",
            "loss 3.949728195486636e-06 average time 0.002558996790003221 iter num 100\n",
            "loss 3.924612224016278e-06 average time 0.0026963197400255014 iter num 50\n",
            "loss 3.918295592198364e-06 average time 0.002600923060012974 iter num 100\n",
            "loss 3.8938671887769495e-06 average time 0.00262425307999365 iter num 50\n",
            "loss 3.886524529262159e-06 average time 0.0025839027600022746 iter num 100\n",
            "loss 3.9979878633270905e-06 average time 0.0026006051000058506 iter num 50\n",
            "loss 3.86332758590096e-06 average time 0.0027235750099953294 iter num 100\n",
            "loss 4.385149602137837e-06 average time 0.0027407154199772777 iter num 50\n",
            "loss 3.870411165160547e-06 average time 0.0026383543899873986 iter num 100\n",
            "loss 3.843027803247485e-06 average time 0.0026575385399792138 iter num 50\n",
            "loss 3.836280894676708e-06 average time 0.0024895139199816187 iter num 100\n",
            "loss 3.8135251205562436e-06 average time 0.0025044754199825547 iter num 50\n",
            "loss 3.8078535128777687e-06 average time 0.0024328250599819513 iter num 100\n",
            "loss 3.7851134991513736e-06 average time 0.002430978239999604 iter num 50\n",
            "loss 3.779383302719753e-06 average time 0.0026509202399984133 iter num 100\n",
            "loss 3.7596823928835166e-06 average time 0.0025836139199782336 iter num 50\n",
            "loss 3.750942553317187e-06 average time 0.0025004880200003754 iter num 100\n",
            "loss 5.135851878834435e-06 average time 0.0024558101600359807 iter num 50\n",
            "loss 3.8502532947650575e-06 average time 0.002468170139995891 iter num 100\n",
            "loss 3.7870773758599658e-06 average time 0.0026600817399958033 iter num 50\n",
            "loss 3.7797759371126813e-06 average time 0.0025580816799902093 iter num 100\n",
            "loss 3.754487659263342e-06 average time 0.002734944420008105 iter num 50\n",
            "loss 3.7486664009159987e-06 average time 0.002629688630026976 iter num 100\n",
            "loss 3.726094026953155e-06 average time 0.002360704380016614 iter num 50\n",
            "loss 3.720560504826118e-06 average time 0.002423472380014573 iter num 100\n",
            "loss 3.6986115331316898e-06 average time 0.0026473468200128992 iter num 50\n",
            "loss 3.6931333080225696e-06 average time 0.002530193650000001 iter num 100\n",
            "loss 3.671204949095873e-06 average time 0.0026684530199963773 iter num 50\n",
            "loss 3.665698559738494e-06 average time 0.002545573229976981 iter num 100\n",
            "loss 3.6435488488440807e-06 average time 0.0024317057599910186 iter num 50\n",
            "loss 3.637967738349448e-06 average time 0.002438317629989797 iter num 100\n",
            "loss 3.6154756079695093e-06 average time 0.0023778715200023725 iter num 50\n",
            "loss 3.609787019997389e-06 average time 0.0024009784699956073 iter num 100\n",
            "loss 3.6022246518019337e-06 average time 0.002323529059995053 iter num 50\n",
            "loss 3.5817935845632347e-06 average time 0.0023454865299800077 iter num 100\n",
            "loss 4.540419881203036e-06 average time 0.0024287878399991314 iter num 50\n",
            "loss 3.610534013703349e-06 average time 0.002395262080012799 iter num 100\n",
            "loss 3.5790189481370047e-06 average time 0.0024249650000092516 iter num 50\n",
            "loss 3.5734047682287047e-06 average time 0.002439242769996781 iter num 100\n",
            "loss 3.553207270613855e-06 average time 0.002594912820031823 iter num 50\n",
            "loss 3.548255647498775e-06 average time 0.002499063900013425 iter num 100\n",
            "loss 3.5285296533984867e-06 average time 0.0023543004200200814 iter num 50\n",
            "loss 3.5235826709518737e-06 average time 0.0024499807300207978 iter num 100\n",
            "loss 3.5036909507789675e-06 average time 0.0027422249199935324 iter num 50\n",
            "loss 3.4986712409956215e-06 average time 0.002626462159996663 iter num 100\n",
            "loss 3.478569937637782e-06 average time 0.002710434600021472 iter num 50\n",
            "loss 3.4733799824974634e-06 average time 0.0026652508300094267 iter num 100\n",
            "loss 4.692315434856873e-06 average time 0.002595463859997835 iter num 50\n",
            "loss 3.533031269394595e-06 average time 0.002559932340004707 iter num 100\n",
            "loss 3.4899613571720984e-06 average time 0.0026844535599957454 iter num 50\n",
            "loss 3.4826448071404274e-06 average time 0.0025965257999951063 iter num 100\n",
            "loss 3.461400865190765e-06 average time 0.0025505904399960853 iter num 50\n",
            "loss 3.4563628149678296e-06 average time 0.0027613390199985586 iter num 100\n",
            "loss 3.436554189685475e-06 average time 0.0025593704199764035 iter num 50\n",
            "loss 3.431636807816657e-06 average time 0.00263517122000394 iter num 100\n",
            "loss 3.412024128263127e-06 average time 0.002465300980002212 iter num 50\n",
            "loss 3.407112524383978e-06 average time 0.0025972313099964593 iter num 100\n",
            "loss 3.3873421405003353e-06 average time 0.0027855461000217473 iter num 50\n",
            "loss 3.382370771286392e-06 average time 0.002559943900014332 iter num 100\n",
            "loss 3.3622984861907884e-06 average time 0.002644149180005115 iter num 50\n",
            "loss 3.35723231988551e-06 average time 0.0025375593800026765 iter num 100\n",
            "loss 3.350878407566941e-06 average time 0.0025615998800549277 iter num 50\n",
            "loss 3.33246452066911e-06 average time 0.0024580675100241934 iter num 100\n",
            "loss 4.623198776200874e-06 average time 0.00247945337996498 iter num 50\n",
            "loss 3.4443349173502855e-06 average time 0.002480910400004177 iter num 100\n",
            "loss 3.3816905168478496e-06 average time 0.0025419767599851183 iter num 50\n",
            "loss 3.374816523951855e-06 average time 0.0024387593499750437 iter num 100\n",
            "loss 3.3518897220380324e-06 average time 0.002840731899932507 iter num 50\n",
            "loss 3.3467486978244024e-06 average time 0.0028616827899713825 iter num 100\n",
            "loss 3.3271382193767888e-06 average time 0.0024889507399802825 iter num 50\n",
            "loss 3.3223944298633783e-06 average time 0.0024433807399918805 iter num 100\n",
            "loss 3.3036697226152613e-06 average time 0.0028235107799628166 iter num 50\n",
            "loss 3.2990377225454146e-06 average time 0.0025974663499710006 iter num 100\n",
            "loss 3.2805408125280865e-06 average time 0.0028235479600152756 iter num 50\n",
            "loss 3.2759169668952594e-06 average time 0.002587512750019414 iter num 100\n",
            "loss 3.257616697994907e-06 average time 0.0023989079400143967 iter num 50\n",
            "loss 3.252715040385412e-06 average time 0.0023898423500077117 iter num 100\n",
            "loss 3.2742250916804957e-06 average time 0.002368078820009032 iter num 50\n",
            "loss 3.2343520149022456e-06 average time 0.0023805622600184505 iter num 100\n",
            "loss 3.2165941540345204e-06 average time 0.0023579496000638756 iter num 50\n",
            "loss 3.2121939998937073e-06 average time 0.002367284050001217 iter num 100\n",
            "loss 3.1944746881909256e-06 average time 0.0024257838999801606 iter num 50\n",
            "loss 3.189997535550223e-06 average time 0.002396360859975175 iter num 100\n",
            "loss 3.231190311295619e-06 average time 0.0023426126999947883 iter num 50\n",
            "loss 3.1683292565791986e-06 average time 0.0023286812300239036 iter num 100\n",
            "loss 5.586313027123068e-06 average time 0.0024843787200006773 iter num 50\n",
            "loss 3.2980693465360704e-06 average time 0.0025257737199808616 iter num 100\n",
            "loss 3.2268891717658543e-06 average time 0.002781436080013009 iter num 50\n",
            "loss 3.218561689551632e-06 average time 0.002660374869997213 iter num 100\n",
            "loss 3.195996678406518e-06 average time 0.0028045966600166138 iter num 50\n",
            "loss 3.191047868522799e-06 average time 0.002588041920012074 iter num 100\n",
            "loss 3.172299327054318e-06 average time 0.0025212434400418717 iter num 50\n",
            "loss 3.1678081172223776e-06 average time 0.0025322750900249958 iter num 100\n",
            "loss 3.150137507069273e-06 average time 0.0026190962000328 iter num 50\n",
            "loss 3.1457808904167105e-06 average time 0.002589572610036157 iter num 100\n",
            "loss 3.128430162121938e-06 average time 0.0026290418799999314 iter num 50\n",
            "loss 3.1241052057082525e-06 average time 0.002507609809990754 iter num 100\n",
            "loss 3.1067718668730825e-06 average time 0.002443609360007031 iter num 50\n",
            "loss 3.102425140878956e-06 average time 0.002390013610001915 iter num 100\n",
            "loss 3.0849467701783906e-06 average time 0.00265011845999652 iter num 50\n",
            "loss 3.080544313712482e-06 average time 0.0025837668700069115 iter num 100\n",
            "loss 3.0701594199816275e-06 average time 0.0024896095599706313 iter num 50\n",
            "loss 3.0589852509474474e-06 average time 0.002475753719986642 iter num 100\n",
            "loss 3.0910690796271382e-06 average time 0.0025267924799936736 iter num 50\n",
            "loss 3.040539953216596e-06 average time 0.0025326272099982818 iter num 100\n",
            "loss 3.0884475875109687e-06 average time 0.0025148880799588367 iter num 50\n",
            "loss 3.0234513765083186e-06 average time 0.0025365921599859575 iter num 100\n",
            "loss 4.041472142441212e-06 average time 0.002535562780003602 iter num 50\n",
            "loss 3.0402550613587242e-06 average time 0.002502581070011729 iter num 100\n",
            "loss 3.0183253248923563e-06 average time 0.00233549653998125 iter num 50\n",
            "loss 3.0129699069089e-06 average time 0.002486746339995989 iter num 100\n",
            "loss 2.995705476710921e-06 average time 0.002300969559992154 iter num 50\n",
            "loss 2.991472449264738e-06 average time 0.002357901229979689 iter num 100\n",
            "loss 2.9745463275074507e-06 average time 0.00237551368003551 iter num 50\n",
            "loss 2.970316449873355e-06 average time 0.0024426995000203532 iter num 100\n",
            "loss 2.9532756723616983e-06 average time 0.0024658814200302004 iter num 50\n",
            "loss 2.94898693673476e-06 average time 0.0024323345700213396 iter num 100\n",
            "loss 2.965741204582607e-06 average time 0.0024383918199600884 iter num 50\n",
            "loss 2.928010930481576e-06 average time 0.0024181443199813657 iter num 100\n",
            "loss 4.878362529418805e-06 average time 0.002464053059993603 iter num 50\n",
            "loss 3.0212497016797526e-06 average time 0.002466880790007053 iter num 100\n",
            "loss 2.970547344264243e-06 average time 0.002723433800010753 iter num 50\n",
            "loss 2.964497449915852e-06 average time 0.0025780354599964994 iter num 100\n",
            "loss 2.9475135325077917e-06 average time 0.002494199259990637 iter num 50\n",
            "loss 2.9437363433816685e-06 average time 0.0024321917299948838 iter num 100\n",
            "loss 2.9293670494794313e-06 average time 0.002492881279995345 iter num 50\n",
            "loss 2.925895877186829e-06 average time 0.002485412959972564 iter num 100\n",
            "loss 2.912257686223881e-06 average time 0.002409771919992636 iter num 50\n",
            "loss 2.908883870228769e-06 average time 0.00248612071000025 iter num 100\n",
            "loss 2.895407999819705e-06 average time 0.0025161885199850077 iter num 50\n",
            "loss 2.8920416915631894e-06 average time 0.0024076020499751395 iter num 100\n",
            "loss 2.8785088371345146e-06 average time 0.0025991061000058837 iter num 50\n",
            "loss 2.8751098325650835e-06 average time 0.002582788510003411 iter num 100\n",
            "loss 2.8614032221130415e-06 average time 0.0026589274800153362 iter num 50\n",
            "loss 2.8579433013066535e-06 average time 0.002522141820008983 iter num 100\n",
            "loss 2.84398918566897e-06 average time 0.002473971379959039 iter num 50\n",
            "loss 2.8404604617415708e-06 average time 0.0025122991299758725 iter num 100\n",
            "loss 2.828109451262667e-06 average time 0.0025882705399817496 iter num 50\n",
            "loss 2.822631014826003e-06 average time 0.002599940459977006 iter num 100\n",
            "loss 7.656227949648638e-06 average time 0.002385001640022892 iter num 50\n",
            "loss 2.9283039853800403e-06 average time 0.002451301610021801 iter num 100\n",
            "loss 2.8699059091605496e-06 average time 0.0027294417400207747 iter num 50\n",
            "loss 2.864165329283946e-06 average time 0.0025437246500177933 iter num 100\n",
            "loss 2.845975826634151e-06 average time 0.002493860820031841 iter num 50\n",
            "loss 2.841997350710946e-06 average time 0.002454659840036584 iter num 100\n",
            "loss 2.8269425032993564e-06 average time 0.0023606971200206316 iter num 50\n",
            "loss 2.823343293217832e-06 average time 0.0023939854000263948 iter num 100\n",
            "loss 2.8091963969306997e-06 average time 0.0024824905800323906 iter num 50\n",
            "loss 2.80571067824383e-06 average time 0.0024616668800263143 iter num 100\n",
            "loss 2.7918377951659076e-06 average time 0.0023994729400055805 iter num 50\n",
            "loss 2.788372641697921e-06 average time 0.002436267380012396 iter num 100\n",
            "loss 2.7745047481894518e-06 average time 0.0024341848799849684 iter num 50\n",
            "loss 2.7710325894621104e-06 average time 0.0025407411899732325 iter num 100\n",
            "loss 2.7570387945564794e-06 average time 0.002832745339956091 iter num 50\n",
            "loss 2.753527893932848e-06 average time 0.0027484161599659273 iter num 100\n",
            "loss 2.7393411018469975e-06 average time 0.0025750631199844064 iter num 50\n",
            "loss 2.7357611420608966e-06 average time 0.002482168549945527 iter num 100\n",
            "loss 2.721315636866759e-06 average time 0.0023295741200308837 iter num 50\n",
            "loss 2.71767885711802e-06 average time 0.002314540079987637 iter num 100\n",
            "loss 2.7443902147947365e-06 average time 0.0024343644000236965 iter num 50\n",
            "loss 2.6998184424594554e-06 average time 0.0024320157100282814 iter num 100\n",
            "loss 5.441633159020978e-06 average time 0.002469531900042057 iter num 50\n",
            "loss 2.8111494769146744e-06 average time 0.0024224359200525213 iter num 100\n",
            "loss 2.753778806699508e-06 average time 0.0024805405599909137 iter num 50\n",
            "loss 2.7475349372757015e-06 average time 0.0026106258599793365 iter num 100\n",
            "loss 2.7291265975104903e-06 average time 0.002700159040014114 iter num 50\n",
            "loss 2.72509165403391e-06 average time 0.002578079370018713 iter num 100\n",
            "loss 2.709811230770873e-06 average time 0.002478896579959837 iter num 50\n",
            "loss 2.7061637613020466e-06 average time 0.0027311150300056396 iter num 100\n",
            "loss 2.6917971521285246e-06 average time 0.0027993925599639623 iter num 50\n",
            "loss 2.6882698880731203e-06 average time 0.002611576809977123 iter num 100\n",
            "loss 2.674203220519903e-06 average time 0.002503374300003998 iter num 50\n",
            "loss 2.6707138908429705e-06 average time 0.0025351947999843107 iter num 100\n",
            "loss 2.6567045685566353e-06 average time 0.0023729808799907914 iter num 50\n",
            "loss 2.653204270314066e-06 average time 0.0023624404800148113 iter num 100\n",
            "loss 2.639114913139368e-06 average time 0.002502749940040303 iter num 50\n",
            "loss 2.6355717902823688e-06 average time 0.0024610742200184177 iter num 100\n",
            "loss 2.628510790456658e-06 average time 0.0024304816799849503 iter num 50\n",
            "loss 2.6187406737973685e-06 average time 0.002384171679996143 iter num 100\n",
            "loss 2.7207866480017298e-06 average time 0.002383829520031213 iter num 50\n",
            "loss 2.610093681378666e-06 average time 0.0024376276800194318 iter num 100\n",
            "loss 2.596501332180781e-06 average time 0.0026253132800047752 iter num 50\n",
            "loss 2.5925956469036237e-06 average time 0.0025823666099813636 iter num 100\n",
            "loss 2.5790246729030383e-06 average time 0.002467072039989944 iter num 50\n",
            "loss 2.57543857697058e-06 average time 0.002496588239991979 iter num 100\n",
            "loss 4.1593046073985e-06 average time 0.0026019139400068527 iter num 50\n",
            "loss 2.61827500370179e-06 average time 0.00260685810000723 iter num 100\n",
            "loss 2.5892694629531127e-06 average time 0.0025570595799763395 iter num 50\n",
            "loss 2.584621808307803e-06 average time 0.0025589748599986707 iter num 100\n",
            "loss 2.570110277136297e-06 average time 0.002378095160029261 iter num 50\n",
            "loss 2.566691621067709e-06 average time 0.0023971180500166156 iter num 100\n",
            "loss 2.5532449956147107e-06 average time 0.0025752989600096045 iter num 50\n",
            "loss 2.5499392492380716e-06 average time 0.0024808897399907436 iter num 100\n",
            "loss 2.536691637401359e-06 average time 0.002519332379970365 iter num 50\n",
            "loss 2.53338850851872e-06 average time 0.0024506161799899926 iter num 100\n",
            "loss 2.5200814661713998e-06 average time 0.0023995892799302965 iter num 50\n",
            "loss 2.5167399619988217e-06 average time 0.0023641659999702825 iter num 100\n",
            "loss 2.503263968332308e-06 average time 0.0024471330800315627 iter num 50\n",
            "loss 2.49986327548028e-06 average time 0.002591682020033659 iter num 100\n",
            "loss 3.103540541851899e-06 average time 0.002477087440001924 iter num 50\n",
            "loss 2.498620707147626e-06 average time 0.0024393718399915087 iter num 100\n",
            "loss 2.4872444459282933e-06 average time 0.002794295920029981 iter num 50\n",
            "loss 2.4814371437184455e-06 average time 0.0026478484699919134 iter num 100\n",
            "loss 2.4854711241620633e-06 average time 0.0025629318399842307 iter num 50\n",
            "loss 2.4683241214949894e-06 average time 0.002477387049993922 iter num 100\n",
            "loss 2.874713503678682e-06 average time 0.002343638900019869 iter num 50\n",
            "loss 2.48901091486594e-06 average time 0.002434363110000959 iter num 100\n",
            "loss 2.469432053844306e-06 average time 0.0027990318599859167 iter num 50\n",
            "loss 2.465966096504755e-06 average time 0.0027272825100226328 iter num 100\n",
            "loss 2.4528101954540337e-06 average time 0.002559964579977532 iter num 50\n",
            "loss 2.4496313239756333e-06 average time 0.0024762486399959017 iter num 100\n",
            "loss 2.436985067417103e-06 average time 0.00249332407995098 iter num 50\n",
            "loss 2.4338316730094642e-06 average time 0.0025015923599630697 iter num 100\n",
            "loss 2.4212110098656145e-06 average time 0.0024389163200157784 iter num 50\n",
            "loss 2.418036782049385e-06 average time 0.0024315516500064403 iter num 100\n",
            "loss 2.4063842451680817e-06 average time 0.0023250444600216723 iter num 50\n",
            "loss 2.402115389527994e-06 average time 0.0023585548000255585 iter num 100\n",
            "loss 3.616809145561173e-06 average time 0.0024700571000084892 iter num 50\n",
            "loss 2.421953140098465e-06 average time 0.0024504108100290977 iter num 100\n",
            "loss 2.40389194802155e-06 average time 0.002416656339992187 iter num 50\n",
            "loss 2.400045875767421e-06 average time 0.002400361379977767 iter num 100\n",
            "loss 2.3883211632720215e-06 average time 0.002736552219967052 iter num 50\n",
            "loss 2.385493231378214e-06 average time 0.0025562653399902047 iter num 100\n",
            "loss 2.374244859028165e-06 average time 0.0023882244200012794 iter num 50\n",
            "loss 2.371450399545166e-06 average time 0.0025870704100088913 iter num 100\n",
            "loss 2.3602979500284523e-06 average time 0.0024208254199766088 iter num 50\n",
            "loss 2.3573930770660252e-06 average time 0.002376107909963139 iter num 100\n",
            "loss 5.471105167087733e-06 average time 0.0024800918399796503 iter num 50\n",
            "loss 2.391834813854644e-06 average time 0.0024968306399887298 iter num 100\n",
            "loss 2.3680292110573185e-06 average time 0.0025958618599815964 iter num 50\n",
            "loss 2.363385131855221e-06 average time 0.002480999789963789 iter num 100\n",
            "loss 2.3512981189161356e-06 average time 0.0024623452599917073 iter num 50\n",
            "loss 2.348419274632823e-06 average time 0.00240905928999382 iter num 100\n",
            "loss 2.3370860463358948e-06 average time 0.002657982999962769 iter num 50\n",
            "loss 2.334285615914716e-06 average time 0.002565083100012089 iter num 100\n",
            "loss 2.3230912788833115e-06 average time 0.00249646194001798 iter num 50\n",
            "loss 2.3202870431238495e-06 average time 0.002472063150007671 iter num 100\n",
            "loss 2.309016865542723e-06 average time 0.0023636852799972987 iter num 50\n",
            "loss 2.3061850285527974e-06 average time 0.0023583773499831293 iter num 100\n",
            "loss 2.294842090569861e-06 average time 0.0023620314599702395 iter num 50\n",
            "loss 2.2918815898193524e-06 average time 0.002338913559992761 iter num 100\n",
            "loss 5.485804229311609e-06 average time 0.0024127956999654998 iter num 50\n",
            "loss 2.330482782875875e-06 average time 0.0024330471799657973 iter num 100\n",
            "loss 2.3039320231793845e-06 average time 0.0025484334399607177 iter num 50\n",
            "loss 2.3002571801724202e-06 average time 0.002494497659990884 iter num 100\n",
            "loss 2.2877404690862244e-06 average time 0.0028441276399826165 iter num 50\n",
            "loss 2.2847805493977347e-06 average time 0.002672675719986728 iter num 100\n",
            "loss 2.273137707198881e-06 average time 0.0028374112600340596 iter num 50\n",
            "loss 2.270270098434432e-06 average time 0.002640671900012421 iter num 100\n",
            "loss 2.2588099231206967e-06 average time 0.0026379215200176987 iter num 50\n",
            "loss 2.2559413447579953e-06 average time 0.0028007131900312744 iter num 100\n",
            "loss 2.2444442977499063e-06 average time 0.002900582940046661 iter num 50\n",
            "loss 2.2415475223778435e-06 average time 0.002635896400006459 iter num 100\n",
            "loss 2.2893562541360064e-06 average time 0.00274609237995719 iter num 50\n",
            "loss 2.2277833177221282e-06 average time 0.0027296592299944676 iter num 100\n",
            "loss 2.803279407300525e-06 average time 0.0024928454999917447 iter num 50\n",
            "loss 2.2335884232315116e-06 average time 0.002499639239977114 iter num 100\n",
            "loss 2.220291295664059e-06 average time 0.002460833739978625 iter num 50\n",
            "loss 2.2175048206482797e-06 average time 0.0025634879400013235 iter num 100\n",
            "loss 2.2069401657232064e-06 average time 0.0023600835199977157 iter num 50\n",
            "loss 2.204312273370638e-06 average time 0.0023761568800000532 iter num 100\n",
            "loss 2.2040410303308875e-06 average time 0.0025094185600573835 iter num 50\n",
            "loss 2.1913506990109537e-06 average time 0.0024312493200159225 iter num 100\n",
            "loss 3.861761643839885e-06 average time 0.0025035576799746196 iter num 50\n",
            "loss 2.2875296537810732e-06 average time 0.0024218080399850805 iter num 100\n",
            "loss 2.238016012676215e-06 average time 0.002397170959966388 iter num 50\n",
            "loss 2.2318235089305086e-06 average time 0.002422438299990972 iter num 100\n",
            "loss 2.2171528077426147e-06 average time 0.0026360422799916704 iter num 50\n",
            "loss 2.2139881747905833e-06 average time 0.002552380339998308 iter num 100\n",
            "loss 2.202172636425268e-06 average time 0.0024648164399695815 iter num 50\n",
            "loss 2.1993706391012327e-06 average time 0.002440001949998987 iter num 100\n",
            "loss 2.188471584482702e-06 average time 0.0025552365399562405 iter num 50\n",
            "loss 2.1858097033696876e-06 average time 0.002498591439989468 iter num 100\n",
            "loss 2.1752742631465618e-06 average time 0.0024998637799853894 iter num 50\n",
            "loss 2.1726608227966937e-06 average time 0.0026570254500120427 iter num 100\n",
            "loss 2.1622465870278152e-06 average time 0.00243309948001297 iter num 50\n",
            "loss 2.1596470877753785e-06 average time 0.0023974401600116834 iter num 100\n",
            "loss 2.149229169718206e-06 average time 0.002469157220057241 iter num 50\n",
            "loss 2.146622225346347e-06 average time 0.002717757220025305 iter num 100\n",
            "loss 2.1361309970431164e-06 average time 0.0027948598399962065 iter num 50\n",
            "loss 2.1334847484826937e-06 average time 0.0026923801499833646 iter num 100\n",
            "loss 2.122981261789163e-06 average time 0.0026453855399995517 iter num 50\n",
            "loss 2.120174770603738e-06 average time 0.0027399288999913553 iter num 100\n",
            "loss 2.178222233595249e-06 average time 0.0026683746000253448 iter num 50\n",
            "loss 2.113374202873276e-06 average time 0.0027866043900348814 iter num 100\n",
            "loss 2.3193975910306855e-06 average time 0.002544591660016522 iter num 50\n",
            "loss 2.110736411723355e-06 average time 0.002443562450016543 iter num 100\n",
            "loss 2.0995772087557805e-06 average time 0.0024786411800596397 iter num 50\n",
            "loss 2.0970641763342043e-06 average time 0.0024299169600544702 iter num 100\n",
            "loss 2.088226380564323e-06 average time 0.00233451763994708 iter num 50\n",
            "loss 2.0846464629890114e-06 average time 0.0023086308599704355 iter num 100\n",
            "loss 3.0742654712223294e-06 average time 0.0024111140799413988 iter num 50\n",
            "loss 2.130602141360391e-06 average time 0.002428520069984188 iter num 100\n",
            "loss 2.1047614532348577e-06 average time 0.0024620048599717846 iter num 50\n",
            "loss 2.100709330438628e-06 average time 0.0026573709399599467 iter num 100\n",
            "loss 2.089012980954706e-06 average time 0.0026971835200129135 iter num 50\n",
            "loss 2.0863123851801895e-06 average time 0.0025282367600311774 iter num 100\n",
            "loss 2.0758731155684327e-06 average time 0.0024004699200486355 iter num 50\n",
            "loss 2.0733264470441783e-06 average time 0.002550663050028561 iter num 100\n",
            "loss 2.063245171658876e-06 average time 0.0026727406600093673 iter num 50\n",
            "loss 2.0607432619465946e-06 average time 0.0026002368600165937 iter num 100\n",
            "loss 2.05074026378356e-06 average time 0.002492274499973064 iter num 50\n",
            "loss 2.0482363919279882e-06 average time 0.002483790299984321 iter num 100\n",
            "loss 2.0381930673788187e-06 average time 0.002511833739990834 iter num 50\n",
            "loss 2.035670148319858e-06 average time 0.002645638889989641 iter num 100\n",
            "loss 2.0390534752728005e-06 average time 0.0025533832399742096 iter num 50\n",
            "loss 2.0233733662421757e-06 average time 0.0025603644399870974 iter num 100\n",
            "loss 2.877542905471639e-06 average time 0.003048316660015189 iter num 50\n",
            "loss 2.0468632851598363e-06 average time 0.002830194879998089 iter num 100\n",
            "loss 2.0301941531253244e-06 average time 0.002388033240004006 iter num 50\n",
            "loss 2.027087879550286e-06 average time 0.002365344019995064 iter num 100\n",
            "loss 2.017618623375941e-06 average time 0.0024014280600204076 iter num 50\n",
            "loss 2.0153661957050436e-06 average time 0.002394751580004595 iter num 100\n",
            "loss 2.0065025444326086e-06 average time 0.002470948180016421 iter num 50\n",
            "loss 2.0043110260284167e-06 average time 0.0024173450900070747 iter num 100\n",
            "loss 1.995546966950429e-06 average time 0.0024555785000029574 iter num 50\n",
            "loss 1.993351817875687e-06 average time 0.002478276280016871 iter num 100\n",
            "loss 1.99127870044485e-06 average time 0.0025246689199684624 iter num 50\n",
            "loss 1.9823756266085124e-06 average time 0.002513078949982628 iter num 100\n",
            "loss 3.7792795797016445e-06 average time 0.002400699639965751 iter num 50\n",
            "loss 2.0725858082932505e-06 average time 0.0025582263899696046 iter num 100\n",
            "loss 2.0264752513538642e-06 average time 0.0026426723999793465 iter num 50\n",
            "loss 2.021254741538686e-06 average time 0.002592512979995263 iter num 100\n",
            "loss 2.0082226178052927e-06 average time 0.00269090586002676 iter num 50\n",
            "loss 2.0054264574735033e-06 average time 0.0026158954600077776 iter num 100\n",
            "loss 1.995009813489595e-06 average time 0.0025312260600003356 iter num 50\n",
            "loss 1.992558396257916e-06 average time 0.002610055649993228 iter num 100\n",
            "loss 1.9830144758366238e-06 average time 0.0026591495200591455 iter num 50\n",
            "loss 1.980693563120631e-06 average time 0.0026468146100523883 iter num 100\n",
            "loss 1.971529914463193e-06 average time 0.0024027696200300853 iter num 50\n",
            "loss 1.9692721312582207e-06 average time 0.002529725100030191 iter num 100\n",
            "loss 1.960258764535094e-06 average time 0.0025232698400213848 iter num 50\n",
            "loss 1.958013429323938e-06 average time 0.0024902164800096214 iter num 100\n",
            "loss 1.9490390132084348e-06 average time 0.0023613109400412212 iter num 50\n",
            "loss 1.9467928441170862e-06 average time 0.0024531272600324884 iter num 100\n",
            "loss 1.9377724296161374e-06 average time 0.0023677079000208323 iter num 50\n",
            "loss 1.9355142231571013e-06 average time 0.002387848830016992 iter num 100\n",
            "loss 1.92639356722584e-06 average time 0.0024566734000563886 iter num 50\n",
            "loss 1.9241017123879934e-06 average time 0.002474465380055335 iter num 100\n",
            "loss 1.9159790201974104e-06 average time 0.0024299836799764308 iter num 50\n",
            "loss 1.912639170095579e-06 average time 0.0026987558299924786 iter num 100\n",
            "loss 4.356941864754453e-06 average time 0.002440592920011113 iter num 50\n",
            "loss 1.9474585887750204e-06 average time 0.0024408446399911553 iter num 100\n",
            "loss 1.927317591367944e-06 average time 0.0023550588200123455 iter num 50\n",
            "loss 1.9244566148990885e-06 average time 0.002414447260034649 iter num 100\n",
            "loss 1.915416148453448e-06 average time 0.002622804399979941 iter num 50\n",
            "loss 1.9133243082876245e-06 average time 0.0025080347299899584 iter num 100\n",
            "loss 1.9051861595022012e-06 average time 0.003019089579966021 iter num 50\n",
            "loss 1.9031980990071465e-06 average time 0.0028515481999875194 iter num 100\n",
            "loss 1.895290566652387e-06 average time 0.0023768542400193836 iter num 50\n",
            "loss 1.8933226746026807e-06 average time 0.00238412060004066 iter num 100\n",
            "loss 1.8854310113211739e-06 average time 0.0023390722799558716 iter num 50\n",
            "loss 1.8834507284770933e-06 average time 0.0023569670299821155 iter num 100\n",
            "loss 1.8755659123721747e-06 average time 0.0023719286599953194 iter num 50\n",
            "loss 1.8734831722181274e-06 average time 0.0024051993900093293 iter num 100\n",
            "loss 2.4831242751758708e-06 average time 0.002547763520014996 iter num 50\n",
            "loss 1.9051611851397783e-06 average time 0.0024647419499933677 iter num 100\n",
            "loss 1.886093904243068e-06 average time 0.0024335015200267662 iter num 50\n",
            "loss 1.883446139800639e-06 average time 0.0024939619499946275 iter num 100\n",
            "loss 1.8741525233017763e-06 average time 0.0027881503999833512 iter num 50\n",
            "loss 1.8719791407203411e-06 average time 0.0027006138600199846 iter num 100\n",
            "loss 1.8634936350361905e-06 average time 0.002775128200037216 iter num 50\n",
            "loss 1.8614157476113034e-06 average time 0.0027282388200137574 iter num 100\n",
            "loss 1.8531463594977333e-06 average time 0.002480801540013999 iter num 50\n",
            "loss 1.8510941948017068e-06 average time 0.0025022367300243788 iter num 100\n",
            "loss 1.8428467368857047e-06 average time 0.0023999653799910447 iter num 50\n",
            "loss 1.8407819508770139e-06 average time 0.0024422997799638324 iter num 100\n",
            "loss 1.8324770035079751e-06 average time 0.0024600179600474804 iter num 50\n",
            "loss 1.8303899311258406e-06 average time 0.0025202540500322357 iter num 100\n",
            "loss 1.822161937103416e-06 average time 0.0025507295800161955 iter num 50\n",
            "loss 1.8198912795504162e-06 average time 0.002598683409996738 iter num 100\n",
            "loss 4.125503949585644e-06 average time 0.002406455679965802 iter num 50\n",
            "loss 1.9172794234352473e-06 average time 0.0023912660699852493 iter num 100\n",
            "loss 1.8686429907162422e-06 average time 0.0026226859600137686 iter num 50\n",
            "loss 1.8641258355892328e-06 average time 0.0025007419500207104 iter num 100\n",
            "loss 1.8506174363778939e-06 average time 0.0024774006600273425 iter num 50\n",
            "loss 1.8477800301328185e-06 average time 0.0024633455200228126 iter num 100\n",
            "loss 1.8373556222700266e-06 average time 0.0025753681999776744 iter num 50\n",
            "loss 1.8349280416336827e-06 average time 0.0025395546699928675 iter num 100\n",
            "loss 1.8255806539044198e-06 average time 0.0026503904800028975 iter num 50\n",
            "loss 1.823317738958092e-06 average time 0.0027156644600199796 iter num 100\n",
            "loss 1.814456526340555e-06 average time 0.002523216500030685 iter num 50\n",
            "loss 1.8122871518897533e-06 average time 0.0025100575600208687 iter num 100\n",
            "loss 1.803659225113029e-06 average time 0.002656681980024587 iter num 50\n",
            "loss 1.8015222284033198e-06 average time 0.0027848918800100365 iter num 100\n",
            "loss 1.8033653102797999e-06 average time 0.002757904520003649 iter num 50\n",
            "loss 1.7911131576886224e-06 average time 0.0026168886799950997 iter num 100\n",
            "loss 1.814610136286375e-06 average time 0.002495659199994407 iter num 50\n",
            "loss 1.7861669739305871e-06 average time 0.0024732824099919525 iter num 100\n",
            "loss 1.7779994730925695e-06 average time 0.0026041734800037377 iter num 50\n",
            "loss 1.7758689523711345e-06 average time 0.002502834879992406 iter num 100\n",
            "loss 1.768001735239292e-06 average time 0.002457757779984604 iter num 50\n",
            "loss 1.7660408117691356e-06 average time 0.0024278207500083226 iter num 100\n",
            "loss 1.7584333870475292e-06 average time 0.002714027880001595 iter num 50\n",
            "loss 1.7561635845324244e-06 average time 0.002590281950001554 iter num 100\n",
            "loss 3.2441212809783024e-06 average time 0.0028635209199910604 iter num 50\n",
            "loss 1.7921513590915071e-06 average time 0.002734276729984231 iter num 100\n",
            "loss 1.7724756688232601e-06 average time 0.002455625879983927 iter num 50\n",
            "loss 1.7689617438752471e-06 average time 0.0025277721099928384 iter num 100\n",
            "loss 1.7599946820594393e-06 average time 0.0024968419799824915 iter num 50\n",
            "loss 1.75791527922074e-06 average time 0.0025038022399985495 iter num 100\n",
            "loss 1.7498737920792916e-06 average time 0.002343902960010382 iter num 50\n",
            "loss 1.7479096054794442e-06 average time 0.0024414934000151333 iter num 100\n",
            "loss 1.7401241026646833e-06 average time 0.0024382761000651954 iter num 50\n",
            "loss 1.7381891943431791e-06 average time 0.002449198140006956 iter num 100\n",
            "loss 1.7304594684648893e-06 average time 0.0024524040799678913 iter num 50\n",
            "loss 1.7285329241378928e-06 average time 0.002445573819973106 iter num 100\n",
            "loss 1.7207778110192552e-06 average time 0.0024599796399888876 iter num 50\n",
            "loss 1.7188178200005608e-06 average time 0.0024733011800026363 iter num 100\n",
            "loss 1.7216954449296296e-06 average time 0.002470123179991788 iter num 50\n",
            "loss 1.7091236135648684e-06 average time 0.00245368320999205 iter num 100\n",
            "loss 3.1498809541588223e-06 average time 0.0025336088400035807 iter num 50\n",
            "loss 1.7281623947934837e-06 average time 0.0024473393899916117 iter num 100\n",
            "loss 1.7149337521565532e-06 average time 0.0027416999200340795 iter num 50\n",
            "loss 1.7126707287264852e-06 average time 0.002689262470007634 iter num 100\n",
            "loss 1.7048506603838789e-06 average time 0.0027803596800094967 iter num 50\n",
            "loss 1.7030110754707122e-06 average time 0.0027176077600097413 iter num 100\n",
            "loss 1.6957778266382774e-06 average time 0.0029239758399944547 iter num 50\n",
            "loss 1.6939962858019797e-06 average time 0.0028000152599952344 iter num 100\n",
            "loss 1.6874236334554032e-06 average time 0.002458656700009669 iter num 50\n",
            "loss 1.6851027063855972e-06 average time 0.0024246477800079448 iter num 100\n",
            "loss 1.7850255450210025e-06 average time 0.0023837271399679595 iter num 50\n",
            "loss 1.7335287753236258e-06 average time 0.0025003753199780475 iter num 100\n",
            "loss 1.707376378660871e-06 average time 0.0028701355399880415 iter num 50\n",
            "loss 1.7044361582929557e-06 average time 0.002631581799992091 iter num 100\n",
            "loss 1.695283798288187e-06 average time 0.002575123819979126 iter num 50\n",
            "loss 1.693225477786261e-06 average time 0.002673526190001212 iter num 100\n",
            "loss 1.6853680910964957e-06 average time 0.003015103539974007 iter num 50\n",
            "loss 1.6834805241174294e-06 average time 0.0027446888100121215 iter num 100\n",
            "loss 1.6760652274856575e-06 average time 0.002537325020020944 iter num 50\n",
            "loss 1.6742444318578103e-06 average time 0.0024491718400258835 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3be0cc-7ec7-4b6f-f8b8-dc8109dcbf5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_oldmethod_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b149d652-e0ea-4172-b2b1-28f19ac2e3d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da66bc57-f197-4c01-dc90-53562ee38c6f"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_oldmethod_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423fa1ab-c1a9-4e12-938b-cb7a18f0f113"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=7, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=256, bias=True)\n",
            "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc5): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc6): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17cb6fb-5435-4b0d-b90f-8ab49592ff1a"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-4, amsgrad=True) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 200) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    # print(x.shape)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "    y_pred = model(x.float())\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    # def compute_deltas(x):\n",
        "    #   inputs = x.float()\n",
        "    #   inputs.requires_grad = True\n",
        "    #   first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "    #   return first_order_gradient[0][[3]]  # Now index 3 is stock price, not 2\n",
        "\n",
        "    # deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    # y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # # print(y_pred)\n",
        "    # # print(y_pred.shape)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 1000)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_oldmethod_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 6.554336670104425e-07 average time 0.003143744149997474 iter num 100\n",
            "loss 6.551586608850832e-07 average time 0.003082120500009751 iter num 200\n",
            "loss 6.550032111198911e-07 average time 0.0031057335400419108 iter num 100\n",
            "loss 6.549777453198205e-07 average time 0.0030874962500274704 iter num 200\n",
            "loss 6.548256633734097e-07 average time 0.0030462571299540286 iter num 100\n",
            "loss 6.548009834864573e-07 average time 0.003042743699984385 iter num 200\n",
            "loss 6.546499768307583e-07 average time 0.003021619589972033 iter num 100\n",
            "loss 6.546232043715435e-07 average time 0.0029962531599858266 iter num 200\n",
            "loss 6.544712722431525e-07 average time 0.0030123791599817196 iter num 100\n",
            "loss 6.544420666615075e-07 average time 0.002997378705003939 iter num 200\n",
            "loss 6.542893204691303e-07 average time 0.003056841440002245 iter num 100\n",
            "loss 6.542620031048353e-07 average time 0.003067551004980942 iter num 200\n",
            "loss 6.541123415451903e-07 average time 0.0030918573300095886 iter num 100\n",
            "loss 6.540819757940188e-07 average time 0.003046199125005842 iter num 200\n",
            "loss 6.53926978717945e-07 average time 0.0030793578099792287 iter num 100\n",
            "loss 6.538972162559855e-07 average time 0.0030349352399775854 iter num 200\n",
            "loss 6.537468385656112e-07 average time 0.003073441029978312 iter num 100\n",
            "loss 6.537182455861595e-07 average time 0.0030825858649836846 iter num 200\n",
            "loss 6.535625474754277e-07 average time 0.0030577804500171624 iter num 100\n",
            "loss 6.5353765073561e-07 average time 0.003024757725006566 iter num 200\n",
            "loss 6.53380157443567e-07 average time 0.0031111162300112485 iter num 100\n",
            "loss 6.533512201134251e-07 average time 0.0031740576699917254 iter num 200\n",
            "loss 6.531963611467557e-07 average time 0.0030583308800169106 iter num 100\n",
            "loss 6.531694094260287e-07 average time 0.003042185144988707 iter num 200\n",
            "loss 6.530150846100976e-07 average time 0.0030387058399901435 iter num 100\n",
            "loss 6.529861437918734e-07 average time 0.003016724934998365 iter num 200\n",
            "loss 6.528302220483434e-07 average time 0.002975463500029036 iter num 100\n",
            "loss 6.528030304111383e-07 average time 0.0030413055350209106 iter num 200\n",
            "loss 6.526451493549707e-07 average time 0.0030857007599979624 iter num 100\n",
            "loss 6.526190059738468e-07 average time 0.003125721879998764 iter num 200\n",
            "loss 6.524655708177493e-07 average time 0.0030319727799997054 iter num 100\n",
            "loss 6.524358006887811e-07 average time 0.0030371726800103717 iter num 200\n",
            "loss 6.522794830230627e-07 average time 0.002975161939989448 iter num 100\n",
            "loss 6.522524270446875e-07 average time 0.0030230824599993865 iter num 200\n",
            "loss 6.520981855560897e-07 average time 0.003025531470034366 iter num 100\n",
            "loss 6.520687281795387e-07 average time 0.0030587874050206666 iter num 200\n",
            "loss 6.519155782236653e-07 average time 0.003045345270011239 iter num 100\n",
            "loss 6.518843253775127e-07 average time 0.0030354680599998574 iter num 200\n",
            "loss 6.517311449917643e-07 average time 0.002959045440020418 iter num 100\n",
            "loss 6.517018212897461e-07 average time 0.002984942500006582 iter num 200\n",
            "loss 6.515462680499023e-07 average time 0.0031530534099965734 iter num 100\n",
            "loss 6.51519476577531e-07 average time 0.0031169402250020538 iter num 200\n",
            "loss 6.513632002751065e-07 average time 0.003081512900034795 iter num 100\n",
            "loss 6.513356688499197e-07 average time 0.0030402542850129066 iter num 200\n",
            "loss 6.511796807690125e-07 average time 0.00295297618999939 iter num 100\n",
            "loss 6.511527155991935e-07 average time 0.002978504814991538 iter num 200\n",
            "loss 6.509967993458813e-07 average time 0.002979994440011069 iter num 100\n",
            "loss 6.509682528744028e-07 average time 0.0029792807799913137 iter num 200\n",
            "loss 6.50813941289045e-07 average time 0.0029954598600170357 iter num 100\n",
            "loss 6.507870512889493e-07 average time 0.002986750535008014 iter num 200\n",
            "loss 6.506310011025432e-07 average time 0.0031644412799687414 iter num 100\n",
            "loss 6.506039148946493e-07 average time 0.003066126629989867 iter num 200\n",
            "loss 6.504501003374763e-07 average time 0.0030923952599914628 iter num 100\n",
            "loss 6.504226415161279e-07 average time 0.0030902072800131465 iter num 200\n",
            "loss 6.502661308417957e-07 average time 0.0031214132999957656 iter num 100\n",
            "loss 6.502371298627514e-07 average time 0.0031000332950088705 iter num 200\n",
            "loss 6.500843698086417e-07 average time 0.0029966024000168544 iter num 100\n",
            "loss 6.500544536156171e-07 average time 0.002981911109984594 iter num 200\n",
            "loss 6.49901290587174e-07 average time 0.003009972669992749 iter num 100\n",
            "loss 6.498745812214819e-07 average time 0.0030020007649864057 iter num 200\n",
            "loss 6.497207244941749e-07 average time 0.0029862444000036703 iter num 100\n",
            "loss 6.49691008673322e-07 average time 0.0030842420300155027 iter num 200\n",
            "loss 6.495373451666251e-07 average time 0.0030979312199906415 iter num 100\n",
            "loss 6.495076197011512e-07 average time 0.0030522199199890564 iter num 200\n",
            "loss 6.493538911703826e-07 average time 0.002987467660009315 iter num 100\n",
            "loss 6.493291559847629e-07 average time 0.003021131720001904 iter num 200\n",
            "loss 6.491724097673449e-07 average time 0.003022998779997579 iter num 100\n",
            "loss 6.491459591250393e-07 average time 0.003078756184997928 iter num 200\n",
            "loss 6.489904450926997e-07 average time 0.003052177659992594 iter num 100\n",
            "loss 6.489641378846164e-07 average time 0.003008267349994185 iter num 200\n",
            "loss 6.4880854115696e-07 average time 0.0029437776500344627 iter num 100\n",
            "loss 6.487832439167949e-07 average time 0.002978723150015412 iter num 200\n",
            "loss 6.48627795813602e-07 average time 0.003090288939988568 iter num 100\n",
            "loss 6.485993369269013e-07 average time 0.003037459510005647 iter num 200\n",
            "loss 6.484438572105249e-07 average time 0.003063187780012413 iter num 100\n",
            "loss 6.484168626201935e-07 average time 0.0030624602550142297 iter num 200\n",
            "loss 6.482657756764086e-07 average time 0.0029910658199924 iter num 100\n",
            "loss 6.482361821664235e-07 average time 0.0030385401899957285 iter num 200\n",
            "loss 6.480825577585696e-07 average time 0.0029850756599853413 iter num 100\n",
            "loss 6.48054162289213e-07 average time 0.003127417704995423 iter num 200\n",
            "loss 6.47901878748366e-07 average time 0.0030248206100122843 iter num 100\n",
            "loss 6.478739966612855e-07 average time 0.003011010280006303 iter num 200\n",
            "loss 6.47720563598635e-07 average time 0.0029691830599767855 iter num 100\n",
            "loss 6.476927269188888e-07 average time 0.00297415417499451 iter num 200\n",
            "loss 6.475407242907522e-07 average time 0.0029777918400350246 iter num 100\n",
            "loss 6.475120690306744e-07 average time 0.002969364810026036 iter num 200\n",
            "loss 6.4735811061422e-07 average time 0.003125421050008299 iter num 100\n",
            "loss 6.473291730140028e-07 average time 0.0030813802050147386 iter num 200\n",
            "loss 6.471763949237903e-07 average time 0.002993661399987104 iter num 100\n",
            "loss 6.471503019079534e-07 average time 0.002977470729990728 iter num 200\n",
            "loss 6.469977634787242e-07 average time 0.003128998059996775 iter num 100\n",
            "loss 6.469713340451079e-07 average time 0.0030752493449972464 iter num 200\n",
            "loss 6.468164388326916e-07 average time 0.003246514880001996 iter num 100\n",
            "loss 6.46789034408234e-07 average time 0.0031784670199886023 iter num 200\n",
            "loss 6.466363910905121e-07 average time 0.003001737069980663 iter num 100\n",
            "loss 6.466067069981033e-07 average time 0.0031249355799741352 iter num 200\n",
            "loss 6.464561778402714e-07 average time 0.003209866389970557 iter num 100\n",
            "loss 6.464290096807584e-07 average time 0.0030980940199970062 iter num 200\n",
            "loss 6.462744621160186e-07 average time 0.0030129599499878167 iter num 100\n",
            "loss 6.462462275817284e-07 average time 0.003013255049995678 iter num 200\n",
            "loss 6.460944831672888e-07 average time 0.003052176180026436 iter num 100\n",
            "loss 6.460669134037543e-07 average time 0.0030828255700203044 iter num 200\n",
            "loss 6.459162949180133e-07 average time 0.0029715333999865835 iter num 100\n",
            "loss 6.458858759789647e-07 average time 0.0030857772650074366 iter num 200\n",
            "loss 6.457368138150379e-07 average time 0.0029803671599847805 iter num 100\n",
            "loss 6.457073984655359e-07 average time 0.0030534732999694825 iter num 200\n",
            "loss 6.455541825295748e-07 average time 0.00309062799001822 iter num 100\n",
            "loss 6.455264058615988e-07 average time 0.0031063801449977293 iter num 200\n",
            "loss 6.453744003394798e-07 average time 0.003018319580005482 iter num 100\n",
            "loss 6.453480990526815e-07 average time 0.003039300155003275 iter num 200\n",
            "loss 6.451966534035961e-07 average time 0.003062468520006405 iter num 100\n",
            "loss 6.451663468213099e-07 average time 0.0030878291750173047 iter num 200\n",
            "loss 6.450158866375055e-07 average time 0.0030186924600002383 iter num 100\n",
            "loss 6.449882157391951e-07 average time 0.003030147800006944 iter num 200\n",
            "loss 6.44836327241421e-07 average time 0.003096917970010509 iter num 100\n",
            "loss 6.448071139399298e-07 average time 0.0031309536999992813 iter num 200\n",
            "loss 6.446582725287536e-07 average time 0.0031142618200192375 iter num 100\n",
            "loss 6.446293664430258e-07 average time 0.003076876905010977 iter num 200\n",
            "loss 6.444771643048973e-07 average time 0.003091661240009671 iter num 100\n",
            "loss 6.444514830091941e-07 average time 0.003132425860010244 iter num 200\n",
            "loss 6.442988562447284e-07 average time 0.0029935074900049583 iter num 100\n",
            "loss 6.442697005877135e-07 average time 0.0030082608000043367 iter num 200\n",
            "loss 6.441186233791212e-07 average time 0.003013993279978422 iter num 100\n",
            "loss 6.440927738467323e-07 average time 0.002991195859997333 iter num 200\n",
            "loss 6.439423336252557e-07 average time 0.002956051110027147 iter num 100\n",
            "loss 6.439151725007568e-07 average time 0.0029717140650041074 iter num 200\n",
            "loss 6.437636294065837e-07 average time 0.002987759289985661 iter num 100\n",
            "loss 6.437358527444345e-07 average time 0.003023059864995048 iter num 200\n",
            "loss 6.435844116505389e-07 average time 0.002983178620002036 iter num 100\n",
            "loss 6.435568026443338e-07 average time 0.003035758190003435 iter num 200\n",
            "loss 6.434077549022791e-07 average time 0.0030509632399753172 iter num 100\n",
            "loss 6.433795845817567e-07 average time 0.0030444940199868143 iter num 200\n",
            "loss 6.432277896351351e-07 average time 0.003335930099997313 iter num 100\n",
            "loss 6.432016006929468e-07 average time 0.003167991355014692 iter num 200\n",
            "loss 6.430499672672515e-07 average time 0.00300706418000118 iter num 100\n",
            "loss 6.430221388402896e-07 average time 0.0030227126049908295 iter num 200\n",
            "loss 6.428703772469239e-07 average time 0.0030167265500313078 iter num 100\n",
            "loss 6.42841893544174e-07 average time 0.003069134795025548 iter num 200\n",
            "loss 6.426931134051873e-07 average time 0.0030775152700198306 iter num 100\n",
            "loss 6.426647796707493e-07 average time 0.003039177355019547 iter num 200\n",
            "loss 6.425155188252817e-07 average time 0.0030179231199826974 iter num 100\n",
            "loss 6.424857073068252e-07 average time 0.0030079797249959483 iter num 200\n",
            "loss 6.423378896778275e-07 average time 0.0030802462100064077 iter num 100\n",
            "loss 6.423090810619558e-07 average time 0.003109310020006433 iter num 200\n",
            "loss 6.421590736232153e-07 average time 0.0030531215299970426 iter num 100\n",
            "loss 6.421299384004912e-07 average time 0.0030059286550067556 iter num 200\n",
            "loss 6.419805751732e-07 average time 0.002982890479997877 iter num 100\n",
            "loss 6.419531470681547e-07 average time 0.002993079130001206 iter num 200\n",
            "loss 6.418033194850346e-07 average time 0.0029579893100162733 iter num 100\n",
            "loss 6.417763695445523e-07 average time 0.0029859962000114136 iter num 200\n",
            "loss 6.416242929579314e-07 average time 0.003003006430012647 iter num 100\n",
            "loss 6.415974590353229e-07 average time 0.0030087473400021735 iter num 200\n",
            "loss 6.414479109900936e-07 average time 0.003093114279990914 iter num 100\n",
            "loss 6.414206542610598e-07 average time 0.0030478182349907 iter num 200\n",
            "loss 6.412709368855405e-07 average time 0.0031358164399989618 iter num 100\n",
            "loss 6.412446831718372e-07 average time 0.003094658719994641 iter num 200\n",
            "loss 6.410941047228198e-07 average time 0.002990351520011245 iter num 100\n",
            "loss 6.410668004999156e-07 average time 0.003037834190006379 iter num 200\n",
            "loss 6.409176924359027e-07 average time 0.0031033768400175175 iter num 100\n",
            "loss 6.408884337438364e-07 average time 0.0030387078750004548 iter num 200\n",
            "loss 6.40739108784378e-07 average time 0.0029775157499716443 iter num 100\n",
            "loss 6.407126623782355e-07 average time 0.00303693014998089 iter num 200\n",
            "loss 6.405647457629681e-07 average time 0.0031549321200100167 iter num 100\n",
            "loss 6.405375039882597e-07 average time 0.0030966088650006894 iter num 200\n",
            "loss 6.403844560250459e-07 average time 0.0031087511700070534 iter num 100\n",
            "loss 6.403579942777935e-07 average time 0.0030491990949894896 iter num 200\n",
            "loss 6.402090073963408e-07 average time 0.0029643413899793814 iter num 100\n",
            "loss 6.401829945200424e-07 average time 0.002969109169985131 iter num 200\n",
            "loss 6.400327841347203e-07 average time 0.0030241230900355733 iter num 100\n",
            "loss 6.400072785948174e-07 average time 0.003001769310023974 iter num 200\n",
            "loss 6.398562640380039e-07 average time 0.0030101665400161437 iter num 100\n",
            "loss 6.398311455422649e-07 average time 0.003011446675000116 iter num 200\n",
            "loss 6.396840434488412e-07 average time 0.0029989900200189366 iter num 100\n",
            "loss 6.396539225971219e-07 average time 0.002986103524999635 iter num 200\n",
            "loss 6.395041462296145e-07 average time 0.003140469140003006 iter num 100\n",
            "loss 6.394763688810781e-07 average time 0.0031197631700206328 iter num 200\n",
            "loss 6.393279623181305e-07 average time 0.003038119769967125 iter num 100\n",
            "loss 6.393002173723943e-07 average time 0.003013280264988225 iter num 200\n",
            "loss 6.391501353151556e-07 average time 0.0034120530800055347 iter num 100\n",
            "loss 6.391245847500696e-07 average time 0.0031966255399902364 iter num 200\n",
            "loss 6.38974078501097e-07 average time 0.003118276750001314 iter num 100\n",
            "loss 6.389478031649387e-07 average time 0.0030960018100017806 iter num 200\n",
            "loss 6.387994939989144e-07 average time 0.0030010885299816435 iter num 100\n",
            "loss 6.387730224194436e-07 average time 0.0029908719399873007 iter num 200\n",
            "loss 6.386246408928207e-07 average time 0.0030320361900248827 iter num 100\n",
            "loss 6.385982591765658e-07 average time 0.0029913191400032704 iter num 200\n",
            "loss 6.384484589560163e-07 average time 0.0029747182099936252 iter num 100\n",
            "loss 6.384229270942462e-07 average time 0.003047852904996944 iter num 200\n",
            "loss 6.38273608252343e-07 average time 0.003044164519978949 iter num 100\n",
            "loss 6.382450023492765e-07 average time 0.003005473489988617 iter num 200\n",
            "loss 6.380955825794787e-07 average time 0.0031091739200155644 iter num 100\n",
            "loss 6.38070442494649e-07 average time 0.0030795991950162715 iter num 200\n",
            "loss 6.379236694175379e-07 average time 0.0029639551199943525 iter num 100\n",
            "loss 6.378954691835995e-07 average time 0.002963251319997653 iter num 200\n",
            "loss 6.377454133649822e-07 average time 0.002998925710012372 iter num 100\n",
            "loss 6.37718616190308e-07 average time 0.0029948559650119933 iter num 200\n",
            "loss 6.37572258469929e-07 average time 0.0029763262699771076 iter num 100\n",
            "loss 6.375441892240774e-07 average time 0.003081639649997214 iter num 200\n",
            "loss 6.373941543116745e-07 average time 0.0029681644300262634 iter num 100\n",
            "loss 6.373713996841973e-07 average time 0.0029846610600134226 iter num 200\n",
            "loss 6.372225616457815e-07 average time 0.0030467624100219835 iter num 100\n",
            "loss 6.371962091295119e-07 average time 0.0030960104050086557 iter num 200\n",
            "loss 6.370481312235803e-07 average time 0.0029843474999961474 iter num 100\n",
            "loss 6.37021847506189e-07 average time 0.0030197792499984644 iter num 200\n",
            "loss 6.36872073631536e-07 average time 0.003112992100045631 iter num 100\n",
            "loss 6.368454748864826e-07 average time 0.0031379094600310965 iter num 200\n",
            "loss 6.366980514474537e-07 average time 0.003070347210014006 iter num 100\n",
            "loss 6.366702237887866e-07 average time 0.0030459885150139598 iter num 200\n",
            "loss 6.365267148164739e-07 average time 0.003027321530007612 iter num 100\n",
            "loss 6.364954566536833e-07 average time 0.0030626420900034647 iter num 200\n",
            "loss 6.363500875181974e-07 average time 0.003034994840018044 iter num 100\n",
            "loss 6.363210139253079e-07 average time 0.0030440532450120373 iter num 200\n",
            "loss 6.361730288370728e-07 average time 0.0030173431600178445 iter num 100\n",
            "loss 6.361462392674105e-07 average time 0.003045703065008638 iter num 200\n",
            "loss 6.359986951328445e-07 average time 0.0029946398199899704 iter num 100\n",
            "loss 6.359732787382115e-07 average time 0.0030271326199977013 iter num 200\n",
            "loss 6.35826540088237e-07 average time 0.0030133385299950534 iter num 100\n",
            "loss 6.357988725021305e-07 average time 0.0030473685150104756 iter num 200\n",
            "loss 6.356527902320043e-07 average time 0.0030459284499829665 iter num 100\n",
            "loss 6.356273323571268e-07 average time 0.0031101047549896067 iter num 200\n",
            "loss 6.354779961530762e-07 average time 0.002954503860009936 iter num 100\n",
            "loss 6.354488432601011e-07 average time 0.0029609753999943677 iter num 200\n",
            "loss 6.353039142692405e-07 average time 0.003081170839996048 iter num 100\n",
            "loss 6.352779869490521e-07 average time 0.0030493151749919887 iter num 200\n",
            "loss 6.351321475386037e-07 average time 0.002997866939990672 iter num 100\n",
            "loss 6.351024484420322e-07 average time 0.002985670019995723 iter num 200\n",
            "loss 6.349585418690575e-07 average time 0.0032185304200174868 iter num 100\n",
            "loss 6.349296236608776e-07 average time 0.0030800698400253166 iter num 200\n",
            "loss 6.347834011027571e-07 average time 0.002990715970004203 iter num 100\n",
            "loss 6.34754586426151e-07 average time 0.0030189279549972525 iter num 200\n",
            "loss 6.346112376759227e-07 average time 0.0029622227299887526 iter num 100\n",
            "loss 6.345816649649506e-07 average time 0.0029943266599980235 iter num 200\n",
            "loss 6.344372059243658e-07 average time 0.0032154770899569486 iter num 100\n",
            "loss 6.344086252255187e-07 average time 0.0031202959749839464 iter num 200\n",
            "loss 6.342634667713979e-07 average time 0.0031699548200094796 iter num 100\n",
            "loss 6.342383187691625e-07 average time 0.0031445266849891595 iter num 200\n",
            "loss 6.340909965796407e-07 average time 0.0032033324200074274 iter num 100\n",
            "loss 6.340620473820643e-07 average time 0.0030941019950250846 iter num 200\n",
            "loss 6.339187727659987e-07 average time 0.003074570809994839 iter num 100\n",
            "loss 6.338911373072932e-07 average time 0.003087185749982382 iter num 200\n",
            "loss 6.337455462246162e-07 average time 0.0030202756199923897 iter num 100\n",
            "loss 6.337188514649464e-07 average time 0.00303230971001085 iter num 200\n",
            "loss 6.33572526749911e-07 average time 0.0029785252899864646 iter num 100\n",
            "loss 6.335478985008621e-07 average time 0.0029727809799942407 iter num 200\n",
            "loss 6.334010169276913e-07 average time 0.0029794274400092038 iter num 100\n",
            "loss 6.333747231558474e-07 average time 0.0030862963599997783 iter num 200\n",
            "loss 6.332261040235594e-07 average time 0.003097099079977852 iter num 100\n",
            "loss 6.331999325075192e-07 average time 0.0031080834549834435 iter num 200\n",
            "loss 6.330533307874787e-07 average time 0.0030572626799812495 iter num 100\n",
            "loss 6.330272034721165e-07 average time 0.003027789444990958 iter num 200\n",
            "loss 6.328819616709979e-07 average time 0.0030113810900138562 iter num 100\n",
            "loss 6.328527632127253e-07 average time 0.003061587450013121 iter num 200\n",
            "loss 6.327111458104433e-07 average time 0.0030633972900022855 iter num 100\n",
            "loss 6.326834227450491e-07 average time 0.003016015174987388 iter num 200\n",
            "loss 6.325381886205185e-07 average time 0.0029710558099577612 iter num 100\n",
            "loss 6.325099240495628e-07 average time 0.002986662344990236 iter num 200\n",
            "loss 6.323656047616606e-07 average time 0.003057543990016711 iter num 100\n",
            "loss 6.323393567002143e-07 average time 0.003023373170008199 iter num 200\n",
            "loss 6.321940567556804e-07 average time 0.003042188719996375 iter num 100\n",
            "loss 6.321674580966508e-07 average time 0.00299648052999828 iter num 200\n",
            "loss 6.32021681086064e-07 average time 0.00312385479000568 iter num 100\n",
            "loss 6.319945542351147e-07 average time 0.0030700275900017006 iter num 200\n",
            "loss 6.318499293429864e-07 average time 0.003021242710024126 iter num 100\n",
            "loss 6.318229011220626e-07 average time 0.0030372508200071026 iter num 200\n",
            "loss 6.316794172349324e-07 average time 0.0030832365699779984 iter num 100\n",
            "loss 6.316531731519597e-07 average time 0.003061524944982921 iter num 200\n",
            "loss 6.315070767029386e-07 average time 0.0029988326299962864 iter num 100\n",
            "loss 6.314796225274997e-07 average time 0.0030545358650056185 iter num 200\n",
            "loss 6.313345291681173e-07 average time 0.00319231416999628 iter num 100\n",
            "loss 6.313104408579089e-07 average time 0.0031657364349985074 iter num 200\n",
            "loss 6.311639704716664e-07 average time 0.003027433590018518 iter num 100\n",
            "loss 6.311380217482488e-07 average time 0.003203642010009844 iter num 200\n",
            "loss 6.309929982166531e-07 average time 0.0031077279999772145 iter num 100\n",
            "loss 6.309662139837824e-07 average time 0.0030897103049801443 iter num 200\n",
            "loss 6.308197457244583e-07 average time 0.003158585000005587 iter num 100\n",
            "loss 6.30794951915986e-07 average time 0.003061465870000575 iter num 200\n",
            "loss 6.306491525168142e-07 average time 0.0031430868399775136 iter num 100\n",
            "loss 6.306240225476492e-07 average time 0.0030827331099817458 iter num 200\n",
            "loss 6.304785910001955e-07 average time 0.003120442929989622 iter num 100\n",
            "loss 6.304512539380291e-07 average time 0.003101117879980393 iter num 200\n",
            "loss 6.303073055473135e-07 average time 0.0032663460300273073 iter num 100\n",
            "loss 6.302812320715795e-07 average time 0.0031931641750043128 iter num 200\n",
            "loss 6.301360433123915e-07 average time 0.003057013080015167 iter num 100\n",
            "loss 6.301121308891916e-07 average time 0.0030494848400007867 iter num 200\n",
            "loss 6.299666030745926e-07 average time 0.0029817737999928793 iter num 100\n",
            "loss 6.299392271263896e-07 average time 0.0029755102249873744 iter num 200\n",
            "loss 6.297982678419377e-07 average time 0.0030780308700104797 iter num 100\n",
            "loss 6.297700096468726e-07 average time 0.0030474974700177883 iter num 200\n",
            "loss 6.296268831385326e-07 average time 0.003249290039993866 iter num 100\n",
            "loss 6.295994624431786e-07 average time 0.003144086624990905 iter num 200\n",
            "loss 6.294558251289899e-07 average time 0.0030678143900149733 iter num 100\n",
            "loss 6.294307517293406e-07 average time 0.0030782985650103 iter num 200\n",
            "loss 6.292859079963856e-07 average time 0.0030000646399957987 iter num 100\n",
            "loss 6.292584423339622e-07 average time 0.0030020388450020617 iter num 200\n",
            "loss 6.291138292648089e-07 average time 0.002985735219995149 iter num 100\n",
            "loss 6.290885768775758e-07 average time 0.002996761229999265 iter num 200\n",
            "loss 6.289451497178822e-07 average time 0.0031418559100166023 iter num 100\n",
            "loss 6.289188305177562e-07 average time 0.0030491454950106345 iter num 200\n",
            "loss 6.287756534736472e-07 average time 0.003005494549988725 iter num 100\n",
            "loss 6.287490938708704e-07 average time 0.002997301439993407 iter num 200\n",
            "loss 6.286045774656875e-07 average time 0.0030181403999904434 iter num 100\n",
            "loss 6.285784243941748e-07 average time 0.0030265762949943566 iter num 200\n",
            "loss 6.284352722283113e-07 average time 0.0030336488499824554 iter num 100\n",
            "loss 6.284084051237509e-07 average time 0.0030350663749868544 iter num 200\n",
            "loss 6.282656649897294e-07 average time 0.0029792937499814797 iter num 100\n",
            "loss 6.282391028221058e-07 average time 0.0029990641699805566 iter num 200\n",
            "loss 6.280954914080733e-07 average time 0.0030719246099806695 iter num 100\n",
            "loss 6.280702201667119e-07 average time 0.0030880888599835996 iter num 200\n",
            "loss 6.279245446499319e-07 average time 0.003008869909972418 iter num 100\n",
            "loss 6.2790019374212e-07 average time 0.0030357741899979373 iter num 200\n",
            "loss 6.277558645618052e-07 average time 0.0030168889799915633 iter num 100\n",
            "loss 6.277316627157512e-07 average time 0.0029977073200007 iter num 200\n",
            "loss 6.275876116951703e-07 average time 0.0031272206199855646 iter num 100\n",
            "loss 6.275607282711794e-07 average time 0.0031233713249980612 iter num 200\n",
            "loss 6.274185919854789e-07 average time 0.002987454560006881 iter num 100\n",
            "loss 6.273916766196578e-07 average time 0.003021919809987139 iter num 200\n",
            "loss 6.272506584901575e-07 average time 0.002983530889978283 iter num 100\n",
            "loss 6.272220380527767e-07 average time 0.0030037844849994146 iter num 200\n",
            "loss 6.270804229011984e-07 average time 0.003016070330040748 iter num 100\n",
            "loss 6.270551188985904e-07 average time 0.003009687055030099 iter num 200\n",
            "loss 6.269112029753126e-07 average time 0.0030737248399873352 iter num 100\n",
            "loss 6.26885302049992e-07 average time 0.0030346990549946894 iter num 200\n",
            "loss 6.267422080020923e-07 average time 0.0030178543099964373 iter num 100\n",
            "loss 6.267148864689672e-07 average time 0.0030405115949906757 iter num 200\n",
            "loss 6.265726279594156e-07 average time 0.0030131923800172443 iter num 100\n",
            "loss 6.265477456994287e-07 average time 0.0030474950800225995 iter num 200\n",
            "loss 6.264053614061243e-07 average time 0.003100287360034599 iter num 100\n",
            "loss 6.263798807606247e-07 average time 0.0030320466650186973 iter num 200\n",
            "loss 6.26237146592186e-07 average time 0.0029898207499763885 iter num 100\n",
            "loss 6.262118377730104e-07 average time 0.0030527181599904907 iter num 200\n",
            "loss 6.260687313833763e-07 average time 0.0031741849999934856 iter num 100\n",
            "loss 6.260420651934024e-07 average time 0.003175752224997268 iter num 200\n",
            "loss 6.258990490122412e-07 average time 0.0030403781499762773 iter num 100\n",
            "loss 6.258731204693543e-07 average time 0.0030617399599827877 iter num 200\n",
            "loss 6.257307085827375e-07 average time 0.003019579389997489 iter num 100\n",
            "loss 6.257026390774021e-07 average time 0.0030197574100088785 iter num 200\n",
            "loss 6.255640798500374e-07 average time 0.003077794309988349 iter num 100\n",
            "loss 6.255365571149879e-07 average time 0.003103812494989597 iter num 200\n",
            "loss 6.25394638527509e-07 average time 0.0029533200700461748 iter num 100\n",
            "loss 6.25369225693155e-07 average time 0.002998109620023115 iter num 200\n",
            "loss 6.252272589042615e-07 average time 0.003133686799997122 iter num 100\n",
            "loss 6.252003720931371e-07 average time 0.0030843916799904035 iter num 200\n",
            "loss 6.250588205122769e-07 average time 0.003030917070000214 iter num 100\n",
            "loss 6.250342753482579e-07 average time 0.0030054979749911583 iter num 200\n",
            "loss 6.24891280314953e-07 average time 0.0030831910199913184 iter num 100\n",
            "loss 6.248644166082512e-07 average time 0.003022096240003975 iter num 200\n",
            "loss 6.247254958128001e-07 average time 0.003036891950014251 iter num 100\n",
            "loss 6.246974893553714e-07 average time 0.002994567440021001 iter num 200\n",
            "loss 6.245583000548303e-07 average time 0.003018700390043705 iter num 100\n",
            "loss 6.245315604174623e-07 average time 0.0030676733500331465 iter num 200\n",
            "loss 6.243905249457274e-07 average time 0.003099203909964672 iter num 100\n",
            "loss 6.243611842409509e-07 average time 0.0030400113449718445 iter num 200\n",
            "loss 6.24222382787597e-07 average time 0.003065042949974668 iter num 100\n",
            "loss 6.241966006638026e-07 average time 0.0030448838949860145 iter num 200\n",
            "loss 6.240528263187081e-07 average time 0.0030475034200162553 iter num 100\n",
            "loss 6.240277651603028e-07 average time 0.003001097910005228 iter num 200\n",
            "loss 6.238871915431203e-07 average time 0.00297000307999042 iter num 100\n",
            "loss 6.238607238874451e-07 average time 0.0029815232149803706 iter num 200\n",
            "loss 6.237193164072713e-07 average time 0.002957928800019545 iter num 100\n",
            "loss 6.236950091983543e-07 average time 0.0029938839850296973 iter num 200\n",
            "loss 6.23554552639267e-07 average time 0.0029588551100141556 iter num 100\n",
            "loss 6.235258484546755e-07 average time 0.002965231290008887 iter num 200\n",
            "loss 6.233856779390012e-07 average time 0.0030350130400074705 iter num 100\n",
            "loss 6.233598314241337e-07 average time 0.0030126384049981424 iter num 200\n",
            "loss 6.232176588915277e-07 average time 0.0031103033500266973 iter num 100\n",
            "loss 6.231923329173381e-07 average time 0.0030387138150126704 iter num 200\n",
            "loss 6.230525757874601e-07 average time 0.0031448057499892457 iter num 100\n",
            "loss 6.230272861563066e-07 average time 0.003095962645002146 iter num 200\n",
            "loss 6.228854685560841e-07 average time 0.003001601670025593 iter num 100\n",
            "loss 6.228607512943108e-07 average time 0.00299946542999578 iter num 200\n",
            "loss 6.227204314506608e-07 average time 0.00300414543999068 iter num 100\n",
            "loss 6.226936409766782e-07 average time 0.0030229063149909054 iter num 200\n",
            "loss 6.225522284950553e-07 average time 0.003026061679997838 iter num 100\n",
            "loss 6.22527563202473e-07 average time 0.0030096014700029625 iter num 200\n",
            "loss 6.223843608267535e-07 average time 0.003014486580023004 iter num 100\n",
            "loss 6.223612072069951e-07 average time 0.0030075562750130303 iter num 200\n",
            "loss 6.222192119181942e-07 average time 0.0030751706300179648 iter num 100\n",
            "loss 6.221946466354384e-07 average time 0.0030282551950267587 iter num 200\n",
            "loss 6.220547620805033e-07 average time 0.0029901188099665885 iter num 100\n",
            "loss 6.22028056211077e-07 average time 0.0030349704449895397 iter num 200\n",
            "loss 6.218865664376745e-07 average time 0.0029932417100280873 iter num 100\n",
            "loss 6.218644242713816e-07 average time 0.0029779849700139493 iter num 200\n",
            "loss 6.21720892440457e-07 average time 0.003135896169969783 iter num 100\n",
            "loss 6.216951784983891e-07 average time 0.003112338539983739 iter num 200\n",
            "loss 6.21555425570767e-07 average time 0.003029408209999929 iter num 100\n",
            "loss 6.215316728661698e-07 average time 0.003019955460008532 iter num 200\n",
            "loss 6.213899246259599e-07 average time 0.002999990450025507 iter num 100\n",
            "loss 6.213646009042381e-07 average time 0.003003181455021604 iter num 200\n",
            "loss 6.212249588273199e-07 average time 0.002985354220013505 iter num 100\n",
            "loss 6.21200843442533e-07 average time 0.0030803080800092175 iter num 200\n",
            "loss 6.210593845826197e-07 average time 0.0030061786299893354 iter num 100\n",
            "loss 6.210341859830913e-07 average time 0.0030179444050099846 iter num 200\n",
            "loss 6.208942584888558e-07 average time 0.002950444610014529 iter num 100\n",
            "loss 6.208686771448405e-07 average time 0.0029615281950123064 iter num 200\n",
            "loss 6.207297979830696e-07 average time 0.0029602047100161146 iter num 100\n",
            "loss 6.207030875782148e-07 average time 0.0029961343750142077 iter num 200\n",
            "loss 6.205629389246393e-07 average time 0.003267838820029283 iter num 100\n",
            "loss 6.205389693109934e-07 average time 0.0031458126700204046 iter num 200\n",
            "loss 6.203984539878852e-07 average time 0.003165603659977023 iter num 100\n",
            "loss 6.20371678535423e-07 average time 0.0031217035049758124 iter num 200\n",
            "loss 6.202311974651039e-07 average time 0.003055451940022067 iter num 100\n",
            "loss 6.202077930915452e-07 average time 0.003062818600021728 iter num 200\n",
            "loss 6.200656834851877e-07 average time 0.003118809600014174 iter num 100\n",
            "loss 6.200427116810622e-07 average time 0.003035088825006369 iter num 200\n",
            "loss 6.199024862361768e-07 average time 0.0030119909299673965 iter num 100\n",
            "loss 6.198790181871262e-07 average time 0.003163726394968762 iter num 200\n",
            "loss 6.197380217477723e-07 average time 0.0030515981799953806 iter num 100\n",
            "loss 6.197141252092547e-07 average time 0.003036767445005353 iter num 200\n",
            "loss 6.195728542393735e-07 average time 0.0029946230099994866 iter num 100\n",
            "loss 6.195492238550551e-07 average time 0.0029874361450015386 iter num 200\n",
            "loss 6.194089785999575e-07 average time 0.0029990213099790708 iter num 100\n",
            "loss 6.193835846465165e-07 average time 0.0029983195599879763 iter num 200\n",
            "loss 6.192444613367002e-07 average time 0.003150330270004815 iter num 100\n",
            "loss 6.192191409217454e-07 average time 0.0031360554150001007 iter num 200\n",
            "loss 6.190798007788126e-07 average time 0.003097366240026531 iter num 100\n",
            "loss 6.19053549853318e-07 average time 0.003135406275030164 iter num 200\n",
            "loss 6.189137204939748e-07 average time 0.003197760389989526 iter num 100\n",
            "loss 6.188916207968043e-07 average time 0.003155185504990641 iter num 200\n",
            "loss 6.187493846623776e-07 average time 0.0029842496499941262 iter num 100\n",
            "loss 6.187258407276678e-07 average time 0.002986895859994547 iter num 200\n",
            "loss 6.185882859162885e-07 average time 0.0030394440599957307 iter num 100\n",
            "loss 6.18561440251054e-07 average time 0.003146144039981209 iter num 200\n",
            "loss 6.184233509432517e-07 average time 0.002954902270016646 iter num 100\n",
            "loss 6.183976180717854e-07 average time 0.0029546663049995914 iter num 200\n",
            "loss 6.18260486709224e-07 average time 0.0031411881500162052 iter num 100\n",
            "loss 6.182331220374465e-07 average time 0.003114214055017328 iter num 200\n",
            "loss 6.180965553263513e-07 average time 0.0031332541299752847 iter num 100\n",
            "loss 6.180704638739891e-07 average time 0.0030915128549781914 iter num 200\n",
            "loss 6.179313932326353e-07 average time 0.0031017946699921596 iter num 100\n",
            "loss 6.179058728315942e-07 average time 0.003079101540013198 iter num 200\n",
            "loss 6.177662454864899e-07 average time 0.003021817990015734 iter num 100\n",
            "loss 6.177437705305147e-07 average time 0.0029904168950179153 iter num 200\n",
            "loss 6.176041902957714e-07 average time 0.0030245728400132067 iter num 100\n",
            "loss 6.175786168751433e-07 average time 0.003137931520004713 iter num 200\n",
            "loss 6.17439812998648e-07 average time 0.003306775449964334 iter num 100\n",
            "loss 6.174165659718188e-07 average time 0.00312687680497902 iter num 200\n",
            "loss 6.172771399873837e-07 average time 0.0030306623499700438 iter num 100\n",
            "loss 6.172520849613909e-07 average time 0.0031670397799825876 iter num 200\n",
            "loss 6.171153762894165e-07 average time 0.0031828165899878513 iter num 100\n",
            "loss 6.170891542773044e-07 average time 0.0031225456349875456 iter num 200\n",
            "loss 6.169498561054247e-07 average time 0.0030069468699866777 iter num 100\n",
            "loss 6.169246054765854e-07 average time 0.0030210664549986175 iter num 200\n",
            "loss 6.167885126395577e-07 average time 0.0030542281399812054 iter num 100\n",
            "loss 6.167638482019836e-07 average time 0.0030210363049786794 iter num 200\n",
            "loss 6.166245661417083e-07 average time 0.0029678460299874133 iter num 100\n",
            "loss 6.165998451653109e-07 average time 0.0029614287449953734 iter num 200\n",
            "loss 6.164620398912862e-07 average time 0.0031970389800198974 iter num 100\n",
            "loss 6.164367302077532e-07 average time 0.0030896582300033513 iter num 200\n",
            "loss 6.162995884767262e-07 average time 0.0029954819000113275 iter num 100\n",
            "loss 6.162740933568055e-07 average time 0.003026991834997261 iter num 200\n",
            "loss 6.161352893705858e-07 average time 0.003005425650048892 iter num 100\n",
            "loss 6.16111136989126e-07 average time 0.002981500380030866 iter num 200\n",
            "loss 6.159705106268171e-07 average time 0.0033243282899775295 iter num 100\n",
            "loss 6.159483413678665e-07 average time 0.0032164628249870477 iter num 200\n",
            "loss 6.158092146493321e-07 average time 0.0030122102199993607 iter num 100\n",
            "loss 6.157860071549514e-07 average time 0.002994256975002827 iter num 200\n",
            "loss 6.156493640937918e-07 average time 0.0029955914000356645 iter num 100\n",
            "loss 6.156221218959889e-07 average time 0.0030563620200155127 iter num 200\n",
            "loss 6.15484685327346e-07 average time 0.0029818931300133046 iter num 100\n",
            "loss 6.154607130135369e-07 average time 0.0029902940050146754 iter num 200\n",
            "loss 6.153237687613161e-07 average time 0.0030546374700043087 iter num 100\n",
            "loss 6.152978189222192e-07 average time 0.0030238690250075704 iter num 200\n",
            "loss 6.151625466239941e-07 average time 0.0031141888800175366 iter num 100\n",
            "loss 6.151372441397222e-07 average time 0.0031303366600150184 iter num 200\n",
            "loss 6.150010488537879e-07 average time 0.00307476682000015 iter num 100\n",
            "loss 6.149733796741365e-07 average time 0.003025878219993956 iter num 200\n",
            "loss 6.148348363479226e-07 average time 0.0029809471499947903 iter num 100\n",
            "loss 6.148137796336998e-07 average time 0.0029757297099899916 iter num 200\n",
            "loss 6.146750095103683e-07 average time 0.002975989039991873 iter num 100\n",
            "loss 6.146501668544251e-07 average time 0.0031166789750113823 iter num 200\n",
            "loss 6.145159930877404e-07 average time 0.0030415984700130137 iter num 100\n",
            "loss 6.144885884905181e-07 average time 0.003035808119996091 iter num 200\n",
            "loss 6.143518827495968e-07 average time 0.0031933884599857267 iter num 100\n",
            "loss 6.14328599635631e-07 average time 0.0030841925550112136 iter num 200\n",
            "loss 6.141901001111692e-07 average time 0.0030108472900110426 iter num 100\n",
            "loss 6.141660430558234e-07 average time 0.00307487941001682 iter num 200\n",
            "loss 6.140300089936172e-07 average time 0.0029855484500103556 iter num 100\n",
            "loss 6.140027951661239e-07 average time 0.002983480354992025 iter num 200\n",
            "loss 6.138672833132326e-07 average time 0.0032012084599864466 iter num 100\n",
            "loss 6.138422101099622e-07 average time 0.003268761429992537 iter num 200\n",
            "loss 6.137073439473185e-07 average time 0.003021684459995413 iter num 100\n",
            "loss 6.13682801331641e-07 average time 0.003047595555001408 iter num 200\n",
            "loss 6.135443351754347e-07 average time 0.0032178632300156095 iter num 100\n",
            "loss 6.135194914250483e-07 average time 0.003226225770013116 iter num 200\n",
            "loss 6.133838235733706e-07 average time 0.0031372241700182713 iter num 100\n",
            "loss 6.133588449558332e-07 average time 0.003168978860014704 iter num 200\n",
            "loss 6.132210115185314e-07 average time 0.00313234631999876 iter num 100\n",
            "loss 6.131972507925491e-07 average time 0.0030964119150121406 iter num 200\n",
            "loss 6.130611966003644e-07 average time 0.003175308019986005 iter num 100\n",
            "loss 6.130390760807026e-07 average time 0.0031971323049970126 iter num 200\n",
            "loss 6.129018204376359e-07 average time 0.0031480817999863576 iter num 100\n",
            "loss 6.12876871585818e-07 average time 0.0031639834799875644 iter num 200\n",
            "loss 6.127383841049474e-07 average time 0.0029599431299993737 iter num 100\n",
            "loss 6.127140852508003e-07 average time 0.0029611886200109436 iter num 200\n",
            "loss 6.125790014272685e-07 average time 0.003192643920019691 iter num 100\n",
            "loss 6.125558855576219e-07 average time 0.003106099160015674 iter num 200\n",
            "loss 6.124174286861017e-07 average time 0.0029856017800193512 iter num 100\n",
            "loss 6.123928884774137e-07 average time 0.0031052170850239236 iter num 200\n",
            "loss 6.122606732088765e-07 average time 0.0029970843400133162 iter num 100\n",
            "loss 6.122345849656747e-07 average time 0.002966455044995655 iter num 200\n",
            "loss 6.120984123943271e-07 average time 0.0029924829399897135 iter num 100\n",
            "loss 6.120742382935505e-07 average time 0.003042088169991075 iter num 200\n",
            "loss 6.119378594360631e-07 average time 0.0029658638099817835 iter num 100\n",
            "loss 6.119126495370556e-07 average time 0.0030234993000044594 iter num 200\n",
            "loss 6.117775470815407e-07 average time 0.003025381069987816 iter num 100\n",
            "loss 6.117540024495945e-07 average time 0.0030256990900034 iter num 200\n",
            "loss 6.116163004400171e-07 average time 0.003043415370002549 iter num 100\n",
            "loss 6.115915948358686e-07 average time 0.0030456447949950417 iter num 200\n",
            "loss 6.114567901628394e-07 average time 0.003003115210017313 iter num 100\n",
            "loss 6.114329571306615e-07 average time 0.0029950928350149296 iter num 200\n",
            "loss 6.112941309288657e-07 average time 0.0030116171000145187 iter num 100\n",
            "loss 6.112729323188949e-07 average time 0.003004338015000485 iter num 200\n",
            "loss 6.111368848034891e-07 average time 0.0029743591899841705 iter num 100\n",
            "loss 6.111122338929455e-07 average time 0.003054228644982686 iter num 200\n",
            "loss 6.109772795886057e-07 average time 0.003060991829993327 iter num 100\n",
            "loss 6.109531290163051e-07 average time 0.0030328150750028727 iter num 200\n",
            "loss 6.10817998715119e-07 average time 0.002990937579988895 iter num 100\n",
            "loss 6.107924866668608e-07 average time 0.0030654357100002015 iter num 200\n",
            "loss 6.106563414396302e-07 average time 0.0030745945499984374 iter num 100\n",
            "loss 6.106310552251434e-07 average time 0.0030140405849965646 iter num 200\n",
            "loss 6.104981095920277e-07 average time 0.003066398780010786 iter num 100\n",
            "loss 6.104735091554374e-07 average time 0.003027423375012859 iter num 200\n",
            "loss 6.103396969482628e-07 average time 0.0030513080500077193 iter num 100\n",
            "loss 6.103145264713892e-07 average time 0.003149609910003619 iter num 200\n",
            "loss 6.101782202633678e-07 average time 0.00302692407001814 iter num 100\n",
            "loss 6.101547795244611e-07 average time 0.003004655200004436 iter num 200\n",
            "loss 6.100195526844493e-07 average time 0.003076934609980526 iter num 100\n",
            "loss 6.099959011134176e-07 average time 0.0030446671599816 iter num 200\n",
            "loss 6.098590743981964e-07 average time 0.0030062680399896634 iter num 100\n",
            "loss 6.09836382205625e-07 average time 0.0030003806549939328 iter num 200\n",
            "loss 6.097005955236104e-07 average time 0.0030363842400129215 iter num 100\n",
            "loss 6.096778670112475e-07 average time 0.003065562214999318 iter num 200\n",
            "loss 6.095430710567839e-07 average time 0.003127405709988125 iter num 100\n",
            "loss 6.095184267300448e-07 average time 0.0030741364499931477 iter num 200\n",
            "loss 6.093825635633913e-07 average time 0.0029557701699968676 iter num 100\n",
            "loss 6.093602462568521e-07 average time 0.002962964875002854 iter num 200\n",
            "loss 6.092240206423664e-07 average time 0.0030946885599769302 iter num 100\n",
            "loss 6.092006969690709e-07 average time 0.0030250583599763558 iter num 200\n",
            "loss 6.090637060608956e-07 average time 0.0029617684000004373 iter num 100\n",
            "loss 6.09042554989488e-07 average time 0.003019535149985586 iter num 200\n",
            "loss 6.089078017801015e-07 average time 0.0030180094600200393 iter num 100\n",
            "loss 6.088822137552522e-07 average time 0.0029977934249927783 iter num 200\n",
            "loss 6.087504356790245e-07 average time 0.0029858995899667206 iter num 100\n",
            "loss 6.087258901000859e-07 average time 0.0030701999049733784 iter num 200\n",
            "loss 6.08592215209443e-07 average time 0.0031052826500126683 iter num 100\n",
            "loss 6.085663101112533e-07 average time 0.0030757670600178246 iter num 200\n",
            "loss 6.084329121773206e-07 average time 0.0032595209300143326 iter num 100\n",
            "loss 6.084079861397677e-07 average time 0.0031226097850003496 iter num 200\n",
            "loss 6.082735658336735e-07 average time 0.003002735919976658 iter num 100\n",
            "loss 6.082509554052754e-07 average time 0.003117480139972031 iter num 200\n",
            "loss 6.081167197121566e-07 average time 0.0029648562799911814 iter num 100\n",
            "loss 6.080902693929151e-07 average time 0.002979224509999767 iter num 200\n",
            "loss 6.079556919534812e-07 average time 0.0029658031900180503 iter num 100\n",
            "loss 6.079316653218689e-07 average time 0.002973419465006373 iter num 200\n",
            "loss 6.077995338527596e-07 average time 0.0029808692300230177 iter num 100\n",
            "loss 6.077764848315343e-07 average time 0.00299445705000835 iter num 200\n",
            "loss 6.076418962744108e-07 average time 0.0030219926099653094 iter num 100\n",
            "loss 6.076161960968816e-07 average time 0.0030545562199881717 iter num 200\n",
            "loss 6.074822867078397e-07 average time 0.0031084140999837473 iter num 100\n",
            "loss 6.07457927416026e-07 average time 0.003191026539984705 iter num 200\n",
            "loss 6.073253600905045e-07 average time 0.003122053079991929 iter num 100\n",
            "loss 6.0730219327793e-07 average time 0.003048712899999373 iter num 200\n",
            "loss 6.071684064545236e-07 average time 0.0033035570800120694 iter num 100\n",
            "loss 6.071454419509838e-07 average time 0.0032047738750088685 iter num 200\n",
            "loss 6.07011054629008e-07 average time 0.0030221244799759008 iter num 100\n",
            "loss 6.069860893715903e-07 average time 0.0030815527299796488 iter num 200\n",
            "loss 6.068510311795526e-07 average time 0.0030841524600009507 iter num 100\n",
            "loss 6.06826012589579e-07 average time 0.0030380859399974725 iter num 200\n",
            "loss 6.066933023863027e-07 average time 0.0032485514499876445 iter num 100\n",
            "loss 6.066695561833304e-07 average time 0.003229668469994067 iter num 200\n",
            "loss 6.065380420313339e-07 average time 0.00312978337999084 iter num 100\n",
            "loss 6.065149127405981e-07 average time 0.0030693971600089754 iter num 200\n",
            "loss 6.063802001195188e-07 average time 0.0029887874399810244 iter num 100\n",
            "loss 6.063559852638146e-07 average time 0.0030314476599937732 iter num 200\n",
            "loss 6.062226918076072e-07 average time 0.0030040539400170018 iter num 100\n",
            "loss 6.061977529973625e-07 average time 0.002985639330020149 iter num 200\n",
            "loss 6.060660568691604e-07 average time 0.0029902875799916726 iter num 100\n",
            "loss 6.060421195031409e-07 average time 0.0030069010900092507 iter num 200\n",
            "loss 6.059096909817692e-07 average time 0.003026735120020021 iter num 100\n",
            "loss 6.058845626874658e-07 average time 0.0030391004200168935 iter num 200\n",
            "loss 6.057542449889847e-07 average time 0.0031702296700177614 iter num 100\n",
            "loss 6.057285724083251e-07 average time 0.0031016640250027196 iter num 200\n",
            "loss 6.055954300305699e-07 average time 0.002983494569994036 iter num 100\n",
            "loss 6.055726195849319e-07 average time 0.002978735480000978 iter num 200\n",
            "loss 6.054383561621137e-07 average time 0.0032144357400329684 iter num 100\n",
            "loss 6.054147064436396e-07 average time 0.0031044608750039515 iter num 200\n",
            "loss 6.052820661121765e-07 average time 0.0030215144000067086 iter num 100\n",
            "loss 6.052564378675328e-07 average time 0.0030349337750158155 iter num 200\n",
            "loss 6.051268400125755e-07 average time 0.0030081452800277473 iter num 100\n",
            "loss 6.051020730159968e-07 average time 0.0030109092350130593 iter num 200\n",
            "loss 6.049703407754989e-07 average time 0.0030126293199737118 iter num 100\n",
            "loss 6.049451701762383e-07 average time 0.003087731964988052 iter num 200\n",
            "loss 6.048141734066455e-07 average time 0.003007004390019574 iter num 100\n",
            "loss 6.047901688232288e-07 average time 0.003013914954995016 iter num 200\n",
            "loss 6.046565054299053e-07 average time 0.0031571806000283685 iter num 100\n",
            "loss 6.046347046235174e-07 average time 0.0031586234700012026 iter num 200\n",
            "loss 6.04500737465637e-07 average time 0.002975993160002872 iter num 100\n",
            "loss 6.044765599760029e-07 average time 0.003000992265012883 iter num 200\n",
            "loss 6.043429730754616e-07 average time 0.0029963321299874223 iter num 100\n",
            "loss 6.043193068232499e-07 average time 0.0029876332549861216 iter num 200\n",
            "loss 6.041872979217917e-07 average time 0.0030029805999856763 iter num 100\n",
            "loss 6.041658777056128e-07 average time 0.003006680289995529 iter num 200\n",
            "loss 6.040326950949334e-07 average time 0.003062360559993067 iter num 100\n",
            "loss 6.040088144572183e-07 average time 0.003021035050005594 iter num 200\n",
            "loss 6.038790862189697e-07 average time 0.003086613010032124 iter num 100\n",
            "loss 6.038529017452374e-07 average time 0.00317921180500889 iter num 200\n",
            "loss 6.037192443818472e-07 average time 0.0031433231499795513 iter num 100\n",
            "loss 6.036971418069909e-07 average time 0.0031302164449812154 iter num 200\n",
            "loss 6.035653025461549e-07 average time 0.0029797714599953907 iter num 100\n",
            "loss 6.035411583656887e-07 average time 0.003055975739985115 iter num 200\n",
            "loss 6.034089626865561e-07 average time 0.003017020060010509 iter num 100\n",
            "loss 6.033862040068729e-07 average time 0.003109865310018449 iter num 200\n",
            "loss 6.03251699379079e-07 average time 0.0031275664900203993 iter num 100\n",
            "loss 6.032299879883922e-07 average time 0.0030836726250072386 iter num 200\n",
            "loss 6.030983723103614e-07 average time 0.003005192990026444 iter num 100\n",
            "loss 6.030751599466057e-07 average time 0.0030143537950152676 iter num 200\n",
            "loss 6.029416561297736e-07 average time 0.0030528673899834755 iter num 100\n",
            "loss 6.029191749435101e-07 average time 0.003020042130008278 iter num 200\n",
            "loss 6.027861573293163e-07 average time 0.003006830339982116 iter num 100\n",
            "loss 6.027636947252311e-07 average time 0.0030695399899786937 iter num 200\n",
            "loss 6.026334886166996e-07 average time 0.002979121620032856 iter num 100\n",
            "loss 6.026085452836588e-07 average time 0.0029879675850042987 iter num 200\n",
            "loss 6.024756440801039e-07 average time 0.0030069990799802327 iter num 100\n",
            "loss 6.024556167643395e-07 average time 0.003077771019979991 iter num 200\n",
            "loss 6.023208453877038e-07 average time 0.0030694634600058634 iter num 100\n",
            "loss 6.023005929065706e-07 average time 0.0030689454249863955 iter num 200\n",
            "loss 6.021666526044724e-07 average time 0.0029618522000055235 iter num 100\n",
            "loss 6.021451054196861e-07 average time 0.0029806082750087628 iter num 200\n",
            "loss 6.020133098779416e-07 average time 0.0030000407099851144 iter num 100\n",
            "loss 6.01989865022999e-07 average time 0.0030655196099860405 iter num 200\n",
            "loss 6.018576325819009e-07 average time 0.003012864939978499 iter num 100\n",
            "loss 6.018339093123175e-07 average time 0.003053658044991607 iter num 200\n",
            "loss 6.017029019331892e-07 average time 0.003273060780029482 iter num 100\n",
            "loss 6.016800255625928e-07 average time 0.0031452990550224057 iter num 200\n",
            "loss 6.015480922241004e-07 average time 0.003036846320001132 iter num 100\n",
            "loss 6.015261138089462e-07 average time 0.003038937049991546 iter num 200\n",
            "loss 6.013949087133572e-07 average time 0.0030193565599938663 iter num 100\n",
            "loss 6.013707603363442e-07 average time 0.0029978201949734285 iter num 200\n",
            "loss 6.012397126315906e-07 average time 0.0030012425299901224 iter num 100\n",
            "loss 6.012145851792882e-07 average time 0.0030067496649826353 iter num 200\n",
            "loss 6.010852249460172e-07 average time 0.002980209430020295 iter num 100\n",
            "loss 6.010636686317328e-07 average time 0.0029773869650125562 iter num 200\n",
            "loss 6.00930895811107e-07 average time 0.0029661537500032865 iter num 100\n",
            "loss 6.009095335679982e-07 average time 0.0030245210249950105 iter num 200\n",
            "loss 6.007765751594064e-07 average time 0.0029902943899969615 iter num 100\n",
            "loss 6.007516915097636e-07 average time 0.0030274276099885354 iter num 200\n",
            "loss 6.006223583006362e-07 average time 0.002997455849999824 iter num 100\n",
            "loss 6.005993896953188e-07 average time 0.002991951155006518 iter num 200\n",
            "loss 6.004679399021107e-07 average time 0.0030012792799561793 iter num 100\n",
            "loss 6.004467999806737e-07 average time 0.0029935157299905766 iter num 200\n",
            "loss 6.003148602100077e-07 average time 0.00313983287996507 iter num 100\n",
            "loss 6.002923219063873e-07 average time 0.0031277987949852103 iter num 200\n",
            "loss 6.001609580954037e-07 average time 0.002952912659998219 iter num 100\n",
            "loss 6.001358378681436e-07 average time 0.002993448289994376 iter num 200\n",
            "loss 6.000052593584839e-07 average time 0.0030903890199624586 iter num 100\n",
            "loss 5.999842667757062e-07 average time 0.003046973339990018 iter num 200\n",
            "loss 5.998539440363853e-07 average time 0.0029788362899898857 iter num 100\n",
            "loss 5.998301717777826e-07 average time 0.002974301479987389 iter num 200\n",
            "loss 5.996996790958388e-07 average time 0.003270026089994644 iter num 100\n",
            "loss 5.996766612787957e-07 average time 0.003139343234986427 iter num 200\n",
            "loss 5.995465151903054e-07 average time 0.0029812860399897547 iter num 100\n",
            "loss 5.995239866670619e-07 average time 0.0029858625749943713 iter num 200\n",
            "loss 5.993920029155261e-07 average time 0.0030653881399757667 iter num 100\n",
            "loss 5.993697846197461e-07 average time 0.0030164184899763315 iter num 200\n",
            "loss 5.992383806464145e-07 average time 0.0031541775999585297 iter num 100\n",
            "loss 5.99217011734383e-07 average time 0.0031114238049872256 iter num 200\n",
            "loss 5.990865439117605e-07 average time 0.0030204243599928305 iter num 100\n",
            "loss 5.990634707602799e-07 average time 0.0030332996900301625 iter num 200\n",
            "loss 5.989331537119261e-07 average time 0.0029779574399935883 iter num 100\n",
            "loss 5.989105129764707e-07 average time 0.0030548593699882077 iter num 200\n",
            "loss 5.987797138161695e-07 average time 0.0029957215400145286 iter num 100\n",
            "loss 5.987561676880973e-07 average time 0.0029881304050127255 iter num 200\n",
            "loss 5.986254927985816e-07 average time 0.003013834929984114 iter num 100\n",
            "loss 5.98600991721015e-07 average time 0.0029928488749965255 iter num 200\n",
            "loss 5.984731251400784e-07 average time 0.003030006819949449 iter num 100\n",
            "loss 5.984501618229258e-07 average time 0.0030578064299720607 iter num 200\n",
            "loss 5.983239123221151e-07 average time 0.0029605681399561944 iter num 100\n",
            "loss 5.982983472673684e-07 average time 0.0029710066299753635 iter num 200\n",
            "loss 5.981690323195041e-07 average time 0.0029982395600018207 iter num 100\n",
            "loss 5.981467179856515e-07 average time 0.003000619430004008 iter num 200\n",
            "loss 5.980159871441526e-07 average time 0.0029702497099833637 iter num 100\n",
            "loss 5.979928294848501e-07 average time 0.0030319257949849997 iter num 200\n",
            "loss 5.9786446716157e-07 average time 0.003009137950020886 iter num 100\n",
            "loss 5.978431592490331e-07 average time 0.0029984416199977204 iter num 200\n",
            "loss 5.97710876686682e-07 average time 0.003024222779986303 iter num 100\n",
            "loss 5.976892863248764e-07 average time 0.003003789109993704 iter num 200\n",
            "loss 5.975610735749778e-07 average time 0.0029704287399999886 iter num 100\n",
            "loss 5.975375837082883e-07 average time 0.003038614980018792 iter num 200\n",
            "loss 5.97406701032856e-07 average time 0.0029717440000104033 iter num 100\n",
            "loss 5.973836189046661e-07 average time 0.0029921029099978114 iter num 200\n",
            "loss 5.972545520831622e-07 average time 0.003118553080012134 iter num 100\n",
            "loss 5.972319954579507e-07 average time 0.003037308530008431 iter num 200\n",
            "loss 5.971036992569705e-07 average time 0.002993525709985079 iter num 100\n",
            "loss 5.970779331586695e-07 average time 0.0030017871449854283 iter num 200\n",
            "loss 5.969515978838128e-07 average time 0.0029958510300139097 iter num 100\n",
            "loss 5.969290799662027e-07 average time 0.0030673006900087786 iter num 200\n",
            "loss 5.967982046833346e-07 average time 0.0030903534200069772 iter num 100\n",
            "loss 5.967756663418493e-07 average time 0.0030341809650099093 iter num 200\n",
            "loss 5.966468926260708e-07 average time 0.003064132850004171 iter num 100\n",
            "loss 5.966240103397026e-07 average time 0.0030316877350151116 iter num 200\n",
            "loss 5.964948028562475e-07 average time 0.002967707719972168 iter num 100\n",
            "loss 5.964714402839387e-07 average time 0.0029754046199832376 iter num 200\n",
            "loss 5.963423420555814e-07 average time 0.003113575169982141 iter num 100\n",
            "loss 5.96322105321406e-07 average time 0.0030555451549798816 iter num 200\n",
            "loss 5.961919381167477e-07 average time 0.002987084259971198 iter num 100\n",
            "loss 5.961681261086597e-07 average time 0.0031203398299862782 iter num 200\n",
            "loss 5.960397741269283e-07 average time 0.002991401599992969 iter num 100\n",
            "loss 5.960170398113332e-07 average time 0.0029880110399972183 iter num 200\n",
            "loss 5.958893760783511e-07 average time 0.003029326540022339 iter num 100\n",
            "loss 5.958659437450231e-07 average time 0.002994384470014211 iter num 200\n",
            "loss 5.957361658421948e-07 average time 0.0031534637599907 iter num 100\n",
            "loss 5.95713899726479e-07 average time 0.0031645822450013837 iter num 200\n",
            "loss 5.955856306926457e-07 average time 0.003057960430037383 iter num 100\n",
            "loss 5.955636893156618e-07 average time 0.0030210786000134248 iter num 200\n",
            "loss 5.954346634211142e-07 average time 0.002976503780009807 iter num 100\n",
            "loss 5.954127433651369e-07 average time 0.0029853660799972204 iter num 200\n",
            "loss 5.952821914336236e-07 average time 0.0030876464499897337 iter num 100\n",
            "loss 5.952595819413604e-07 average time 0.003051215319997027 iter num 200\n",
            "loss 5.951330202439363e-07 average time 0.003161636209979406 iter num 100\n",
            "loss 5.951107557526896e-07 average time 0.0031070364099991823 iter num 200\n",
            "loss 5.949815218586943e-07 average time 0.0029805847299985543 iter num 100\n",
            "loss 5.949570943323103e-07 average time 0.0030128389749825147 iter num 200\n",
            "loss 5.948328258410063e-07 average time 0.0030584115100236887 iter num 100\n",
            "loss 5.948075795425659e-07 average time 0.003098663650011986 iter num 200\n",
            "loss 5.946819182351804e-07 average time 0.0030152664999968694 iter num 100\n",
            "loss 5.946571795665799e-07 average time 0.0030191272099955313 iter num 200\n",
            "loss 5.945297743117632e-07 average time 0.003073376920033297 iter num 100\n",
            "loss 5.94506740515203e-07 average time 0.0030960322550185994 iter num 200\n",
            "loss 5.94378974196011e-07 average time 0.003051298790001056 iter num 100\n",
            "loss 5.943546760969422e-07 average time 0.003021766590009065 iter num 200\n",
            "loss 5.942273812534371e-07 average time 0.0029819096400024135 iter num 100\n",
            "loss 5.94204395162802e-07 average time 0.0030864337650041308 iter num 200\n",
            "loss 5.940790069803356e-07 average time 0.003073820639974656 iter num 100\n",
            "loss 5.940544638475804e-07 average time 0.0030266929299909862 iter num 200\n",
            "loss 5.939288650693164e-07 average time 0.0030862275000117733 iter num 100\n",
            "loss 5.939046895850626e-07 average time 0.0031002623049880638 iter num 200\n",
            "loss 5.937774426214958e-07 average time 0.003010022290022789 iter num 100\n",
            "loss 5.937549876222543e-07 average time 0.003041912495004908 iter num 200\n",
            "loss 5.936281121768506e-07 average time 0.003196022390006874 iter num 100\n",
            "loss 5.936049629948523e-07 average time 0.0031035125800121933 iter num 200\n",
            "loss 5.934783974052835e-07 average time 0.003349394400011079 iter num 100\n",
            "loss 5.934529896285666e-07 average time 0.0032362897300026814 iter num 200\n",
            "loss 5.933275995742755e-07 average time 0.0031410441299703964 iter num 100\n",
            "loss 5.933045279820942e-07 average time 0.0030818464650064926 iter num 200\n",
            "loss 5.931795366400874e-07 average time 0.0030957610299765293 iter num 100\n",
            "loss 5.931545665276133e-07 average time 0.0030655170700015335 iter num 200\n",
            "loss 5.930285143943362e-07 average time 0.0029937116100154527 iter num 100\n",
            "loss 5.930053493554246e-07 average time 0.0030708791600068254 iter num 200\n",
            "loss 5.928756209542373e-07 average time 0.0030048334900038753 iter num 100\n",
            "loss 5.928524066678728e-07 average time 0.0029891353399989383 iter num 200\n",
            "loss 5.927283592483515e-07 average time 0.0030611180399955627 iter num 100\n",
            "loss 5.927054183590309e-07 average time 0.0031215628099948846 iter num 200\n",
            "loss 5.925769680869368e-07 average time 0.0030605284400007805 iter num 100\n",
            "loss 5.925539779763157e-07 average time 0.0030616233200225908 iter num 200\n",
            "loss 5.924273572538706e-07 average time 0.0031344570500141345 iter num 100\n",
            "loss 5.924072191187742e-07 average time 0.0031126090900056623 iter num 200\n",
            "loss 5.922793831916768e-07 average time 0.002969902909985649 iter num 100\n",
            "loss 5.922574095617693e-07 average time 0.0030116042500117146 iter num 200\n",
            "loss 5.921289159186489e-07 average time 0.003104669820008894 iter num 100\n",
            "loss 5.921070717158429e-07 average time 0.0030667410749902046 iter num 200\n",
            "loss 5.919790614806687e-07 average time 0.003262138210016019 iter num 100\n",
            "loss 5.919597760021012e-07 average time 0.0031790203200057474 iter num 200\n",
            "loss 5.918289693301528e-07 average time 0.003071700770019561 iter num 100\n",
            "loss 5.918069461177234e-07 average time 0.003174455020002824 iter num 200\n",
            "loss 5.916815487751906e-07 average time 0.003227213390009638 iter num 100\n",
            "loss 5.916592518038241e-07 average time 0.0031187757350039647 iter num 200\n",
            "loss 5.91534178340648e-07 average time 0.003029499700010092 iter num 100\n",
            "loss 5.915105898727433e-07 average time 0.003130881395002234 iter num 200\n",
            "loss 5.913839652288557e-07 average time 0.0031002631700175697 iter num 100\n",
            "loss 5.913622969613635e-07 average time 0.003081403985038378 iter num 200\n",
            "loss 5.912354433532181e-07 average time 0.0030364904399993974 iter num 100\n",
            "loss 5.912108723155458e-07 average time 0.003091961705001722 iter num 200\n",
            "loss 5.910859616265701e-07 average time 0.003001617359982447 iter num 100\n",
            "loss 5.910627362891536e-07 average time 0.00298713804500494 iter num 200\n",
            "loss 5.909382766718483e-07 average time 0.0029906793099917194 iter num 100\n",
            "loss 5.909143647419269e-07 average time 0.002982819289993586 iter num 200\n",
            "loss 5.907879365936833e-07 average time 0.003064435430010235 iter num 100\n",
            "loss 5.907670543568052e-07 average time 0.0030227916649914733 iter num 200\n",
            "loss 5.906387026021419e-07 average time 0.0030386454899917225 iter num 100\n",
            "loss 5.906175570682641e-07 average time 0.0031067919349902695 iter num 200\n",
            "loss 5.90492322338364e-07 average time 0.0030112206399644493 iter num 100\n",
            "loss 5.904688321681784e-07 average time 0.0029955157899871666 iter num 200\n",
            "loss 5.903444988489614e-07 average time 0.002986077969994767 iter num 100\n",
            "loss 5.903209201845636e-07 average time 0.003018088869989697 iter num 200\n",
            "loss 5.901933385777365e-07 average time 0.002953252140000586 iter num 100\n",
            "loss 5.90173927566837e-07 average time 0.0029744078599969726 iter num 200\n",
            "loss 5.900472013403544e-07 average time 0.002990021399996294 iter num 100\n",
            "loss 5.900236085255398e-07 average time 0.002999696844974551 iter num 200\n",
            "loss 5.898978674939643e-07 average time 0.0029976341699466504 iter num 100\n",
            "loss 5.898762510898678e-07 average time 0.003035855759981132 iter num 200\n",
            "loss 5.897486823935016e-07 average time 0.003077577489975738 iter num 100\n",
            "loss 5.897280676225331e-07 average time 0.0030481951599858805 iter num 200\n",
            "loss 5.896024772509008e-07 average time 0.0030885819399918547 iter num 100\n",
            "loss 5.895814349130067e-07 average time 0.0031032576149959824 iter num 200\n",
            "loss 5.894537463676291e-07 average time 0.0029872828499992467 iter num 100\n",
            "loss 5.894338964592047e-07 average time 0.003033715145011229 iter num 200\n",
            "loss 5.893078805781848e-07 average time 0.002958044169990899 iter num 100\n",
            "loss 5.89285518756723e-07 average time 0.003006008444997406 iter num 200\n",
            "loss 5.891574774024857e-07 average time 0.003025582820000636 iter num 100\n",
            "loss 5.891364198289804e-07 average time 0.0031137617899980796 iter num 200\n",
            "loss 5.89011018765209e-07 average time 0.0029862647300205936 iter num 100\n",
            "loss 5.889906027120191e-07 average time 0.002977251925019573 iter num 200\n",
            "loss 5.888618398552991e-07 average time 0.0031174167700055476 iter num 100\n",
            "loss 5.888403058798708e-07 average time 0.0030400939500100322 iter num 200\n",
            "loss 5.887159632073754e-07 average time 0.002958224730027723 iter num 100\n",
            "loss 5.886950872334317e-07 average time 0.0030170268000051692 iter num 200\n",
            "loss 5.885695047800217e-07 average time 0.0030596188900153722 iter num 100\n",
            "loss 5.885484086263305e-07 average time 0.0030478238149930803 iter num 200\n",
            "loss 5.884206769422024e-07 average time 0.0030825892800339714 iter num 100\n",
            "loss 5.883985805732671e-07 average time 0.0030174917150043255 iter num 200\n",
            "loss 5.882735702802434e-07 average time 0.002969168540021201 iter num 100\n",
            "loss 5.882511674837805e-07 average time 0.00296699217000878 iter num 200\n",
            "loss 5.881260621139142e-07 average time 0.0030775223700129574 iter num 100\n",
            "loss 5.881038864476264e-07 average time 0.003029670910009372 iter num 200\n",
            "loss 5.879763146482233e-07 average time 0.003015197730001091 iter num 100\n",
            "loss 5.879569644172351e-07 average time 0.0029934129300113453 iter num 200\n",
            "loss 5.878331869772067e-07 average time 0.0030641859799925442 iter num 100\n",
            "loss 5.87809415756701e-07 average time 0.003034294379999665 iter num 200\n",
            "loss 5.876849916013607e-07 average time 0.003083185720001893 iter num 100\n",
            "loss 5.876642942852683e-07 average time 0.0031209740450026404 iter num 200\n",
            "loss 5.875413321651926e-07 average time 0.0030979166699898995 iter num 100\n",
            "loss 5.875169032913233e-07 average time 0.00305749545497747 iter num 200\n",
            "loss 5.873917267568086e-07 average time 0.003014926110054148 iter num 100\n",
            "loss 5.873684843906331e-07 average time 0.003042741095034671 iter num 200\n",
            "loss 5.872449598872991e-07 average time 0.003051850359988748 iter num 100\n",
            "loss 5.872245965545762e-07 average time 0.0031926233949980086 iter num 200\n",
            "loss 5.870992709938032e-07 average time 0.002985700389981503 iter num 100\n",
            "loss 5.870774098992108e-07 average time 0.0030094088449982336 iter num 200\n",
            "loss 5.869528099530362e-07 average time 0.00299999537999156 iter num 100\n",
            "loss 5.869291627819784e-07 average time 0.002989689509984146 iter num 200\n",
            "loss 5.868068701307386e-07 average time 0.0029590744899678613 iter num 100\n",
            "loss 5.867835505854883e-07 average time 0.0030799744849923628 iter num 200\n",
            "loss 5.866597682888481e-07 average time 0.0031211579599357718 iter num 100\n",
            "loss 5.866362877540732e-07 average time 0.003124875514965879 iter num 200\n",
            "loss 5.865149746470785e-07 average time 0.002963556850008899 iter num 100\n",
            "loss 5.864920021734362e-07 average time 0.0029835410450118616 iter num 200\n",
            "loss 5.863652435735411e-07 average time 0.0030800969199935935 iter num 100\n",
            "loss 5.863427135399874e-07 average time 0.003044200700012425 iter num 200\n",
            "loss 5.862192029170652e-07 average time 0.0030985199000269858 iter num 100\n",
            "loss 5.861975375761885e-07 average time 0.003131398505022389 iter num 200\n",
            "loss 5.860734231397556e-07 average time 0.0030998432499745833 iter num 100\n",
            "loss 5.860524191799138e-07 average time 0.0030617395250033042 iter num 200\n",
            "loss 5.859268123271718e-07 average time 0.0030140649100349037 iter num 100\n",
            "loss 5.859046076449417e-07 average time 0.0030442995850216903 iter num 200\n",
            "loss 5.857840839310686e-07 average time 0.0031918459100097607 iter num 100\n",
            "loss 5.857607717686992e-07 average time 0.003101923215019724 iter num 200\n",
            "loss 5.856360430921726e-07 average time 0.0029887397399807015 iter num 100\n",
            "loss 5.856144000852294e-07 average time 0.003031609649992788 iter num 200\n",
            "loss 5.854902748653433e-07 average time 0.0030230443300160916 iter num 100\n",
            "loss 5.854670011676156e-07 average time 0.00300457523499972 iter num 200\n",
            "loss 5.853469819503536e-07 average time 0.003087472710003567 iter num 100\n",
            "loss 5.853232818976614e-07 average time 0.0030552018549951754 iter num 200\n",
            "loss 5.852011435713258e-07 average time 0.0031271310699867174 iter num 100\n",
            "loss 5.851770489617212e-07 average time 0.0031395848799957095 iter num 200\n",
            "loss 5.850524000023226e-07 average time 0.003016204879959332 iter num 100\n",
            "loss 5.850321979468656e-07 average time 0.003062534709974898 iter num 200\n",
            "loss 5.849090979045832e-07 average time 0.0032759068400582693 iter num 100\n",
            "loss 5.848867971839007e-07 average time 0.003110892960039564 iter num 200\n",
            "loss 5.847639785336927e-07 average time 0.003117439660022683 iter num 100\n",
            "loss 5.84742948475436e-07 average time 0.003132783905004999 iter num 200\n",
            "loss 5.846182264233008e-07 average time 0.003027552570001717 iter num 100\n",
            "loss 5.84597848803507e-07 average time 0.0030197513700068158 iter num 200\n",
            "loss 5.844747795339571e-07 average time 0.0030995388799829014 iter num 100\n",
            "loss 5.844509791094488e-07 average time 0.0030390213349869555 iter num 200\n",
            "loss 5.843266599422055e-07 average time 0.002987237579995963 iter num 100\n",
            "loss 5.843065482983643e-07 average time 0.0030364149799925145 iter num 200\n",
            "loss 5.841830901707006e-07 average time 0.003284677749975344 iter num 100\n",
            "loss 5.841613747899183e-07 average time 0.0031492708699897777 iter num 200\n",
            "loss 5.840383268746035e-07 average time 0.0030371795799783285 iter num 100\n",
            "loss 5.840166054800778e-07 average time 0.0030314078149899617 iter num 200\n",
            "loss 5.838919567609186e-07 average time 0.003013948390025689 iter num 100\n",
            "loss 5.83871837947691e-07 average time 0.0030525182850033162 iter num 200\n",
            "loss 5.837486567043082e-07 average time 0.003039792529989427 iter num 100\n",
            "loss 5.837265746722922e-07 average time 0.0030028862700078205 iter num 200\n",
            "loss 5.836036999224548e-07 average time 0.0029829904600137524 iter num 100\n",
            "loss 5.835807720327962e-07 average time 0.003046259324987659 iter num 200\n",
            "loss 5.834590381069615e-07 average time 0.002989914490017327 iter num 100\n",
            "loss 5.834378862819241e-07 average time 0.003030644204991404 iter num 200\n",
            "loss 5.833131919359381e-07 average time 0.0029931342799955018 iter num 100\n",
            "loss 5.832933046311264e-07 average time 0.003087365449978279 iter num 200\n",
            "loss 5.831703063995005e-07 average time 0.002983656360020177 iter num 100\n",
            "loss 5.831485579509741e-07 average time 0.002971506220005722 iter num 200\n",
            "loss 5.830264918072959e-07 average time 0.003213134199977503 iter num 100\n",
            "loss 5.830046124666087e-07 average time 0.003101436034978633 iter num 200\n",
            "loss 5.828812201192554e-07 average time 0.003037240199973894 iter num 100\n",
            "loss 5.828602765327123e-07 average time 0.003022831969969957 iter num 200\n",
            "loss 5.827370605542166e-07 average time 0.0029699579500265826 iter num 100\n",
            "loss 5.827151616165238e-07 average time 0.002968697510007132 iter num 200\n",
            "loss 5.825937630649329e-07 average time 0.00302827984999567 iter num 100\n",
            "loss 5.825725869287297e-07 average time 0.003024357719998534 iter num 200\n",
            "loss 5.82448882432926e-07 average time 0.003000410660010857 iter num 100\n",
            "loss 5.824250890366659e-07 average time 0.0030903735299966685 iter num 200\n",
            "loss 5.823051345515548e-07 average time 0.00296939485999701 iter num 100\n",
            "loss 5.822825604453811e-07 average time 0.0029769159349871187 iter num 200\n",
            "loss 5.8215949282652e-07 average time 0.003025804450035139 iter num 100\n",
            "loss 5.821387031137993e-07 average time 0.0030819161550243736 iter num 200\n",
            "loss 5.820160819346209e-07 average time 0.0029640228799826217 iter num 100\n",
            "loss 5.819953388768508e-07 average time 0.0029903864599805275 iter num 200\n",
            "loss 5.818722844205106e-07 average time 0.0031201350100218406 iter num 100\n",
            "loss 5.818502251529874e-07 average time 0.0031007390750073683 iter num 200\n",
            "loss 5.81727328231755e-07 average time 0.0029954053700021175 iter num 100\n",
            "loss 5.817072258194473e-07 average time 0.003004512334987339 iter num 200\n",
            "loss 5.815848454278651e-07 average time 0.0030501343500054646 iter num 100\n",
            "loss 5.815632557915494e-07 average time 0.0030239169650053553 iter num 200\n",
            "loss 5.814418640460878e-07 average time 0.003006640969979344 iter num 100\n",
            "loss 5.814194287909782e-07 average time 0.0030236406649919445 iter num 200\n",
            "loss 5.812995802796987e-07 average time 0.002967332330017598 iter num 100\n",
            "loss 5.812781307482062e-07 average time 0.003103799009998056 iter num 200\n",
            "loss 5.811535113832757e-07 average time 0.003132382209987554 iter num 100\n",
            "loss 5.811322338817558e-07 average time 0.0030513083899904813 iter num 200\n",
            "loss 5.81012161017844e-07 average time 0.002952145889998974 iter num 100\n",
            "loss 5.809920483303939e-07 average time 0.002958316304991513 iter num 200\n",
            "loss 5.808691902603176e-07 average time 0.0030123662599862654 iter num 100\n",
            "loss 5.808481868680973e-07 average time 0.0030118303249992095 iter num 200\n",
            "loss 5.807249809501144e-07 average time 0.003004837000007683 iter num 100\n",
            "loss 5.807027959097846e-07 average time 0.0030393708100041293 iter num 200\n",
            "loss 5.805841251578279e-07 average time 0.003024437639992357 iter num 100\n",
            "loss 5.805618531122645e-07 average time 0.00302249751999625 iter num 200\n",
            "loss 5.804390798010311e-07 average time 0.0031126262799989492 iter num 100\n",
            "loss 5.804174770105193e-07 average time 0.00304411036998772 iter num 200\n",
            "loss 5.802951887080047e-07 average time 0.003007545760015091 iter num 100\n",
            "loss 5.802754413272748e-07 average time 0.0030664944900081537 iter num 200\n",
            "loss 5.801526678218134e-07 average time 0.0029587874199978616 iter num 100\n",
            "loss 5.801316163111514e-07 average time 0.0029751994750131417 iter num 200\n",
            "loss 5.800108220634685e-07 average time 0.0031517804999612055 iter num 100\n",
            "loss 5.799904312785541e-07 average time 0.0030784981149918167 iter num 200\n",
            "loss 5.798675898440908e-07 average time 0.002973262280002018 iter num 100\n",
            "loss 5.798453788495246e-07 average time 0.0030025718300112202 iter num 200\n",
            "loss 5.797246381728859e-07 average time 0.0029687991899891132 iter num 100\n",
            "loss 5.797044078426679e-07 average time 0.002971034109998527 iter num 200\n",
            "loss 5.795836542842689e-07 average time 0.0030121506700061217 iter num 100\n",
            "loss 5.795604972526683e-07 average time 0.002998308035014361 iter num 200\n",
            "loss 5.794385610323163e-07 average time 0.0029879898600029264 iter num 100\n",
            "loss 5.794174698692456e-07 average time 0.003052439599998706 iter num 200\n",
            "loss 5.792989458545591e-07 average time 0.0029686517599930083 iter num 100\n",
            "loss 5.792753238643025e-07 average time 0.003018425884997669 iter num 200\n",
            "loss 5.79154780285456e-07 average time 0.0029739627699837 iter num 100\n",
            "loss 5.791339111413077e-07 average time 0.003065355124999769 iter num 200\n",
            "loss 5.790122511352071e-07 average time 0.002943153910023284 iter num 100\n",
            "loss 5.789924866231077e-07 average time 0.0029794791550079936 iter num 200\n",
            "loss 5.788710211175582e-07 average time 0.002976156239978991 iter num 100\n",
            "loss 5.788516105556762e-07 average time 0.003017693474985208 iter num 200\n",
            "loss 5.78729959327989e-07 average time 0.00310534934998941 iter num 100\n",
            "loss 5.787054387601085e-07 average time 0.003097050944993498 iter num 200\n",
            "loss 5.785861893001542e-07 average time 0.003224320889999035 iter num 100\n",
            "loss 5.785634820694646e-07 average time 0.0031241923449806565 iter num 200\n",
            "loss 5.784435556608933e-07 average time 0.002999194989984062 iter num 100\n",
            "loss 5.784238764783822e-07 average time 0.0029990594049922946 iter num 200\n",
            "loss 5.783010962210319e-07 average time 0.00303032119999898 iter num 100\n",
            "loss 5.782811007368426e-07 average time 0.003036847845014563 iter num 200\n",
            "loss 5.781608741783463e-07 average time 0.003131915639978615 iter num 100\n",
            "loss 5.781392199706719e-07 average time 0.0031322592499714117 iter num 200\n",
            "loss 5.780202653026726e-07 average time 0.003087674720018185 iter num 100\n",
            "loss 5.779975550725099e-07 average time 0.003102808030005235 iter num 200\n",
            "loss 5.778788696747546e-07 average time 0.003015404559969284 iter num 100\n",
            "loss 5.778554009897496e-07 average time 0.0030457535299842674 iter num 200\n",
            "loss 5.777345775135876e-07 average time 0.003147918249983377 iter num 100\n",
            "loss 5.777147183076451e-07 average time 0.0031310050949855395 iter num 200\n",
            "loss 5.775966571199011e-07 average time 0.003317194170003859 iter num 100\n",
            "loss 5.775737534205248e-07 average time 0.003153166245010652 iter num 200\n",
            "loss 5.774533247750724e-07 average time 0.0029696163400012663 iter num 100\n",
            "loss 5.774320447913214e-07 average time 0.002962403849999191 iter num 200\n",
            "loss 5.77312079504037e-07 average time 0.0030579735799847185 iter num 100\n",
            "loss 5.772893854650904e-07 average time 0.0030109204099949237 iter num 200\n",
            "loss 5.771700434629815e-07 average time 0.0029784263899864526 iter num 100\n",
            "loss 5.771494943085113e-07 average time 0.0029814385899953775 iter num 200\n",
            "loss 5.770287235772917e-07 average time 0.0029654960899870276 iter num 100\n",
            "loss 5.770087836535718e-07 average time 0.002978132764990278 iter num 200\n",
            "loss 5.768879073159657e-07 average time 0.0030081614699702186 iter num 100\n",
            "loss 5.768663631077821e-07 average time 0.00305390186998693 iter num 200\n",
            "loss 5.767477761914829e-07 average time 0.0030371936499750517 iter num 100\n",
            "loss 5.76726590469579e-07 average time 0.0030435835899811537 iter num 200\n",
            "loss 5.766067492166635e-07 average time 0.0031118313600200054 iter num 100\n",
            "loss 5.765853310152563e-07 average time 0.0030528088200185268 iter num 200\n",
            "loss 5.764642473065462e-07 average time 0.002983027100012805 iter num 100\n",
            "loss 5.764424864710244e-07 average time 0.003058034870005031 iter num 200\n",
            "loss 5.763244479209909e-07 average time 0.0029741077499875245 iter num 100\n",
            "loss 5.763018180369229e-07 average time 0.002994344980002097 iter num 200\n",
            "loss 5.761831709703695e-07 average time 0.0030968482799926277 iter num 100\n",
            "loss 5.761613577114408e-07 average time 0.0030436563449984535 iter num 200\n",
            "loss 5.760415241448904e-07 average time 0.003186184739993223 iter num 100\n",
            "loss 5.760243319716384e-07 average time 0.0030877478449951924 iter num 200\n",
            "loss 5.759041574968274e-07 average time 0.0030023330399899352 iter num 100\n",
            "loss 5.758814777964877e-07 average time 0.0030122425949912213 iter num 200\n",
            "loss 5.757609306513805e-07 average time 0.0030120506299954287 iter num 100\n",
            "loss 5.757419472002583e-07 average time 0.002990322099994955 iter num 200\n",
            "loss 5.75622070676212e-07 average time 0.002971056409992343 iter num 100\n",
            "loss 5.75602617052215e-07 average time 0.003045244205004565 iter num 200\n",
            "loss 5.754808828504693e-07 average time 0.0029798974700179313 iter num 100\n",
            "loss 5.754603857969708e-07 average time 0.0029984918300101525 iter num 200\n",
            "loss 5.753413084401718e-07 average time 0.003078709670016906 iter num 100\n",
            "loss 5.753198437008171e-07 average time 0.003035090625007797 iter num 200\n",
            "loss 5.752003558442214e-07 average time 0.0029725560299948485 iter num 100\n",
            "loss 5.751801469847662e-07 average time 0.0029865540749892715 iter num 200\n",
            "loss 5.75059228232217e-07 average time 0.003000054909989558 iter num 100\n",
            "loss 5.750393886312445e-07 average time 0.002989011245003894 iter num 200\n",
            "loss 5.749201888843463e-07 average time 0.003038523469995198 iter num 100\n",
            "loss 5.749010469399997e-07 average time 0.0030559439899957397 iter num 200\n",
            "loss 5.74780215256181e-07 average time 0.003323317360013789 iter num 100\n",
            "loss 5.747579736559731e-07 average time 0.003166344220016981 iter num 200\n",
            "loss 5.746402795164875e-07 average time 0.0029958802999999535 iter num 100\n",
            "loss 5.746199829586795e-07 average time 0.003003980989997217 iter num 200\n",
            "loss 5.745006847642081e-07 average time 0.0030240645299818426 iter num 100\n",
            "loss 5.744821131498358e-07 average time 0.0030277919449940784 iter num 200\n",
            "loss 5.743611601408367e-07 average time 0.0032025971000166466 iter num 100\n",
            "loss 5.743409668728188e-07 average time 0.00314789063999342 iter num 200\n",
            "loss 5.742201168520503e-07 average time 0.003010428150014377 iter num 100\n",
            "loss 5.742007714006113e-07 average time 0.0030330853299938097 iter num 200\n",
            "loss 5.740822371360214e-07 average time 0.0031324505999918983 iter num 100\n",
            "loss 5.740601793248172e-07 average time 0.0030629558700002234 iter num 200\n",
            "loss 5.739415579472285e-07 average time 0.003129541420021269 iter num 100\n",
            "loss 5.739212610125504e-07 average time 0.0030410317000246325 iter num 200\n",
            "loss 5.738026919367553e-07 average time 0.0031051670499937247 iter num 100\n",
            "loss 5.737819833371231e-07 average time 0.0031066840549920016 iter num 200\n",
            "loss 5.736629539686803e-07 average time 0.0030384491999802775 iter num 100\n",
            "loss 5.736428602907462e-07 average time 0.0029977873399661802 iter num 200\n",
            "loss 5.735241997097164e-07 average time 0.0030967610600237095 iter num 100\n",
            "loss 5.735051434617842e-07 average time 0.0030770689300038613 iter num 200\n",
            "loss 5.733851025972662e-07 average time 0.003104515149980216 iter num 100\n",
            "loss 5.733630687255804e-07 average time 0.0030521861450006327 iter num 200\n",
            "loss 5.732444517950627e-07 average time 0.002986279770002511 iter num 100\n",
            "loss 5.732249561531182e-07 average time 0.0030504260399993655 iter num 200\n",
            "loss 5.731093849018934e-07 average time 0.0029840596000303777 iter num 100\n",
            "loss 5.730853595149355e-07 average time 0.0030016902200077312 iter num 200\n",
            "loss 5.729670861624021e-07 average time 0.0029763539899931857 iter num 100\n",
            "loss 5.729483687128555e-07 average time 0.002972367984984885 iter num 200\n",
            "loss 5.72828148794035e-07 average time 0.002943224750019908 iter num 100\n",
            "loss 5.728076365723493e-07 average time 0.002949956980007755 iter num 200\n",
            "loss 5.726921358449494e-07 average time 0.0029852733899815576 iter num 100\n",
            "loss 5.726687399479684e-07 average time 0.003005380554980093 iter num 200\n",
            "loss 5.725528776629772e-07 average time 0.0030294011600017256 iter num 100\n",
            "loss 5.725320939198108e-07 average time 0.003014589319989227 iter num 200\n",
            "loss 5.724123034206546e-07 average time 0.0029899436800224066 iter num 100\n",
            "loss 5.723919471602073e-07 average time 0.0029832160600085445 iter num 200\n",
            "loss 5.722745850874512e-07 average time 0.0031644724500347367 iter num 100\n",
            "loss 5.722548282349199e-07 average time 0.003191512000014427 iter num 200\n",
            "loss 5.72135290381205e-07 average time 0.0031836214499708147 iter num 100\n",
            "loss 5.721149939928063e-07 average time 0.003086587109987704 iter num 200\n",
            "loss 5.719995925994413e-07 average time 0.0031913725900221835 iter num 100\n",
            "loss 5.71974928304257e-07 average time 0.003094907144989065 iter num 200\n",
            "loss 5.718594114997888e-07 average time 0.0029848274500000114 iter num 100\n",
            "loss 5.718389631645454e-07 average time 0.002984674025003642 iter num 200\n",
            "loss 5.717201301460858e-07 average time 0.0032578813999998603 iter num 100\n",
            "loss 5.71699141702314e-07 average time 0.0031620595949993914 iter num 200\n",
            "loss 5.715837817237023e-07 average time 0.002999567890028629 iter num 100\n",
            "loss 5.715624725711166e-07 average time 0.002995227580017854 iter num 200\n",
            "loss 5.714449188820981e-07 average time 0.0031387818900202548 iter num 100\n",
            "loss 5.714232316279267e-07 average time 0.0030863847150271796 iter num 200\n",
            "loss 5.713070596914778e-07 average time 0.002968094459988606 iter num 100\n",
            "loss 5.712841662223871e-07 average time 0.003031594819997281 iter num 200\n",
            "loss 5.711680074702982e-07 average time 0.0030117154000072334 iter num 100\n",
            "loss 5.711484113554641e-07 average time 0.002999481005006146 iter num 200\n",
            "loss 5.710303490760144e-07 average time 0.0029873559700081386 iter num 100\n",
            "loss 5.710092329636876e-07 average time 0.0029936574049975206 iter num 200\n",
            "loss 5.708903510520076e-07 average time 0.0029978733600137275 iter num 100\n",
            "loss 5.708711903518537e-07 average time 0.0030397911349973584 iter num 200\n",
            "loss 5.70753645644939e-07 average time 0.0031098585700010516 iter num 100\n",
            "loss 5.707343213823771e-07 average time 0.003081248369990135 iter num 200\n",
            "loss 5.706155259533794e-07 average time 0.0032280773300226427 iter num 100\n",
            "loss 5.705953558889721e-07 average time 0.0031019684700140716 iter num 200\n",
            "loss 5.704801231431531e-07 average time 0.0030831190400431295 iter num 100\n",
            "loss 5.704583399731272e-07 average time 0.0031209374400282285 iter num 200\n",
            "loss 5.703406593387758e-07 average time 0.0029746630699992237 iter num 100\n",
            "loss 5.703211859716283e-07 average time 0.0030320412349919934 iter num 200\n",
            "loss 5.70205413151272e-07 average time 0.0030210310200027377 iter num 100\n",
            "loss 5.701826307870642e-07 average time 0.003045843370016428 iter num 200\n",
            "loss 5.700661571215344e-07 average time 0.0029872682199766133 iter num 100\n",
            "loss 5.700462186588618e-07 average time 0.002986408839976775 iter num 200\n",
            "loss 5.699279913067113e-07 average time 0.0030561223000404427 iter num 100\n",
            "loss 5.699090740444954e-07 average time 0.003085988470002121 iter num 200\n",
            "loss 5.697895861717757e-07 average time 0.0031265725599905637 iter num 100\n",
            "loss 5.697690640068752e-07 average time 0.0030876465199980883 iter num 200\n",
            "loss 5.69653064413819e-07 average time 0.0029939897200074485 iter num 100\n",
            "loss 5.696322148480481e-07 average time 0.002985797499995897 iter num 200\n",
            "loss 5.695160645116738e-07 average time 0.0031115607199899385 iter num 100\n",
            "loss 5.694975315301073e-07 average time 0.003056871275005051 iter num 200\n",
            "loss 5.693797717405191e-07 average time 0.0030079687700254 iter num 100\n",
            "loss 5.693573726483162e-07 average time 0.0030028535550104605 iter num 200\n",
            "loss 5.692437998885396e-07 average time 0.0030321952400072403 iter num 100\n",
            "loss 5.692205545561157e-07 average time 0.0030241501800082914 iter num 200\n",
            "loss 5.69106103280339e-07 average time 0.003006629300016357 iter num 100\n",
            "loss 5.690872844876145e-07 average time 0.0031074702900036754 iter num 200\n",
            "loss 5.689686142762143e-07 average time 0.0030331437699715027 iter num 100\n",
            "loss 5.689488152553395e-07 average time 0.0030416943649856877 iter num 200\n",
            "loss 5.688307076874768e-07 average time 0.003002699609987758 iter num 100\n",
            "loss 5.688118980375452e-07 average time 0.0029696396250142244 iter num 200\n",
            "loss 5.686936843231707e-07 average time 0.0029814230200145174 iter num 100\n",
            "loss 5.686761946100688e-07 average time 0.0029920091950202733 iter num 200\n",
            "loss 5.685602580122021e-07 average time 0.0032091287200091754 iter num 100\n",
            "loss 5.685388959005157e-07 average time 0.0031138251049992504 iter num 200\n",
            "loss 5.684237519069048e-07 average time 0.003037540039972555 iter num 100\n",
            "loss 5.684016260339047e-07 average time 0.0030714563849846854 iter num 200\n",
            "loss 5.68286132437697e-07 average time 0.003003828099999737 iter num 100\n",
            "loss 5.682664271900204e-07 average time 0.0030341791950036167 iter num 200\n",
            "loss 5.681484200455479e-07 average time 0.003026871920028498 iter num 100\n",
            "loss 5.681295112031697e-07 average time 0.00301661380001633 iter num 200\n",
            "loss 5.680108837038908e-07 average time 0.0029812701000082598 iter num 100\n",
            "loss 5.679925191247095e-07 average time 0.0029959859600012352 iter num 200\n",
            "loss 5.678776183827765e-07 average time 0.003092649440018249 iter num 100\n",
            "loss 5.678570280745586e-07 average time 0.0030440316149997673 iter num 200\n",
            "loss 5.677382214186017e-07 average time 0.003134879919989544 iter num 100\n",
            "loss 5.677189237904734e-07 average time 0.0030644372649953765 iter num 200\n",
            "loss 5.676059309434255e-07 average time 0.0029307437999977993 iter num 100\n",
            "loss 5.675846475729031e-07 average time 0.002968537864994687 iter num 200\n",
            "loss 5.674684814021829e-07 average time 0.0030422836500019913 iter num 100\n",
            "loss 5.674463027553667e-07 average time 0.002996797010018781 iter num 200\n",
            "loss 5.673319873588757e-07 average time 0.0029993750200083013 iter num 100\n",
            "loss 5.673098618991972e-07 average time 0.0030447136349903304 iter num 200\n",
            "loss 5.671967831801637e-07 average time 0.0029589091199886753 iter num 100\n",
            "loss 5.671750593482715e-07 average time 0.00296063919499602 iter num 200\n",
            "loss 5.670595969821264e-07 average time 0.0029636825800434963 iter num 100\n",
            "loss 5.670401616185121e-07 average time 0.0030475650800349287 iter num 200\n",
            "loss 5.669245056292605e-07 average time 0.002982403639994118 iter num 100\n",
            "loss 5.6690456323468e-07 average time 0.0029776647999983654 iter num 200\n",
            "loss 5.667892139372667e-07 average time 0.0030497525100008716 iter num 100\n",
            "loss 5.6676891177549e-07 average time 0.0030524344399987056 iter num 200\n",
            "loss 5.666532499749262e-07 average time 0.002985186219971183 iter num 100\n",
            "loss 5.666335506938451e-07 average time 0.002991672039988771 iter num 200\n",
            "loss 5.665175135225861e-07 average time 0.0029505663899863067 iter num 100\n",
            "loss 5.664964518963603e-07 average time 0.0029948096449948025 iter num 200\n",
            "loss 5.66380249727462e-07 average time 0.003082776959995499 iter num 100\n",
            "loss 5.663628077159957e-07 average time 0.003079992250000032 iter num 200\n",
            "loss 5.662463861072163e-07 average time 0.0029922713900259624 iter num 100\n",
            "loss 5.662280675663941e-07 average time 0.002976637480021509 iter num 200\n",
            "loss 5.661100563147551e-07 average time 0.0029450035599711557 iter num 100\n",
            "loss 5.660928051764583e-07 average time 0.0030065755349778555 iter num 200\n",
            "loss 5.65974466829835e-07 average time 0.003112432749967411 iter num 100\n",
            "loss 5.659556598974021e-07 average time 0.003043590969982688 iter num 200\n",
            "loss 5.658404038923784e-07 average time 0.0031162340999935623 iter num 100\n",
            "loss 5.658202607602787e-07 average time 0.003065649569996367 iter num 200\n",
            "loss 5.657060003373829e-07 average time 0.003085321270004897 iter num 100\n",
            "loss 5.656857021726054e-07 average time 0.0030402779999985797 iter num 200\n",
            "loss 5.655706091938146e-07 average time 0.003014345460005643 iter num 100\n",
            "loss 5.655505454725117e-07 average time 0.0030290778700054945 iter num 200\n",
            "loss 5.654359624441535e-07 average time 0.002983761999971648 iter num 100\n",
            "loss 5.654151570958311e-07 average time 0.00306115414497981 iter num 200\n",
            "loss 5.653008737492076e-07 average time 0.0033123926800044503 iter num 100\n",
            "loss 5.652794437637798e-07 average time 0.0031573932550008977 iter num 200\n",
            "loss 5.651644327144827e-07 average time 0.0029686756300179697 iter num 100\n",
            "loss 5.651453698761909e-07 average time 0.00298159497998995 iter num 200\n",
            "loss 5.650302884096198e-07 average time 0.003046813449991532 iter num 100\n",
            "loss 5.650121989293645e-07 average time 0.0030062189050022426 iter num 200\n",
            "loss 5.64895698108725e-07 average time 0.003014662380005575 iter num 100\n",
            "loss 5.648749422178967e-07 average time 0.0029981945549866396 iter num 200\n",
            "loss 5.647604543280531e-07 average time 0.002952426959991499 iter num 100\n",
            "loss 5.647412639579e-07 average time 0.003033187545001965 iter num 200\n",
            "loss 5.646261196325085e-07 average time 0.003089692309995371 iter num 100\n",
            "loss 5.646059169265758e-07 average time 0.0030561654399980397 iter num 200\n",
            "loss 5.64492476947147e-07 average time 0.0029801373000009333 iter num 100\n",
            "loss 5.644736069855584e-07 average time 0.0030391256500229245 iter num 200\n",
            "loss 5.643575069239726e-07 average time 0.0029841950099989845 iter num 100\n",
            "loss 5.643381569398957e-07 average time 0.0029982674350003437 iter num 200\n",
            "loss 5.642239131051142e-07 average time 0.003012995399999454 iter num 100\n",
            "loss 5.642035239980415e-07 average time 0.0030371584400018035 iter num 200\n",
            "loss 5.640896922918646e-07 average time 0.002984176139984811 iter num 100\n",
            "loss 5.640698395670752e-07 average time 0.0030055298400043285 iter num 200\n",
            "loss 5.639550304213642e-07 average time 0.003042179760009276 iter num 100\n",
            "loss 5.639350189945942e-07 average time 0.0030098094200116067 iter num 200\n",
            "loss 5.638220853646466e-07 average time 0.0029706910799995965 iter num 100\n",
            "loss 5.638019661604603e-07 average time 0.003063810519997787 iter num 200\n",
            "loss 5.636884162075395e-07 average time 0.002980917920003776 iter num 100\n",
            "loss 5.636657623619062e-07 average time 0.0029853043800017074 iter num 200\n",
            "loss 5.635548153979771e-07 average time 0.0033151391599903946 iter num 100\n",
            "loss 5.635317148189291e-07 average time 0.0031657494749879334 iter num 200\n",
            "loss 5.634193033906345e-07 average time 0.003018066560011903 iter num 100\n",
            "loss 5.633992533362606e-07 average time 0.0030000756500226087 iter num 200\n",
            "loss 5.632832334763993e-07 average time 0.0029593870499866172 iter num 100\n",
            "loss 5.632645975923033e-07 average time 0.002973989839999831 iter num 200\n",
            "loss 5.631507647632064e-07 average time 0.002991403789974356 iter num 100\n",
            "loss 5.631317927340244e-07 average time 0.0030120647050171103 iter num 200\n",
            "loss 5.63016717085604e-07 average time 0.0029598972799885813 iter num 100\n",
            "loss 5.629985705493732e-07 average time 0.003036283589983668 iter num 200\n",
            "loss 5.628830326731506e-07 average time 0.002964136880027581 iter num 100\n",
            "loss 5.628642012332931e-07 average time 0.0029639753150217983 iter num 200\n",
            "loss 5.627521132985532e-07 average time 0.0029847503399969355 iter num 100\n",
            "loss 5.627299997347711e-07 average time 0.00305134901999736 iter num 200\n",
            "loss 5.626175683047409e-07 average time 0.002995357119975779 iter num 100\n",
            "loss 5.625971681962124e-07 average time 0.003091607869978361 iter num 200\n",
            "loss 5.624842505697134e-07 average time 0.0030723741600058928 iter num 100\n",
            "loss 5.62461257200084e-07 average time 0.0030256760400061465 iter num 200\n",
            "loss 5.623494765011481e-07 average time 0.0029920409200485665 iter num 100\n",
            "loss 5.623315618567767e-07 average time 0.00310278566503257 iter num 200\n",
            "loss 5.622176193389625e-07 average time 0.0029895687700081906 iter num 100\n",
            "loss 5.621970864224168e-07 average time 0.0029915701649952096 iter num 200\n",
            "loss 5.620846587827452e-07 average time 0.002998549940011799 iter num 100\n",
            "loss 5.620635508083639e-07 average time 0.0030303616800119925 iter num 200\n",
            "loss 5.619498020736257e-07 average time 0.0031102857899895755 iter num 100\n",
            "loss 5.61930667200646e-07 average time 0.0030773343949749687 iter num 200\n",
            "loss 5.61816849537509e-07 average time 0.0030232044800095536 iter num 100\n",
            "loss 5.617976345850831e-07 average time 0.0031224564600006487 iter num 200\n",
            "loss 5.616854928396429e-07 average time 0.0029920277700375662 iter num 100\n",
            "loss 5.6166525228515e-07 average time 0.0029951804850293228 iter num 200\n",
            "loss 5.615514339424565e-07 average time 0.002977426240008754 iter num 100\n",
            "loss 5.615331090233222e-07 average time 0.0030686487400112127 iter num 200\n",
            "loss 5.614199148412517e-07 average time 0.003007910610003819 iter num 100\n",
            "loss 5.613995066764112e-07 average time 0.003044703564989959 iter num 200\n",
            "loss 5.61287517689276e-07 average time 0.003089877109978261 iter num 100\n",
            "loss 5.612647741922035e-07 average time 0.003114485204982884 iter num 200\n",
            "loss 5.611522280190055e-07 average time 0.003078083050013447 iter num 100\n",
            "loss 5.611333567693321e-07 average time 0.003026834744991902 iter num 200\n",
            "loss 5.610193700059798e-07 average time 0.0029649416499796643 iter num 100\n",
            "loss 5.610000812542074e-07 average time 0.0030890291099876775 iter num 200\n",
            "loss 5.608866526772907e-07 average time 0.002972121300035724 iter num 100\n",
            "loss 5.608665095648394e-07 average time 0.002983821400018769 iter num 200\n",
            "loss 5.607543348659391e-07 average time 0.0030199326600222776 iter num 100\n",
            "loss 5.607343623732998e-07 average time 0.003062122615017415 iter num 200\n",
            "loss 5.606218705952807e-07 average time 0.003028139570028543 iter num 100\n",
            "loss 5.606017559905156e-07 average time 0.0030183510550205028 iter num 200\n",
            "loss 5.604903527761861e-07 average time 0.0030076805899898317 iter num 100\n",
            "loss 5.604705361286938e-07 average time 0.003015383359986572 iter num 200\n",
            "loss 5.603558640021236e-07 average time 0.003105012299997725 iter num 100\n",
            "loss 5.603357558648305e-07 average time 0.003037395720002678 iter num 200\n",
            "loss 5.602249203642293e-07 average time 0.0030111813999701554 iter num 100\n",
            "loss 5.60206411038849e-07 average time 0.002983994419980718 iter num 200\n",
            "loss 5.600939391149822e-07 average time 0.0030780479199984255 iter num 100\n",
            "loss 5.600742091330346e-07 average time 0.003080297635003717 iter num 200\n",
            "loss 5.599624361751686e-07 average time 0.0032630695700163414 iter num 100\n",
            "loss 5.599422744754978e-07 average time 0.003155937010017169 iter num 200\n",
            "loss 5.598294754286028e-07 average time 0.0030871878599873525 iter num 100\n",
            "loss 5.598097548681876e-07 average time 0.0030640304099893 iter num 200\n",
            "loss 5.59695711126807e-07 average time 0.002992745430024115 iter num 100\n",
            "loss 5.596777865764896e-07 average time 0.002979703140001675 iter num 200\n",
            "loss 5.595653299865153e-07 average time 0.002981278299985206 iter num 100\n",
            "loss 5.595458885252117e-07 average time 0.0030047613249985262 iter num 200\n",
            "loss 5.594328155443299e-07 average time 0.0029632700499996646 iter num 100\n",
            "loss 5.5941382822427e-07 average time 0.0029901911850083707 iter num 200\n",
            "loss 5.593028690024831e-07 average time 0.003046586999962528 iter num 100\n",
            "loss 5.59281610687187e-07 average time 0.0030101641999726782 iter num 200\n",
            "loss 5.591697080280579e-07 average time 0.002998266529993998 iter num 100\n",
            "loss 5.591501654505076e-07 average time 0.0030432872700043843 iter num 200\n",
            "loss 5.590368772870973e-07 average time 0.0029505561000087253 iter num 100\n",
            "loss 5.590169016203061e-07 average time 0.002979818009994233 iter num 200\n",
            "loss 5.589063304007331e-07 average time 0.0030519596100111812 iter num 100\n",
            "loss 5.588851583108867e-07 average time 0.0030775072400138015 iter num 200\n",
            "loss 5.587754101200491e-07 average time 0.0029914750799935066 iter num 100\n",
            "loss 5.587545393139288e-07 average time 0.0029934424800012495 iter num 200\n",
            "loss 5.586438199983112e-07 average time 0.003065504679975675 iter num 100\n",
            "loss 5.586238948880816e-07 average time 0.0030546907999769246 iter num 200\n",
            "loss 5.585113987897361e-07 average time 0.00302216182004031 iter num 100\n",
            "loss 5.584926992210115e-07 average time 0.0030366950600250677 iter num 200\n",
            "loss 5.583827620916476e-07 average time 0.0030140202200027487 iter num 100\n",
            "loss 5.583611468682777e-07 average time 0.0030161209249968124 iter num 200\n",
            "loss 5.582504457789071e-07 average time 0.0030272816300112027 iter num 100\n",
            "loss 5.582309817055606e-07 average time 0.0030032332200153177 iter num 200\n",
            "loss 5.581183727523385e-07 average time 0.0029909373299869912 iter num 100\n",
            "loss 5.58100351411513e-07 average time 0.003001263395008209 iter num 200\n",
            "loss 5.579864832576047e-07 average time 0.0031220668300147737 iter num 100\n",
            "loss 5.579674838981012e-07 average time 0.0030374537799912104 iter num 200\n",
            "loss 5.578570322193383e-07 average time 0.002989886660020602 iter num 100\n",
            "loss 5.578359634534578e-07 average time 0.00298154874500824 iter num 200\n",
            "loss 5.577258913406011e-07 average time 0.0030093289500064202 iter num 100\n",
            "loss 5.577063656845649e-07 average time 0.0030221335000032924 iter num 200\n",
            "loss 5.575938605881027e-07 average time 0.002982176110008368 iter num 100\n",
            "loss 5.575750015388961e-07 average time 0.0029783935900081814 iter num 200\n",
            "loss 5.574641505760742e-07 average time 0.00304709636998723 iter num 100\n",
            "loss 5.574451937973388e-07 average time 0.0031083157649868555 iter num 200\n",
            "loss 5.573317655532666e-07 average time 0.0030008631999726277 iter num 100\n",
            "loss 5.573142489178662e-07 average time 0.002979734639984599 iter num 200\n",
            "loss 5.572011646809004e-07 average time 0.003050509509998847 iter num 100\n",
            "loss 5.571844792518465e-07 average time 0.003047142260004421 iter num 200\n",
            "loss 5.570711074076005e-07 average time 0.003202565690003212 iter num 100\n",
            "loss 5.570543501204589e-07 average time 0.003129933585003073 iter num 200\n",
            "loss 5.569402043093454e-07 average time 0.003126382070013278 iter num 100\n",
            "loss 5.569210425925455e-07 average time 0.0030611822650030264 iter num 200\n",
            "loss 5.568108111968566e-07 average time 0.0029744490099892574 iter num 100\n",
            "loss 5.567928064793775e-07 average time 0.0029825330800008487 iter num 200\n",
            "loss 5.566821733893905e-07 average time 0.0029831149599704077 iter num 100\n",
            "loss 5.566622279869486e-07 average time 0.002968631444982748 iter num 200\n",
            "loss 5.565481539142324e-07 average time 0.003142474290016253 iter num 100\n",
            "loss 5.565318197477388e-07 average time 0.0030669190350045028 iter num 200\n",
            "loss 5.564211943820885e-07 average time 0.0031138322900051206 iter num 100\n",
            "loss 5.563989444552246e-07 average time 0.0030880327199952263 iter num 200\n",
            "loss 5.562903686938689e-07 average time 0.0029585007700188726 iter num 100\n",
            "loss 5.56270439089279e-07 average time 0.002970444535008028 iter num 200\n",
            "loss 5.561589167807139e-07 average time 0.0031535533899887014 iter num 100\n",
            "loss 5.561415077030728e-07 average time 0.003129506629984462 iter num 200\n",
            "loss 5.560291297691641e-07 average time 0.0030310635499881753 iter num 100\n",
            "loss 5.56010991090154e-07 average time 0.0030306494049887078 iter num 200\n",
            "loss 5.558969668346044e-07 average time 0.002977952710016325 iter num 100\n",
            "loss 5.558817566961644e-07 average time 0.002997376085011183 iter num 200\n",
            "loss 5.557710488324997e-07 average time 0.003206024669980252 iter num 100\n",
            "loss 5.557487964630182e-07 average time 0.0031622184349953384 iter num 200\n",
            "loss 5.556418669559415e-07 average time 0.0030626907600071716 iter num 100\n",
            "loss 5.556211595345855e-07 average time 0.0030614567000156966 iter num 200\n",
            "loss 5.555104608122474e-07 average time 0.0030846946399788067 iter num 100\n",
            "loss 5.554906558013294e-07 average time 0.003056294899981822 iter num 200\n",
            "loss 5.553810476976518e-07 average time 0.0031752139100262866 iter num 100\n",
            "loss 5.553611083863372e-07 average time 0.0031099598000241713 iter num 200\n",
            "loss 5.552502819485245e-07 average time 0.003015215289979096 iter num 100\n",
            "loss 5.552298703806141e-07 average time 0.003061675399987962 iter num 200\n",
            "loss 5.551210542246632e-07 average time 0.0030215274799684265 iter num 100\n",
            "loss 5.551034361241598e-07 average time 0.0030033152199894176 iter num 200\n",
            "loss 5.549923375158809e-07 average time 0.0030690043100139518 iter num 100\n",
            "loss 5.549707428612645e-07 average time 0.0030594255100095326 iter num 200\n",
            "loss 5.54862885635034e-07 average time 0.0030574619999742935 iter num 100\n",
            "loss 5.548429844343943e-07 average time 0.003119683789982446 iter num 200\n",
            "loss 5.547334359780049e-07 average time 0.003115931190000083 iter num 100\n",
            "loss 5.547126003804162e-07 average time 0.00306394261999003 iter num 200\n",
            "loss 5.546051591302774e-07 average time 0.0029716904600127236 iter num 100\n",
            "loss 5.545851550836204e-07 average time 0.003031523775016467 iter num 200\n",
            "loss 5.54475342249077e-07 average time 0.0030274909999843656 iter num 100\n",
            "loss 5.544531417175429e-07 average time 0.0030663382149896277 iter num 200\n",
            "loss 5.543446998620669e-07 average time 0.0030565767199823312 iter num 100\n",
            "loss 5.543254639272517e-07 average time 0.0030329112399863334 iter num 200\n",
            "loss 5.54217419149905e-07 average time 0.002987225069964552 iter num 100\n",
            "loss 5.54197689149349e-07 average time 0.002989580324990584 iter num 200\n",
            "loss 5.540872227593536e-07 average time 0.0029831601500200124 iter num 100\n",
            "loss 5.540687598974501e-07 average time 0.0029647101550108346 iter num 200\n",
            "loss 5.539589375019834e-07 average time 0.003050187949975225 iter num 100\n",
            "loss 5.539391707552127e-07 average time 0.003116997534980328 iter num 200\n",
            "loss 5.538307502424903e-07 average time 0.003089958190021207 iter num 100\n",
            "loss 5.538103354858158e-07 average time 0.0031592052600058194 iter num 200\n",
            "loss 5.536990602815015e-07 average time 0.002991258139982165 iter num 100\n",
            "loss 5.536810157344855e-07 average time 0.00304509144500571 iter num 200\n",
            "loss 5.535712218153276e-07 average time 0.002981679620047544 iter num 100\n",
            "loss 5.535519820787645e-07 average time 0.0030153463800252213 iter num 200\n",
            "loss 5.534422804124532e-07 average time 0.00305493703999673 iter num 100\n",
            "loss 5.534251484750797e-07 average time 0.0030132322150006985 iter num 200\n",
            "loss 5.533147151762924e-07 average time 0.003005359039975701 iter num 100\n",
            "loss 5.532962587229433e-07 average time 0.003058773359985025 iter num 200\n",
            "loss 5.531856598503592e-07 average time 0.003016637039995658 iter num 100\n",
            "loss 5.531664164801776e-07 average time 0.003069801820017801 iter num 200\n",
            "loss 5.530576438605872e-07 average time 0.0030719246300031956 iter num 100\n",
            "loss 5.530386048810899e-07 average time 0.003022346294994804 iter num 200\n",
            "loss 5.529296518186096e-07 average time 0.0029787380299831057 iter num 100\n",
            "loss 5.529084796065784e-07 average time 0.0030160449599907226 iter num 200\n",
            "loss 5.528010833086273e-07 average time 0.003133015860016712 iter num 100\n",
            "loss 5.527809925223617e-07 average time 0.0030511409850055315 iter num 200\n",
            "loss 5.526714858493369e-07 average time 0.0032050392700011796 iter num 100\n",
            "loss 5.526551414861798e-07 average time 0.003119676499986781 iter num 200\n",
            "loss 5.525438265536101e-07 average time 0.003050262169990674 iter num 100\n",
            "loss 5.525256888796167e-07 average time 0.003026096274979864 iter num 200\n",
            "loss 5.524142331885081e-07 average time 0.002936268859980373 iter num 100\n",
            "loss 5.523977852607266e-07 average time 0.0029402542549883037 iter num 200\n",
            "loss 5.522869144033952e-07 average time 0.003184054519997517 iter num 100\n",
            "loss 5.522695539935072e-07 average time 0.003160944030016708 iter num 200\n",
            "loss 5.521592209584696e-07 average time 0.003166260949979005 iter num 100\n",
            "loss 5.521408055504387e-07 average time 0.0030876005400000393 iter num 200\n",
            "loss 5.520301368745128e-07 average time 0.002963096560006306 iter num 100\n",
            "loss 5.520126824414564e-07 average time 0.0029752732400015703 iter num 200\n",
            "loss 5.519037047546633e-07 average time 0.002990506300029665 iter num 100\n",
            "loss 5.518841282359554e-07 average time 0.0030131429950142775 iter num 200\n",
            "loss 5.517758863452068e-07 average time 0.0030141594600218012 iter num 100\n",
            "loss 5.517561444857623e-07 average time 0.0030411834100254964 iter num 200\n",
            "loss 5.516462207607236e-07 average time 0.0030420748699634716 iter num 100\n",
            "loss 5.516285106864741e-07 average time 0.0030237076099638217 iter num 200\n",
            "loss 5.515211120218899e-07 average time 0.003118178039967461 iter num 100\n",
            "loss 5.515015296865713e-07 average time 0.0030864575599889575 iter num 200\n",
            "loss 5.513926874959134e-07 average time 0.0030026509600247662 iter num 100\n",
            "loss 5.513729428847354e-07 average time 0.00301682554999843 iter num 200\n",
            "loss 5.512644468342369e-07 average time 0.0029876176900006613 iter num 100\n",
            "loss 5.512466122451121e-07 average time 0.003050776824998138 iter num 200\n",
            "loss 5.511368686681511e-07 average time 0.002967067439994935 iter num 100\n",
            "loss 5.511199260541136e-07 average time 0.002963007144983294 iter num 200\n",
            "loss 5.51009741498978e-07 average time 0.002970059280005444 iter num 100\n",
            "loss 5.509916304685953e-07 average time 0.002992520509994847 iter num 200\n",
            "loss 5.508835093985299e-07 average time 0.002975207559957198 iter num 100\n",
            "loss 5.508613637602393e-07 average time 0.002988827264973679 iter num 200\n",
            "loss 5.507554721365719e-07 average time 0.0029533462400013377 iter num 100\n",
            "loss 5.507374855281906e-07 average time 0.002991108624996741 iter num 200\n",
            "loss 5.506277570931667e-07 average time 0.0031469423399948936 iter num 100\n",
            "loss 5.506086261269643e-07 average time 0.003150091150002936 iter num 200\n",
            "loss 5.505014930400509e-07 average time 0.0029626764999966327 iter num 100\n",
            "loss 5.504839185636767e-07 average time 0.0030305091299919697 iter num 200\n",
            "loss 5.503736191058176e-07 average time 0.00309909486997185 iter num 100\n",
            "loss 5.503543284029905e-07 average time 0.0030693561149882954 iter num 200\n",
            "loss 5.50247999418124e-07 average time 0.0031131381600107487 iter num 100\n",
            "loss 5.502299359793426e-07 average time 0.0030622563150245695 iter num 200\n",
            "loss 5.501203400548945e-07 average time 0.002966347960013991 iter num 100\n",
            "loss 5.501001704999477e-07 average time 0.0029887874249970993 iter num 200\n",
            "loss 5.499917660768848e-07 average time 0.002997806489984214 iter num 100\n",
            "loss 5.499751708010352e-07 average time 0.0030095100249923233 iter num 200\n",
            "loss 5.498654914938328e-07 average time 0.003046641530013403 iter num 100\n",
            "loss 5.49846345537757e-07 average time 0.0030291824049913886 iter num 200\n",
            "loss 5.497403740163233e-07 average time 0.0031606072800150287 iter num 100\n",
            "loss 5.497198527943292e-07 average time 0.003133475105021262 iter num 200\n",
            "loss 5.496112209806954e-07 average time 0.003020973440002308 iter num 100\n",
            "loss 5.495939794711422e-07 average time 0.003009384815006797 iter num 200\n",
            "loss 5.494868615537875e-07 average time 0.0029697597500171467 iter num 100\n",
            "loss 5.494676402347458e-07 average time 0.0029896948200143925 iter num 200\n",
            "loss 5.493602928482124e-07 average time 0.0030553631999782737 iter num 100\n",
            "loss 5.493406125834697e-07 average time 0.003031356294993657 iter num 200\n",
            "loss 5.492308037587629e-07 average time 0.002970030169990423 iter num 100\n",
            "loss 5.492139493228808e-07 average time 0.0030223269499992968 iter num 200\n",
            "loss 5.49104941678227e-07 average time 0.0030186575399829964 iter num 100\n",
            "loss 5.49087845453771e-07 average time 0.0031327542050030386 iter num 200\n",
            "loss 5.48978229959158e-07 average time 0.003161244079997232 iter num 100\n",
            "loss 5.489601826654139e-07 average time 0.003155164799989052 iter num 200\n",
            "loss 5.488542524940628e-07 average time 0.0029738569200117126 iter num 100\n",
            "loss 5.488338624887702e-07 average time 0.0029843472500124335 iter num 200\n",
            "loss 5.487271294538311e-07 average time 0.0029865434700104745 iter num 100\n",
            "loss 5.487081423044667e-07 average time 0.003027170515015314 iter num 200\n",
            "loss 5.485986284490984e-07 average time 0.0031361900799993236 iter num 100\n",
            "loss 5.485846862522939e-07 average time 0.0030873859949906548 iter num 200\n",
            "loss 5.484735622744572e-07 average time 0.0031343094099884183 iter num 100\n",
            "loss 5.484567027089528e-07 average time 0.003072378194999601 iter num 200\n",
            "loss 5.483496530967205e-07 average time 0.002963991709966649 iter num 100\n",
            "loss 5.483300882064571e-07 average time 0.002973162689979745 iter num 200\n",
            "loss 5.482234145267961e-07 average time 0.0029854431299963834 iter num 100\n",
            "loss 5.48203888005912e-07 average time 0.0030247682700155565 iter num 200\n",
            "loss 5.480959112286978e-07 average time 0.003058499280000433 iter num 100\n",
            "loss 5.480786660223668e-07 average time 0.00304041591500436 iter num 200\n",
            "loss 5.47971837350972e-07 average time 0.0029748237999501726 iter num 100\n",
            "loss 5.479530494970929e-07 average time 0.003038643904969831 iter num 200\n",
            "loss 5.478451300905067e-07 average time 0.003010552199984886 iter num 100\n",
            "loss 5.478260795636601e-07 average time 0.0030051387499906925 iter num 200\n",
            "loss 5.477203814657981e-07 average time 0.003064574740014905 iter num 100\n",
            "loss 5.477003224801141e-07 average time 0.0030176226750040767 iter num 200\n",
            "loss 5.475923480512533e-07 average time 0.0032231963699996414 iter num 100\n",
            "loss 5.475751125443632e-07 average time 0.0031508532949987966 iter num 200\n",
            "loss 5.474673617200959e-07 average time 0.0031369704500002625 iter num 100\n",
            "loss 5.474503257468e-07 average time 0.0030648730899929433 iter num 200\n",
            "loss 5.473430274029393e-07 average time 0.003029760300023554 iter num 100\n",
            "loss 5.47324540228681e-07 average time 0.003078778999995393 iter num 200\n",
            "loss 5.47216547078108e-07 average time 0.0030089582600157884 iter num 100\n",
            "loss 5.471988429550076e-07 average time 0.0030224971950133296 iter num 200\n",
            "loss 5.470932411168056e-07 average time 0.0031907756799819252 iter num 100\n",
            "loss 5.470731440465143e-07 average time 0.003150559064988556 iter num 200\n",
            "loss 5.469643492113701e-07 average time 0.0029977906499607343 iter num 100\n",
            "loss 5.469479370333599e-07 average time 0.0030044536950049406 iter num 200\n",
            "loss 5.468413531450335e-07 average time 0.0030610037800079225 iter num 100\n",
            "loss 5.46821221947357e-07 average time 0.003097804940016431 iter num 200\n",
            "loss 5.467143282384391e-07 average time 0.0031798489899938433 iter num 100\n",
            "loss 5.466958829224269e-07 average time 0.00306086518998427 iter num 200\n",
            "loss 5.465901556778717e-07 average time 0.0029561623799600058 iter num 100\n",
            "loss 5.465726824470784e-07 average time 0.0030450988199800123 iter num 200\n",
            "loss 5.464654855588189e-07 average time 0.0030275351800537463 iter num 100\n",
            "loss 5.464459182570799e-07 average time 0.0030489564650383726 iter num 200\n",
            "loss 5.463393396614919e-07 average time 0.002981315409988383 iter num 100\n",
            "loss 5.463227214888138e-07 average time 0.0030453482449865985 iter num 200\n",
            "loss 5.462141440677812e-07 average time 0.0029863862499905735 iter num 100\n",
            "loss 5.461967767586579e-07 average time 0.0030364535650005565 iter num 200\n",
            "loss 5.460905044372431e-07 average time 0.0030563238500326407 iter num 100\n",
            "loss 5.460709690416393e-07 average time 0.003003998450008112 iter num 200\n",
            "loss 5.459657089926172e-07 average time 0.0029891495999982 iter num 100\n",
            "loss 5.459452587028185e-07 average time 0.0030239618650034572 iter num 200\n",
            "loss 5.458415812763602e-07 average time 0.0029810025599681465 iter num 100\n",
            "loss 5.45822073465283e-07 average time 0.002975984684983359 iter num 200\n",
            "loss 5.45716774899284e-07 average time 0.0030560532400386365 iter num 100\n",
            "loss 5.456976550768671e-07 average time 0.003003542360015672 iter num 200\n",
            "loss 5.455911996466516e-07 average time 0.0029932425199967836 iter num 100\n",
            "loss 5.455724034553204e-07 average time 0.002983040799997525 iter num 200\n",
            "loss 5.454668512976937e-07 average time 0.0030103337900118277 iter num 100\n",
            "loss 5.454492387093832e-07 average time 0.0030055499850209343 iter num 200\n",
            "loss 5.453429509748121e-07 average time 0.0029916387099638087 iter num 100\n",
            "loss 5.453245752134668e-07 average time 0.002995752434969745 iter num 200\n",
            "loss 5.452174363039875e-07 average time 0.0029645880999760264 iter num 100\n",
            "loss 5.452000406095398e-07 average time 0.0029845429149963822 iter num 200\n",
            "loss 5.450923860735165e-07 average time 0.003000427229990237 iter num 100\n",
            "loss 5.450730800739061e-07 average time 0.0029899036599840656 iter num 200\n",
            "loss 5.449689009297722e-07 average time 0.0030224290900150662 iter num 100\n",
            "loss 5.449514146256951e-07 average time 0.0031803275900188057 iter num 200\n",
            "loss 5.448441121528618e-07 average time 0.0030722247299945593 iter num 100\n",
            "loss 5.448266240509898e-07 average time 0.0030366738899920164 iter num 200\n",
            "loss 5.447210856779517e-07 average time 0.0032044775299982574 iter num 100\n",
            "loss 5.447031579278878e-07 average time 0.003158398359992134 iter num 200\n",
            "loss 5.445951878891594e-07 average time 0.0029853334799963705 iter num 100\n",
            "loss 5.445773488300519e-07 average time 0.002978179864987851 iter num 200\n",
            "loss 5.444730105217179e-07 average time 0.003000026660020012 iter num 100\n",
            "loss 5.44453453914268e-07 average time 0.003041574959997888 iter num 200\n",
            "loss 5.443488329262622e-07 average time 0.0031926395600066827 iter num 100\n",
            "loss 5.443280844825969e-07 average time 0.003192670669989184 iter num 200\n",
            "loss 5.44226107484995e-07 average time 0.00298992279999311 iter num 100\n",
            "loss 5.442057704822555e-07 average time 0.0029832796049890932 iter num 200\n",
            "loss 5.441005188423124e-07 average time 0.0029756195099980686 iter num 100\n",
            "loss 5.440827790255716e-07 average time 0.002983661450002728 iter num 200\n",
            "loss 5.439766033817279e-07 average time 0.003032789000030789 iter num 100\n",
            "loss 5.439569727538019e-07 average time 0.0031052226500014515 iter num 200\n",
            "loss 5.438527711625485e-07 average time 0.0030494481200094015 iter num 100\n",
            "loss 5.438337671408672e-07 average time 0.003027468919999592 iter num 200\n",
            "loss 5.437284046459973e-07 average time 0.0030789418500307875 iter num 100\n",
            "loss 5.437099176215634e-07 average time 0.0030227746000218757 iter num 200\n",
            "loss 5.436065103990173e-07 average time 0.0030659809800226865 iter num 100\n",
            "loss 5.435879627273242e-07 average time 0.0030241918000001532 iter num 200\n",
            "loss 5.434815575154456e-07 average time 0.0031773201699843412 iter num 100\n",
            "loss 5.43463318441049e-07 average time 0.0032077433999961615 iter num 200\n",
            "loss 5.433592064979378e-07 average time 0.003254487139993216 iter num 100\n",
            "loss 5.433411261880147e-07 average time 0.003115349444999538 iter num 200\n",
            "loss 5.43236242457959e-07 average time 0.003107526160006273 iter num 100\n",
            "loss 5.432156070392612e-07 average time 0.0031248867549925306 iter num 200\n",
            "loss 5.431117812215487e-07 average time 0.0031711918600149146 iter num 100\n",
            "loss 5.430965150071406e-07 average time 0.00308819947500524 iter num 200\n",
            "loss 5.429873058685932e-07 average time 0.003040375939981459 iter num 100\n",
            "loss 5.429703612249621e-07 average time 0.003050592129980032 iter num 200\n",
            "loss 5.428647885739001e-07 average time 0.0029622928399976444 iter num 100\n",
            "loss 5.428468337493752e-07 average time 0.0030499436999843967 iter num 200\n",
            "loss 5.427421045575053e-07 average time 0.0030708073400046486 iter num 100\n",
            "loss 5.427243774365963e-07 average time 0.003032726865001223 iter num 200\n",
            "loss 5.426187609161973e-07 average time 0.0030030432700004894 iter num 100\n",
            "loss 5.426003554445359e-07 average time 0.003017254700000649 iter num 200\n",
            "loss 5.424980532909564e-07 average time 0.002956443170005514 iter num 100\n",
            "loss 5.424788696051426e-07 average time 0.0030964385600032074 iter num 200\n",
            "loss 5.423736102913179e-07 average time 0.0031667327299828687 iter num 100\n",
            "loss 5.423550343923167e-07 average time 0.0030796103399893583 iter num 200\n",
            "loss 5.422495865111519e-07 average time 0.0030267217299979167 iter num 100\n",
            "loss 5.422311721394238e-07 average time 0.003037149025005874 iter num 200\n",
            "loss 5.421259282995269e-07 average time 0.0031387007600187644 iter num 100\n",
            "loss 5.421088515579382e-07 average time 0.0030769792600108302 iter num 200\n",
            "loss 5.420051086065465e-07 average time 0.003010911040028077 iter num 100\n",
            "loss 5.419859159950722e-07 average time 0.0030006747150036974 iter num 200\n",
            "loss 5.418810651312584e-07 average time 0.0030192810500011547 iter num 100\n",
            "loss 5.418638159118662e-07 average time 0.0030689536799968662 iter num 200\n",
            "loss 5.417590030337397e-07 average time 0.002964248720004434 iter num 100\n",
            "loss 5.417438347675658e-07 average time 0.002957911900000454 iter num 200\n",
            "loss 5.416365403582878e-07 average time 0.0029956894599990846 iter num 100\n",
            "loss 5.416188711360457e-07 average time 0.0030340714999965712 iter num 200\n",
            "loss 5.415131105053001e-07 average time 0.0029834209400132748 iter num 100\n",
            "loss 5.414971001122608e-07 average time 0.003003665450009976 iter num 200\n",
            "loss 5.413920788642917e-07 average time 0.003042148580011599 iter num 100\n",
            "loss 5.413740285479847e-07 average time 0.0030185427549963605 iter num 200\n",
            "loss 5.412677774945989e-07 average time 0.00310406411000713 iter num 100\n",
            "loss 5.412511636180524e-07 average time 0.0031138556700125264 iter num 200\n",
            "loss 5.411461778513068e-07 average time 0.003267155790003926 iter num 100\n",
            "loss 5.411280945237946e-07 average time 0.003154562605018327 iter num 200\n",
            "loss 5.410244183932732e-07 average time 0.003081328909993317 iter num 100\n",
            "loss 5.410076194545923e-07 average time 0.003121868129983341 iter num 200\n",
            "loss 5.409011977793138e-07 average time 0.0030342701999916243 iter num 100\n",
            "loss 5.408851536025977e-07 average time 0.0030677542350076694 iter num 200\n",
            "loss 5.407786653036099e-07 average time 0.003062881590003599 iter num 100\n",
            "loss 5.407608970837518e-07 average time 0.0030930913399924973 iter num 200\n",
            "loss 5.406582018870582e-07 average time 0.0030713122199949794 iter num 100\n",
            "loss 5.406394563663851e-07 average time 0.003131721570011905 iter num 200\n",
            "loss 5.405353132322641e-07 average time 0.0031345939399761848 iter num 100\n",
            "loss 5.405190681969622e-07 average time 0.003107205049998356 iter num 200\n",
            "loss 5.404149240056377e-07 average time 0.003003378319986041 iter num 100\n",
            "loss 5.403958552075208e-07 average time 0.0030396964800161186 iter num 200\n",
            "loss 5.402924413253779e-07 average time 0.0029912757499732834 iter num 100\n",
            "loss 5.402736267454094e-07 average time 0.0030094013249936324 iter num 200\n",
            "loss 5.401687314092045e-07 average time 0.0030466266900020856 iter num 100\n",
            "loss 5.401527248549799e-07 average time 0.003015536960008376 iter num 200\n",
            "loss 5.40046723471723e-07 average time 0.00297984256001655 iter num 100\n",
            "loss 5.400306120551107e-07 average time 0.0029874844999972085 iter num 200\n",
            "loss 5.399277249617381e-07 average time 0.0029763328699800697 iter num 100\n",
            "loss 5.399104150964191e-07 average time 0.003021078900005705 iter num 200\n",
            "loss 5.398035802418685e-07 average time 0.0030199047499945664 iter num 100\n",
            "loss 5.397881611261067e-07 average time 0.0029876489499952186 iter num 200\n",
            "loss 5.396826358723084e-07 average time 0.002977606139997988 iter num 100\n",
            "loss 5.396671206871994e-07 average time 0.0030277837149969857 iter num 200\n",
            "loss 5.395601099484303e-07 average time 0.0029795762300091153 iter num 100\n",
            "loss 5.395428820591637e-07 average time 0.0029709206850088775 iter num 200\n",
            "loss 5.39439651312148e-07 average time 0.0029923450100113767 iter num 100\n",
            "loss 5.394224354650114e-07 average time 0.0031095099400022265 iter num 200\n",
            "loss 5.393197660660653e-07 average time 0.0029613547399958405 iter num 100\n",
            "loss 5.39299951758796e-07 average time 0.0030103785300048 iter num 200\n",
            "loss 5.391955229691203e-07 average time 0.002996991309987607 iter num 100\n",
            "loss 5.391793923336002e-07 average time 0.003023400959978062 iter num 200\n",
            "loss 5.390747269300607e-07 average time 0.003000367980016563 iter num 100\n",
            "loss 5.390588825675921e-07 average time 0.003088705029990706 iter num 200\n",
            "loss 5.389560114740574e-07 average time 0.002977417849988342 iter num 100\n",
            "loss 5.389363858850057e-07 average time 0.0029783661649867097 iter num 200\n",
            "loss 5.388346367697828e-07 average time 0.0029921671299962325 iter num 100\n",
            "loss 5.388165604976973e-07 average time 0.0029904679450032745 iter num 200\n",
            "loss 5.387132015034284e-07 average time 0.0029634768699952473 iter num 100\n",
            "loss 5.386954168103929e-07 average time 0.0029784795000068697 iter num 200\n",
            "loss 5.385922903174646e-07 average time 0.0031154538900136685 iter num 100\n",
            "loss 5.385719312435773e-07 average time 0.00304084492001266 iter num 200\n",
            "loss 5.384711582281422e-07 average time 0.002981441379984062 iter num 100\n",
            "loss 5.384529860564406e-07 average time 0.0030017981799869632 iter num 200\n",
            "loss 5.38349745309399e-07 average time 0.0030292346000169346 iter num 100\n",
            "loss 5.383328943254801e-07 average time 0.0030632830200170245 iter num 200\n",
            "loss 5.382287419274492e-07 average time 0.0030300356499947155 iter num 100\n",
            "loss 5.382109550392584e-07 average time 0.0030026811749917213 iter num 200\n",
            "loss 5.381090718437267e-07 average time 0.0030333651099772397 iter num 100\n",
            "loss 5.380904816098518e-07 average time 0.0030245810699898357 iter num 200\n",
            "loss 5.37986825309568e-07 average time 0.0029808787999627386 iter num 100\n",
            "loss 5.379700426205473e-07 average time 0.002975684049972642 iter num 200\n",
            "loss 5.37867140977727e-07 average time 0.0029776062700102557 iter num 100\n",
            "loss 5.378486161569672e-07 average time 0.003008297335009047 iter num 200\n",
            "loss 5.377473272907406e-07 average time 0.002988214119995973 iter num 100\n",
            "loss 5.377282347862046e-07 average time 0.003002903964995767 iter num 200\n",
            "loss 5.376262780437647e-07 average time 0.0030763052999782303 iter num 100\n",
            "loss 5.37607361118863e-07 average time 0.0030373388449834236 iter num 200\n",
            "loss 5.375056653631116e-07 average time 0.0030420526800162408 iter num 100\n",
            "loss 5.374871764779463e-07 average time 0.0030619828899943967 iter num 200\n",
            "loss 5.373853653184767e-07 average time 0.0029494494299979125 iter num 100\n",
            "loss 5.373658677509759e-07 average time 0.002964410140007203 iter num 200\n",
            "loss 5.372642943143952e-07 average time 0.003153746060024787 iter num 100\n",
            "loss 5.372441984050706e-07 average time 0.003063818315026765 iter num 200\n",
            "loss 5.371424436138801e-07 average time 0.002990450460010834 iter num 100\n",
            "loss 5.371266987524606e-07 average time 0.003036500485009128 iter num 200\n",
            "loss 5.370222385412794e-07 average time 0.0029794141100137495 iter num 100\n",
            "loss 5.370053501702465e-07 average time 0.003027110835012081 iter num 200\n",
            "loss 5.369035864977805e-07 average time 0.00305350443999032 iter num 100\n",
            "loss 5.368852254217185e-07 average time 0.003028382209975007 iter num 200\n",
            "loss 5.367844023495081e-07 average time 0.0029795707000403126 iter num 100\n",
            "loss 5.367654792552834e-07 average time 0.0029701749250193644 iter num 200\n",
            "loss 5.366635423571488e-07 average time 0.0030751933700094015 iter num 100\n",
            "loss 5.366464173896885e-07 average time 0.00306805634499824 iter num 200\n",
            "loss 5.365430628489985e-07 average time 0.0029952401699983966 iter num 100\n",
            "loss 5.365274771298685e-07 average time 0.003029704969994782 iter num 200\n",
            "loss 5.364240009530575e-07 average time 0.0029852879299915 iter num 100\n",
            "loss 5.364067716357723e-07 average time 0.0029780568999967726 iter num 200\n",
            "loss 5.363046278787642e-07 average time 0.003019139500001984 iter num 100\n",
            "loss 5.362866796283511e-07 average time 0.0030801894750061365 iter num 200\n",
            "loss 5.361839545063436e-07 average time 0.0029937575300209573 iter num 100\n",
            "loss 5.361663395657441e-07 average time 0.0029915882850059462 iter num 200\n",
            "loss 5.360654491120093e-07 average time 0.0029772745199989005 iter num 100\n",
            "loss 5.360477913913703e-07 average time 0.0029625113849874652 iter num 200\n",
            "loss 5.359435275006673e-07 average time 0.0029512782599795174 iter num 100\n",
            "loss 5.359272913159153e-07 average time 0.0029657462100090015 iter num 200\n",
            "loss 5.358250456711909e-07 average time 0.0029666355399831446 iter num 100\n",
            "loss 5.35807078452467e-07 average time 0.003024252484983663 iter num 200\n",
            "loss 5.357048286991512e-07 average time 0.00297766737000984 iter num 100\n",
            "loss 5.35688122606735e-07 average time 0.0030003980650212727 iter num 200\n",
            "loss 5.35585369217717e-07 average time 0.0030727703800130257 iter num 100\n",
            "loss 5.355696810661934e-07 average time 0.0030544958400219 iter num 200\n",
            "loss 5.354664344260983e-07 average time 0.003018581980009003 iter num 100\n",
            "loss 5.35448998296802e-07 average time 0.0029887068050175004 iter num 200\n",
            "loss 5.35349188223796e-07 average time 0.002971564639997268 iter num 100\n",
            "loss 5.353284867684169e-07 average time 0.0029730786999971315 iter num 200\n",
            "loss 5.352252986214864e-07 average time 0.002974834689985073 iter num 100\n",
            "loss 5.352103873725777e-07 average time 0.002969452814993474 iter num 200\n",
            "loss 5.35108424292558e-07 average time 0.002968611459996282 iter num 100\n",
            "loss 5.350938100654713e-07 average time 0.0029668991550079226 iter num 200\n",
            "loss 5.349898528459152e-07 average time 0.0030135728200184533 iter num 100\n",
            "loss 5.349718828020223e-07 average time 0.003001675844998317 iter num 200\n",
            "loss 5.348707595139981e-07 average time 0.002988880209986746 iter num 100\n",
            "loss 5.348526809593011e-07 average time 0.003072668284987685 iter num 200\n",
            "loss 5.347512597897247e-07 average time 0.0030361625999967144 iter num 100\n",
            "loss 5.347330302776461e-07 average time 0.003083289625010366 iter num 200\n",
            "loss 5.346336733666674e-07 average time 0.0031009575299913196 iter num 100\n",
            "loss 5.346152990473636e-07 average time 0.003065266854994206 iter num 200\n",
            "loss 5.345134173984808e-07 average time 0.0029525283699831564 iter num 100\n",
            "loss 5.344968724861718e-07 average time 0.0029597495049961254 iter num 200\n",
            "loss 5.34394373155378e-07 average time 0.0029917183900124654 iter num 100\n",
            "loss 5.343760562242055e-07 average time 0.003061863835002896 iter num 200\n",
            "loss 5.342769681388071e-07 average time 0.002960197629963659 iter num 100\n",
            "loss 5.342574380110413e-07 average time 0.0029775351949820105 iter num 200\n",
            "loss 5.341545854191079e-07 average time 0.003068004949977876 iter num 100\n",
            "loss 5.34139904847667e-07 average time 0.0030131138299998384 iter num 200\n",
            "loss 5.340376722059858e-07 average time 0.0031398715399836873 iter num 100\n",
            "loss 5.34019983892508e-07 average time 0.003071834225002021 iter num 200\n",
            "loss 5.339186663998955e-07 average time 0.0030174943700149014 iter num 100\n",
            "loss 5.339036497077526e-07 average time 0.003048665870021523 iter num 200\n",
            "loss 5.337985918993652e-07 average time 0.0030679935700072746 iter num 100\n",
            "loss 5.337823781657378e-07 average time 0.0030210208750122546 iter num 200\n",
            "loss 5.336800246682502e-07 average time 0.003106816470008198 iter num 100\n",
            "loss 5.336649147186622e-07 average time 0.003070879914996567 iter num 200\n",
            "loss 5.335624308184668e-07 average time 0.003016577140010668 iter num 100\n",
            "loss 5.335457247917445e-07 average time 0.003006861635014957 iter num 200\n",
            "loss 5.33444339812232e-07 average time 0.0029982765699969605 iter num 100\n",
            "loss 5.334295054964681e-07 average time 0.003087315745012802 iter num 200\n",
            "loss 5.333266408696731e-07 average time 0.0029959182699803933 iter num 100\n",
            "loss 5.33309929257382e-07 average time 0.003020141134986716 iter num 200\n",
            "loss 5.332081415235867e-07 average time 0.0030782118399883983 iter num 100\n",
            "loss 5.331924449091921e-07 average time 0.0031033616449985856 iter num 200\n",
            "loss 5.330923110103941e-07 average time 0.0029912520199968638 iter num 100\n",
            "loss 5.330745844442718e-07 average time 0.0030271448949952175 iter num 200\n",
            "loss 5.329724686817269e-07 average time 0.0029916354699935253 iter num 100\n",
            "loss 5.329560479083621e-07 average time 0.0030342034899945246 iter num 200\n",
            "loss 5.328545454812578e-07 average time 0.0030132601099830936 iter num 100\n",
            "loss 5.328394151857293e-07 average time 0.0030022381249887077 iter num 200\n",
            "loss 5.32735510343403e-07 average time 0.003013560030040026 iter num 100\n",
            "loss 5.32718137596091e-07 average time 0.0030999354750201745 iter num 200\n",
            "loss 5.326183700777171e-07 average time 0.003091144459976931 iter num 100\n",
            "loss 5.326003225987214e-07 average time 0.00305530886500037 iter num 200\n",
            "loss 5.324967362536429e-07 average time 0.003027315239987729 iter num 100\n",
            "loss 5.324837102072232e-07 average time 0.003077204349990552 iter num 200\n",
            "loss 5.323837634487169e-07 average time 0.0029842510300022696 iter num 100\n",
            "loss 5.323660678608121e-07 average time 0.003012922430002618 iter num 200\n",
            "loss 5.322658745607305e-07 average time 0.0030246103500030587 iter num 100\n",
            "loss 5.322463136020221e-07 average time 0.0030438798350019168 iter num 200\n",
            "loss 5.321465505466746e-07 average time 0.0030978049099712733 iter num 100\n",
            "loss 5.321287691675408e-07 average time 0.0030386957449741203 iter num 200\n",
            "loss 5.320307681108286e-07 average time 0.0030535315400038597 iter num 100\n",
            "loss 5.320112544886013e-07 average time 0.0030433384349953484 iter num 200\n",
            "loss 5.319105765908979e-07 average time 0.002972747559979325 iter num 100\n",
            "loss 5.318957307654208e-07 average time 0.0030043398449856795 iter num 200\n",
            "loss 5.317931400150461e-07 average time 0.0033757825399879948 iter num 100\n",
            "loss 5.317768391655904e-07 average time 0.0032850993149895658 iter num 200\n",
            "loss 5.316772637831197e-07 average time 0.003112991860002694 iter num 100\n",
            "loss 5.316571955887957e-07 average time 0.0030452264549899154 iter num 200\n",
            "loss 5.315573378178943e-07 average time 0.0030630910600120843 iter num 100\n",
            "loss 5.315428614449547e-07 average time 0.003048935370009076 iter num 200\n",
            "loss 5.314394228411999e-07 average time 0.0029649524999740607 iter num 100\n",
            "loss 5.314246430468684e-07 average time 0.0030594606499971633 iter num 200\n",
            "loss 5.313231409479178e-07 average time 0.0030272807600294984 iter num 100\n",
            "loss 5.313070790724446e-07 average time 0.0029979297550175943 iter num 200\n",
            "loss 5.312062736321916e-07 average time 0.0030278473699627286 iter num 100\n",
            "loss 5.311892753711629e-07 average time 0.0030145231549795425 iter num 200\n",
            "loss 5.310904435143565e-07 average time 0.0030110522199993284 iter num 100\n",
            "loss 5.310723228381958e-07 average time 0.0029971082199995182 iter num 200\n",
            "loss 5.309737702384368e-07 average time 0.003055401259994142 iter num 100\n",
            "loss 5.309552315602566e-07 average time 0.003039577944980465 iter num 200\n",
            "loss 5.308550852137874e-07 average time 0.003114714240023204 iter num 100\n",
            "loss 5.30839243348688e-07 average time 0.003073901255013425 iter num 200\n",
            "loss 5.307383172349184e-07 average time 0.0029919670799699815 iter num 100\n",
            "loss 5.307200711069967e-07 average time 0.003002290339989031 iter num 200\n",
            "loss 5.306208645278812e-07 average time 0.0029838463299938665 iter num 100\n",
            "loss 5.306035515609863e-07 average time 0.002983146244996533 iter num 200\n",
            "loss 5.305040096984754e-07 average time 0.0029465955199975725 iter num 100\n",
            "loss 5.304874157530168e-07 average time 0.002961826309990556 iter num 200\n",
            "loss 5.303877162752523e-07 average time 0.0029889970099611675 iter num 100\n",
            "loss 5.303702546392548e-07 average time 0.0030073775899813883 iter num 200\n",
            "loss 5.302714345005376e-07 average time 0.003016425699984211 iter num 100\n",
            "loss 5.302530460459135e-07 average time 0.003009187934987949 iter num 200\n",
            "loss 5.301545231248705e-07 average time 0.0030890491799800655 iter num 100\n",
            "loss 5.301368052035501e-07 average time 0.0030423396350033726 iter num 200\n",
            "loss 5.300388520364366e-07 average time 0.0029628508100040564 iter num 100\n",
            "loss 5.300186793681576e-07 average time 0.003007024015003026 iter num 200\n",
            "loss 5.299211106755677e-07 average time 0.003010627490011757 iter num 100\n",
            "loss 5.299045259817885e-07 average time 0.0030147355700091793 iter num 200\n",
            "loss 5.298032132022813e-07 average time 0.0030672639000067646 iter num 100\n",
            "loss 5.297863988591928e-07 average time 0.0030248219150075784 iter num 200\n",
            "loss 5.296871614086902e-07 average time 0.003029961369998091 iter num 100\n",
            "loss 5.296705905905602e-07 average time 0.0029999645649877492 iter num 200\n",
            "loss 5.295701158778262e-07 average time 0.002967435540031147 iter num 100\n",
            "loss 5.295532957034419e-07 average time 0.003039384540034007 iter num 200\n",
            "loss 5.294568489271365e-07 average time 0.003160571359990172 iter num 100\n",
            "loss 5.294379988477326e-07 average time 0.0030687555749909735 iter num 200\n",
            "loss 5.293382430896949e-07 average time 0.0031802844800131423 iter num 100\n",
            "loss 5.293201853604712e-07 average time 0.00310916604499198 iter num 200\n",
            "loss 5.292214680688938e-07 average time 0.002948319940055626 iter num 100\n",
            "loss 5.292050954382645e-07 average time 0.002951005880017874 iter num 200\n",
            "loss 5.291040571126195e-07 average time 0.003196300459999293 iter num 100\n",
            "loss 5.290895535932765e-07 average time 0.003190196035000099 iter num 200\n",
            "loss 5.289904244250349e-07 average time 0.003033262050007579 iter num 100\n",
            "loss 5.289736461516715e-07 average time 0.0030024606850020063 iter num 200\n",
            "loss 5.288717957421363e-07 average time 0.002973171280000315 iter num 100\n",
            "loss 5.288562662356159e-07 average time 0.0029929601750200165 iter num 200\n",
            "loss 5.287568029867139e-07 average time 0.0032467391099953603 iter num 100\n",
            "loss 5.287402491906851e-07 average time 0.0031307410399995204 iter num 200\n",
            "loss 5.286426565174254e-07 average time 0.003073825290025525 iter num 100\n",
            "loss 5.286233027672565e-07 average time 0.0030384281750207263 iter num 200\n",
            "loss 5.28524933214584e-07 average time 0.0030387195600224005 iter num 100\n",
            "loss 5.285072956830658e-07 average time 0.0030574932050058123 iter num 200\n",
            "loss 5.284104125456765e-07 average time 0.0029483447500160765 iter num 100\n",
            "loss 5.283936024653817e-07 average time 0.0029564355500087914 iter num 200\n",
            "loss 5.28294386254375e-07 average time 0.0029949612600194088 iter num 100\n",
            "loss 5.282777196005907e-07 average time 0.002981197980016077 iter num 200\n",
            "loss 5.281774792065189e-07 average time 0.0030554931700407907 iter num 100\n",
            "loss 5.281599456388293e-07 average time 0.003017101380025906 iter num 200\n",
            "loss 5.280624660693426e-07 average time 0.0029674516600152858 iter num 100\n",
            "loss 5.280452715227054e-07 average time 0.0030391946999998253 iter num 200\n",
            "loss 5.279459458661716e-07 average time 0.003010564119981609 iter num 100\n",
            "loss 5.2793101111308e-07 average time 0.0030728208649975384 iter num 200\n",
            "loss 5.278317165636468e-07 average time 0.00304727267999624 iter num 100\n",
            "loss 5.278136310703792e-07 average time 0.0030587309399948024 iter num 200\n",
            "loss 5.277147792338118e-07 average time 0.0030362554200291923 iter num 100\n",
            "loss 5.276997003361364e-07 average time 0.0030147449750143095 iter num 200\n",
            "loss 5.276006986478285e-07 average time 0.003075235089991111 iter num 100\n",
            "loss 5.275825836761952e-07 average time 0.0030651211200120087 iter num 200\n",
            "loss 5.27483779722745e-07 average time 0.0029521671699967557 iter num 100\n",
            "loss 5.274674297946304e-07 average time 0.0029818313850023515 iter num 200\n",
            "loss 5.273699949240492e-07 average time 0.003172018329951243 iter num 100\n",
            "loss 5.273513340860842e-07 average time 0.0030766764649774814 iter num 200\n",
            "loss 5.272545613025008e-07 average time 0.0029807281999956105 iter num 100\n",
            "loss 5.272384435778616e-07 average time 0.002977674539988584 iter num 200\n",
            "loss 5.27140201230782e-07 average time 0.0029908577099877222 iter num 100\n",
            "loss 5.271225067034021e-07 average time 0.0029978004449844773 iter num 200\n",
            "loss 5.270242837201214e-07 average time 0.0030256967400237047 iter num 100\n",
            "loss 5.270071458367478e-07 average time 0.003054179810021651 iter num 200\n",
            "loss 5.269072905406165e-07 average time 0.0029648644800045077 iter num 100\n",
            "loss 5.268921914855414e-07 average time 0.002995997335010543 iter num 200\n",
            "loss 5.267947447978461e-07 average time 0.003063366959995619 iter num 100\n",
            "loss 5.267777051293975e-07 average time 0.00301528636500052 iter num 200\n",
            "loss 5.26678313797715e-07 average time 0.003036115710001468 iter num 100\n",
            "loss 5.266637782998324e-07 average time 0.0030395427700000257 iter num 200\n",
            "loss 5.265641598388262e-07 average time 0.002955900610008939 iter num 100\n",
            "loss 5.265460402530296e-07 average time 0.0029695305550012565 iter num 200\n",
            "loss 5.264478865299512e-07 average time 0.003135164409968638 iter num 100\n",
            "loss 5.264311289709873e-07 average time 0.003044998734985711 iter num 200\n",
            "loss 5.26336270413469e-07 average time 0.003051186569996389 iter num 100\n",
            "loss 5.263164926344014e-07 average time 0.003087287075002223 iter num 200\n",
            "loss 5.262188687364968e-07 average time 0.0030334006099565157 iter num 100\n",
            "loss 5.262022971014256e-07 average time 0.0030582414099808376 iter num 200\n",
            "loss 5.261034753025813e-07 average time 0.00307107136000468 iter num 100\n",
            "loss 5.260887092993698e-07 average time 0.003066863094989003 iter num 200\n",
            "loss 5.259892612193674e-07 average time 0.0029517140000052677 iter num 100\n",
            "loss 5.259712044235209e-07 average time 0.0030245044850062187 iter num 200\n",
            "loss 5.258751030681292e-07 average time 0.0029781450900100027 iter num 100\n",
            "loss 5.258581311190066e-07 average time 0.0029619367749978663 iter num 200\n",
            "loss 5.257609776199328e-07 average time 0.0030292136000070967 iter num 100\n",
            "loss 5.257439931032271e-07 average time 0.0030505790000006526 iter num 200\n",
            "loss 5.256475206157761e-07 average time 0.0030016563700201006 iter num 100\n",
            "loss 5.256296754470348e-07 average time 0.0030573764149994533 iter num 200\n",
            "loss 5.255300121513593e-07 average time 0.0030966703100421002 iter num 100\n",
            "loss 5.255146267190794e-07 average time 0.003044882145018164 iter num 200\n",
            "loss 5.254180029175326e-07 average time 0.003038517700010743 iter num 100\n",
            "loss 5.254007105357769e-07 average time 0.003046067909988324 iter num 200\n",
            "loss 5.253040229262548e-07 average time 0.0029985377500133838 iter num 100\n",
            "loss 5.252866069702192e-07 average time 0.003019375585006401 iter num 200\n",
            "loss 5.251879182177262e-07 average time 0.0029919160199824548 iter num 100\n",
            "loss 5.251717628124141e-07 average time 0.0030082150649764117 iter num 200\n",
            "loss 5.250734979543544e-07 average time 0.0029789105799818572 iter num 100\n",
            "loss 5.250561378729658e-07 average time 0.003009734209979342 iter num 200\n",
            "loss 5.249581156744816e-07 average time 0.0030438334600057715 iter num 100\n",
            "loss 5.249439131782687e-07 average time 0.0030292020100205264 iter num 200\n",
            "loss 5.248454008844529e-07 average time 0.003033346229976814 iter num 100\n",
            "loss 5.248294490054148e-07 average time 0.003011124749980354 iter num 200\n",
            "loss 5.247307598790031e-07 average time 0.0030791717199963385 iter num 100\n",
            "loss 5.247147325444095e-07 average time 0.0030259703250067106 iter num 200\n",
            "loss 5.246160673726655e-07 average time 0.003021923649980636 iter num 100\n",
            "loss 5.24601916076327e-07 average time 0.0030064656600006854 iter num 200\n",
            "loss 5.245060675174728e-07 average time 0.0030577337300064754 iter num 100\n",
            "loss 5.244896788073577e-07 average time 0.0030804516899979716 iter num 200\n",
            "loss 5.24392272185133e-07 average time 0.003179425169987553 iter num 100\n",
            "loss 5.243724595952752e-07 average time 0.003121833880002214 iter num 200\n",
            "loss 5.242760872759466e-07 average time 0.0031923400800133095 iter num 100\n",
            "loss 5.242600765469179e-07 average time 0.0032142557649876835 iter num 200\n",
            "loss 5.241643264578092e-07 average time 0.002978578950001065 iter num 100\n",
            "loss 5.241462112512289e-07 average time 0.0030223289949981337 iter num 200\n",
            "loss 5.240485178947304e-07 average time 0.0030816178199665955 iter num 100\n",
            "loss 5.240300135509774e-07 average time 0.0030646225999703346 iter num 200\n",
            "loss 5.239364459207198e-07 average time 0.0030664292599931284 iter num 100\n",
            "loss 5.239181682554007e-07 average time 0.0030538420899915764 iter num 200\n",
            "loss 5.238229090724805e-07 average time 0.0029886381299957065 iter num 100\n",
            "loss 5.238032910343693e-07 average time 0.0030365004299983413 iter num 200\n",
            "loss 5.237087732073844e-07 average time 0.003133688269963386 iter num 100\n",
            "loss 5.236906005258063e-07 average time 0.0030558250349827175 iter num 200\n",
            "loss 5.235936294475195e-07 average time 0.002978561540012379 iter num 100\n",
            "loss 5.235789392241732e-07 average time 0.003045797355002833 iter num 200\n",
            "loss 5.234830142758735e-07 average time 0.0030773428100292224 iter num 100\n",
            "loss 5.234649064507951e-07 average time 0.003020228515015333 iter num 200\n",
            "loss 5.233674633939766e-07 average time 0.003041562129974409 iter num 100\n",
            "loss 5.233527883003692e-07 average time 0.0030384607949918065 iter num 200\n",
            "loss 5.232543144935629e-07 average time 0.0030378627699974457 iter num 100\n",
            "loss 5.232376234933665e-07 average time 0.0030676120499902025 iter num 200\n",
            "loss 5.231418248628974e-07 average time 0.0030092231999879005 iter num 100\n",
            "loss 5.231270000190218e-07 average time 0.0030054547699796784 iter num 200\n",
            "loss 5.230283186342744e-07 average time 0.0030057881999800886 iter num 100\n",
            "loss 5.23012883686054e-07 average time 0.0030152406649949625 iter num 200\n",
            "loss 5.22915288295965e-07 average time 0.0030117802899849266 iter num 100\n",
            "loss 5.228996308309882e-07 average time 0.0030237493750018985 iter num 200\n",
            "loss 5.228036697865079e-07 average time 0.0030799272500007645 iter num 100\n",
            "loss 5.227847978784124e-07 average time 0.003027788104989213 iter num 200\n",
            "loss 5.22689484809252e-07 average time 0.0030850170899930163 iter num 100\n",
            "loss 5.226725000122686e-07 average time 0.0030568182650040398 iter num 200\n",
            "loss 5.225759000782738e-07 average time 0.0030417872999714745 iter num 100\n",
            "loss 5.225599982033994e-07 average time 0.003000206819979212 iter num 200\n",
            "loss 5.22464118187717e-07 average time 0.003063777300012589 iter num 100\n",
            "loss 5.224471927289859e-07 average time 0.003124344135021602 iter num 200\n",
            "loss 5.223510918157444e-07 average time 0.002969344709977122 iter num 100\n",
            "loss 5.223347586531765e-07 average time 0.0029981073399903834 iter num 200\n",
            "loss 5.222396956249342e-07 average time 0.003219781820002936 iter num 100\n",
            "loss 5.222225521330179e-07 average time 0.0031001724899874716 iter num 200\n",
            "loss 5.221245520220374e-07 average time 0.002962618299980022 iter num 100\n",
            "loss 5.221105772698089e-07 average time 0.0030342055699770754 iter num 200\n",
            "loss 5.22012190607476e-07 average time 0.0030812394400163614 iter num 100\n",
            "loss 5.219958849457562e-07 average time 0.0030310145999942507 iter num 200\n",
            "loss 5.218995920767337e-07 average time 0.003118963040010385 iter num 100\n",
            "loss 5.218834956630796e-07 average time 0.003051491860001079 iter num 200\n",
            "loss 5.217873062393703e-07 average time 0.0031040682800176 iter num 100\n",
            "loss 5.217701896363875e-07 average time 0.003099098370005322 iter num 200\n",
            "loss 5.216742153377319e-07 average time 0.002981743109994568 iter num 100\n",
            "loss 5.216583086729463e-07 average time 0.0030246648399997866 iter num 200\n",
            "loss 5.215616882875945e-07 average time 0.002977893350021077 iter num 100\n",
            "loss 5.215466680713061e-07 average time 0.0029858983700137287 iter num 200\n",
            "loss 5.214493396814825e-07 average time 0.0030252768200034552 iter num 100\n",
            "loss 5.214345396318949e-07 average time 0.0030497643900048387 iter num 200\n",
            "loss 5.213397127221704e-07 average time 0.003254766450013449 iter num 100\n",
            "loss 5.213230995713154e-07 average time 0.0031394253700068476 iter num 200\n",
            "loss 5.212252160018267e-07 average time 0.0029971268700273866 iter num 100\n",
            "loss 5.212087022984425e-07 average time 0.0031289846450295047 iter num 200\n",
            "loss 5.211145508045422e-07 average time 0.003158209449993592 iter num 100\n",
            "loss 5.211000244416377e-07 average time 0.0030522403349959857 iter num 200\n",
            "loss 5.210023532384608e-07 average time 0.003004378509967864 iter num 100\n",
            "loss 5.209851907145729e-07 average time 0.002988718339981915 iter num 200\n",
            "loss 5.208888322529504e-07 average time 0.002975493229987478 iter num 100\n",
            "loss 5.208713581395873e-07 average time 0.0029926782649954474 iter num 200\n",
            "loss 5.207760646344889e-07 average time 0.002971797660002267 iter num 100\n",
            "loss 5.207611203302297e-07 average time 0.0029794302800110017 iter num 200\n",
            "loss 5.20664575496625e-07 average time 0.002981779740002821 iter num 100\n",
            "loss 5.206455271156444e-07 average time 0.0029856662849897476 iter num 200\n",
            "loss 5.205536765331581e-07 average time 0.0029731515899948137 iter num 100\n",
            "loss 5.205371589869575e-07 average time 0.0030971269600013327 iter num 200\n",
            "loss 5.204424313644694e-07 average time 0.003138724449977417 iter num 100\n",
            "loss 5.204237588773141e-07 average time 0.0030411928599733073 iter num 200\n",
            "loss 5.203287666006856e-07 average time 0.002985764470022332 iter num 100\n",
            "loss 5.203126844939964e-07 average time 0.00296453869000743 iter num 200\n",
            "loss 5.202177160215148e-07 average time 0.003126528940015305 iter num 100\n",
            "loss 5.202017444990225e-07 average time 0.0030482659750123275 iter num 200\n",
            "loss 5.201048934872176e-07 average time 0.0030841499799907977 iter num 100\n",
            "loss 5.200895712330555e-07 average time 0.0031224961000066286 iter num 200\n",
            "loss 5.199943386999225e-07 average time 0.0029967672399925506 iter num 100\n",
            "loss 5.199795758332315e-07 average time 0.0030091157649872 iter num 200\n",
            "loss 5.198828998935736e-07 average time 0.0029752512299774025 iter num 100\n",
            "loss 5.198660752834376e-07 average time 0.0029953307899836545 iter num 200\n",
            "loss 5.197707227805563e-07 average time 0.003304301570001371 iter num 100\n",
            "loss 5.197529796076198e-07 average time 0.0031780515800051033 iter num 200\n",
            "loss 5.196586039619455e-07 average time 0.0030206573099940213 iter num 100\n",
            "loss 5.196428690459319e-07 average time 0.003005694144994777 iter num 200\n",
            "loss 5.195467999689358e-07 average time 0.003028040229983162 iter num 100\n",
            "loss 5.195303908107752e-07 average time 0.0030308244350089808 iter num 200\n",
            "loss 5.194371424697483e-07 average time 0.0030116412000097625 iter num 100\n",
            "loss 5.194218747927397e-07 average time 0.002997492890021931 iter num 200\n",
            "loss 5.193229332113648e-07 average time 0.003143686849975893 iter num 100\n",
            "loss 5.193091595921818e-07 average time 0.003069397075003053 iter num 200\n",
            "loss 5.192123979319101e-07 average time 0.003057454540003164 iter num 100\n",
            "loss 5.191972038067782e-07 average time 0.003049530735006556 iter num 200\n",
            "loss 5.191018712919589e-07 average time 0.0030247528500194675 iter num 100\n",
            "loss 5.190865404660797e-07 average time 0.0030175164750130532 iter num 200\n",
            "loss 5.189920622129638e-07 average time 0.0030548306099854017 iter num 100\n",
            "loss 5.189733191360269e-07 average time 0.0030680633150018365 iter num 200\n",
            "loss 5.188776490691708e-07 average time 0.0030044890299950567 iter num 100\n",
            "loss 5.188636165671671e-07 average time 0.003002249545004361 iter num 200\n",
            "loss 5.187671705708909e-07 average time 0.003022321480020764 iter num 100\n",
            "loss 5.187539917568393e-07 average time 0.0029938793700284806 iter num 200\n",
            "loss 5.18658994338233e-07 average time 0.0032708054499971696 iter num 100\n",
            "loss 5.186439210121351e-07 average time 0.0031240951100039637 iter num 200\n",
            "loss 5.185470872924144e-07 average time 0.002956912319987168 iter num 100\n",
            "loss 5.185297109022566e-07 average time 0.0029762771149944457 iter num 200\n",
            "loss 5.18435432823347e-07 average time 0.003008238930015068 iter num 100\n",
            "loss 5.184204748650203e-07 average time 0.002995664860004581 iter num 200\n",
            "loss 5.18324960184183e-07 average time 0.002983835240015651 iter num 100\n",
            "loss 5.183080235907583e-07 average time 0.0030228566949972446 iter num 200\n",
            "loss 5.182156014040267e-07 average time 0.002993045530015479 iter num 100\n",
            "loss 5.181981015089445e-07 average time 0.003019144820011661 iter num 200\n",
            "loss 5.181046271299144e-07 average time 0.0030992848799996863 iter num 100\n",
            "loss 5.180887382788623e-07 average time 0.003060932504995435 iter num 200\n",
            "loss 5.179952332056931e-07 average time 0.002949532669981636 iter num 100\n",
            "loss 5.17977736428953e-07 average time 0.0029694337399837423 iter num 200\n",
            "loss 5.17882780247971e-07 average time 0.003023922330003188 iter num 100\n",
            "loss 5.178657846218618e-07 average time 0.0030307401150025727 iter num 200\n",
            "loss 5.177728271887643e-07 average time 0.002985618929960765 iter num 100\n",
            "loss 5.177563447168592e-07 average time 0.0029757063599754473 iter num 200\n",
            "loss 5.176613951623616e-07 average time 0.002971868880022157 iter num 100\n",
            "loss 5.176454493149827e-07 average time 0.0030501543250034046 iter num 200\n",
            "loss 5.17551148091085e-07 average time 0.0030541445799872236 iter num 100\n",
            "loss 5.175356242122389e-07 average time 0.003007456159996309 iter num 200\n",
            "loss 5.174395425496692e-07 average time 0.0029997939399845565 iter num 100\n",
            "loss 5.174231856826103e-07 average time 0.003018449949988735 iter num 200\n",
            "loss 5.173313877291586e-07 average time 0.003034057349982504 iter num 100\n",
            "loss 5.173142726568036e-07 average time 0.0030653735149962814 iter num 200\n",
            "loss 5.172202534321925e-07 average time 0.003019070899981671 iter num 100\n",
            "loss 5.172032216016497e-07 average time 0.0030107281149798835 iter num 200\n",
            "loss 5.171097257460358e-07 average time 0.0029845170899761796 iter num 100\n",
            "loss 5.170951902164314e-07 average time 0.002984734464992016 iter num 200\n",
            "loss 5.169995251265118e-07 average time 0.0030983976999732475 iter num 100\n",
            "loss 5.169844653095218e-07 average time 0.0030375844549894284 iter num 200\n",
            "loss 5.168896652997382e-07 average time 0.0032200471099804416 iter num 100\n",
            "loss 5.168740229538005e-07 average time 0.0030924043049981266 iter num 200\n",
            "loss 5.1677990887145e-07 average time 0.003037676220010326 iter num 100\n",
            "loss 5.167641246717414e-07 average time 0.0031018038499814795 iter num 200\n",
            "loss 5.166686711570917e-07 average time 0.0030165723400114074 iter num 100\n",
            "loss 5.166552551123748e-07 average time 0.0030050557850040604 iter num 200\n",
            "loss 5.165599765978571e-07 average time 0.0030544018599857736 iter num 100\n",
            "loss 5.165444372602414e-07 average time 0.0030106225049939895 iter num 200\n",
            "loss 5.164490048445262e-07 average time 0.0029760631999943142 iter num 100\n",
            "loss 5.16434740285961e-07 average time 0.003040056179995645 iter num 200\n",
            "loss 5.163398544858354e-07 average time 0.0031328473600024153 iter num 100\n",
            "loss 5.163211513330174e-07 average time 0.0030618091900123543 iter num 200\n",
            "loss 5.162295738295841e-07 average time 0.0029691327199998343 iter num 100\n",
            "loss 5.162149025590681e-07 average time 0.0030209795549922093 iter num 200\n",
            "loss 5.161195812441643e-07 average time 0.0030914489100041464 iter num 100\n",
            "loss 5.161035743907349e-07 average time 0.003051792435001062 iter num 200\n",
            "loss 5.160103982988369e-07 average time 0.0029786342600027636 iter num 100\n",
            "loss 5.159940348362584e-07 average time 0.002972548735012879 iter num 200\n",
            "loss 5.159011391427325e-07 average time 0.0029628839400129437 iter num 100\n",
            "loss 5.158848657074912e-07 average time 0.0029768353300073614 iter num 200\n",
            "loss 5.157901288215741e-07 average time 0.003024185200010834 iter num 100\n",
            "loss 5.157761307243013e-07 average time 0.003007418789991334 iter num 200\n",
            "loss 5.156821541071241e-07 average time 0.003038285350012302 iter num 100\n",
            "loss 5.156666194106247e-07 average time 0.0030175403250109413 iter num 200\n",
            "loss 5.155734786023835e-07 average time 0.002989643800015074 iter num 100\n",
            "loss 5.155563114068624e-07 average time 0.002989910445000987 iter num 200\n",
            "loss 5.154619230251459e-07 average time 0.002995507479995467 iter num 100\n",
            "loss 5.154472476919415e-07 average time 0.0030076586549967034 iter num 200\n",
            "loss 5.153523876982593e-07 average time 0.003127394780008217 iter num 100\n",
            "loss 5.153379319838522e-07 average time 0.003135061939995012 iter num 200\n",
            "loss 5.152440618234491e-07 average time 0.0030132641200179932 iter num 100\n",
            "loss 5.15227101202317e-07 average time 0.003021194959992499 iter num 200\n",
            "loss 5.151347574637121e-07 average time 0.0029853523100382517 iter num 100\n",
            "loss 5.151198012560026e-07 average time 0.002968406670015611 iter num 200\n",
            "loss 5.150276575791873e-07 average time 0.002968485820015303 iter num 100\n",
            "loss 5.150103755042235e-07 average time 0.0029537534700193645 iter num 200\n",
            "loss 5.149153155304993e-07 average time 0.0030665003799913394 iter num 100\n",
            "loss 5.14901236631879e-07 average time 0.0030164445850004995 iter num 200\n",
            "loss 5.148058033783482e-07 average time 0.003139584459991056 iter num 100\n",
            "loss 5.147919768493716e-07 average time 0.0031165616100111036 iter num 200\n",
            "loss 5.146982206537324e-07 average time 0.0030290723999723923 iter num 100\n",
            "loss 5.146844255081228e-07 average time 0.003027293109976199 iter num 200\n",
            "loss 5.145896880370943e-07 average time 0.002995334560005176 iter num 100\n",
            "loss 5.145725065589697e-07 average time 0.002987660464993951 iter num 200\n",
            "loss 5.144809497300843e-07 average time 0.0030112530000087645 iter num 100\n",
            "loss 5.144630250172961e-07 average time 0.0030776122749966816 iter num 200\n",
            "loss 5.143736621895946e-07 average time 0.0032496971200407644 iter num 100\n",
            "loss 5.143555961169795e-07 average time 0.003113414960021146 iter num 200\n",
            "loss 5.142624693990704e-07 average time 0.0031863376699857327 iter num 100\n",
            "loss 5.14247608663964e-07 average time 0.003080265229998531 iter num 200\n",
            "loss 5.141537101614027e-07 average time 0.002993855030003942 iter num 100\n",
            "loss 5.141387382210048e-07 average time 0.0030045287300026757 iter num 200\n",
            "loss 5.140470885264191e-07 average time 0.003038296889990306 iter num 100\n",
            "loss 5.140294007839026e-07 average time 0.003009454875002575 iter num 200\n",
            "loss 5.139368185262718e-07 average time 0.0029560386199864296 iter num 100\n",
            "loss 5.139203540671514e-07 average time 0.002960188360002576 iter num 200\n",
            "loss 5.138280687310889e-07 average time 0.0029646308300198144 iter num 100\n",
            "loss 5.138116303041513e-07 average time 0.0030472354050061767 iter num 200\n",
            "loss 5.137166837309978e-07 average time 0.0030267050000065865 iter num 100\n",
            "loss 5.137028491068281e-07 average time 0.002999596890012981 iter num 200\n",
            "loss 5.136120796311604e-07 average time 0.0031375301400157697 iter num 100\n",
            "loss 5.135946589910562e-07 average time 0.0030455903949996354 iter num 200\n",
            "loss 5.135016768890067e-07 average time 0.0031028312999751505 iter num 100\n",
            "loss 5.134859634563459e-07 average time 0.003117606059972786 iter num 200\n",
            "loss 5.133930345611647e-07 average time 0.0030623944900207787 iter num 100\n",
            "loss 5.133788285793982e-07 average time 0.003070304025002315 iter num 200\n",
            "loss 5.132845349768742e-07 average time 0.002983489580014975 iter num 100\n",
            "loss 5.132689758526495e-07 average time 0.002983106805006628 iter num 200\n",
            "loss 5.13176210122548e-07 average time 0.0031314007999935713 iter num 100\n",
            "loss 5.131599550117735e-07 average time 0.003042006255002434 iter num 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqpasxVi0hx3"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "        dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx2)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.02]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.]*numstocks) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "B = 0.8 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# # option price\n",
        "# print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# # delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "# print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "zkzUgfoXrO8h",
        "outputId": "1f259d7f-c614-481d-f863-315e60502522"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())[0][0]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "correct_call_prices = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    correct_call_prices.append(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T))\n",
        "\n",
        "# plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "# plt.plot(prices, correct_call_prices, label = \"JAX_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(correct_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "7fb22c93-5b3b-47ad-df35-3140ad783c6c"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())[0][1]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.0, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"JAX_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgD1NjP7DOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6e52a8ed-2aa2-44d1-ed5e-2481da60cc9b"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.775, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())[0][1]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 0.775, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"JAX_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7MfujMQ7oij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "61e45c3d-4b03-4109-aa88-fdcde73d2974"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.225, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())[0][1]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.225, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"JAX_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvPo2XQllGVs"
      },
      "source": [
        "# Comparison on speed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWPtmiEVr77L",
        "outputId": "5522129a-a2c9-463d-d20d-d659a8b826f6"
      },
      "source": [
        "# time difference for computing delta\n",
        "%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)\n",
        "%timeit compute_delta(1).item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 415 ms per loop\n",
            "The slowest run took 941.36 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 365 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Co3op0Wfq_"
      },
      "source": [
        "# Heat Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fM2UuYBWfzY",
        "outputId": "fdd51358-e54c-4d65-a699-4bf0f256efe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cupy\n",
        "\n",
        "df = pd.DataFrame(np.zeros((25,10)))\n",
        "df.columns = np.around(np.linspace(0.005, 0.05, 10), 3)\n",
        "df.index = np.around(np.arange(0.77, 1.27, 0.02), 2)\n",
        "\n",
        "for col_num,approx_width in enumerate(df.columns):\n",
        "  base = []\n",
        "  approx = [] # approximated value calculated from base\n",
        "  model_value = [] # value given by model\n",
        "  for S in df.index:\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S - approx_width, 0.25, 0.02, 0.02]]).cuda()\n",
        "    base.append(model(inputs.float())[0][0])\n",
        "    approx.append(model(inputs.float())[0][0] + ((model(inputs.float())[0][1:]) * approx_width).sum())\n",
        "    inputs2 = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    model_value.append(model(inputs2.float())[0][0])\n",
        "  abs_error = (np.absolute(np.array(approx)-np.array(model_value))).astype(float)\n",
        "  df.iloc[:,col_num] = abs_error\n",
        "\n",
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.005</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.015</th>\n",
              "      <th>0.020</th>\n",
              "      <th>0.025</th>\n",
              "      <th>0.030</th>\n",
              "      <th>0.035</th>\n",
              "      <th>0.040</th>\n",
              "      <th>0.045</th>\n",
              "      <th>0.050</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.77</th>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.001707</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.002024</td>\n",
              "      <td>0.002091</td>\n",
              "      <td>0.002117</td>\n",
              "      <td>0.002113</td>\n",
              "      <td>0.002087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.79</th>\n",
              "      <td>0.001054</td>\n",
              "      <td>0.002033</td>\n",
              "      <td>0.002937</td>\n",
              "      <td>0.003748</td>\n",
              "      <td>0.004420</td>\n",
              "      <td>0.004947</td>\n",
              "      <td>0.005352</td>\n",
              "      <td>0.005670</td>\n",
              "      <td>0.005890</td>\n",
              "      <td>0.006019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.81</th>\n",
              "      <td>0.001377</td>\n",
              "      <td>0.002767</td>\n",
              "      <td>0.004211</td>\n",
              "      <td>0.005545</td>\n",
              "      <td>0.006794</td>\n",
              "      <td>0.007939</td>\n",
              "      <td>0.008981</td>\n",
              "      <td>0.009904</td>\n",
              "      <td>0.010657</td>\n",
              "      <td>0.011246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.83</th>\n",
              "      <td>0.001494</td>\n",
              "      <td>0.002946</td>\n",
              "      <td>0.004375</td>\n",
              "      <td>0.005870</td>\n",
              "      <td>0.007512</td>\n",
              "      <td>0.009159</td>\n",
              "      <td>0.010852</td>\n",
              "      <td>0.012412</td>\n",
              "      <td>0.013856</td>\n",
              "      <td>0.015167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.85</th>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.002455</td>\n",
              "      <td>0.004148</td>\n",
              "      <td>0.005951</td>\n",
              "      <td>0.007739</td>\n",
              "      <td>0.009474</td>\n",
              "      <td>0.011178</td>\n",
              "      <td>0.012944</td>\n",
              "      <td>0.014852</td>\n",
              "      <td>0.016755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.87</th>\n",
              "      <td>0.000963</td>\n",
              "      <td>0.002015</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.004420</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>0.007441</td>\n",
              "      <td>0.009434</td>\n",
              "      <td>0.011540</td>\n",
              "      <td>0.013622</td>\n",
              "      <td>0.015641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.89</th>\n",
              "      <td>0.000798</td>\n",
              "      <td>0.001726</td>\n",
              "      <td>0.002768</td>\n",
              "      <td>0.003915</td>\n",
              "      <td>0.005139</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.007870</td>\n",
              "      <td>0.009396</td>\n",
              "      <td>0.011068</td>\n",
              "      <td>0.012983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.91</th>\n",
              "      <td>0.000692</td>\n",
              "      <td>0.001478</td>\n",
              "      <td>0.002349</td>\n",
              "      <td>0.003299</td>\n",
              "      <td>0.004355</td>\n",
              "      <td>0.005540</td>\n",
              "      <td>0.006840</td>\n",
              "      <td>0.008246</td>\n",
              "      <td>0.009731</td>\n",
              "      <td>0.011310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.93</th>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.001844</td>\n",
              "      <td>0.002660</td>\n",
              "      <td>0.003591</td>\n",
              "      <td>0.004620</td>\n",
              "      <td>0.005739</td>\n",
              "      <td>0.006942</td>\n",
              "      <td>0.008254</td>\n",
              "      <td>0.009696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.95</th>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>0.001609</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>0.003070</td>\n",
              "      <td>0.003907</td>\n",
              "      <td>0.004842</td>\n",
              "      <td>0.005894</td>\n",
              "      <td>0.007064</td>\n",
              "      <td>0.008336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.97</th>\n",
              "      <td>0.000424</td>\n",
              "      <td>0.000910</td>\n",
              "      <td>0.001463</td>\n",
              "      <td>0.002085</td>\n",
              "      <td>0.002778</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>0.004352</td>\n",
              "      <td>0.005275</td>\n",
              "      <td>0.006269</td>\n",
              "      <td>0.007336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.99</th>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.000825</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.001891</td>\n",
              "      <td>0.002517</td>\n",
              "      <td>0.003209</td>\n",
              "      <td>0.003970</td>\n",
              "      <td>0.004805</td>\n",
              "      <td>0.005713</td>\n",
              "      <td>0.006687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.01</th>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>0.001607</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.002818</td>\n",
              "      <td>0.003517</td>\n",
              "      <td>0.004280</td>\n",
              "      <td>0.005109</td>\n",
              "      <td>0.006007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.03</th>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>0.001165</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.002811</td>\n",
              "      <td>0.003512</td>\n",
              "      <td>0.004278</td>\n",
              "      <td>0.005109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.05</th>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001601</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>0.002693</td>\n",
              "      <td>0.003349</td>\n",
              "      <td>0.004080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.07</th>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.001043</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.002229</td>\n",
              "      <td>0.002743</td>\n",
              "      <td>0.003336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.09</th>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.002547</td>\n",
              "      <td>0.003043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.11</th>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.001027</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.001684</td>\n",
              "      <td>0.002068</td>\n",
              "      <td>0.002494</td>\n",
              "      <td>0.002960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.13</th>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.001507</td>\n",
              "      <td>0.001890</td>\n",
              "      <td>0.002305</td>\n",
              "      <td>0.002757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.15</th>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.001213</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>0.001943</td>\n",
              "      <td>0.002380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.17</th>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.001646</td>\n",
              "      <td>0.002020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.19</th>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000286</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>0.000778</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.001245</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.001845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.21</th>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>0.001440</td>\n",
              "      <td>0.001732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.23</th>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>0.000973</td>\n",
              "      <td>0.001231</td>\n",
              "      <td>0.001516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.25</th>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.000927</td>\n",
              "      <td>0.001183</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0.005     0.010     0.015  ...     0.040     0.045     0.050\n",
              "0.77  0.000590  0.001057  0.001418  ...  0.002117  0.002113  0.002087\n",
              "0.79  0.001054  0.002033  0.002937  ...  0.005670  0.005890  0.006019\n",
              "0.81  0.001377  0.002767  0.004211  ...  0.009904  0.010657  0.011246\n",
              "0.83  0.001494  0.002946  0.004375  ...  0.012412  0.013856  0.015167\n",
              "0.85  0.001117  0.002455  0.004148  ...  0.012944  0.014852  0.016755\n",
              "0.87  0.000963  0.002015  0.003165  ...  0.011540  0.013622  0.015641\n",
              "0.89  0.000798  0.001726  0.002768  ...  0.009396  0.011068  0.012983\n",
              "0.91  0.000692  0.001478  0.002349  ...  0.008246  0.009731  0.011310\n",
              "0.93  0.000536  0.001142  0.001844  ...  0.006942  0.008254  0.009696\n",
              "0.95  0.000478  0.001013  0.001609  ...  0.005894  0.007064  0.008336\n",
              "0.97  0.000424  0.000910  0.001463  ...  0.005275  0.006269  0.007336\n",
              "0.99  0.000382  0.000825  0.001327  ...  0.004805  0.005713  0.006687\n",
              "1.01  0.000294  0.000658  0.001095  ...  0.004280  0.005109  0.006007\n",
              "1.03  0.000178  0.000427  0.000760  ...  0.003512  0.004278  0.005109\n",
              "1.05  0.000133  0.000312  0.000546  ...  0.002693  0.003349  0.004080\n",
              "1.07  0.000132  0.000302  0.000509  ...  0.002229  0.002743  0.003336\n",
              "1.09  0.000136  0.000306  0.000509  ...  0.002103  0.002547  0.003043\n",
              "1.11  0.000126  0.000295  0.000507  ...  0.002068  0.002494  0.002960\n",
              "1.13  0.000088  0.000216  0.000386  ...  0.001890  0.002305  0.002757\n",
              "1.15  0.000063  0.000158  0.000289  ...  0.001555  0.001943  0.002380\n",
              "1.17  0.000063  0.000150  0.000263  ...  0.001317  0.001646  0.002020\n",
              "1.19  0.000073  0.000169  0.000286  ...  0.001245  0.001526  0.001845\n",
              "1.21  0.000049  0.000127  0.000235  ...  0.001177  0.001440  0.001732\n",
              "1.23  0.000017  0.000064  0.000139  ...  0.000973  0.001231  0.001516\n",
              "1.25  0.000009  0.000006  0.000049  ...  0.000704  0.000927  0.001183\n",
              "\n",
              "[25 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XsbWXELWf5D",
        "outputId": "059701d4-8b7c-48ec-b921-82981bd24577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "fig, ax = plt.subplots(figsize=(11, 9))\n",
        "sb.heatmap(df, cmap='Blues')\n",
        "plt.title('Absolute Errors between Linear-Approximated Prices and NN Model Prices (K=1, B=0.8)')\n",
        "plt.xlabel('Approximation Width')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}