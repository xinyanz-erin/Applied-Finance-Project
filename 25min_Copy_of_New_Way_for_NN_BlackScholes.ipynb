{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of New Way for NN_BlackScholes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/25min_Copy_of_New_Way_for_NN_BlackScholes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtECD9BnR2NC"
      },
      "source": [
        "# import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UddgD2b1Rz0p"
      },
      "source": [
        "# iv = 0.005\n",
        "# rnd_s0 = np.arange(0.75, 1.25 + iv, iv)\n",
        "# print(len(rnd_s0))\n",
        "# initial_stocks = np.array([rnd_s0[i] for i in np.random.randint(0, len(rnd_s0)+1, 2)])\n",
        "# initial_stocks"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2gIC6kxF0ks"
      },
      "source": [
        "# Use Black-Scholes to Generate Data\n",
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5_EBlCgHsFt",
        "outputId": "c217c30d-71cd-4241-b2b5-b7f27f04e122"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0   5851      0 --:--:-- --:--:-- --:--:--  5851\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg1VnsiZF3GM",
        "outputId": "61606c57-9638-48fd-e104-162b5f1855ca"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "    return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "          \n",
        "          ################# intervals\n",
        "\n",
        "          iv_s0 = 0.005\n",
        "          iv_sigma = 0.005\n",
        "          iv_r = 0.005\n",
        "          iv_K = 0.005\n",
        "\n",
        "          # iv_s0 = (1.25-0.75)/80\n",
        "          # iv_sigma = (0.45-0.15)/40\n",
        "          # iv_r = (0.6-0.25)/40\n",
        "          # iv_K = (1.25-0.75)/80\n",
        "\n",
        "          # initial_stocks = np.array(np.random.randint(75,125,self.N_STOCKS)/100)\n",
        "          # iv_s0 = 0.005\n",
        "          rnd_s0 = np.arange(0.75, 1.25 + iv_s0, iv_s0)\n",
        "          initial_stocks = np.array([rnd_s0[i] for i in np.random.randint(0, len(rnd_s0), self.N_STOCKS)])\n",
        "\n",
        "          corr = np.diag(np.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "\n",
        "          # sigma = np.array(np.random.randint(15,45,self.N_STOCKS)/100)\n",
        "\n",
        "          # iv_sigma = 0.005\n",
        "          rnd_sigma = np.arange(0.15, 0.45 + iv_sigma, iv_sigma)\n",
        "          sigma = np.array([rnd_sigma[i] for i in np.random.randint(0, len(rnd_sigma), self.N_STOCKS)])\n",
        "\n",
        "          cov = (np.diag(sigma)).dot(corr).dot(np.diag(sigma))\n",
        "\n",
        "          # r = np.repeat(np.array(np.random.randint(25,60)/100), self.N_STOCKS)\n",
        "          # iv_r = 0.005\n",
        "          rnd_r = np.arange(0.25, 0.6 + iv_r, iv_r)\n",
        "          r = np.repeat(rnd_r[np.random.randint(0,len(rnd_r), 1)], self.N_STOCKS)\n",
        "\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          # K = np.random.randint(75,125)/100\n",
        "          # iv_K = 0.005\n",
        "          rnd_K = np.arange(0.75, 1.25 + iv_K, iv_K)\n",
        "          K = rnd_K[np.random.randint(0, len(rnd_K), 1)]\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "\n",
        "          European_Call_price = bs_call(initial_stocks,K,T,r,sigma)\n",
        "          Deltas = bs_delta(initial_stocks,K,T,r,sigma)\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price[0]\n",
        "          Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (np.repeat(np.array(T), self.N_STOCKS), np.repeat(np.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNmazdSvGEho",
        "outputId": "ae23097c-e60b-4841-e7d3-884a9afa2acb"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.35, 0.35]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.25, 0.25]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJNy_JNHGFGh"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNp9_sibGQSJ",
        "outputId": "84ddac06-855f-4bd9-e74a-1965bbb3d8cd"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.7)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lZmDAkPiGWyz",
        "outputId": "a3fa2cd1-fdf4-477f-a458-00d579ea9ea7"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 5000000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 1000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.4448133111000061 average time 0.0405348619000506 iter num 20\n",
            "loss 0.31619197130203247 average time 0.03590754235001441 iter num 40\n",
            "loss 0.29904985427856445 average time 0.034349599033157575 iter num 60\n",
            "loss 0.30566641688346863 average time 0.03379308505000154 iter num 80\n",
            "loss 0.3078719973564148 average time 0.03347563996001554 iter num 100\n",
            "loss 0.2587352693080902 average time 0.03225377674989431 iter num 20\n",
            "loss 0.24022358655929565 average time 0.03150175707478411 iter num 40\n",
            "loss 0.2511121332645416 average time 0.03178981366660689 iter num 60\n",
            "loss 0.2122032791376114 average time 0.031626615862433025 iter num 80\n",
            "loss 0.20464788377285004 average time 0.03145031098985782 iter num 100\n",
            "loss 0.17693118751049042 average time 0.032991700250022404 iter num 20\n",
            "loss 0.16536381840705872 average time 0.03258067655006016 iter num 40\n",
            "loss 0.14080579578876495 average time 0.032364361799955076 iter num 60\n",
            "loss 0.1359681338071823 average time 0.03229869812498691 iter num 80\n",
            "loss 0.1398068070411682 average time 0.032220461289871306 iter num 100\n",
            "loss 0.11300078779459 average time 0.033483372149567 iter num 20\n",
            "loss 0.08996444940567017 average time 0.03252401219970125 iter num 40\n",
            "loss 0.07844118028879166 average time 0.03257316041635931 iter num 60\n",
            "loss 0.055877089500427246 average time 0.032374652462158336 iter num 80\n",
            "loss 0.06844257563352585 average time 0.03216173414970399 iter num 100\n",
            "loss 0.03847464174032211 average time 0.0328365222994762 iter num 20\n",
            "loss 0.027613891288638115 average time 0.032251849324984504 iter num 40\n",
            "loss 0.024585282430052757 average time 0.032215899349951845 iter num 60\n",
            "loss 0.018916785717010498 average time 0.03200509571229304 iter num 80\n",
            "loss 0.020518537610769272 average time 0.03186796982987289 iter num 100\n",
            "loss 0.011178849264979362 average time 0.032337459299924376 iter num 20\n",
            "loss 0.01815461926162243 average time 0.03203788672462906 iter num 40\n",
            "loss 0.017262883484363556 average time 0.03213804579960803 iter num 60\n",
            "loss 0.015510422177612782 average time 0.03198526234982637 iter num 80\n",
            "loss 0.01884464919567108 average time 0.031838048539902956 iter num 100\n",
            "loss 0.011618375778198242 average time 0.03284577385020384 iter num 20\n",
            "loss 0.013163420371711254 average time 0.032460439999977095 iter num 40\n",
            "loss 0.01372029073536396 average time 0.0322066510332661 iter num 60\n",
            "loss 0.016594095155596733 average time 0.03214326649999748 iter num 80\n",
            "loss 0.012839075177907944 average time 0.03216877024005953 iter num 100\n",
            "loss 0.008982162922620773 average time 0.0329701868500706 iter num 20\n",
            "loss 0.018081502988934517 average time 0.032733887975155085 iter num 40\n",
            "loss 0.009914027526974678 average time 0.032744189266607764 iter num 60\n",
            "loss 0.013388440944254398 average time 0.032487428824970264 iter num 80\n",
            "loss 0.014872375875711441 average time 0.03235021590007818 iter num 100\n",
            "loss 0.009727013297379017 average time 0.03237233284962713 iter num 20\n",
            "loss 0.013333342038094997 average time 0.03204385639965039 iter num 40\n",
            "loss 0.010189159773290157 average time 0.03178785618317003 iter num 60\n",
            "loss 0.008240689523518085 average time 0.03188084474973039 iter num 80\n",
            "loss 0.015538892708718777 average time 0.03189791301985679 iter num 100\n",
            "loss 0.014869997277855873 average time 0.03344683949926548 iter num 20\n",
            "loss 0.010832383297383785 average time 0.032629916274618156 iter num 40\n",
            "loss 0.009729159995913506 average time 0.032653910599886635 iter num 60\n",
            "loss 0.014596515335142612 average time 0.032584101737347736 iter num 80\n",
            "loss 0.01716696284711361 average time 0.03236590371983766 iter num 100\n",
            "loss 0.007779385428875685 average time 0.03331262675055768 iter num 20\n",
            "loss 0.010581941343843937 average time 0.03267785102525522 iter num 40\n",
            "loss 0.008922873064875603 average time 0.03263905260027968 iter num 60\n",
            "loss 0.014947512187063694 average time 0.03244563872526669 iter num 80\n",
            "loss 0.017167463898658752 average time 0.03237090555023315 iter num 100\n",
            "loss 0.008672495372593403 average time 0.03303978335043212 iter num 20\n",
            "loss 0.009640367701649666 average time 0.032507970000369826 iter num 40\n",
            "loss 0.014447090215981007 average time 0.03268018203349735 iter num 60\n",
            "loss 0.012992366217076778 average time 0.032591289824949854 iter num 80\n",
            "loss 0.01131737045943737 average time 0.03251450352992833 iter num 100\n",
            "loss 0.0072135706432163715 average time 0.033710378499563376 iter num 20\n",
            "loss 0.011112881824374199 average time 0.03263015474967688 iter num 40\n",
            "loss 0.01564822904765606 average time 0.03268990976630448 iter num 60\n",
            "loss 0.005963844247162342 average time 0.03241729073724855 iter num 80\n",
            "loss 0.011610371060669422 average time 0.032260982409788995 iter num 100\n",
            "loss 0.006166666280478239 average time 0.03277695800024958 iter num 20\n",
            "loss 0.010606303811073303 average time 0.03200290490021871 iter num 40\n",
            "loss 0.010415039956569672 average time 0.03198171133335563 iter num 60\n",
            "loss 0.01429490465670824 average time 0.03205187114995169 iter num 80\n",
            "loss 0.014726006425917149 average time 0.032098120959963124 iter num 100\n",
            "loss 0.015647051855921745 average time 0.032903146049829954 iter num 20\n",
            "loss 0.007225247099995613 average time 0.03241634129981321 iter num 40\n",
            "loss 0.016997402533888817 average time 0.03297923693310925 iter num 60\n",
            "loss 0.009501637890934944 average time 0.03305848761228845 iter num 80\n",
            "loss 0.02700638771057129 average time 0.032856526619834764 iter num 100\n",
            "loss 0.006354959681630135 average time 0.03246087425013684 iter num 20\n",
            "loss 0.012085530906915665 average time 0.03242736655010958 iter num 40\n",
            "loss 0.00567243667319417 average time 0.032395118666621175 iter num 60\n",
            "loss 0.00712892971932888 average time 0.032198775612414465 iter num 80\n",
            "loss 0.008183951489627361 average time 0.03198534684001061 iter num 100\n",
            "loss 0.007231336086988449 average time 0.03296134359952703 iter num 20\n",
            "loss 0.010653285309672356 average time 0.03228819787473185 iter num 40\n",
            "loss 0.014150110073387623 average time 0.032176665249850585 iter num 60\n",
            "loss 0.010161939077079296 average time 0.0321352023124291 iter num 80\n",
            "loss 0.012623731978237629 average time 0.032071673219943475 iter num 100\n",
            "loss 0.013673711568117142 average time 0.03331067765029729 iter num 20\n",
            "loss 0.019896255806088448 average time 0.032822955424944664 iter num 40\n",
            "loss 0.011305327527225018 average time 0.03276953304963778 iter num 60\n",
            "loss 0.01117030717432499 average time 0.03260833616222954 iter num 80\n",
            "loss 0.007282466161996126 average time 0.032692489619657865 iter num 100\n",
            "loss 0.014703381806612015 average time 0.03247464729975036 iter num 20\n",
            "loss 0.013516385108232498 average time 0.03202336397471299 iter num 40\n",
            "loss 0.00907102134078741 average time 0.03191572778305272 iter num 60\n",
            "loss 0.009303317405283451 average time 0.032177678999732964 iter num 80\n",
            "loss 0.00942144449800253 average time 0.032105014519838736 iter num 100\n",
            "loss 0.010108082555234432 average time 0.03332529109975439 iter num 20\n",
            "loss 0.010132795199751854 average time 0.03299947972482187 iter num 40\n",
            "loss 0.008788215927779675 average time 0.03258397573323843 iter num 60\n",
            "loss 0.014022653922438622 average time 0.032335909225093926 iter num 80\n",
            "loss 0.0067080906592309475 average time 0.03205959978007741 iter num 100\n",
            "loss 0.010253208689391613 average time 0.03299076550047175 iter num 20\n",
            "loss 0.008853232488036156 average time 0.03285778810022748 iter num 40\n",
            "loss 0.008794154971837997 average time 0.03259742780004065 iter num 60\n",
            "loss 0.0067106448113918304 average time 0.03233598716260531 iter num 80\n",
            "loss 0.007317491807043552 average time 0.03228084023008705 iter num 100\n",
            "loss 0.00634849863126874 average time 0.033886760850327845 iter num 20\n",
            "loss 0.009989378973841667 average time 0.03318975490010416 iter num 40\n",
            "loss 0.008483153767883778 average time 0.03268288451672561 iter num 60\n",
            "loss 0.004316023550927639 average time 0.032407778625156425 iter num 80\n",
            "loss 0.008773010224103928 average time 0.032236544460138246 iter num 100\n",
            "loss 0.010470401495695114 average time 0.03290562965030404 iter num 20\n",
            "loss 0.008945832028985023 average time 0.03254608787538018 iter num 40\n",
            "loss 0.0057237353175878525 average time 0.03207349938365951 iter num 60\n",
            "loss 0.011614772491157055 average time 0.032040704900282435 iter num 80\n",
            "loss 0.007455012295395136 average time 0.031926809620126735 iter num 100\n",
            "loss 0.007354004308581352 average time 0.032774166200033504 iter num 20\n",
            "loss 0.010418652556836605 average time 0.032604863225242295 iter num 40\n",
            "loss 0.008155192248523235 average time 0.03237330726691046 iter num 60\n",
            "loss 0.012461993843317032 average time 0.032116163462569604 iter num 80\n",
            "loss 0.0058023943565785885 average time 0.032033246620158023 iter num 100\n",
            "loss 0.007202551234513521 average time 0.03443481390004308 iter num 20\n",
            "loss 0.010202900506556034 average time 0.03340714362484505 iter num 40\n",
            "loss 0.010439454577863216 average time 0.03283925419970425 iter num 60\n",
            "loss 0.008362030610442162 average time 0.0324541552622577 iter num 80\n",
            "loss 0.004030484240502119 average time 0.03221173596975859 iter num 100\n",
            "loss 0.004674943629652262 average time 0.033067126099740565 iter num 20\n",
            "loss 0.008784844540059566 average time 0.032925003950185786 iter num 40\n",
            "loss 0.007351327687501907 average time 0.032667777883458864 iter num 60\n",
            "loss 0.0070352195762097836 average time 0.03247308346258251 iter num 80\n",
            "loss 0.006002307869493961 average time 0.03226927768006135 iter num 100\n",
            "loss 0.0039154114201664925 average time 0.03226228184994397 iter num 20\n",
            "loss 0.005179367959499359 average time 0.03217772514990429 iter num 40\n",
            "loss 0.006889284588396549 average time 0.031992247150022496 iter num 60\n",
            "loss 0.007639885414391756 average time 0.03214848336251634 iter num 80\n",
            "loss 0.013064186088740826 average time 0.03218677655004285 iter num 100\n",
            "loss 0.012059819884598255 average time 0.03372241084998677 iter num 20\n",
            "loss 0.006569224875420332 average time 0.033192776899886665 iter num 40\n",
            "loss 0.00847790390253067 average time 0.032865879033124656 iter num 60\n",
            "loss 0.007379034534096718 average time 0.03286104494991378 iter num 80\n",
            "loss 0.00562466261908412 average time 0.032727050189969305 iter num 100\n",
            "loss 0.009814667515456676 average time 0.032776607149935445 iter num 20\n",
            "loss 0.00421183742582798 average time 0.0326570112751142 iter num 40\n",
            "loss 0.004750235006213188 average time 0.032251808616698935 iter num 60\n",
            "loss 0.00722168292850256 average time 0.03214105685015056 iter num 80\n",
            "loss 0.005707345437258482 average time 0.032156273970213076 iter num 100\n",
            "loss 0.004553701728582382 average time 0.033667318200059526 iter num 20\n",
            "loss 0.003793619107455015 average time 0.03300482715003454 iter num 40\n",
            "loss 0.0062298825941979885 average time 0.0331877608000165 iter num 60\n",
            "loss 0.007352923043072224 average time 0.03299859291273606 iter num 80\n",
            "loss 0.005300772376358509 average time 0.032985805750104194 iter num 100\n",
            "loss 0.01010765042155981 average time 0.03370921650002856 iter num 20\n",
            "loss 0.002216959837824106 average time 0.0334095849501864 iter num 40\n",
            "loss 0.00597462197765708 average time 0.0327749782334043 iter num 60\n",
            "loss 0.0049867513589560986 average time 0.03265375751261672 iter num 80\n",
            "loss 0.008496805094182491 average time 0.0325899995801592 iter num 100\n",
            "loss 0.002577259438112378 average time 0.03408150985032989 iter num 20\n",
            "loss 0.008586594834923744 average time 0.032785110225358946 iter num 40\n",
            "loss 0.0036691196728497744 average time 0.03235302780024843 iter num 60\n",
            "loss 0.008471223525702953 average time 0.032186802937621904 iter num 80\n",
            "loss 0.007622183300554752 average time 0.03210124900018854 iter num 100\n",
            "loss 0.007572826463729143 average time 0.03342252004968031 iter num 20\n",
            "loss 0.01283351518213749 average time 0.032541106624830715 iter num 40\n",
            "loss 0.007242000661790371 average time 0.032192240083228775 iter num 60\n",
            "loss 0.006047938950359821 average time 0.03224821674994018 iter num 80\n",
            "loss 0.003839918179437518 average time 0.032152762989862825 iter num 100\n",
            "loss 0.009051622822880745 average time 0.034643444249741154 iter num 20\n",
            "loss 0.003877986455336213 average time 0.03322799172510713 iter num 40\n",
            "loss 0.004645492881536484 average time 0.03261632810002387 iter num 60\n",
            "loss 0.008234240114688873 average time 0.03252393576244685 iter num 80\n",
            "loss 0.006163773592561483 average time 0.03239728887980164 iter num 100\n",
            "loss 0.004372898954898119 average time 0.03364631520016701 iter num 20\n",
            "loss 0.0032168824691325426 average time 0.032903530325256725 iter num 40\n",
            "loss 0.006957958452403545 average time 0.03244086683334899 iter num 60\n",
            "loss 0.007892285473644733 average time 0.0321311235624762 iter num 80\n",
            "loss 0.007884209975600243 average time 0.03225479000007908 iter num 100\n",
            "loss 0.002326695481315255 average time 0.033491723100269155 iter num 20\n",
            "loss 0.005227372981607914 average time 0.03244508330017197 iter num 40\n",
            "loss 0.004174917470663786 average time 0.03219377961674278 iter num 60\n",
            "loss 0.0018895863322541118 average time 0.032083154725069105 iter num 80\n",
            "loss 0.002884372603148222 average time 0.031952264110041144 iter num 100\n",
            "loss 0.004621093161404133 average time 0.032307730950014955 iter num 20\n",
            "loss 0.0029331394471228123 average time 0.03179701102490071 iter num 40\n",
            "loss 0.003082430921494961 average time 0.03169997498328788 iter num 60\n",
            "loss 0.008696001954376698 average time 0.03175846174972321 iter num 80\n",
            "loss 0.002487955382093787 average time 0.03183410078971065 iter num 100\n",
            "loss 0.004764029756188393 average time 0.033455461200355786 iter num 20\n",
            "loss 0.003053242340683937 average time 0.032691311325288554 iter num 40\n",
            "loss 0.009185465052723885 average time 0.03221137558345314 iter num 60\n",
            "loss 0.004987776279449463 average time 0.03198192312515857 iter num 80\n",
            "loss 0.004921642132103443 average time 0.03203190637010266 iter num 100\n",
            "loss 0.004789163824170828 average time 0.033371606850414536 iter num 20\n",
            "loss 0.0028769883792847395 average time 0.03271105322528456 iter num 40\n",
            "loss 0.0025303722359240055 average time 0.032209397416772846 iter num 60\n",
            "loss 0.003613339038565755 average time 0.03196784257497711 iter num 80\n",
            "loss 0.002384814200922847 average time 0.03185717445005139 iter num 100\n",
            "loss 0.003427784191444516 average time 0.032952465000016674 iter num 20\n",
            "loss 0.0036654863506555557 average time 0.032116711225262405 iter num 40\n",
            "loss 0.004849929828196764 average time 0.032287011116750364 iter num 60\n",
            "loss 0.0031758334953337908 average time 0.03244269718757096 iter num 80\n",
            "loss 0.00817143265157938 average time 0.03253919589005818 iter num 100\n",
            "loss 0.00755220465362072 average time 0.033232194099946356 iter num 20\n",
            "loss 0.0028015512507408857 average time 0.03256860944975415 iter num 40\n",
            "loss 0.001126385759562254 average time 0.032174024416660056 iter num 60\n",
            "loss 0.0019894058350473642 average time 0.03204344506243615 iter num 80\n",
            "loss 0.005684535950422287 average time 0.03192392135002592 iter num 100\n",
            "loss 0.0038260677829384804 average time 0.03420652850018087 iter num 20\n",
            "loss 0.0052068536169826984 average time 0.0330145626254307 iter num 40\n",
            "loss 0.006593756377696991 average time 0.032710258816829686 iter num 60\n",
            "loss 0.003666124539449811 average time 0.03243528926268482 iter num 80\n",
            "loss 0.004206669516861439 average time 0.03248488073026237 iter num 100\n",
            "loss 0.0047004204243421555 average time 0.03304502919982042 iter num 20\n",
            "loss 0.0023373530711978674 average time 0.03256438094995247 iter num 40\n",
            "loss 0.0018110217060893774 average time 0.03234666761648744 iter num 60\n",
            "loss 0.00510069727897644 average time 0.032160161712408805 iter num 80\n",
            "loss 0.001634232234209776 average time 0.032083386890008114 iter num 100\n",
            "loss 0.005553677212446928 average time 0.03314031575009722 iter num 20\n",
            "loss 0.005350810941308737 average time 0.03255792505015052 iter num 40\n",
            "loss 0.0029545887373387814 average time 0.03255123418354439 iter num 60\n",
            "loss 0.005791439674794674 average time 0.03231056793774769 iter num 80\n",
            "loss 0.0019188440637663007 average time 0.032155497560161166 iter num 100\n",
            "loss 0.0036453469656407833 average time 0.033448945550480856 iter num 20\n",
            "loss 0.004190939944237471 average time 0.03281397590026245 iter num 40\n",
            "loss 0.0024591945111751556 average time 0.03265078651675139 iter num 60\n",
            "loss 0.001762698288075626 average time 0.03260642555005688 iter num 80\n",
            "loss 0.0031088029500097036 average time 0.03254711383004178 iter num 100\n",
            "loss 0.00401072483509779 average time 0.03277946084981522 iter num 20\n",
            "loss 0.0015445428434759378 average time 0.032288865249756785 iter num 40\n",
            "loss 0.002223951043561101 average time 0.03200650594978167 iter num 60\n",
            "loss 0.0009491503587923944 average time 0.03203620043741466 iter num 80\n",
            "loss 0.0006417623953893781 average time 0.03195804967006552 iter num 100\n",
            "loss 0.00555309047922492 average time 0.032930691549699985 iter num 20\n",
            "loss 0.0027616049628704786 average time 0.03256694127485389 iter num 40\n",
            "loss 0.002187584526836872 average time 0.03244982991654979 iter num 60\n",
            "loss 0.0029159034602344036 average time 0.03236246422475233 iter num 80\n",
            "loss 0.0020441580563783646 average time 0.03252107884978614 iter num 100\n",
            "loss 0.0033250506967306137 average time 0.03306714840000495 iter num 20\n",
            "loss 0.001724271452985704 average time 0.03234871047488923 iter num 40\n",
            "loss 0.002166574588045478 average time 0.032181132566680995 iter num 60\n",
            "loss 0.0022151796147227287 average time 0.03210194771240822 iter num 80\n",
            "loss 0.0007601660909131169 average time 0.032139648779957494 iter num 100\n",
            "loss 0.001206896617077291 average time 0.032725089900122836 iter num 20\n",
            "loss 0.0013779508881270885 average time 0.03255257059990981 iter num 40\n",
            "loss 0.0026420357171446085 average time 0.032489696733258216 iter num 60\n",
            "loss 0.0034643453545868397 average time 0.03245499526237836 iter num 80\n",
            "loss 0.004136358853429556 average time 0.03247171880993847 iter num 100\n",
            "loss 0.0008188009378500283 average time 0.03248994215009589 iter num 20\n",
            "loss 0.002318487735465169 average time 0.03220203400014725 iter num 40\n",
            "loss 0.0016223021084442735 average time 0.032214821083349915 iter num 60\n",
            "loss 0.0007341459859162569 average time 0.032159288725051735 iter num 80\n",
            "loss 0.0010043970542028546 average time 0.03218313057008345 iter num 100\n",
            "loss 0.0012004171730950475 average time 0.03259365970006911 iter num 20\n",
            "loss 0.0013838527956977487 average time 0.032423105574980585 iter num 40\n",
            "loss 0.001166741014458239 average time 0.032153915250091814 iter num 60\n",
            "loss 0.0012475312687456608 average time 0.0319892760874609 iter num 80\n",
            "loss 0.0018917166162282228 average time 0.03209321051999723 iter num 100\n",
            "loss 0.0018814363284036517 average time 0.03262385204998282 iter num 20\n",
            "loss 0.0014229664811864495 average time 0.03223673142520056 iter num 40\n",
            "loss 0.0017714111600071192 average time 0.03197806888335132 iter num 60\n",
            "loss 0.0014117761747911572 average time 0.03206494483742972 iter num 80\n",
            "loss 0.002120459685102105 average time 0.03189963153996359 iter num 100\n",
            "loss 0.0015169157413765788 average time 0.03299556930105609 iter num 20\n",
            "loss 0.0009503144538030028 average time 0.033170907450494266 iter num 40\n",
            "loss 0.0031290778424590826 average time 0.032807653717039406 iter num 60\n",
            "loss 0.000696162402164191 average time 0.032919248637563216 iter num 80\n",
            "loss 0.00046892333193682134 average time 0.03294173767004395 iter num 100\n",
            "loss 0.0011311041889712214 average time 0.034053809199758686 iter num 20\n",
            "loss 0.0018970320234075189 average time 0.0329848492493511 iter num 40\n",
            "loss 0.0010246551828458905 average time 0.03247585538295728 iter num 60\n",
            "loss 0.0026271247770637274 average time 0.03240987109970774 iter num 80\n",
            "loss 0.001862351200543344 average time 0.03230904825984908 iter num 100\n",
            "loss 0.0030660186894237995 average time 0.0334162691999154 iter num 20\n",
            "loss 0.004160967655479908 average time 0.03301443960008328 iter num 40\n",
            "loss 0.0021367433946579695 average time 0.03256996980004866 iter num 60\n",
            "loss 0.0009417756809853017 average time 0.032411588925060644 iter num 80\n",
            "loss 0.005094411317259073 average time 0.032395365840020535 iter num 100\n",
            "loss 0.001012300723232329 average time 0.03264465919983195 iter num 20\n",
            "loss 0.004815829452127218 average time 0.03208533047481978 iter num 40\n",
            "loss 0.0023558211978524923 average time 0.032474591716527355 iter num 60\n",
            "loss 0.0015270061558112502 average time 0.03252567096233179 iter num 80\n",
            "loss 0.0005713574937544763 average time 0.032384020519784826 iter num 100\n",
            "loss 0.0023920226376503706 average time 0.03320774655057903 iter num 20\n",
            "loss 0.0040073031559586525 average time 0.03303930500023853 iter num 40\n",
            "loss 0.0018544587073847651 average time 0.032720186233442895 iter num 60\n",
            "loss 0.00288794026710093 average time 0.032617461062545774 iter num 80\n",
            "loss 0.0017665671184659004 average time 0.03262463165006921 iter num 100\n",
            "loss 0.002331411698833108 average time 0.033835042150167284 iter num 20\n",
            "loss 0.00037811812944710255 average time 0.03327445859968066 iter num 40\n",
            "loss 0.0028967508114874363 average time 0.03328182063329829 iter num 60\n",
            "loss 0.0007205521105788648 average time 0.03322654767493986 iter num 80\n",
            "loss 0.000762498879339546 average time 0.03317921979993116 iter num 100\n",
            "loss 0.001843585167080164 average time 0.034735085199827154 iter num 20\n",
            "loss 0.0015083338366821408 average time 0.0340918468747077 iter num 40\n",
            "loss 0.0008764042286202312 average time 0.034028289233053025 iter num 60\n",
            "loss 0.0008301563211716712 average time 0.03363066728729791 iter num 80\n",
            "loss 0.0004248667391948402 average time 0.03318324843989103 iter num 100\n",
            "loss 0.003439649473875761 average time 0.033359262549674897 iter num 20\n",
            "loss 0.0007450305856764317 average time 0.032698672749666 iter num 40\n",
            "loss 0.001260096556507051 average time 0.032548624833119294 iter num 60\n",
            "loss 0.001235041650943458 average time 0.0323654662998706 iter num 80\n",
            "loss 0.0009499323787167668 average time 0.032370361479952404 iter num 100\n",
            "loss 0.0004662643768824637 average time 0.033097405000080474 iter num 20\n",
            "loss 0.005686094984412193 average time 0.03237889672491292 iter num 40\n",
            "loss 0.0034612095914781094 average time 0.032407982200068845 iter num 60\n",
            "loss 0.000815706211142242 average time 0.032292921387534076 iter num 80\n",
            "loss 0.0037986095994710922 average time 0.032220098589968986 iter num 100\n",
            "loss 0.0005047698505222797 average time 0.033758781449796514 iter num 20\n",
            "loss 0.0008245565113611519 average time 0.032843667899760474 iter num 40\n",
            "loss 0.0013711984502151608 average time 0.03291110395002761 iter num 60\n",
            "loss 0.0015838575782254338 average time 0.032591699049999076 iter num 80\n",
            "loss 0.0009688674472272396 average time 0.03246795565992215 iter num 100\n",
            "loss 0.0014365753158926964 average time 0.03314553199961665 iter num 20\n",
            "loss 0.00046105703222565353 average time 0.032268720474985454 iter num 40\n",
            "loss 0.0004937545163556933 average time 0.03194965429993317 iter num 60\n",
            "loss 0.0009899063734337687 average time 0.031941073249981855 iter num 80\n",
            "loss 0.0027869001496583223 average time 0.031918077919835926 iter num 100\n",
            "loss 0.0017652249662205577 average time 0.03254453659956198 iter num 20\n",
            "loss 0.000953224953263998 average time 0.03221002489981402 iter num 40\n",
            "loss 0.0011936572846025229 average time 0.03210503333308831 iter num 60\n",
            "loss 0.001974796876311302 average time 0.03201800167480542 iter num 80\n",
            "loss 0.00043233760516159236 average time 0.0318937475598068 iter num 100\n",
            "loss 0.0011348557891324162 average time 0.033241028399606874 iter num 20\n",
            "loss 0.00041243553278036416 average time 0.032762495599581595 iter num 40\n",
            "loss 0.0013069264823570848 average time 0.03245891256641092 iter num 60\n",
            "loss 0.0006358563550747931 average time 0.03227722984979664 iter num 80\n",
            "loss 0.0012065591290593147 average time 0.03224438105975423 iter num 100\n",
            "loss 0.00029090401949360967 average time 0.033334306999859106 iter num 20\n",
            "loss 0.0003368840552866459 average time 0.03254876197506747 iter num 40\n",
            "loss 0.0014260266907513142 average time 0.03237578993345475 iter num 60\n",
            "loss 0.000596974219661206 average time 0.0323031573874232 iter num 80\n",
            "loss 0.005174537189304829 average time 0.03213583495988132 iter num 100\n",
            "loss 0.000774637795984745 average time 0.032862879700223856 iter num 20\n",
            "loss 0.0005415474879555404 average time 0.032265216050655 iter num 40\n",
            "loss 0.0017699217423796654 average time 0.032361266600310044 iter num 60\n",
            "loss 0.001818707911297679 average time 0.0322017118376607 iter num 80\n",
            "loss 0.002711135195568204 average time 0.032242617440206234 iter num 100\n",
            "loss 0.0004896544269286096 average time 0.033048168400091525 iter num 20\n",
            "loss 0.0015586554072797298 average time 0.032330636775077436 iter num 40\n",
            "loss 0.0006760058458894491 average time 0.03208881128333208 iter num 60\n",
            "loss 0.0028128447011113167 average time 0.03198185741243833 iter num 80\n",
            "loss 0.000966541119851172 average time 0.03197595119996549 iter num 100\n",
            "loss 0.0009894814575091004 average time 0.033701646049667036 iter num 20\n",
            "loss 0.0017544194124639034 average time 0.03249501457494262 iter num 40\n",
            "loss 0.0007236040546558797 average time 0.032333150233231814 iter num 60\n",
            "loss 0.0012480436125770211 average time 0.032376817374915844 iter num 80\n",
            "loss 0.0015889108180999756 average time 0.03224049983993609 iter num 100\n",
            "loss 0.0011768937110900879 average time 0.032626411300589096 iter num 20\n",
            "loss 0.00046125592780299485 average time 0.03205547387515253 iter num 40\n",
            "loss 0.0008795391186140478 average time 0.03209629638337598 iter num 60\n",
            "loss 0.0012100128224119544 average time 0.03197305975008931 iter num 80\n",
            "loss 0.0007381748873740435 average time 0.03194631243994081 iter num 100\n",
            "loss 0.0005604344187304378 average time 0.032519646950095196 iter num 20\n",
            "loss 0.0009674527682363987 average time 0.031872570149789684 iter num 40\n",
            "loss 0.0005720751942135394 average time 0.03182982006649884 iter num 60\n",
            "loss 0.0004726672195829451 average time 0.031862728599799085 iter num 80\n",
            "loss 0.0005779833882115781 average time 0.03182455806982034 iter num 100\n",
            "loss 0.0024630289990454912 average time 0.033743631100151106 iter num 20\n",
            "loss 0.0014841887168586254 average time 0.03323984405005831 iter num 40\n",
            "loss 0.0018966123461723328 average time 0.03309124450006493 iter num 60\n",
            "loss 0.0010764484759420156 average time 0.03268545483761045 iter num 80\n",
            "loss 0.0005608652136288583 average time 0.03251531389007141 iter num 100\n",
            "loss 0.0003450555959716439 average time 0.032469771799515004 iter num 20\n",
            "loss 0.00027921772561967373 average time 0.03239142509974045 iter num 40\n",
            "loss 0.000617534329649061 average time 0.03207122604968996 iter num 60\n",
            "loss 0.00029924255795776844 average time 0.03241325457493076 iter num 80\n",
            "loss 0.00034712220076471567 average time 0.032388267970054584 iter num 100\n",
            "loss 0.0006470117368735373 average time 0.03314248500028043 iter num 20\n",
            "loss 0.00030396401416510344 average time 0.03254398267508805 iter num 40\n",
            "loss 0.0007850616239011288 average time 0.032449702916649885 iter num 60\n",
            "loss 0.0017774314619600773 average time 0.032351351687384525 iter num 80\n",
            "loss 0.0011094098445028067 average time 0.03229838573995949 iter num 100\n",
            "loss 0.00037048282683826983 average time 0.032782984250115985 iter num 20\n",
            "loss 0.00022600723605137318 average time 0.03237924077511707 iter num 40\n",
            "loss 0.0005675209686160088 average time 0.03219147396697129 iter num 60\n",
            "loss 0.0011950681218877435 average time 0.03221317378765889 iter num 80\n",
            "loss 0.0006468080682680011 average time 0.03203125717005605 iter num 100\n",
            "loss 0.002267286414280534 average time 0.032669708349567374 iter num 20\n",
            "loss 0.0017211528029292822 average time 0.03208078767484039 iter num 40\n",
            "loss 0.0003634628956206143 average time 0.03196901006673822 iter num 60\n",
            "loss 0.0005311163840815425 average time 0.03202633780010729 iter num 80\n",
            "loss 0.0008856825297698379 average time 0.0318640862301254 iter num 100\n",
            "loss 0.0005666955839842558 average time 0.03408273314998951 iter num 20\n",
            "loss 0.00034090300323441625 average time 0.032946342774721415 iter num 40\n",
            "loss 0.00038565334398299456 average time 0.0326630917830092 iter num 60\n",
            "loss 0.0003565825754776597 average time 0.03222998119981639 iter num 80\n",
            "loss 0.0004951363662257791 average time 0.03204813451982773 iter num 100\n",
            "loss 0.0003517298318911344 average time 0.032685548800145625 iter num 20\n",
            "loss 0.000686227751430124 average time 0.03242810114998065 iter num 40\n",
            "loss 0.0007170221069827676 average time 0.03230545073347457 iter num 60\n",
            "loss 0.0007568703149445355 average time 0.03219167798752096 iter num 80\n",
            "loss 0.00019672809867188334 average time 0.03220102488998236 iter num 100\n",
            "loss 0.00047387718223035336 average time 0.03318845034991682 iter num 20\n",
            "loss 0.0022631692700088024 average time 0.03226671934990009 iter num 40\n",
            "loss 0.0017233628313988447 average time 0.032001126399882195 iter num 60\n",
            "loss 0.0006535316351801157 average time 0.031965850562392005 iter num 80\n",
            "loss 0.0005064885481260717 average time 0.03185370786988642 iter num 100\n",
            "loss 0.0001886939280666411 average time 0.03288030214989703 iter num 20\n",
            "loss 0.0006907054339535534 average time 0.0321300074249848 iter num 40\n",
            "loss 0.0024385862052440643 average time 0.03186086113346391 iter num 60\n",
            "loss 0.0012535136193037033 average time 0.031883788125151115 iter num 80\n",
            "loss 0.00171495764516294 average time 0.03184481757012691 iter num 100\n",
            "loss 0.0004916037432849407 average time 0.03264515390073939 iter num 20\n",
            "loss 0.001060059410519898 average time 0.03236058937563939 iter num 40\n",
            "loss 0.0028999948408454657 average time 0.03200059620079022 iter num 60\n",
            "loss 0.0003818612021859735 average time 0.031868379487968924 iter num 80\n",
            "loss 0.0009667446138337255 average time 0.0319062364403726 iter num 100\n",
            "loss 0.00023103554849512875 average time 0.033173092800279844 iter num 20\n",
            "loss 0.00046196591574698687 average time 0.03275283880020652 iter num 40\n",
            "loss 0.0003466042981017381 average time 0.03251758978349244 iter num 60\n",
            "loss 0.0006771586486138403 average time 0.03238969335006914 iter num 80\n",
            "loss 0.0004282816080376506 average time 0.03228684948000591 iter num 100\n",
            "loss 0.00021653245494235307 average time 0.03305299064995779 iter num 20\n",
            "loss 0.0003331897023599595 average time 0.032575953225023116 iter num 40\n",
            "loss 0.0006681633531115949 average time 0.03230353256676608 iter num 60\n",
            "loss 0.002129840198904276 average time 0.03226427106255869 iter num 80\n",
            "loss 0.0004086880071554333 average time 0.03214874757999496 iter num 100\n",
            "loss 0.0008039546082727611 average time 0.03269343019965163 iter num 20\n",
            "loss 0.00024903647135943174 average time 0.032279229650066556 iter num 40\n",
            "loss 0.00022154016187414527 average time 0.031987637616839495 iter num 60\n",
            "loss 0.0002709577092900872 average time 0.03186661107515647 iter num 80\n",
            "loss 0.0007703023147769272 average time 0.03197656571021071 iter num 100\n",
            "loss 0.0002669805253390223 average time 0.032708620699850144 iter num 20\n",
            "loss 0.001196548342704773 average time 0.03314023647490103 iter num 40\n",
            "loss 0.00028172999736852944 average time 0.03306820029993105 iter num 60\n",
            "loss 0.0011389350984245539 average time 0.03274227284978224 iter num 80\n",
            "loss 0.0004101817321497947 average time 0.03269991183973616 iter num 100\n",
            "loss 0.0008990052156150341 average time 0.033123225500094125 iter num 20\n",
            "loss 0.0008479689131490886 average time 0.0324548089003656 iter num 40\n",
            "loss 0.0004384962667245418 average time 0.03226187390009727 iter num 60\n",
            "loss 0.003256182884797454 average time 0.032060209962628504 iter num 80\n",
            "loss 0.00045215050340630114 average time 0.03196860143016238 iter num 100\n",
            "loss 0.0010715090902522206 average time 0.03272001419991284 iter num 20\n",
            "loss 0.00036210991675034165 average time 0.03192399780000414 iter num 40\n",
            "loss 0.0003966135554946959 average time 0.03203540524997758 iter num 60\n",
            "loss 0.002188536338508129 average time 0.03189648202496755 iter num 80\n",
            "loss 0.0002688237582333386 average time 0.03207727909990354 iter num 100\n",
            "loss 0.00064487149938941 average time 0.035411424599442395 iter num 20\n",
            "loss 0.0003194143937435001 average time 0.03352674727466365 iter num 40\n",
            "loss 0.0002774125023279339 average time 0.03299970586649579 iter num 60\n",
            "loss 0.0007438852335326374 average time 0.0328196335623943 iter num 80\n",
            "loss 0.0007591257453896105 average time 0.03271527219993004 iter num 100\n",
            "loss 0.0010980754159390926 average time 0.032442717149751846 iter num 20\n",
            "loss 0.00020322522323112935 average time 0.032157836525129824 iter num 40\n",
            "loss 0.0007237978861667216 average time 0.031930394799989395 iter num 60\n",
            "loss 0.0004269807250238955 average time 0.032048516024951824 iter num 80\n",
            "loss 0.0011384034296497703 average time 0.03210052075999556 iter num 100\n",
            "loss 0.0002149785723304376 average time 0.03438945585021429 iter num 20\n",
            "loss 0.00022644706768915057 average time 0.0335561566999786 iter num 40\n",
            "loss 0.0006565011572092772 average time 0.03353781083339224 iter num 60\n",
            "loss 0.00021865896997042 average time 0.03324157908741654 iter num 80\n",
            "loss 0.0023402979131788015 average time 0.0329571969700919 iter num 100\n",
            "loss 0.0006384691223502159 average time 0.032674419250361096 iter num 20\n",
            "loss 0.0039572627283632755 average time 0.032489468800304164 iter num 40\n",
            "loss 0.00027311802841722965 average time 0.032794006216803 iter num 60\n",
            "loss 0.00019109563436359167 average time 0.03282685236267753 iter num 80\n",
            "loss 0.0002035266370512545 average time 0.03269534102018952 iter num 100\n",
            "loss 0.00029181959689594805 average time 0.03304938164983469 iter num 20\n",
            "loss 0.00030879848054610193 average time 0.03236502477511749 iter num 40\n",
            "loss 0.0005130157223902643 average time 0.0320964609167883 iter num 60\n",
            "loss 0.0002590649819467217 average time 0.032170221700152976 iter num 80\n",
            "loss 0.00022826282656751573 average time 0.032080940430205374 iter num 100\n",
            "loss 0.0006090081296861172 average time 0.03276689384983911 iter num 20\n",
            "loss 0.0002968563640024513 average time 0.03228340727482646 iter num 40\n",
            "loss 0.0007148230797611177 average time 0.032156767366541314 iter num 60\n",
            "loss 0.000735744193661958 average time 0.032047676837555626 iter num 80\n",
            "loss 0.0005246323416940868 average time 0.03191203949001647 iter num 100\n",
            "loss 0.00046716289944015443 average time 0.03334895820044039 iter num 20\n",
            "loss 0.0007998349610716105 average time 0.03256025642513123 iter num 40\n",
            "loss 0.002909823087975383 average time 0.0324381673835281 iter num 60\n",
            "loss 0.000271616067038849 average time 0.03243203172514768 iter num 80\n",
            "loss 0.001149043790064752 average time 0.032299440760070866 iter num 100\n",
            "loss 0.0005129605997353792 average time 0.03324436229959247 iter num 20\n",
            "loss 0.0006693211616948247 average time 0.03238434065006004 iter num 40\n",
            "loss 0.0015051537193357944 average time 0.03207100008330599 iter num 60\n",
            "loss 0.00024908289196901023 average time 0.032088809837432564 iter num 80\n",
            "loss 0.0008486098959110677 average time 0.03209726233002584 iter num 100\n",
            "loss 0.0005241434555500746 average time 0.03333924555026897 iter num 20\n",
            "loss 0.00020459710503928363 average time 0.03258138682494973 iter num 40\n",
            "loss 9.998647146858275e-05 average time 0.03232085728329063 iter num 60\n",
            "loss 0.0012309784069657326 average time 0.032144154849947884 iter num 80\n",
            "loss 0.00014123241999186575 average time 0.032067300810012966 iter num 100\n",
            "loss 0.0002231234248029068 average time 0.032428013850403656 iter num 20\n",
            "loss 0.0002959374978672713 average time 0.03200899405028394 iter num 40\n",
            "loss 0.000382011552574113 average time 0.032020003016987174 iter num 60\n",
            "loss 0.0013213102938607335 average time 0.031966223850122336 iter num 80\n",
            "loss 0.0008112909272313118 average time 0.03201756710011978 iter num 100\n",
            "loss 0.0002916848170571029 average time 0.03418971085011435 iter num 20\n",
            "loss 0.00019562577654141933 average time 0.03347843200026546 iter num 40\n",
            "loss 0.0003653850289992988 average time 0.03281578535018828 iter num 60\n",
            "loss 0.0004733885871246457 average time 0.032538868075289426 iter num 80\n",
            "loss 0.0001857121242210269 average time 0.032471515480210655 iter num 100\n",
            "loss 0.00042515600216574967 average time 0.03277015724979719 iter num 20\n",
            "loss 0.0004562881658785045 average time 0.032279708374790064 iter num 40\n",
            "loss 0.0002978653647005558 average time 0.03195283696653253 iter num 60\n",
            "loss 0.0006718406802974641 average time 0.03183136616248703 iter num 80\n",
            "loss 0.00016668585885781795 average time 0.03197882823002146 iter num 100\n",
            "loss 0.0006975345895625651 average time 0.032464430650179565 iter num 20\n",
            "loss 0.00037551429704762995 average time 0.032358988449777824 iter num 40\n",
            "loss 0.0001518139470135793 average time 0.03208699334979125 iter num 60\n",
            "loss 0.0007909114938229322 average time 0.032100180987481505 iter num 80\n",
            "loss 0.00023394718300551176 average time 0.03221025713985 iter num 100\n",
            "loss 0.0007163172122091055 average time 0.03297496320010396 iter num 20\n",
            "loss 0.0002370725414948538 average time 0.03237510845028737 iter num 40\n",
            "loss 0.00015909061767160892 average time 0.03233878898354305 iter num 60\n",
            "loss 0.0006389486370608211 average time 0.032259839262678724 iter num 80\n",
            "loss 0.0011626402847468853 average time 0.03230405621015962 iter num 100\n",
            "loss 0.0011325736995786428 average time 0.03301456464942021 iter num 20\n",
            "loss 0.0014730481198057532 average time 0.03239126027474413 iter num 40\n",
            "loss 0.0012688781134784222 average time 0.0322740754163533 iter num 60\n",
            "loss 0.0016521945362910628 average time 0.03223665077480291 iter num 80\n",
            "loss 0.00023560004774481058 average time 0.03211114165984327 iter num 100\n",
            "loss 0.00012981041800230742 average time 0.03284799439970811 iter num 20\n",
            "loss 0.0002677520096767694 average time 0.03216268699989087 iter num 40\n",
            "loss 0.0002488892641849816 average time 0.032001521883163755 iter num 60\n",
            "loss 0.0041838535107672215 average time 0.0323680368123405 iter num 80\n",
            "loss 0.0006382814608514309 average time 0.032252369389862 iter num 100\n",
            "loss 0.00032770950929261744 average time 0.03306459554987669 iter num 20\n",
            "loss 0.00029205577448010445 average time 0.03230287702508576 iter num 40\n",
            "loss 0.00010044671216746792 average time 0.032394045866870634 iter num 60\n",
            "loss 0.00020223550382070243 average time 0.0324203513876455 iter num 80\n",
            "loss 9.906697960104793e-05 average time 0.032340642610033686 iter num 100\n",
            "loss 0.003114684484899044 average time 0.03320404649966804 iter num 20\n",
            "loss 0.0002489134203642607 average time 0.03291239572490667 iter num 40\n",
            "loss 0.0019429452950134873 average time 0.032552598983420465 iter num 60\n",
            "loss 0.00038178678369149566 average time 0.03254401057506584 iter num 80\n",
            "loss 0.0008938270038925111 average time 0.03244417964011518 iter num 100\n",
            "loss 0.00021887931507080793 average time 0.03285847779989126 iter num 20\n",
            "loss 0.00036151480162516236 average time 0.032798987875048626 iter num 40\n",
            "loss 0.0005092170322313905 average time 0.03252469778320422 iter num 60\n",
            "loss 0.0010651643387973309 average time 0.0324092784498589 iter num 80\n",
            "loss 0.0005601570010185242 average time 0.0324953480599288 iter num 100\n",
            "loss 0.0002021746913669631 average time 0.03328091475013935 iter num 20\n",
            "loss 0.0005252945702522993 average time 0.03286657547532741 iter num 40\n",
            "loss 0.001050898339599371 average time 0.03278180230020856 iter num 60\n",
            "loss 0.00013653526548296213 average time 0.03259781340002519 iter num 80\n",
            "loss 0.0003849034255836159 average time 0.03265116460996069 iter num 100\n",
            "loss 0.00027249351842328906 average time 0.03292953130021488 iter num 20\n",
            "loss 0.00016859937750268728 average time 0.032356481850183624 iter num 40\n",
            "loss 0.0002662742626853287 average time 0.032129581750147435 iter num 60\n",
            "loss 0.00038503509131260216 average time 0.03217145645025994 iter num 80\n",
            "loss 0.00045354553731158376 average time 0.03214572654011136 iter num 100\n",
            "loss 0.0004589353920891881 average time 0.03343593164991034 iter num 20\n",
            "loss 0.0003229365684092045 average time 0.03243376870004795 iter num 40\n",
            "loss 0.0005572964437305927 average time 0.03229703366687318 iter num 60\n",
            "loss 9.44727289606817e-05 average time 0.032072080300258675 iter num 80\n",
            "loss 0.00038648053305223584 average time 0.03192617406013596 iter num 100\n",
            "loss 0.00041899969801306725 average time 0.03282175004969758 iter num 20\n",
            "loss 0.0003240172518417239 average time 0.032333182374986794 iter num 40\n",
            "loss 0.00043037274735979736 average time 0.032246309349951235 iter num 60\n",
            "loss 0.00020951336773578078 average time 0.03225489198748619 iter num 80\n",
            "loss 0.0009987781522795558 average time 0.032348395970038836 iter num 100\n",
            "loss 0.000300627201795578 average time 0.033182273599777545 iter num 20\n",
            "loss 9.930253145284951e-05 average time 0.03249346842476371 iter num 40\n",
            "loss 0.00025056072627194226 average time 0.032240897382992746 iter num 60\n",
            "loss 0.00020521791884675622 average time 0.032100150412179576 iter num 80\n",
            "loss 0.00042253616265952587 average time 0.032304979449545496 iter num 100\n",
            "loss 0.0008971419301815331 average time 0.03460950534936273 iter num 20\n",
            "loss 0.0004755449772346765 average time 0.03348269744965364 iter num 40\n",
            "loss 0.0007557060453109443 average time 0.03296150011634988 iter num 60\n",
            "loss 0.00023497080837842077 average time 0.03277287821224491 iter num 80\n",
            "loss 0.0001743385655572638 average time 0.0327706915197632 iter num 100\n",
            "loss 0.0003547572123352438 average time 0.033243811399916015 iter num 20\n",
            "loss 0.003021269803866744 average time 0.032568443400123215 iter num 40\n",
            "loss 0.00016051963029894978 average time 0.032633641116808575 iter num 60\n",
            "loss 0.00029473722679540515 average time 0.03233833025024069 iter num 80\n",
            "loss 0.00011173237726325169 average time 0.03231455048022326 iter num 100\n",
            "loss 0.0002598520368337631 average time 0.03352520200060098 iter num 20\n",
            "loss 0.0012800706317648292 average time 0.03258744242493776 iter num 40\n",
            "loss 0.00025394087424501777 average time 0.03260155008319998 iter num 60\n",
            "loss 0.0001366447249893099 average time 0.032626454749879484 iter num 80\n",
            "loss 0.0004867202660534531 average time 0.032405529359966746 iter num 100\n",
            "loss 0.001214787014760077 average time 0.0331079651006803 iter num 20\n",
            "loss 0.0008425413980148733 average time 0.0322688760253186 iter num 40\n",
            "loss 0.00020263409533072263 average time 0.032248325250050884 iter num 60\n",
            "loss 0.0004570205637719482 average time 0.032122146812571375 iter num 80\n",
            "loss 0.0007561018574051559 average time 0.03207508368988783 iter num 100\n",
            "loss 0.00023891015734989196 average time 0.03325026060028904 iter num 20\n",
            "loss 0.00024395622313022614 average time 0.032543008500033464 iter num 40\n",
            "loss 0.00017338673933409154 average time 0.03258144904993969 iter num 60\n",
            "loss 0.00019436758884694427 average time 0.03238115531244148 iter num 80\n",
            "loss 0.0006922870525158942 average time 0.032336139969957 iter num 100\n",
            "loss 0.000465592835098505 average time 0.03325664034964575 iter num 20\n",
            "loss 0.0004987556021660566 average time 0.03318117999970127 iter num 40\n",
            "loss 0.0013365622144192457 average time 0.03305017259978437 iter num 60\n",
            "loss 0.0004645700682885945 average time 0.03278746121231961 iter num 80\n",
            "loss 0.0012580965412780643 average time 0.03276472607987671 iter num 100\n",
            "loss 0.0001738693390507251 average time 0.035158365800634785 iter num 20\n",
            "loss 0.0018202688079327345 average time 0.03397204827497262 iter num 40\n",
            "loss 0.00023634324315935373 average time 0.03370846368355463 iter num 60\n",
            "loss 0.00017008723807521164 average time 0.03345947302518652 iter num 80\n",
            "loss 0.0001716194674372673 average time 0.03337048848025006 iter num 100\n",
            "loss 0.0009371333289891481 average time 0.03389378439987922 iter num 20\n",
            "loss 0.0009921378223225474 average time 0.033325855075418076 iter num 40\n",
            "loss 0.0005539693520404398 average time 0.03304585763368474 iter num 60\n",
            "loss 0.0008833144675008953 average time 0.0329912060753486 iter num 80\n",
            "loss 0.00161021389067173 average time 0.032951120450161395 iter num 100\n",
            "loss 0.0003340577823109925 average time 0.032690784799888205 iter num 20\n",
            "loss 0.000269243901129812 average time 0.032544850674730695 iter num 40\n",
            "loss 0.00045976339606568217 average time 0.03220442588329509 iter num 60\n",
            "loss 0.0005911531625315547 average time 0.0322166967001067 iter num 80\n",
            "loss 0.00047459773486480117 average time 0.03239779391009506 iter num 100\n",
            "loss 0.0010204481659457088 average time 0.033925131750220314 iter num 20\n",
            "loss 0.00010989837755914778 average time 0.032978620675203275 iter num 40\n",
            "loss 0.0008508535102009773 average time 0.03247055431681171 iter num 60\n",
            "loss 0.0005161436274647713 average time 0.03238929631265819 iter num 80\n",
            "loss 0.00029395613819360733 average time 0.032346872740090474 iter num 100\n",
            "loss 0.00026266268105246127 average time 0.033629414900315166 iter num 20\n",
            "loss 0.00020430638687685132 average time 0.033084099700226945 iter num 40\n",
            "loss 0.0005399849615059793 average time 0.03277994916667618 iter num 60\n",
            "loss 0.00015510032244492322 average time 0.03272913031250937 iter num 80\n",
            "loss 0.00016271998174488544 average time 0.03275817240002652 iter num 100\n",
            "loss 9.48755259742029e-05 average time 0.033506116249918706 iter num 20\n",
            "loss 0.0018723621033132076 average time 0.03289675492469542 iter num 40\n",
            "loss 0.0006091829272918403 average time 0.03292593326647572 iter num 60\n",
            "loss 0.00036893782089464366 average time 0.03304414027479652 iter num 80\n",
            "loss 0.0003078263544011861 average time 0.03275817267982348 iter num 100\n",
            "loss 0.0011476754443719983 average time 0.03338267135004571 iter num 20\n",
            "loss 0.00027479318669065833 average time 0.032948420525099206 iter num 40\n",
            "loss 0.00013183770352043211 average time 0.03261728291654435 iter num 60\n",
            "loss 0.00010967366688419133 average time 0.032530369112419064 iter num 80\n",
            "loss 0.00010655583173502237 average time 0.03240992813993216 iter num 100\n",
            "loss 0.00015339204401243478 average time 0.03325650094939192 iter num 20\n",
            "loss 0.0006879538996145129 average time 0.0325080208496729 iter num 40\n",
            "loss 0.0003120720502920449 average time 0.03245574511662805 iter num 60\n",
            "loss 0.00039862783160060644 average time 0.03233436038744912 iter num 80\n",
            "loss 0.0002601499727461487 average time 0.032199346530032924 iter num 100\n",
            "loss 0.0001894549059215933 average time 0.0329316478499095 iter num 20\n",
            "loss 0.00013707233301829547 average time 0.032480446774934536 iter num 40\n",
            "loss 0.0011533527867868543 average time 0.03245623970002877 iter num 60\n",
            "loss 0.00020401060464791954 average time 0.03234001658756824 iter num 80\n",
            "loss 0.0014440853847190738 average time 0.03223251325020101 iter num 100\n",
            "loss 0.0011458953376859426 average time 0.03308706329989945 iter num 20\n",
            "loss 0.00021049835777375847 average time 0.032658969149906625 iter num 40\n",
            "loss 0.0011843605898320675 average time 0.032579362433231535 iter num 60\n",
            "loss 0.000137983777676709 average time 0.032481371899848455 iter num 80\n",
            "loss 0.0004985841806046665 average time 0.03245962869983487 iter num 100\n",
            "loss 0.0011680042371153831 average time 0.033612620650455935 iter num 20\n",
            "loss 0.0014634481631219387 average time 0.032590821850317295 iter num 40\n",
            "loss 0.0001400462060701102 average time 0.03245220103369017 iter num 60\n",
            "loss 0.0003467320348136127 average time 0.03240555266274896 iter num 80\n",
            "loss 0.0006773872883059084 average time 0.03229294736025622 iter num 100\n",
            "loss 0.00018112812540493906 average time 0.033516093700200145 iter num 20\n",
            "loss 0.00018784365965984762 average time 0.03283878380007081 iter num 40\n",
            "loss 0.00011919042299268767 average time 0.03272804351648422 iter num 60\n",
            "loss 0.001471002702601254 average time 0.03255703968738999 iter num 80\n",
            "loss 0.00029464546241797507 average time 0.03241350344982493 iter num 100\n",
            "loss 0.0015568388625979424 average time 0.03369398479990195 iter num 20\n",
            "loss 0.00030555491684935987 average time 0.03240639082496273 iter num 40\n",
            "loss 0.00011960660776821896 average time 0.03218327819989402 iter num 60\n",
            "loss 0.00012405797315295786 average time 0.03209956138739471 iter num 80\n",
            "loss 0.00016086778487078846 average time 0.03212849316994834 iter num 100\n",
            "loss 0.0002291184791829437 average time 0.03309865124956559 iter num 20\n",
            "loss 0.0003917256253771484 average time 0.03280925682483939 iter num 40\n",
            "loss 9.255573968403041e-05 average time 0.032376183266508936 iter num 60\n",
            "loss 0.0007347782957367599 average time 0.03215119113729088 iter num 80\n",
            "loss 0.0002633599506225437 average time 0.03260486398987268 iter num 100\n",
            "loss 0.000605319975875318 average time 0.03445285695052007 iter num 20\n",
            "loss 0.00015580409672111273 average time 0.03339946692531157 iter num 40\n",
            "loss 0.0007424956420436502 average time 0.032954320116793195 iter num 60\n",
            "loss 0.0001731243246467784 average time 0.032995992975065745 iter num 80\n",
            "loss 0.0005287504172883928 average time 0.03288334349013894 iter num 100\n",
            "loss 0.0002275817678309977 average time 0.03364703345032467 iter num 20\n",
            "loss 0.00021084722538944334 average time 0.03261442405027992 iter num 40\n",
            "loss 0.0005724763032048941 average time 0.032419957233469174 iter num 60\n",
            "loss 0.00017332835705019534 average time 0.032272575437718845 iter num 80\n",
            "loss 0.00039401979302056134 average time 0.03221369320017402 iter num 100\n",
            "loss 0.00019080805941484869 average time 0.03272465659956651 iter num 20\n",
            "loss 0.0003571240231394768 average time 0.032002924800053734 iter num 40\n",
            "loss 0.00036895755329169333 average time 0.031813621449994875 iter num 60\n",
            "loss 0.00019174769113305956 average time 0.03190401144988755 iter num 80\n",
            "loss 0.00010703938460210338 average time 0.03184366697987571 iter num 100\n",
            "loss 0.00013448360550682992 average time 0.03441708634982206 iter num 20\n",
            "loss 0.00017873640172183514 average time 0.03334750739986703 iter num 40\n",
            "loss 0.0026851126458495855 average time 0.033310841299802024 iter num 60\n",
            "loss 0.0013277886901050806 average time 0.0329949548249715 iter num 80\n",
            "loss 0.00011524520232342184 average time 0.03287226355991152 iter num 100\n",
            "loss 0.00015438717673532665 average time 0.03324145155002043 iter num 20\n",
            "loss 0.00046300774556584656 average time 0.03287940744967273 iter num 40\n",
            "loss 0.0003366631572134793 average time 0.03284390014978271 iter num 60\n",
            "loss 0.0002431800530757755 average time 0.03278764693732228 iter num 80\n",
            "loss 0.00023898303334135562 average time 0.032800267369893846 iter num 100\n",
            "loss 0.0001766938075888902 average time 0.03366149154917366 iter num 20\n",
            "loss 0.00019291299395263195 average time 0.032714152649350584 iter num 40\n",
            "loss 0.0009316115756519139 average time 0.03263814399948994 iter num 60\n",
            "loss 0.00017581306747160852 average time 0.03256246759970054 iter num 80\n",
            "loss 0.0004165177815593779 average time 0.03266797580978164 iter num 100\n",
            "loss 0.0004149197484366596 average time 0.03317739574995358 iter num 20\n",
            "loss 0.00019129477732349187 average time 0.03292724487509986 iter num 40\n",
            "loss 0.00041149466414935887 average time 0.032622192516707095 iter num 60\n",
            "loss 0.0005240375176072121 average time 0.03255711282522498 iter num 80\n",
            "loss 0.0003263367689214647 average time 0.0325075583902435 iter num 100\n",
            "loss 0.0001232171489391476 average time 0.03304458534985315 iter num 20\n",
            "loss 0.00013637797383125871 average time 0.03273930110008223 iter num 40\n",
            "loss 0.00022563910169992596 average time 0.03237517688351848 iter num 60\n",
            "loss 0.00022117645130492747 average time 0.032403890062551 iter num 80\n",
            "loss 0.0004656147793866694 average time 0.03235135063998314 iter num 100\n",
            "loss 0.00010283284063916653 average time 0.03401217029968393 iter num 20\n",
            "loss 0.0001343975163763389 average time 0.033200738149844256 iter num 40\n",
            "loss 0.00019737984985113144 average time 0.03318039531677641 iter num 60\n",
            "loss 0.00016486096137668937 average time 0.03286385677502039 iter num 80\n",
            "loss 0.00012967435759492218 average time 0.03272547247994226 iter num 100\n",
            "loss 0.00013022868370171636 average time 0.03341211229999317 iter num 20\n",
            "loss 8.477288793073967e-05 average time 0.03325799342492246 iter num 40\n",
            "loss 0.00015886168694123626 average time 0.0327638925499438 iter num 60\n",
            "loss 0.0011584062594920397 average time 0.032645037212432726 iter num 80\n",
            "loss 0.00025738932890817523 average time 0.03255603434998193 iter num 100\n",
            "loss 0.0003885403275489807 average time 0.03291778480052017 iter num 20\n",
            "loss 0.001017293194308877 average time 0.03227049995020934 iter num 40\n",
            "loss 0.0002274624421261251 average time 0.032188387300205554 iter num 60\n",
            "loss 0.0005849224398843944 average time 0.032063903625157764 iter num 80\n",
            "loss 0.0007803422631695867 average time 0.032310386480276065 iter num 100\n",
            "loss 0.00018533578258939087 average time 0.03352925220006 iter num 20\n",
            "loss 0.00015929076471365988 average time 0.033143102774920406 iter num 40\n",
            "loss 0.00012930248340126127 average time 0.03270537018330894 iter num 60\n",
            "loss 0.00019164761761203408 average time 0.032518206537406516 iter num 80\n",
            "loss 0.00020942377159371972 average time 0.03242193976999261 iter num 100\n",
            "loss 0.00010960234794765711 average time 0.032829326249702716 iter num 20\n",
            "loss 0.0002642147010192275 average time 0.03266472797477036 iter num 40\n",
            "loss 0.0001787442888598889 average time 0.032471703433111544 iter num 60\n",
            "loss 0.0012303253170102835 average time 0.03230501723728594 iter num 80\n",
            "loss 0.0015654988819733262 average time 0.03231028254984267 iter num 100\n",
            "loss 0.0001452690630685538 average time 0.03303861135045736 iter num 20\n",
            "loss 9.268990106647834e-05 average time 0.03262786205023076 iter num 40\n",
            "loss 0.0002577112172730267 average time 0.03244064873324533 iter num 60\n",
            "loss 0.0002103640144923702 average time 0.03222761636238829 iter num 80\n",
            "loss 0.00012917289859615266 average time 0.03218958647994441 iter num 100\n",
            "loss 0.0002627545618452132 average time 0.032734923900352444 iter num 20\n",
            "loss 0.000293516757665202 average time 0.032045110699982615 iter num 40\n",
            "loss 0.00025940517662093043 average time 0.03177632476678506 iter num 60\n",
            "loss 0.00010265973105560988 average time 0.0318121794500712 iter num 80\n",
            "loss 0.00025524001102894545 average time 0.03188432778013521 iter num 100\n",
            "loss 0.0002028997550951317 average time 0.03381699844958348 iter num 20\n",
            "loss 0.00015118852024897933 average time 0.03312403684976743 iter num 40\n",
            "loss 0.001351643237285316 average time 0.03274064014973798 iter num 60\n",
            "loss 0.0001753659889800474 average time 0.03278901962476084 iter num 80\n",
            "loss 0.00012320549285504967 average time 0.03265435285986314 iter num 100\n",
            "loss 0.00060457200743258 average time 0.03341085420051968 iter num 20\n",
            "loss 0.00014219972945284098 average time 0.03293550697508181 iter num 40\n",
            "loss 0.00011556248500710353 average time 0.03252765010001894 iter num 60\n",
            "loss 0.00010735628893598914 average time 0.03260637952498655 iter num 80\n",
            "loss 0.0002668103261385113 average time 0.03255440706991067 iter num 100\n",
            "loss 0.00012168307148385793 average time 0.03420371969950793 iter num 20\n",
            "loss 0.0002675488649401814 average time 0.03416857292477289 iter num 40\n",
            "loss 0.000939663965255022 average time 0.03376997589997093 iter num 60\n",
            "loss 0.0003622520307544619 average time 0.03335883261242998 iter num 80\n",
            "loss 0.00025681377155706286 average time 0.03300298522004596 iter num 100\n",
            "loss 0.00011373674351489171 average time 0.03255929934948654 iter num 20\n",
            "loss 8.717788296053186e-05 average time 0.03201993714965283 iter num 40\n",
            "loss 0.00011861467646667734 average time 0.032556071983223474 iter num 60\n",
            "loss 0.00011851929593831301 average time 0.03238067471238537 iter num 80\n",
            "loss 0.00012386150774545968 average time 0.03236955043001217 iter num 100\n",
            "loss 8.607845666119829e-05 average time 0.03273332649987424 iter num 20\n",
            "loss 0.0002975740353576839 average time 0.03249289167479219 iter num 40\n",
            "loss 0.00017411148292012513 average time 0.032416659366693544 iter num 60\n",
            "loss 0.0003058337315451354 average time 0.0323032760249589 iter num 80\n",
            "loss 0.00017865178233478218 average time 0.03223383063999791 iter num 100\n",
            "loss 0.00011200790322618559 average time 0.03344585960003314 iter num 20\n",
            "loss 0.0001497339690104127 average time 0.032581164925341 iter num 40\n",
            "loss 0.0005207784124650061 average time 0.03254553843362373 iter num 60\n",
            "loss 0.0001904747769003734 average time 0.03257932781270938 iter num 80\n",
            "loss 0.00023326618247665465 average time 0.03274928832011938 iter num 100\n",
            "loss 0.0003329066967125982 average time 0.03335907539985783 iter num 20\n",
            "loss 0.0001250134373549372 average time 0.03236740859965721 iter num 40\n",
            "loss 9.329977910965681e-05 average time 0.032289476433046124 iter num 60\n",
            "loss 0.00010428712994325906 average time 0.03226117468725533 iter num 80\n",
            "loss 6.798804679419845e-05 average time 0.03221827403987845 iter num 100\n",
            "loss 0.0012903745518997312 average time 0.03351355345002958 iter num 20\n",
            "loss 0.0002447318402118981 average time 0.03290191124988269 iter num 40\n",
            "loss 0.0011475352803245187 average time 0.03263903739995536 iter num 60\n",
            "loss 0.0002291855198564008 average time 0.032547395074971064 iter num 80\n",
            "loss 0.00023643461463507265 average time 0.03242235082998377 iter num 100\n",
            "loss 0.0004431591951288283 average time 0.03232035785022162 iter num 20\n",
            "loss 0.0005032599437981844 average time 0.032169938950119104 iter num 40\n",
            "loss 0.00012795474322047085 average time 0.03220113743330633 iter num 60\n",
            "loss 0.0002038013917626813 average time 0.0323685015999672 iter num 80\n",
            "loss 0.00024170246615540236 average time 0.032378537710028465 iter num 100\n",
            "loss 0.000129230524180457 average time 0.03333432294966769 iter num 20\n",
            "loss 0.000824781775008887 average time 0.03294852597500721 iter num 40\n",
            "loss 0.00020099470566492528 average time 0.03270979521669991 iter num 60\n",
            "loss 0.00020687596406787634 average time 0.03271665440001925 iter num 80\n",
            "loss 0.00014822749653831124 average time 0.03273185717003799 iter num 100\n",
            "loss 0.0012950245290994644 average time 0.03279444365016389 iter num 20\n",
            "loss 0.00010886746895266697 average time 0.03234234415031097 iter num 40\n",
            "loss 0.0002437056100461632 average time 0.032338604200109936 iter num 60\n",
            "loss 9.355667134514078e-05 average time 0.0322057950501403 iter num 80\n",
            "loss 0.00014416250633075833 average time 0.032103948570184004 iter num 100\n",
            "loss 0.0004722357261925936 average time 0.03303324134976719 iter num 20\n",
            "loss 0.0011378031922504306 average time 0.032864379974853364 iter num 40\n",
            "loss 0.0005346423713490367 average time 0.03285173739974804 iter num 60\n",
            "loss 9.916082490235567e-05 average time 0.0328954281748338 iter num 80\n",
            "loss 8.913840429158881e-05 average time 0.03270981951005524 iter num 100\n",
            "loss 0.00013134710025042295 average time 0.03272993459977443 iter num 20\n",
            "loss 0.00015157664893195033 average time 0.032407739875088735 iter num 40\n",
            "loss 0.0001256091200048104 average time 0.032235510983385514 iter num 60\n",
            "loss 0.00016382901230826974 average time 0.03235201243751362 iter num 80\n",
            "loss 0.002377511467784643 average time 0.03224079263996828 iter num 100\n",
            "loss 0.00011589220230234787 average time 0.03302620550002757 iter num 20\n",
            "loss 0.00011635810369625688 average time 0.032332685100027445 iter num 40\n",
            "loss 0.00029810285195708275 average time 0.03215733821652975 iter num 60\n",
            "loss 0.00029711046954616904 average time 0.032116641624952535 iter num 80\n",
            "loss 8.602220623288304e-05 average time 0.0320402616699721 iter num 100\n",
            "loss 0.0007839229074306786 average time 0.03320511649999389 iter num 20\n",
            "loss 0.00032420357456430793 average time 0.032662663275186785 iter num 40\n",
            "loss 0.000500361667945981 average time 0.032478836316780266 iter num 60\n",
            "loss 0.00015499768778681755 average time 0.032318042762608454 iter num 80\n",
            "loss 8.71506636030972e-05 average time 0.03230582682004751 iter num 100\n",
            "loss 9.739827510202304e-05 average time 0.03467757949983934 iter num 20\n",
            "loss 0.0001071435835910961 average time 0.03375368177466953 iter num 40\n",
            "loss 0.0007500630454160273 average time 0.033090433249647805 iter num 60\n",
            "loss 0.0002554215316195041 average time 0.032826860387194755 iter num 80\n",
            "loss 0.00026626524049788713 average time 0.032735125839717515 iter num 100\n",
            "loss 0.00010625312279444188 average time 0.033568361950528924 iter num 20\n",
            "loss 0.0001478761259932071 average time 0.03304201035025471 iter num 40\n",
            "loss 0.0002748172846622765 average time 0.032778608900116524 iter num 60\n",
            "loss 0.00013465259689837694 average time 0.03269745055004023 iter num 80\n",
            "loss 0.00015931669622659683 average time 0.03258567637996748 iter num 100\n",
            "loss 0.0013585654087364674 average time 0.033440181049991226 iter num 20\n",
            "loss 0.0011306913802400231 average time 0.03314009372506917 iter num 40\n",
            "loss 0.0001317589485552162 average time 0.032886210833385124 iter num 60\n",
            "loss 0.000373113522073254 average time 0.032585306237524495 iter num 80\n",
            "loss 0.00015296282072085887 average time 0.032320229430079055 iter num 100\n",
            "loss 0.00014464127889368683 average time 0.03234338415059028 iter num 20\n",
            "loss 0.00017749621474649757 average time 0.032203184825266365 iter num 40\n",
            "loss 0.00010921966168098152 average time 0.031963609166935684 iter num 60\n",
            "loss 0.00014841073425486684 average time 0.0320997893252752 iter num 80\n",
            "loss 9.2695678176824e-05 average time 0.03202194835022965 iter num 100\n",
            "loss 0.00013225042494013906 average time 0.03443400484975427 iter num 20\n",
            "loss 0.00023398612393066287 average time 0.03306600980004078 iter num 40\n",
            "loss 0.00017869977455120534 average time 0.03246798799997729 iter num 60\n",
            "loss 0.00019371171947568655 average time 0.03254491604993746 iter num 80\n",
            "loss 0.0006889970973134041 average time 0.032401117789959244 iter num 100\n",
            "loss 0.00017573095101397485 average time 0.033144173700566174 iter num 20\n",
            "loss 0.0005119894631206989 average time 0.03253871385031744 iter num 40\n",
            "loss 0.00011147058103233576 average time 0.03232326418359056 iter num 60\n",
            "loss 0.0001480479259043932 average time 0.03232757828759532 iter num 80\n",
            "loss 0.0012765420833602548 average time 0.0322421620001478 iter num 100\n",
            "loss 0.0011663436889648438 average time 0.03381940299968846 iter num 20\n",
            "loss 0.00016468857938889414 average time 0.033028928575095054 iter num 40\n",
            "loss 0.0006312528857961297 average time 0.03290525108326013 iter num 60\n",
            "loss 0.00013317748380359262 average time 0.03274147004976839 iter num 80\n",
            "loss 0.00020540537661872804 average time 0.03256791483989218 iter num 100\n",
            "loss 0.0003954034182243049 average time 0.03357828619991778 iter num 20\n",
            "loss 0.00016356197011191398 average time 0.03288828582481074 iter num 40\n",
            "loss 0.0001387259690091014 average time 0.03287749174990798 iter num 60\n",
            "loss 0.00014544355508405715 average time 0.03265881476240793 iter num 80\n",
            "loss 0.0014412031741812825 average time 0.03284212925998872 iter num 100\n",
            "loss 0.001326115452684462 average time 0.03310201124968444 iter num 20\n",
            "loss 0.000783829833380878 average time 0.032597248224737994 iter num 40\n",
            "loss 0.00017060557729564607 average time 0.032519224883193 iter num 60\n",
            "loss 0.00013907735410612077 average time 0.03240803238750232 iter num 80\n",
            "loss 0.00014147086767479777 average time 0.0323250681099671 iter num 100\n",
            "loss 7.994553743628785e-05 average time 0.03347488874951523 iter num 20\n",
            "loss 9.183400106849149e-05 average time 0.032913842875132104 iter num 40\n",
            "loss 0.0001130197269958444 average time 0.03268166614998336 iter num 60\n",
            "loss 0.0004696063697338104 average time 0.03255810244991153 iter num 80\n",
            "loss 0.00017670261149760336 average time 0.03235917108995636 iter num 100\n",
            "loss 0.00021453677618410438 average time 0.03273486109974329 iter num 20\n",
            "loss 0.0002675446157809347 average time 0.03254789569973582 iter num 40\n",
            "loss 0.00016203863197006285 average time 0.03214179571653707 iter num 60\n",
            "loss 7.034083682810888e-05 average time 0.03214541487504903 iter num 80\n",
            "loss 0.00010847846715478227 average time 0.03211332909009798 iter num 100\n",
            "loss 0.00010219198156846687 average time 0.03335243179990357 iter num 20\n",
            "loss 0.000611636321991682 average time 0.03265632625016224 iter num 40\n",
            "loss 6.586256495211273e-05 average time 0.03242277271668475 iter num 60\n",
            "loss 0.00017958744138013572 average time 0.03230021234985543 iter num 80\n",
            "loss 8.534995868103579e-05 average time 0.0320610340498024 iter num 100\n",
            "loss 0.00010543905227677897 average time 0.03310830895024992 iter num 20\n",
            "loss 0.0008750412962399423 average time 0.032795781575168804 iter num 40\n",
            "loss 0.001787365647032857 average time 0.0326489720333484 iter num 60\n",
            "loss 0.00017281527107115835 average time 0.03256432913740355 iter num 80\n",
            "loss 0.00035171827767044306 average time 0.03268142730990803 iter num 100\n",
            "loss 0.00013269743067212403 average time 0.032850322549529666 iter num 20\n",
            "loss 0.00010089357238030061 average time 0.032796813049662886 iter num 40\n",
            "loss 0.0001270078937523067 average time 0.032533751599779254 iter num 60\n",
            "loss 0.0005127561744302511 average time 0.0323802335873097 iter num 80\n",
            "loss 0.0001985938142752275 average time 0.0323345801598407 iter num 100\n",
            "loss 0.00048743566730991006 average time 0.03372574949989939 iter num 20\n",
            "loss 0.00020024331752210855 average time 0.03276792490023581 iter num 40\n",
            "loss 0.00016997843340504915 average time 0.032656040250124835 iter num 60\n",
            "loss 0.00044049351708963513 average time 0.032336062437570944 iter num 80\n",
            "loss 0.00030432449420914054 average time 0.0323139055700085 iter num 100\n",
            "loss 0.0001816650910768658 average time 0.034063777300252694 iter num 20\n",
            "loss 9.17278666747734e-05 average time 0.03303015352530565 iter num 40\n",
            "loss 8.070972398854792e-05 average time 0.032847867400050745 iter num 60\n",
            "loss 0.00027158207376487553 average time 0.032789257124886714 iter num 80\n",
            "loss 0.0003635007014963776 average time 0.0328437769998709 iter num 100\n",
            "loss 0.0004459608462639153 average time 0.033501779849575544 iter num 20\n",
            "loss 0.000172790780197829 average time 0.032870178999746716 iter num 40\n",
            "loss 5.399765359470621e-05 average time 0.03269701955002044 iter num 60\n",
            "loss 0.0005238721496425569 average time 0.03292443237514817 iter num 80\n",
            "loss 0.0013356966665014625 average time 0.032984728390292734 iter num 100\n",
            "loss 0.0004031775170005858 average time 0.034986800949263853 iter num 20\n",
            "loss 0.00010479679622221738 average time 0.03440584619947913 iter num 40\n",
            "loss 0.0001073029707185924 average time 0.03389843939979376 iter num 60\n",
            "loss 0.00012360552500467747 average time 0.03381369633739269 iter num 80\n",
            "loss 9.725551353767514e-05 average time 0.03346561455986375 iter num 100\n",
            "loss 0.00010741627920651808 average time 0.033575973399820214 iter num 20\n",
            "loss 0.00035788145032711327 average time 0.0331944841499535 iter num 40\n",
            "loss 0.00029342464404180646 average time 0.0327806604331272 iter num 60\n",
            "loss 9.581022459315136e-05 average time 0.03255529168736757 iter num 80\n",
            "loss 9.830084309214726e-05 average time 0.03256262596984016 iter num 100\n",
            "loss 0.00010704131273087114 average time 0.03462462104980659 iter num 20\n",
            "loss 7.594976341351867e-05 average time 0.034211946224786516 iter num 40\n",
            "loss 9.398380643688142e-05 average time 0.03402703716656106 iter num 60\n",
            "loss 0.00010507890692679211 average time 0.03360317038732319 iter num 80\n",
            "loss 0.00011221985187148675 average time 0.03341598896975483 iter num 100\n",
            "loss 0.00021662221115548164 average time 0.03336165939999773 iter num 20\n",
            "loss 0.0002443654229864478 average time 0.03275791054984438 iter num 40\n",
            "loss 6.309727905318141e-05 average time 0.03250515581664028 iter num 60\n",
            "loss 6.662523082923144e-05 average time 0.032634830125016376 iter num 80\n",
            "loss 5.288244938128628e-05 average time 0.03257657492002181 iter num 100\n",
            "loss 9.583015344105661e-05 average time 0.032325256500371324 iter num 20\n",
            "loss 0.000182372605195269 average time 0.03181276654986505 iter num 40\n",
            "loss 0.00011621496378211305 average time 0.03186806133332235 iter num 60\n",
            "loss 0.00011903521954081953 average time 0.03195608371261187 iter num 80\n",
            "loss 0.001694867038168013 average time 0.032008257410125224 iter num 100\n",
            "loss 0.0014605154283344746 average time 0.03381734230024449 iter num 20\n",
            "loss 8.84360633790493e-05 average time 0.03302963082496717 iter num 40\n",
            "loss 0.00015945105405990034 average time 0.032714018066690186 iter num 60\n",
            "loss 0.0003590034903027117 average time 0.03262848730000769 iter num 80\n",
            "loss 0.0012261142255738378 average time 0.032453960480088424 iter num 100\n",
            "loss 0.0003820655110757798 average time 0.033469040500312984 iter num 20\n",
            "loss 0.00015816283121239394 average time 0.03296385042540351 iter num 40\n",
            "loss 0.00010451373236719519 average time 0.03275722081701436 iter num 60\n",
            "loss 8.785645331954584e-05 average time 0.032713411250279024 iter num 80\n",
            "loss 7.885911327321082e-05 average time 0.032394193590189385 iter num 100\n",
            "loss 0.001361317466944456 average time 0.03312294179977471 iter num 20\n",
            "loss 0.00016028752725105733 average time 0.0325265185749231 iter num 40\n",
            "loss 0.00019263096328359097 average time 0.03219015806662355 iter num 60\n",
            "loss 0.00032881691004149616 average time 0.03221275762498408 iter num 80\n",
            "loss 0.0007258833502419293 average time 0.03219838316992536 iter num 100\n",
            "loss 0.0004213772190269083 average time 0.03386476489995403 iter num 20\n",
            "loss 0.0012347400188446045 average time 0.03299944025029618 iter num 40\n",
            "loss 0.0003079494053963572 average time 0.033061548066810546 iter num 60\n",
            "loss 0.00012414023512974381 average time 0.03275468242513853 iter num 80\n",
            "loss 0.0001599529932718724 average time 0.03253167183000187 iter num 100\n",
            "loss 0.00011237355647608638 average time 0.03309832304930751 iter num 20\n",
            "loss 0.00010635170474415645 average time 0.032519243374827055 iter num 40\n",
            "loss 0.0026018747594207525 average time 0.03273793003348449 iter num 60\n",
            "loss 9.065611084224656e-05 average time 0.03273667723765357 iter num 80\n",
            "loss 0.00011290807742625475 average time 0.0325222022102389 iter num 100\n",
            "loss 0.00011625802289927378 average time 0.03360756859983667 iter num 20\n",
            "loss 0.0001587798324180767 average time 0.03272197612504897 iter num 40\n",
            "loss 0.0001778129517333582 average time 0.03245513008344763 iter num 60\n",
            "loss 0.0002398769574938342 average time 0.033110251800098925 iter num 80\n",
            "loss 0.0002875386271625757 average time 0.03291214631000912 iter num 100\n",
            "loss 0.0015205179806798697 average time 0.032914464900204624 iter num 20\n",
            "loss 8.021926623769104e-05 average time 0.03266814402531963 iter num 40\n",
            "loss 8.302890637423843e-05 average time 0.032645565850301254 iter num 60\n",
            "loss 0.0001648573379497975 average time 0.03253862637520797 iter num 80\n",
            "loss 0.00010290663340128958 average time 0.032395579350049955 iter num 100\n",
            "loss 0.00019622956460807472 average time 0.03316487745032646 iter num 20\n",
            "loss 4.677159813581966e-05 average time 0.032862513849977404 iter num 40\n",
            "loss 0.0013340713921934366 average time 0.032408659133276764 iter num 60\n",
            "loss 0.0001616780791664496 average time 0.03243512936242041 iter num 80\n",
            "loss 0.00023692667309660465 average time 0.03238032300996565 iter num 100\n",
            "loss 6.903814210090786e-05 average time 0.03304989350017422 iter num 20\n",
            "loss 0.000792450737208128 average time 0.03238077424985022 iter num 40\n",
            "loss 0.0002259466564282775 average time 0.03226238603310776 iter num 60\n",
            "loss 6.48190252832137e-05 average time 0.03214599554985398 iter num 80\n",
            "loss 0.00023301262990571558 average time 0.032166617829752796 iter num 100\n",
            "loss 6.240851507754996e-05 average time 0.03227193785005511 iter num 20\n",
            "loss 0.00012969285307917744 average time 0.03197416164985043 iter num 40\n",
            "loss 0.0014912679325789213 average time 0.031922167849855514 iter num 60\n",
            "loss 0.0007373960688710213 average time 0.03193959451241426 iter num 80\n",
            "loss 0.0001288549101445824 average time 0.032214447659898726 iter num 100\n",
            "loss 7.329913933062926e-05 average time 0.03359276239989413 iter num 20\n",
            "loss 0.0003477469726931304 average time 0.032489298700238575 iter num 40\n",
            "loss 0.00015873370284680277 average time 0.032151745250303065 iter num 60\n",
            "loss 0.0003153647412545979 average time 0.03204585146277168 iter num 80\n",
            "loss 0.00016120783402584493 average time 0.031817542650132966 iter num 100\n",
            "loss 0.0003475120756775141 average time 0.033723965299577684 iter num 20\n",
            "loss 0.0002257659361930564 average time 0.032652897124717126 iter num 40\n",
            "loss 0.0001632460771361366 average time 0.03247719288307659 iter num 60\n",
            "loss 0.00021692494919989258 average time 0.032365479212285206 iter num 80\n",
            "loss 0.00036926454049535096 average time 0.032351431079878236 iter num 100\n",
            "loss 0.00016625145508442074 average time 0.03301141434985766 iter num 20\n",
            "loss 0.00017182875308208168 average time 0.032373781399564906 iter num 40\n",
            "loss 0.00022540471400134265 average time 0.03234156679961112 iter num 60\n",
            "loss 0.00013539228530135006 average time 0.032055184387218105 iter num 80\n",
            "loss 0.00011171012738486752 average time 0.03212376513980417 iter num 100\n",
            "loss 0.00016001869516912848 average time 0.03355727964972175 iter num 20\n",
            "loss 0.00016745201719459146 average time 0.03389160830001856 iter num 40\n",
            "loss 0.00032401279895566404 average time 0.033171691249966294 iter num 60\n",
            "loss 0.00015763050760142505 average time 0.03293351212496418 iter num 80\n",
            "loss 0.0008109078044071794 average time 0.03270966568001313 iter num 100\n",
            "loss 0.0005369913997128606 average time 0.03347911165092228 iter num 20\n",
            "loss 0.001859869691543281 average time 0.03261739060035325 iter num 40\n",
            "loss 8.887344301911071e-05 average time 0.03231653655032763 iter num 60\n",
            "loss 0.0001187047382700257 average time 0.032198390050280065 iter num 80\n",
            "loss 0.0008678406593389809 average time 0.03218245215019124 iter num 100\n",
            "loss 8.622254244983196e-05 average time 0.03424286245044641 iter num 20\n",
            "loss 7.656980596948415e-05 average time 0.033649763974972303 iter num 40\n",
            "loss 0.00011099298717454076 average time 0.033211025166444114 iter num 60\n",
            "loss 0.0002930710033979267 average time 0.03309268717471241 iter num 80\n",
            "loss 0.00038101294194348156 average time 0.03319816674982576 iter num 100\n",
            "loss 0.00021587942319456488 average time 0.0344409381998048 iter num 20\n",
            "loss 7.089307473506778e-05 average time 0.033544073274879335 iter num 40\n",
            "loss 0.0001655912201385945 average time 0.033538626599874985 iter num 60\n",
            "loss 0.00010992581519531086 average time 0.033117040724937395 iter num 80\n",
            "loss 0.00010313504026271403 average time 0.03296420296999713 iter num 100\n",
            "loss 0.00028036453295499086 average time 0.03511743945018679 iter num 20\n",
            "loss 0.00013717348338104784 average time 0.034108466700035936 iter num 40\n",
            "loss 6.099433085182682e-05 average time 0.033496320349998616 iter num 60\n",
            "loss 0.0004696935648098588 average time 0.03314316184996642 iter num 80\n",
            "loss 0.00011029248707927763 average time 0.03297712245996081 iter num 100\n",
            "loss 0.0005869372980669141 average time 0.032702932999745825 iter num 20\n",
            "loss 0.00011168206401634961 average time 0.032522411975151044 iter num 40\n",
            "loss 0.00017713570559863 average time 0.03253786031673371 iter num 60\n",
            "loss 0.0009742106776684523 average time 0.032341655887466916 iter num 80\n",
            "loss 0.00016035143926274031 average time 0.03242626303996076 iter num 100\n",
            "loss 0.00017292100528720766 average time 0.03278592985025171 iter num 20\n",
            "loss 0.00019508335390128195 average time 0.0325265085754836 iter num 40\n",
            "loss 7.330462540267035e-05 average time 0.03234225315033352 iter num 60\n",
            "loss 0.000359880126779899 average time 0.03218554793761541 iter num 80\n",
            "loss 0.00014175126852933317 average time 0.03207326009014651 iter num 100\n",
            "loss 0.00044837890891358256 average time 0.03323276189948956 iter num 20\n",
            "loss 0.00031855684937909245 average time 0.03299232202452913 iter num 40\n",
            "loss 0.00016997900092974305 average time 0.032966626749779 iter num 60\n",
            "loss 0.00016686438175383955 average time 0.03289160359986454 iter num 80\n",
            "loss 0.000135311100166291 average time 0.03261002525992808 iter num 100\n",
            "loss 0.0001524488179711625 average time 0.033704938299706555 iter num 20\n",
            "loss 0.00014360282511916012 average time 0.03319545267495414 iter num 40\n",
            "loss 8.018851076485589e-05 average time 0.0329932404999757 iter num 60\n",
            "loss 6.956779543543234e-05 average time 0.032575464362662385 iter num 80\n",
            "loss 7.283126615220681e-05 average time 0.03264455425010965 iter num 100\n",
            "loss 0.00022605492267757654 average time 0.03435386919991288 iter num 20\n",
            "loss 0.00019836313731502742 average time 0.03326778809987445 iter num 40\n",
            "loss 0.00024128697987180203 average time 0.032956577666724725 iter num 60\n",
            "loss 0.00011388688290026039 average time 0.03282299610000337 iter num 80\n",
            "loss 0.00014071163604967296 average time 0.03269728732007934 iter num 100\n",
            "loss 0.0001026340905809775 average time 0.03360235405016283 iter num 20\n",
            "loss 0.00012912273814436048 average time 0.03282420919995275 iter num 40\n",
            "loss 0.00023047644936013967 average time 0.03277237063339271 iter num 60\n",
            "loss 0.00020440008665900677 average time 0.0327438254500521 iter num 80\n",
            "loss 0.00025259016547352076 average time 0.03276893448994087 iter num 100\n",
            "loss 9.472548117628321e-05 average time 0.03318139519997203 iter num 20\n",
            "loss 0.0011052462505176663 average time 0.03280461282483884 iter num 40\n",
            "loss 0.0005937507376074791 average time 0.03247319578301055 iter num 60\n",
            "loss 0.00011214333062525839 average time 0.03240367683729346 iter num 80\n",
            "loss 0.0002440719399601221 average time 0.03226390611005627 iter num 100\n",
            "loss 0.0005485096480697393 average time 0.03292510010014667 iter num 20\n",
            "loss 0.0001214413859997876 average time 0.03276497032529733 iter num 40\n",
            "loss 0.00019510371203068644 average time 0.0326056683334779 iter num 60\n",
            "loss 0.0001441110362065956 average time 0.032577426662646757 iter num 80\n",
            "loss 7.894945883890614e-05 average time 0.03236309622010594 iter num 100\n",
            "loss 0.001685681170783937 average time 0.032513327849665076 iter num 20\n",
            "loss 0.00010115789336850867 average time 0.03248716702464662 iter num 40\n",
            "loss 0.00039836004725657403 average time 0.03220729964984154 iter num 60\n",
            "loss 0.0001409286051057279 average time 0.03226274872495196 iter num 80\n",
            "loss 0.00015133304987102747 average time 0.032191700539879096 iter num 100\n",
            "loss 0.0001558338844915852 average time 0.03321613634925598 iter num 20\n",
            "loss 0.00014542903227265924 average time 0.03242395999968721 iter num 40\n",
            "loss 0.00021751019812654704 average time 0.032037640949783966 iter num 60\n",
            "loss 0.00048041256377473474 average time 0.03193363138730092 iter num 80\n",
            "loss 0.00017680280143395066 average time 0.03189763884976855 iter num 100\n",
            "loss 0.00015311947208829224 average time 0.03361471189982694 iter num 20\n",
            "loss 0.0005244067870080471 average time 0.03265521489947787 iter num 40\n",
            "loss 0.00031867469078861177 average time 0.032543676333201196 iter num 60\n",
            "loss 7.161373650887981e-05 average time 0.03231247083740527 iter num 80\n",
            "loss 0.0010005957446992397 average time 0.03283395415994164 iter num 100\n",
            "loss 5.1930917834397405e-05 average time 0.03276627705054125 iter num 20\n",
            "loss 0.00015259135398082435 average time 0.032460325000374726 iter num 40\n",
            "loss 0.000262202403973788 average time 0.0323470697669715 iter num 60\n",
            "loss 0.0002729800471570343 average time 0.03227625478775735 iter num 80\n",
            "loss 7.223671855172142e-05 average time 0.032243125250242886 iter num 100\n",
            "loss 7.519919017795473e-05 average time 0.032874745999833976 iter num 20\n",
            "loss 0.0005096298991702497 average time 0.032582065649785365 iter num 40\n",
            "loss 0.00010853667481569573 average time 0.032298667749819286 iter num 60\n",
            "loss 4.763919787365012e-05 average time 0.03222647403731571 iter num 80\n",
            "loss 0.0002391325542703271 average time 0.03223625645983702 iter num 100\n",
            "loss 0.00016719585983082652 average time 0.033973383500051565 iter num 20\n",
            "loss 0.00015776374493725598 average time 0.03309536215001572 iter num 40\n",
            "loss 0.00045327102998271585 average time 0.03283544131669866 iter num 60\n",
            "loss 0.0002900731051340699 average time 0.03272646127493317 iter num 80\n",
            "loss 0.0005201119347475469 average time 0.03269555902985303 iter num 100\n",
            "loss 0.0001012511202134192 average time 0.03342065544948127 iter num 20\n",
            "loss 0.00011034008639398962 average time 0.032657702874803365 iter num 40\n",
            "loss 5.3286043112166226e-05 average time 0.03237600844974319 iter num 60\n",
            "loss 0.0001771785318851471 average time 0.032672669724843215 iter num 80\n",
            "loss 0.00012196644820505753 average time 0.032772692379876386 iter num 100\n",
            "loss 0.00014139975246507674 average time 0.03332267785026488 iter num 20\n",
            "loss 0.0001765994675224647 average time 0.03267630787495364 iter num 40\n",
            "loss 0.0001485594839323312 average time 0.0324106260500533 iter num 60\n",
            "loss 0.0004784100456163287 average time 0.03252305738742507 iter num 80\n",
            "loss 4.9561400373931974e-05 average time 0.03241944515986688 iter num 100\n",
            "loss 0.00011495248327264562 average time 0.034024215900171836 iter num 20\n",
            "loss 0.00021798550733365119 average time 0.032934038050279925 iter num 40\n",
            "loss 9.42094557103701e-05 average time 0.03271265030034556 iter num 60\n",
            "loss 3.506327630020678e-05 average time 0.032645647012805055 iter num 80\n",
            "loss 0.0003534270217642188 average time 0.03247656983025081 iter num 100\n",
            "loss 0.00015688422718085349 average time 0.03347132125018106 iter num 20\n",
            "loss 6.156114250188693e-05 average time 0.0335207927000738 iter num 40\n",
            "loss 0.00035177957033738494 average time 0.033428133666772435 iter num 60\n",
            "loss 7.996201748028398e-05 average time 0.03309168827504436 iter num 80\n",
            "loss 9.405102173332125e-05 average time 0.032846701760063296 iter num 100\n",
            "loss 0.00010768663923954591 average time 0.03402651015003357 iter num 20\n",
            "loss 0.00019716133829206228 average time 0.03335373690006236 iter num 40\n",
            "loss 0.00011677089787553996 average time 0.033013368416686714 iter num 60\n",
            "loss 0.0002503652358427644 average time 0.032780550800134735 iter num 80\n",
            "loss 4.7357636503875256e-05 average time 0.03259296329008066 iter num 100\n",
            "loss 0.0008559994748793542 average time 0.03274070860043139 iter num 20\n",
            "loss 0.000130430722492747 average time 0.03261828807499114 iter num 40\n",
            "loss 0.0007583906990475953 average time 0.03282185788351247 iter num 60\n",
            "loss 7.7667114965152e-05 average time 0.03265871582507316 iter num 80\n",
            "loss 0.00020833613234572113 average time 0.032493428039997524 iter num 100\n",
            "loss 9.248600690625608e-05 average time 0.03405516664970491 iter num 20\n",
            "loss 0.00010585050040390342 average time 0.032868898949891444 iter num 40\n",
            "loss 0.00017578026745468378 average time 0.03244571538325545 iter num 60\n",
            "loss 0.00041127990698441863 average time 0.03245850334997158 iter num 80\n",
            "loss 0.00020246555504854769 average time 0.03232090325007448 iter num 100\n",
            "loss 0.0007485704263672233 average time 0.033319344549636296 iter num 20\n",
            "loss 8.237417932832614e-05 average time 0.03272722314950442 iter num 40\n",
            "loss 7.366220961557701e-05 average time 0.032576597766274064 iter num 60\n",
            "loss 0.00021552751422859728 average time 0.03260365908727181 iter num 80\n",
            "loss 0.0004207308520562947 average time 0.03256149229986477 iter num 100\n",
            "loss 0.00012149816029705107 average time 0.033293898099509535 iter num 20\n",
            "loss 0.00015182964853011072 average time 0.032691776449792084 iter num 40\n",
            "loss 0.0002521616406738758 average time 0.03241802038319293 iter num 60\n",
            "loss 0.0002524080337025225 average time 0.032432568499871195 iter num 80\n",
            "loss 0.00029619428096339107 average time 0.0323271059099352 iter num 100\n",
            "loss 0.0001179120154120028 average time 0.034301866900023015 iter num 20\n",
            "loss 0.00046014797408133745 average time 0.033206545049961275 iter num 40\n",
            "loss 3.868473140755668e-05 average time 0.032881604533213246 iter num 60\n",
            "loss 9.44293278735131e-05 average time 0.03279524868730732 iter num 80\n",
            "loss 0.00013117825437802821 average time 0.03267073551996873 iter num 100\n",
            "loss 0.00010218027455266565 average time 0.0332942852501219 iter num 20\n",
            "loss 0.00035233559901826084 average time 0.03297974490014895 iter num 40\n",
            "loss 8.886444265954196e-05 average time 0.03257602311678056 iter num 60\n",
            "loss 0.00010451901471242309 average time 0.0324728976499955 iter num 80\n",
            "loss 0.0008170608198270202 average time 0.03231347853012267 iter num 100\n",
            "loss 6.70319568598643e-05 average time 0.033755959950212855 iter num 20\n",
            "loss 0.0009321180987171829 average time 0.03326869512520716 iter num 40\n",
            "loss 0.00016270957712549716 average time 0.03301710536670725 iter num 60\n",
            "loss 6.727955042151734e-05 average time 0.033331823649996296 iter num 80\n",
            "loss 0.00010109741560881957 average time 0.03316997153004195 iter num 100\n",
            "loss 0.00010784532787511125 average time 0.032509927750288624 iter num 20\n",
            "loss 0.00011650774831650779 average time 0.03237713062490002 iter num 40\n",
            "loss 0.00012987230729777366 average time 0.03290053798318695 iter num 60\n",
            "loss 7.108967838576064e-05 average time 0.0326256170499164 iter num 80\n",
            "loss 0.00014564560842700303 average time 0.03235057832996972 iter num 100\n",
            "loss 0.00011698126763803884 average time 0.03279514739988372 iter num 20\n",
            "loss 0.00010493159788893536 average time 0.03224416544981068 iter num 40\n",
            "loss 8.552256622351706e-05 average time 0.03225251238345663 iter num 60\n",
            "loss 6.970739923417568e-05 average time 0.032088353887547785 iter num 80\n",
            "loss 0.0002631740062497556 average time 0.03223542963998625 iter num 100\n",
            "loss 6.495346315205097e-05 average time 0.03313605004968849 iter num 20\n",
            "loss 0.0001171336552943103 average time 0.03238707117498052 iter num 40\n",
            "loss 0.0004889217671006918 average time 0.032275606816619985 iter num 60\n",
            "loss 0.00041313894325867295 average time 0.032366181912357204 iter num 80\n",
            "loss 0.00018814331269823015 average time 0.0324040858198714 iter num 100\n",
            "loss 7.110628939699382e-05 average time 0.0331265713499306 iter num 20\n",
            "loss 9.140677866525948e-05 average time 0.032432735800011867 iter num 40\n",
            "loss 9.666726691648364e-05 average time 0.03230825123337126 iter num 60\n",
            "loss 0.00013742721057496965 average time 0.03234027737507859 iter num 80\n",
            "loss 5.095695814816281e-05 average time 0.03230414580011711 iter num 100\n",
            "loss 7.349946099566296e-05 average time 0.03412042514974019 iter num 20\n",
            "loss 0.0001402790512656793 average time 0.033495532224969794 iter num 40\n",
            "loss 9.170144039671868e-05 average time 0.03305258985019464 iter num 60\n",
            "loss 0.00013919686898589134 average time 0.032764997887625213 iter num 80\n",
            "loss 0.0002306078968103975 average time 0.03280678107017593 iter num 100\n",
            "loss 0.00012695524492301047 average time 0.034237125949948675 iter num 20\n",
            "loss 0.00026731938123703003 average time 0.0329912396002328 iter num 40\n",
            "loss 6.108636443968862e-05 average time 0.032809802116692786 iter num 60\n",
            "loss 0.00012855515524279326 average time 0.03287334496249059 iter num 80\n",
            "loss 6.300776294665411e-05 average time 0.0327735040900734 iter num 100\n",
            "loss 8.664089546073228e-05 average time 0.03327210615007061 iter num 20\n",
            "loss 0.0002722612698562443 average time 0.03281151212513578 iter num 40\n",
            "loss 0.00011482882109703496 average time 0.03231646211673554 iter num 60\n",
            "loss 7.40448958822526e-05 average time 0.03207206858755853 iter num 80\n",
            "loss 3.3342217648169026e-05 average time 0.03207254391003517 iter num 100\n",
            "loss 0.00041793595300987363 average time 0.03209234675050539 iter num 20\n",
            "loss 7.003027712926269e-05 average time 0.03254058520005856 iter num 40\n",
            "loss 2.884026980609633e-05 average time 0.032424542250313 iter num 60\n",
            "loss 9.137487359112129e-05 average time 0.03235938132525007 iter num 80\n",
            "loss 0.00013162863615434617 average time 0.03227050175024487 iter num 100\n",
            "loss 0.00011082387936767191 average time 0.03290098494981066 iter num 20\n",
            "loss 0.00010307619231753051 average time 0.032413720324984754 iter num 40\n",
            "loss 0.00011600700236158445 average time 0.0321155952500097 iter num 60\n",
            "loss 0.00025521189672872424 average time 0.03207329947504149 iter num 80\n",
            "loss 0.00015731915482319891 average time 0.03202479110004788 iter num 100\n",
            "loss 9.883412712952122e-05 average time 0.03284432759974152 iter num 20\n",
            "loss 0.00035486137494444847 average time 0.032407968899678966 iter num 40\n",
            "loss 8.949889888754115e-05 average time 0.03252375878322103 iter num 60\n",
            "loss 0.00030699922353960574 average time 0.032686747912384816 iter num 80\n",
            "loss 6.803329597460106e-05 average time 0.03279298208999535 iter num 100\n",
            "loss 0.0006203927914611995 average time 0.03452373519976391 iter num 20\n",
            "loss 9.401197166880593e-05 average time 0.03374014522469224 iter num 40\n",
            "loss 6.895875412737951e-05 average time 0.033839190049669315 iter num 60\n",
            "loss 0.00030395941575989127 average time 0.03376063269979568 iter num 80\n",
            "loss 9.346879232907668e-05 average time 0.033609113069869634 iter num 100\n",
            "loss 4.9486006901133806e-05 average time 0.03401179505035543 iter num 20\n",
            "loss 0.0006499552400782704 average time 0.03390008640017186 iter num 40\n",
            "loss 0.0016540217911824584 average time 0.03372710736669736 iter num 60\n",
            "loss 0.00010005106742028147 average time 0.03329134552495816 iter num 80\n",
            "loss 0.0001455322781112045 average time 0.033235152369998106 iter num 100\n",
            "loss 0.00035994601785205305 average time 0.03412171310010308 iter num 20\n",
            "loss 0.00022588338470086455 average time 0.03332193729993378 iter num 40\n",
            "loss 3.6175195418763906e-05 average time 0.03284937808330142 iter num 60\n",
            "loss 7.813511183485389e-05 average time 0.032530055562347115 iter num 80\n",
            "loss 5.885683640372008e-05 average time 0.0324614217499402 iter num 100\n",
            "loss 0.00018624305084813386 average time 0.03366845774999092 iter num 20\n",
            "loss 8.914843783713877e-05 average time 0.03284633412495168 iter num 40\n",
            "loss 9.507738286629319e-05 average time 0.0325474684499568 iter num 60\n",
            "loss 6.063074397388846e-05 average time 0.03235908663750706 iter num 80\n",
            "loss 0.0003538496675901115 average time 0.03233387507018051 iter num 100\n",
            "loss 5.978844274068251e-05 average time 0.03360394120027195 iter num 20\n",
            "loss 0.00010283732990501449 average time 0.03283100567523434 iter num 40\n",
            "loss 5.713494829251431e-05 average time 0.03231834090014066 iter num 60\n",
            "loss 0.00013508539996109903 average time 0.03229547463752169 iter num 80\n",
            "loss 0.00019540781795512885 average time 0.032129401490128655 iter num 100\n",
            "loss 0.00034687723382376134 average time 0.03351792774992646 iter num 20\n",
            "loss 8.628232899354771e-05 average time 0.03262782119982148 iter num 40\n",
            "loss 0.0002612984972074628 average time 0.0321937189998304 iter num 60\n",
            "loss 8.907485607778654e-05 average time 0.03213008813740999 iter num 80\n",
            "loss 7.700207061134279e-05 average time 0.032283081999812564 iter num 100\n",
            "loss 0.0002024534624069929 average time 0.03542174804952083 iter num 20\n",
            "loss 6.160258635645732e-05 average time 0.03426087747475322 iter num 40\n",
            "loss 0.00015447982877958566 average time 0.033521021366565644 iter num 60\n",
            "loss 0.0002255794679513201 average time 0.03338916331235851 iter num 80\n",
            "loss 0.00038132810732349753 average time 0.03316113237990066 iter num 100\n",
            "loss 9.990634862333536e-05 average time 0.03274570954981755 iter num 20\n",
            "loss 0.00013621857215184718 average time 0.03222621734976201 iter num 40\n",
            "loss 0.0011272482806816697 average time 0.032042725849775404 iter num 60\n",
            "loss 0.00010360597661929205 average time 0.03223261728721809 iter num 80\n",
            "loss 0.0016857158625498414 average time 0.03216816038984689 iter num 100\n",
            "loss 0.0009218949126079679 average time 0.032937704349569684 iter num 20\n",
            "loss 0.00021099671721458435 average time 0.032552146799844195 iter num 40\n",
            "loss 0.0001289261708734557 average time 0.03244016869988021 iter num 60\n",
            "loss 6.915952690178528e-05 average time 0.032399535687363826 iter num 80\n",
            "loss 0.00015664793318137527 average time 0.03236586847979197 iter num 100\n",
            "loss 7.169648597482592e-05 average time 0.03305329700015136 iter num 20\n",
            "loss 0.0003367335593793541 average time 0.03238868642501984 iter num 40\n",
            "loss 7.600727258250117e-05 average time 0.03317488308330212 iter num 60\n",
            "loss 0.00044613282079808414 average time 0.033332148099952975 iter num 80\n",
            "loss 5.492407944984734e-05 average time 0.032984595590023674 iter num 100\n",
            "loss 8.216658898163587e-05 average time 0.03363037279996206 iter num 20\n",
            "loss 0.00010187469888478518 average time 0.032621780949648384 iter num 40\n",
            "loss 0.00031410210067406297 average time 0.03273901653313563 iter num 60\n",
            "loss 0.0002745326783042401 average time 0.0326141417373492 iter num 80\n",
            "loss 8.02324284450151e-05 average time 0.0325253318299292 iter num 100\n",
            "loss 0.00022845099738333374 average time 0.032465582300028474 iter num 20\n",
            "loss 8.427618013229221e-05 average time 0.03222261589999107 iter num 40\n",
            "loss 0.0005748392431996763 average time 0.03207145576670882 iter num 60\n",
            "loss 0.00013093769666738808 average time 0.03186761447509525 iter num 80\n",
            "loss 0.00024679358466528356 average time 0.031993141270104386 iter num 100\n",
            "loss 9.442385635338724e-05 average time 0.03299039390021789 iter num 20\n",
            "loss 6.489274528576061e-05 average time 0.033366070125157424 iter num 40\n",
            "loss 0.00012499130389187485 average time 0.03299861996680799 iter num 60\n",
            "loss 0.0005341956857591867 average time 0.032690380350140914 iter num 80\n",
            "loss 6.802596180932596e-05 average time 0.03252012506007304 iter num 100\n",
            "loss 0.00011275853466941044 average time 0.0336008539004979 iter num 20\n",
            "loss 8.616131526650861e-05 average time 0.03273889515021437 iter num 40\n",
            "loss 7.128126890165731e-05 average time 0.032761491883563094 iter num 60\n",
            "loss 5.543444785871543e-05 average time 0.03254804367516044 iter num 80\n",
            "loss 7.44698045309633e-05 average time 0.0324287399700188 iter num 100\n",
            "loss 0.00011040987737942487 average time 0.03313669904982817 iter num 20\n",
            "loss 9.629025589674711e-05 average time 0.032863640199684595 iter num 40\n",
            "loss 6.548684905283153e-05 average time 0.032750308783094324 iter num 60\n",
            "loss 0.0005338254850357771 average time 0.0323240471247118 iter num 80\n",
            "loss 0.00016585845150984824 average time 0.0322557948996473 iter num 100\n",
            "loss 6.228157872101292e-05 average time 0.03314523580047535 iter num 20\n",
            "loss 0.00016960014181677252 average time 0.03247335665009814 iter num 40\n",
            "loss 0.0003870323998853564 average time 0.03255103770018953 iter num 60\n",
            "loss 9.656645124778152e-05 average time 0.03268515086274419 iter num 80\n",
            "loss 7.222137355711311e-05 average time 0.032564290140217056 iter num 100\n",
            "loss 7.589554297737777e-05 average time 0.034189405050346976 iter num 20\n",
            "loss 9.22902618185617e-05 average time 0.03380363665019104 iter num 40\n",
            "loss 0.00010171377653023228 average time 0.033625410716861856 iter num 60\n",
            "loss 0.00023538543609902263 average time 0.033712389750053265 iter num 80\n",
            "loss 0.0006604425725527108 average time 0.03371036121017824 iter num 100\n",
            "loss 0.00011546938185347244 average time 0.032987689650144605 iter num 20\n",
            "loss 0.0003370997728779912 average time 0.03272880432487 iter num 40\n",
            "loss 0.0005590993678197265 average time 0.03271677156662918 iter num 60\n",
            "loss 8.788305422058329e-05 average time 0.03293707227494451 iter num 80\n",
            "loss 0.0003127813106402755 average time 0.03297880571004498 iter num 100\n",
            "loss 0.000110113229311537 average time 0.03485153734982305 iter num 20\n",
            "loss 7.829775859136134e-05 average time 0.03408935692486921 iter num 40\n",
            "loss 0.0005071757477708161 average time 0.03380415158329318 iter num 60\n",
            "loss 0.00011324937077006325 average time 0.03370398236247638 iter num 80\n",
            "loss 3.319830648251809e-05 average time 0.033558654749977 iter num 100\n",
            "loss 7.375965651590377e-05 average time 0.03386040270033845 iter num 20\n",
            "loss 5.282974598230794e-05 average time 0.03285278332496091 iter num 40\n",
            "loss 0.0003400099230930209 average time 0.03302182926660559 iter num 60\n",
            "loss 0.00011435995111241937 average time 0.03325052138748106 iter num 80\n",
            "loss 0.0004486030957195908 average time 0.03315772174002632 iter num 100\n",
            "loss 0.0002027288865065202 average time 0.0345693316001416 iter num 20\n",
            "loss 3.731571268872358e-05 average time 0.03417519394988631 iter num 40\n",
            "loss 9.377414971822873e-05 average time 0.0336596799834903 iter num 60\n",
            "loss 9.970719838747755e-05 average time 0.033403210362575916 iter num 80\n",
            "loss 0.0008994822856038809 average time 0.03322435324997059 iter num 100\n",
            "loss 0.00040475837886333466 average time 0.03384152670023468 iter num 20\n",
            "loss 0.0009364720317535102 average time 0.03322564207492178 iter num 40\n",
            "loss 0.00026524291024543345 average time 0.033440370749910166 iter num 60\n",
            "loss 7.328375795623288e-05 average time 0.03324735896248967 iter num 80\n",
            "loss 0.0014003345277160406 average time 0.0331083798799591 iter num 100\n",
            "loss 0.00022443306806962937 average time 0.03426064785035123 iter num 20\n",
            "loss 9.494711412116885e-05 average time 0.03401991802511475 iter num 40\n",
            "loss 0.00016038992907851934 average time 0.03354114529999303 iter num 60\n",
            "loss 0.00016265489102806896 average time 0.033194500987656285 iter num 80\n",
            "loss 5.802126179332845e-05 average time 0.03344204781016742 iter num 100\n",
            "loss 9.380787378177047e-05 average time 0.03425511549994553 iter num 20\n",
            "loss 0.0001027109901770018 average time 0.03360084715004632 iter num 40\n",
            "loss 0.0003193829325027764 average time 0.03322379829981704 iter num 60\n",
            "loss 5.8964982599718496e-05 average time 0.03314394363728752 iter num 80\n",
            "loss 8.412605529883876e-05 average time 0.03317340097972192 iter num 100\n",
            "loss 0.00022467097733169794 average time 0.03387513809993834 iter num 20\n",
            "loss 0.00011218137660762295 average time 0.0329689114501889 iter num 40\n",
            "loss 5.924675861024298e-05 average time 0.032745469666709444 iter num 60\n",
            "loss 9.390570630785078e-05 average time 0.03269880157508851 iter num 80\n",
            "loss 6.611317076021805e-05 average time 0.03267198417001055 iter num 100\n",
            "loss 0.0006995336734689772 average time 0.03373138234965154 iter num 20\n",
            "loss 0.00024137843865901232 average time 0.03286723434994201 iter num 40\n",
            "loss 5.515823431778699e-05 average time 0.032750890083298144 iter num 60\n",
            "loss 0.00015631267160642892 average time 0.03285352054995201 iter num 80\n",
            "loss 5.705828152713366e-05 average time 0.032614818479924 iter num 100\n",
            "loss 0.00016453817079309374 average time 0.034118457100157686 iter num 20\n",
            "loss 0.0001002373464871198 average time 0.03336529512516791 iter num 40\n",
            "loss 8.973181684268638e-05 average time 0.033083132816682334 iter num 60\n",
            "loss 0.00011782615183619782 average time 0.03286797564992412 iter num 80\n",
            "loss 0.0001234254305018112 average time 0.03286037284993654 iter num 100\n",
            "loss 9.386883903061971e-05 average time 0.033951383700150474 iter num 20\n",
            "loss 0.0001088461431208998 average time 0.03293518465025045 iter num 40\n",
            "loss 0.00010047617979580536 average time 0.03279497246670265 iter num 60\n",
            "loss 8.513755165040493e-05 average time 0.032585071912581046 iter num 80\n",
            "loss 0.0005505846347659826 average time 0.03261012421004125 iter num 100\n",
            "loss 0.00018825924780685455 average time 0.033694013600506875 iter num 20\n",
            "loss 5.8427085605217144e-05 average time 0.03366571780015874 iter num 40\n",
            "loss 0.00010559294605627656 average time 0.03346248115018777 iter num 60\n",
            "loss 9.02461979421787e-05 average time 0.033281364587719506 iter num 80\n",
            "loss 0.0002668309898581356 average time 0.03326333210014127 iter num 100\n",
            "loss 0.00029678820283152163 average time 0.034808660500129915 iter num 20\n",
            "loss 7.672386709600687e-05 average time 0.034133240649953225 iter num 40\n",
            "loss 0.00023475561465602368 average time 0.03367040568324834 iter num 60\n",
            "loss 0.0002714674628805369 average time 0.033546090662457576 iter num 80\n",
            "loss 6.920019950484857e-05 average time 0.03322078314005921 iter num 100\n",
            "loss 0.000509722507558763 average time 0.033659616300064955 iter num 20\n",
            "loss 9.498517465544865e-05 average time 0.03291282942518592 iter num 40\n",
            "loss 0.00015567026275675744 average time 0.032949224516717855 iter num 60\n",
            "loss 8.074041397776455e-05 average time 0.032784338825194936 iter num 80\n",
            "loss 0.00012183896615169942 average time 0.03266086757008452 iter num 100\n",
            "loss 0.0005335133173502982 average time 0.03341006914997706 iter num 20\n",
            "loss 4.194670691504143e-05 average time 0.03259419934975085 iter num 40\n",
            "loss 0.00026255124248564243 average time 0.0325376486331758 iter num 60\n",
            "loss 0.0005974295199848711 average time 0.03230007720003414 iter num 80\n",
            "loss 0.0002230586251243949 average time 0.03214653803010151 iter num 100\n",
            "loss 0.0001386414369335398 average time 0.03378176914993673 iter num 20\n",
            "loss 7.342923345277086e-05 average time 0.032505584649970845 iter num 40\n",
            "loss 9.659874194767326e-05 average time 0.03206925671656791 iter num 60\n",
            "loss 0.00013867717643734068 average time 0.03207009312491209 iter num 80\n",
            "loss 8.030466415220872e-05 average time 0.03195146199981536 iter num 100\n",
            "loss 9.26880820770748e-05 average time 0.0330484478499784 iter num 20\n",
            "loss 0.00011098858522018418 average time 0.03269680375005919 iter num 40\n",
            "loss 6.931054667802528e-05 average time 0.03245401718352999 iter num 60\n",
            "loss 6.124129868112504e-05 average time 0.03224625295001715 iter num 80\n",
            "loss 0.0002670893445611 average time 0.03212507522002852 iter num 100\n",
            "loss 0.00035048462450504303 average time 0.03326821365008072 iter num 20\n",
            "loss 7.486189133487642e-05 average time 0.03288589312514887 iter num 40\n",
            "loss 0.0001343703333986923 average time 0.03264722670028277 iter num 60\n",
            "loss 0.00013487097749020904 average time 0.032350820175270203 iter num 80\n",
            "loss 5.171956945559941e-05 average time 0.03221377243025927 iter num 100\n",
            "loss 0.00017694526468403637 average time 0.032803906900517175 iter num 20\n",
            "loss 8.750782581046224e-05 average time 0.03237776302539715 iter num 40\n",
            "loss 9.675961337052286e-05 average time 0.03261809648350512 iter num 60\n",
            "loss 9.739462257130072e-05 average time 0.03261427692509642 iter num 80\n",
            "loss 0.0005577628035098314 average time 0.03247686213006091 iter num 100\n",
            "loss 7.939134229673073e-05 average time 0.03243241825057339 iter num 20\n",
            "loss 8.453382906736806e-05 average time 0.03233087397520649 iter num 40\n",
            "loss 0.00010236450907541439 average time 0.03243583924995619 iter num 60\n",
            "loss 0.0005113881779834628 average time 0.032279898762590166 iter num 80\n",
            "loss 8.15357780084014e-05 average time 0.032510468600085 iter num 100\n",
            "loss 5.318405601428822e-05 average time 0.033616373199765806 iter num 20\n",
            "loss 7.980842929100618e-05 average time 0.03306972495001901 iter num 40\n",
            "loss 0.0001481820800108835 average time 0.03271028396654098 iter num 60\n",
            "loss 0.0013558375649154186 average time 0.03259866857488305 iter num 80\n",
            "loss 0.00019919670012313873 average time 0.03239774295991083 iter num 100\n",
            "loss 0.00040603140951134264 average time 0.03435398349993193 iter num 20\n",
            "loss 0.0013917024480178952 average time 0.0333114688749447 iter num 40\n",
            "loss 0.00011222463945159689 average time 0.03314739851660609 iter num 60\n",
            "loss 0.00041522408719174564 average time 0.03277968355005214 iter num 80\n",
            "loss 4.5817891077604145e-05 average time 0.032598769319920395 iter num 100\n",
            "loss 0.00015048035129439086 average time 0.033721562700156936 iter num 20\n",
            "loss 0.0004926469991914928 average time 0.03291928632543204 iter num 40\n",
            "loss 0.00010893043508986011 average time 0.032466173816889446 iter num 60\n",
            "loss 0.00014616767293773592 average time 0.03234441946274273 iter num 80\n",
            "loss 6.356343510560691e-05 average time 0.03226572563020454 iter num 100\n",
            "loss 9.177377796731889e-05 average time 0.033315909399789234 iter num 20\n",
            "loss 0.00023695433628745377 average time 0.032654489249853216 iter num 40\n",
            "loss 4.149726373725571e-05 average time 0.032435903149901905 iter num 60\n",
            "loss 9.515247802482918e-05 average time 0.03242523834978783 iter num 80\n",
            "loss 0.00017592404037714005 average time 0.03260224567984551 iter num 100\n",
            "loss 0.0003825768071692437 average time 0.03366854029973183 iter num 20\n",
            "loss 0.0001899220806080848 average time 0.032881524025015096 iter num 40\n",
            "loss 5.8749104937305674e-05 average time 0.03262542850000803 iter num 60\n",
            "loss 6.416191172320396e-05 average time 0.032572001837525025 iter num 80\n",
            "loss 0.00012230298307258636 average time 0.032560332160028335 iter num 100\n",
            "loss 4.3541262130020186e-05 average time 0.03331624470065435 iter num 20\n",
            "loss 0.0007386872894130647 average time 0.03306975310024427 iter num 40\n",
            "loss 6.603745714528486e-05 average time 0.03272831786671304 iter num 60\n",
            "loss 9.527054498903453e-05 average time 0.03262373027487229 iter num 80\n",
            "loss 0.000216800719499588 average time 0.03248947910997231 iter num 100\n",
            "loss 6.942507752683014e-05 average time 0.03434922579999693 iter num 20\n",
            "loss 0.00015926813648547977 average time 0.03310589652501221 iter num 40\n",
            "loss 0.0001602409320184961 average time 0.03277319303336602 iter num 60\n",
            "loss 4.3618158088065684e-05 average time 0.03251595957499376 iter num 80\n",
            "loss 7.395646389340982e-05 average time 0.03244417316996987 iter num 100\n",
            "loss 5.732832141802646e-05 average time 0.03367457279982773 iter num 20\n",
            "loss 0.0006276745698414743 average time 0.032780685849775185 iter num 40\n",
            "loss 0.00027496551047079265 average time 0.032408113816563854 iter num 60\n",
            "loss 8.553267980460078e-05 average time 0.032398617124999876 iter num 80\n",
            "loss 9.1513131337706e-05 average time 0.032327512390111224 iter num 100\n",
            "loss 0.00015243545931298286 average time 0.03325837595020857 iter num 20\n",
            "loss 0.00015944532060530037 average time 0.03261655544993118 iter num 40\n",
            "loss 0.0007386332144960761 average time 0.032624833383427664 iter num 60\n",
            "loss 0.0005286349332891405 average time 0.03235036713745103 iter num 80\n",
            "loss 0.00010802239557961002 average time 0.032168855470081324 iter num 100\n",
            "loss 8.068964962149039e-05 average time 0.03299831995009299 iter num 20\n",
            "loss 6.72768073854968e-05 average time 0.03239151830002811 iter num 40\n",
            "loss 0.00011301990161882713 average time 0.03230478833344629 iter num 60\n",
            "loss 9.129101090366021e-05 average time 0.03215460502510723 iter num 80\n",
            "loss 0.0004043043009005487 average time 0.03206859215017175 iter num 100\n",
            "loss 7.563458348158747e-05 average time 0.03329706529966643 iter num 20\n",
            "loss 0.0003698900400195271 average time 0.033424634749917456 iter num 40\n",
            "loss 0.0003725280112121254 average time 0.03294426064997727 iter num 60\n",
            "loss 0.00020277457952033728 average time 0.03278741587505465 iter num 80\n",
            "loss 7.916524191386998e-05 average time 0.03253564301008737 iter num 100\n",
            "loss 7.89603145676665e-05 average time 0.03394796534958004 iter num 20\n",
            "loss 8.728379907552153e-05 average time 0.03342154679994565 iter num 40\n",
            "loss 0.000449629791546613 average time 0.033073282916666356 iter num 60\n",
            "loss 0.0011230150703340769 average time 0.03271333272487027 iter num 80\n",
            "loss 0.0005289707914926112 average time 0.032663609419832935 iter num 100\n",
            "loss 9.450002107769251e-05 average time 0.03279058969965263 iter num 20\n",
            "loss 4.810496830032207e-05 average time 0.03232219154970153 iter num 40\n",
            "loss 0.0004154962080065161 average time 0.03202065601653885 iter num 60\n",
            "loss 0.00029355494189076126 average time 0.032217862962352226 iter num 80\n",
            "loss 6.787021266063675e-05 average time 0.03240584526989551 iter num 100\n",
            "loss 0.00011581669969018549 average time 0.0325443448000442 iter num 20\n",
            "loss 5.971535574644804e-05 average time 0.032172833675122095 iter num 40\n",
            "loss 7.322194869630039e-05 average time 0.032163340683412873 iter num 60\n",
            "loss 0.00010866982484003529 average time 0.03210736868754793 iter num 80\n",
            "loss 5.062779018771835e-05 average time 0.03206972597014101 iter num 100\n",
            "loss 7.857011951273307e-05 average time 0.03374270769963914 iter num 20\n",
            "loss 9.471202793065459e-05 average time 0.03309247740007777 iter num 40\n",
            "loss 0.00015446032921317965 average time 0.032782404733431275 iter num 60\n",
            "loss 7.717853441135958e-05 average time 0.03259614547505407 iter num 80\n",
            "loss 0.00022679464018438011 average time 0.03252819992008881 iter num 100\n",
            "loss 9.437447442905977e-05 average time 0.033351366599890755 iter num 20\n",
            "loss 9.934473928296939e-05 average time 0.032966317699720096 iter num 40\n",
            "loss 6.787978782085702e-05 average time 0.03301452799981538 iter num 60\n",
            "loss 0.000627462868578732 average time 0.03303769711233144 iter num 80\n",
            "loss 8.134837844409049e-05 average time 0.032661050989881917 iter num 100\n",
            "loss 5.526238237507641e-05 average time 0.03307364749980479 iter num 20\n",
            "loss 5.6016597227426246e-05 average time 0.03266655484994772 iter num 40\n",
            "loss 0.00019213258929084986 average time 0.03278512114999709 iter num 60\n",
            "loss 5.580774450208992e-05 average time 0.032593591887462026 iter num 80\n",
            "loss 3.0439759939326905e-05 average time 0.032532359339893444 iter num 100\n",
            "loss 5.997568950988352e-05 average time 0.033511247000024016 iter num 20\n",
            "loss 9.925539779942483e-05 average time 0.03327591277484317 iter num 40\n",
            "loss 7.797652506269515e-05 average time 0.03269621794994843 iter num 60\n",
            "loss 0.00022563812672160566 average time 0.0326611685124135 iter num 80\n",
            "loss 7.503569941036403e-05 average time 0.03258911773998989 iter num 100\n",
            "loss 7.650101179024205e-05 average time 0.032968138800060845 iter num 20\n",
            "loss 5.368288475438021e-05 average time 0.03259074342495296 iter num 40\n",
            "loss 4.464820813154802e-05 average time 0.032423861299927616 iter num 60\n",
            "loss 6.299855158431455e-05 average time 0.03224309427509979 iter num 80\n",
            "loss 0.0002870826283469796 average time 0.032233516720043556 iter num 100\n",
            "loss 0.000495805055834353 average time 0.033569761749822645 iter num 20\n",
            "loss 0.0003661207447294146 average time 0.0327387336996253 iter num 40\n",
            "loss 0.00013722595758736134 average time 0.03284707488298106 iter num 60\n",
            "loss 0.00011911100591532886 average time 0.032600705224785996 iter num 80\n",
            "loss 6.422589649446309e-05 average time 0.032435177969819054 iter num 100\n",
            "loss 9.424522431800142e-05 average time 0.03321156990077725 iter num 20\n",
            "loss 0.0007962064119055867 average time 0.03269639082536742 iter num 40\n",
            "loss 7.590301538584754e-05 average time 0.032578830966728374 iter num 60\n",
            "loss 0.00020382482034619898 average time 0.032815849299959156 iter num 80\n",
            "loss 8.77522979862988e-05 average time 0.03269881935993908 iter num 100\n",
            "loss 8.090578194241971e-05 average time 0.03315207420055231 iter num 20\n",
            "loss 8.992866787593812e-05 average time 0.03254432087533132 iter num 40\n",
            "loss 0.00046736339572817087 average time 0.032405411933541475 iter num 60\n",
            "loss 7.784215995343402e-05 average time 0.0323982491625884 iter num 80\n",
            "loss 6.706623389618471e-05 average time 0.03253693380003824 iter num 100\n",
            "loss 6.314568599918857e-05 average time 0.033641602900388534 iter num 20\n",
            "loss 8.985988097265363e-05 average time 0.033346286025062 iter num 40\n",
            "loss 9.808824688661844e-05 average time 0.03304525536677829 iter num 60\n",
            "loss 4.729046122520231e-05 average time 0.033069064825031094 iter num 80\n",
            "loss 8.747466199565679e-05 average time 0.03314092624012119 iter num 100\n",
            "loss 9.062062599696219e-05 average time 0.034100575399861555 iter num 20\n",
            "loss 6.538758316310123e-05 average time 0.03366015530000368 iter num 40\n",
            "loss 0.0003390328784007579 average time 0.03344617438324349 iter num 60\n",
            "loss 0.0001199848047690466 average time 0.03329500366248794 iter num 80\n",
            "loss 0.00016742452862672508 average time 0.033161652240087276 iter num 100\n",
            "loss 9.587290696799755e-05 average time 0.033745316650311 iter num 20\n",
            "loss 0.00013880894402973354 average time 0.03285837530056597 iter num 40\n",
            "loss 0.0001550967717776075 average time 0.032370080650374194 iter num 60\n",
            "loss 8.957739919424057e-05 average time 0.03240791530020033 iter num 80\n",
            "loss 0.002062429441139102 average time 0.03239580817018577 iter num 100\n",
            "loss 0.0003980690671596676 average time 0.033110087400200425 iter num 20\n",
            "loss 5.4207783250603825e-05 average time 0.03285274082545584 iter num 40\n",
            "loss 6.174799636937678e-05 average time 0.032510367733508856 iter num 60\n",
            "loss 8.258179877884686e-05 average time 0.032401001925109085 iter num 80\n",
            "loss 4.591656397678889e-05 average time 0.0323194702301771 iter num 100\n",
            "loss 0.00010650230979081243 average time 0.03232251340014045 iter num 20\n",
            "loss 7.326718332478777e-05 average time 0.03191542285012474 iter num 40\n",
            "loss 8.086312300292775e-05 average time 0.032092219533357516 iter num 60\n",
            "loss 9.181529458146542e-05 average time 0.032027820724988484 iter num 80\n",
            "loss 6.419792771339417e-05 average time 0.03209760325989919 iter num 100\n",
            "loss 9.112040424952284e-05 average time 0.0332681258500088 iter num 20\n",
            "loss 0.00018638977780938148 average time 0.03280360859998836 iter num 40\n",
            "loss 8.19349879748188e-05 average time 0.03246135121668582 iter num 60\n",
            "loss 3.8661197322653607e-05 average time 0.03255580724999163 iter num 80\n",
            "loss 7.875102892285213e-05 average time 0.03241146979002224 iter num 100\n",
            "loss 4.3909327359870076e-05 average time 0.03343512694991659 iter num 20\n",
            "loss 7.228690083138645e-05 average time 0.033005772000160506 iter num 40\n",
            "loss 5.6286444305442274e-05 average time 0.032541507283591876 iter num 60\n",
            "loss 9.630227577872574e-05 average time 0.03236054578760559 iter num 80\n",
            "loss 0.00030168367084115744 average time 0.032452846560154285 iter num 100\n",
            "loss 5.568078995565884e-05 average time 0.032997958949999885 iter num 20\n",
            "loss 6.609338015550748e-05 average time 0.032706842299921844 iter num 40\n",
            "loss 5.8133948186878115e-05 average time 0.032512505366442686 iter num 60\n",
            "loss 0.00021845786250196397 average time 0.032373485799598714 iter num 80\n",
            "loss 7.914273737696931e-05 average time 0.03235825027968531 iter num 100\n",
            "loss 0.00018017864204011858 average time 0.034697535049599534 iter num 20\n",
            "loss 7.82032948336564e-05 average time 0.03449859234979158 iter num 40\n",
            "loss 5.981592403259128e-05 average time 0.03352436848308571 iter num 60\n",
            "loss 8.213699766201898e-05 average time 0.03310700683723553 iter num 80\n",
            "loss 0.0010656013619154692 average time 0.03284798048982338 iter num 100\n",
            "loss 0.00019671139307320118 average time 0.03381211350006197 iter num 20\n",
            "loss 0.0001669448392931372 average time 0.03351002602530571 iter num 40\n",
            "loss 9.316773503087461e-05 average time 0.032961961466814196 iter num 60\n",
            "loss 0.0001184558350360021 average time 0.03259067573767425 iter num 80\n",
            "loss 0.000528216827660799 average time 0.03247809004020383 iter num 100\n",
            "loss 0.00011717665620381013 average time 0.03293948759965133 iter num 20\n",
            "loss 6.879701686557382e-05 average time 0.03286573372515704 iter num 40\n",
            "loss 0.0003093083796557039 average time 0.03267892206683124 iter num 60\n",
            "loss 6.8374298280105e-05 average time 0.03255998008767165 iter num 80\n",
            "loss 0.00011233790428377688 average time 0.03250974915012193 iter num 100\n",
            "loss 5.6384011259069666e-05 average time 0.03311873674992967 iter num 20\n",
            "loss 0.00026111805345863104 average time 0.032734841574892926 iter num 40\n",
            "loss 9.054991096490994e-05 average time 0.03283500648340123 iter num 60\n",
            "loss 7.263036968652159e-05 average time 0.032635875575033425 iter num 80\n",
            "loss 0.00011088557948824018 average time 0.03247739271006139 iter num 100\n",
            "loss 4.694627205026336e-05 average time 0.03346205319976434 iter num 20\n",
            "loss 4.1245097236242145e-05 average time 0.03289616822485186 iter num 40\n",
            "loss 8.06199896032922e-05 average time 0.03278629629997643 iter num 60\n",
            "loss 4.995341441826895e-05 average time 0.03254547291248855 iter num 80\n",
            "loss 9.811515337787569e-05 average time 0.03241833483003575 iter num 100\n",
            "loss 0.0001200844271807 average time 0.03381833604980784 iter num 20\n",
            "loss 0.00010083699453389272 average time 0.03324222237506547 iter num 40\n",
            "loss 0.0006522628245875239 average time 0.03271686785016451 iter num 60\n",
            "loss 8.673702541273087e-05 average time 0.032583897175118184 iter num 80\n",
            "loss 8.615765545982867e-05 average time 0.03250367170010577 iter num 100\n",
            "loss 7.70307524362579e-05 average time 0.03328753159985354 iter num 20\n",
            "loss 7.401074253721163e-05 average time 0.032775001024765514 iter num 40\n",
            "loss 0.00028203861438669264 average time 0.03274766661652393 iter num 60\n",
            "loss 6.848540215287358e-05 average time 0.03262044508737745 iter num 80\n",
            "loss 7.285515312105417e-05 average time 0.032578807419849906 iter num 100\n",
            "loss 4.2197585571557283e-05 average time 0.033134474550024606 iter num 20\n",
            "loss 0.0005948870675638318 average time 0.03331749007484177 iter num 40\n",
            "loss 6.632359873037785e-05 average time 0.033064616450064935 iter num 60\n",
            "loss 7.680141425225884e-05 average time 0.03288771645006818 iter num 80\n",
            "loss 4.347706999396905e-05 average time 0.03285925867010519 iter num 100\n",
            "loss 7.102345989551395e-05 average time 0.03350726589960686 iter num 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bcafc712cc0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bcafc712cc0f>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bcafc712cc0f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bcafc712cc0f>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0cc_DoENm4G"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tt0PFffNmVO",
        "outputId": "bbfaa4f0-4ba6-453b-9552-292c5614054e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMkQh_5rNxAY"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_BS_NewNN_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT2g0J3qN284"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eChrShF1N4nu",
        "outputId": "a1073fce-8cdd-4cb7-d0e8-a813400a94a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoOdFI5VN7zN",
        "outputId": "3d98b0cd-5ff3-4300-b751-e222f7282a78"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_BS_NewNN_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVPuJKhrOAet",
        "outputId": "1f36da08-0259-4375-c56e-a7d470f9f8c9"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhKSwGk-O6R_"
      },
      "source": [
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import OptionDataSet\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch import Tensor\n",
        "# from torch.autograd import grad\n",
        "# timer = Timer(average=True)\n",
        "# #model = Net().cuda()\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# #dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "# # dataset = OptionDataSet(max_len = 100, number_path = 500000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "# dataset = OptionDataSet(max_len = 100, number_path = 5000000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     y_pred = model(x)\n",
        "\n",
        "#     def compute_deltas(x):\n",
        "#       inputs = x\n",
        "#       inputs.requires_grad = True\n",
        "#       first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "#       return first_order_gradient[0][[2]]\n",
        "\n",
        "#     deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "#     y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "#     loss_weight = torch.tensor([1, 1]).cuda()  # let delta weight = 0 for testing\n",
        "#     loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "#     loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 10\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs = 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvLBlnD9a4hD"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLOOywC_a5Xf"
      },
      "source": [
        "# import torch\n",
        "# model_save_name = 'jax_european_1stock_BS_NewNN_3.pth'\n",
        "# path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ6lpcXObC0t"
      },
      "source": [
        "# state_dict = torch.load(path)\n",
        "# print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK2WfATbbGxM"
      },
      "source": [
        "# # need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "# from model import Net\n",
        "# model = Net().cuda()\n",
        "\n",
        "# model.load_state_dict(state_dict)\n",
        "# print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdK0IMDvOIl1"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzXW2m_IOBYP",
        "outputId": "a875ba08-6d00-4887-fea2-e837678bfd10"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.3, 0.3]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.27130044, 0.90763223)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.2706]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9151], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq-PF4oaPNlh"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "J4CQ1pnkOKZZ",
        "outputId": "f356b31b-749b-46c5-f941-05040ae160ae"
      },
      "source": [
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "BS_call_prices = []\n",
        "for p in prices:\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    BS_call_prices.append(bs_call(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, BS_call_prices, label = \"BS_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(BS_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD4CAYAAAAUymoqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gVZdrH8e+dQkIJJRBqCAGk10BoVnZRwUqxIYKoKGBb+yusLqy4rnVXF3ujuxQrWFABcXEVgQChBIQECJBQEgiEloSU+/0jo1dkE5KQMjnn3J/rOlfOzDzznHuSkB8zzxRRVYwxxpjS8HO7AGOMMZ7HwsMYY0ypWXgYY4wpNQsPY4wxpWbhYYwxptQC3C6gMjRo0EAjIyPdLsMYYzzK2rVrD6lqWGHLfCI8IiMjiYmJcbsMY4zxKCKyu6hldtjKGGNMqVl4GGOMKTULD2OMMaXmE2MehcnOziYpKYnMzEy3S/EZwcHBhIeHExgY6HYpxpgy8tnwSEpKIiQkhMjISETE7XK8nqpy+PBhkpKSaNmypdvlGGPKyGcPW2VmZlK/fn0LjkoiItSvX9/29IzxEj4bHoAFRyWz77cx3sNnD1sZY4wny8nNY396JslHM9h3NIOM7FxycpXs3Dxy8pSc3Dyyc5W2jUK4qmuTcv98Cw8X+fv706VLF7KzswkICODWW2/loYcews/Pj5iYGGbNmsXUqVPJysriqquu4tChQ0ycOJGmTZsyfvx4AgMDWblyJdWrV3d7U4wxFSQvT9l28DhrEtOI3XuUpLQMko6c4sCxTPJK8Dima7o1tfDwNtWrVyc2NhaAlJQURowYwbFjx3jqqaeIjo4mOjoagPXr1wP81nb8+PFMnDiRkSNHluhzVBVVxc/Pp49SGuMRcvOUjUlHWb0rjdW70ojZfYT0jGwAGoYEEdmgJn1b1adZveo0q1udZvWq07RudWoFBRDgJwT4+xHoLwT4+RHgJ/j5VdDh4l//sHjzq2fPnnqmLVu2/M+8ylazZs3fTe/YsUNDQ0M1Ly9Ply9frldddZUePHhQW7durbVr19Zu3brpW2+9pfXq1dPIyEgdMWKEqqq+8MILGh0drV26dNFJkyapququXbu0bdu2OmrUKO3YsaMmJiYW2a59+/Z65513aseOHfWyyy7TU6dOqapqfHy8DhgwQLt27apRUVGakJBQ5OedOHFCr7zySu3atat26tRJ582bV+g2V4XvuzFV0Y6U4/r84q3a55ml2uLxL7TF41/oH15cro9/tEE/itmrew6f1Ly8vEqtCYjRIv6u2p4H8NTncWzZd6xc++zYtDaTr+lUqnVatWpFbm4uKSkpv81r2LAh7733Hi+99BJffPEFACtXruTqq6/m+uuv59tvvyU+Pp7Vq1ejqlx77bWsWLGCiIgI4uPjmTlzJn379i223dy5c3n33Xe58cYb+fjjjxk5ciS33HILEyZMYOjQoWRmZpKXl1dkP6mpqTRt2pQvv/wSgPT09PL7ZhrjpY5nZvPlxv18uDaJtbuP4CdwSdswJl7Znn6t69MwJNjtEotk4eHhvv32W7799luioqIAOHHiBPHx8URERNCiRQv69u1bbLuWLVvSvXt3AHr27EliYiLHjx8nOTmZoUOHAvkX+J2tn4suuohHHnmExx9/nKuvvpqLLrqoUr8PxniSfUczeG15Ap+sSyIzO4/WYTWZcEV7hkU1o2HtqhsYBVl4QKn3ECrKzp078ff3p2HDhmzdurVE66gqEydOZNy4cb+bn5iYSM2aNUvULigo6Ldpf39/MjIySv15AOvWreOrr77iySefZMCAAUyaNKlE22CMr0g5lsnryxOYu3ovAMN6NOOmXs3p3ryux53KbiOoVURqairjx4/nvvvuK9Uv0cCBA5k2bRonTpwAIDk5+XeHvUrb7lchISGEh4fz2WefAZCVlcWpU6eK7Gffvn3UqFGDkSNH8thjj7Fu3boSb4Mx3u7wiSye+XILF72wnA9W7eG6ns1Y/lh/nruuK1ER9TwuOMD2PFyVkZFB9+7dfztVd9SoUTz88MOl6uPyyy9n69at9OvXD4BatWoxZ84c/P39z6ldQbNnz2bcuHFMmjSJwMBAPvzwwyL7SUhI4LHHHsPPz4/AwEDefPPNUm2HMd4oMzuXN5Yn8N5/d5GZncvQqHAeGNCGiPo13C6tzCR/QN27RUdH65kPg9q6dSsdOnRwqSLfZd934yvWJKbx+Ecb2XnoJFd3bcJDl7WldVgtt8sqFRFZq6rRhS2zPQ9jjClHJ7NyeOHrX5j1826a1a3O7DG9uahNoU9y9WgWHsYYU05+iE9lwseb2Jeeweh+kTw2sB01g7zzz6x3blUJqapHDlR5Kl84RGp806nTOfx1URwLYpJoFVaTD8f1Izoy1O2yKpTPhkdwcDCHDx+227JXEnWe5/Hr9SLGeIvEQycZN3st8SnHubt/ax4Y0IbgwKJPRPEWPhse4eHhJCUlkZqa6nYpPuPXJwka4y2Wb0vhgbnr8fMTZt7hnWMbRfHZ8AgMDLQn2hljzklenvLG9wn8Y8l22jeuzTujetI81PNPvy0Nnw0PY4w5F8czs3lkwQa+3XKQId2b8uywrlSv5v2Hqc5k4WGMMSW0+/BJ7pixhsTDp/jL1R2544JInx0ztfAwxpgS2HbgOCPfX0V2bh5zxvShX+v6bpfkKgsPY4wpxvo9R7ht+hqCA/34cFw/2jQKcbsk11l4GGPMWfyYcIi7ZsUQFhLEnDF9fG5gvCjlclddERkkIttEJEFEJhSyPEhE5jvLV4lIZIFlE53520RkYIH500QkRUQ2n9FXqIgsEZF452u98tgGY4w50zdxB7h9+hqa16vBh+P6WXAUUObwEBF/4HXgCqAjcLOIdDyj2RjgiKqeB7wMPO+s2xEYDnQCBgFvOP0BzHDmnWkCsExV2wDLnGljjClXH69N4p4P1tGxaW3mj+vrMQ9pqizlsefRG0hQ1Z2qehqYBww+o81gYKbz/iNggOSfojAYmKeqWaq6C0hw+kNVVwBphXxewb5mAkPKYRuMMeY3c1fv4ZEPN9C3VSgf3NmHujWquV1SlVMe4dEM2FtgOsmZV2gbVc0B0oH6JVz3TI1Udb/z/gDQqLBGIjJWRGJEJMauIjfGlNTSLQd54tNN9G8Xxvuje3ntjQ3LyqOfJKj5d9or9G57qvqOqkaranRYmO/cMsAYc+5i9x7lvrnr6NysDq+P6OET96g6V+URHslA8wLT4c68QtuISABQBzhcwnXPdFBEmjh9NQGKfpaqMcaUUOKh/AsAG4YE2x5HCZRHeKwB2ohISxGpRv4A+KIz2iwCRjvvrwe+c/YaFgHDnbOxWgJtgNXFfF7BvkYDC8thG4wxPuzwiSxGT1+NqjLj9l6EhQS5XVKVV+bwcMYw7gO+AbYCC1Q1TkSmiMi1TrP3gfoikgA8jHOGlKrGAQuALcDXwL2qmgsgInOBlUA7EUkSkTFOX88Bl4lIPHCpM22MMeck43Qud8yM4UB6Ju+N7kUrD3tUrFt89hnmxhiTk5vH+DlrWfZLCm+N7MnATo3dLqlKOdszzD16wNwYY8rib19uZenWFJ66tpMFRylZeBhjfNLnG/Yx46dExlzYklv7Rbpdjsex8DDG+JydqSeY8PFGeraox4Qr2rtdjkey8DDG+JTM7Fzu+WAd1QL8ePXmKAL97c/gubATmY0xPuWpz+P45cBxpt/ei6Z1q7tdjseyyDXG+IzP1iczd/Ve7u7fmj+0a+h2OR7NwsMY4xMSUk7w50830SuyHo9c1tbtcjyehYcxxutlnM7l3g/WERzoz6s39yDAxjnKzMY8jDFe76+L4tiecpwZt/emcR17Lkd5sPg1xni1z9YnMz9mL/f2P49L2todtsuLhYcxxmslHjrJE844x4OXtnG7HK9i4WGM8UpZObncN3cdAf5+/Gt4lI1zlDMb8zDGeKXnF29jc/Ix3hnV067nqAAWxcYYr7N0y0Gm/biL286P5HK74WGFsPAwxniV/ekZPPrRBjo1rc3EK+2+VRXFwsMY4zVycvN4YG4sp3PyePXmKIIC7BnkFcXGPIwxXuPV7xJYnZjGP2/sZk8ErGC252GM8Qqrdh7m1e/iua5HOMN6hLtdjtez8DDGeLz0U9k8ND+WiNAaTBncye1yfIIdtjLGeDRV5c+fbSLleBYf330+NYPsz1plsD0PY4xH+3hdMl9u3M9Dl7WlW/O6bpfjMyw8jDEeK/HQSSYv3EyflqGMv6S12+X4FAsPY4xHys7N48H5sfj7CS/f1B1/P3G7JJ9iBweNMR5p6rJ4Yvce5fURPez2Iy6wPQ9jjMdZvSuN15cncEPPcK7q2sTtcnxSuYSHiAwSkW0ikiAiEwpZHiQi853lq0QkssCyic78bSIysLg+RWSGiOwSkVjn1b08tsEY4xnSM/JPy20eWoPJ19ppuW4p82ErEfEHXgcuA5KANSKySFW3FGg2BjiiqueJyHDgeeAmEekIDAc6AU2BpSLy68OFz9bnY6r6UVlrN8Z4nkkLN3PgWCYf330+tey0XNeUx55HbyBBVXeq6mlgHjD4jDaDgZnO+4+AASIizvx5qpqlqruABKe/kvRpjPExC2OTWRi7jwcHtKG7nZbrqvIIj2bA3gLTSc68Qtuoag6QDtQ/y7rF9fmMiGwUkZdFJKiwokRkrIjEiEhMampq6bfKGFOlJB/N4MnPNtMjoi5397fTct3miQPmE4H2QC8gFHi8sEaq+o6qRqtqdFiYPbfYGE+Wl6c8siCWvDzl5Zu621MBq4Dy+AkkA80LTIc78wptIyIBQB3g8FnWLbJPVd2v+bKA6eQf4jLGeLH3/ruTn3emMfnaTrSoX9PtcgzlEx5rgDYi0lJEqpE/AL7ojDaLgNHO++uB71RVnfnDnbOxWgJtgNVn61NEmjhfBRgCbC6HbTDGVFFb9h3jxW+2MahTY27oaXfLrSrKfKqCquaIyH3AN4A/ME1V40RkChCjqouA94HZIpIApJEfBjjtFgBbgBzgXlXNBSisT+cjPxCRMECAWGB8WbfBGFM1ZWbn8uD89dStUY2/D+tC/v8ZTVUg+TsA3i06OlpjYmLcLsMYU0pPfR7H9B8TmXlHby5pa2OXlU1E1qpqdGHLbNTJGFMlrdieyvQfE7nt/EgLjirIwsMYU+WkHMvk4QWxtGlYiwlXtHe7HFMIuzzTGFOl5OTmcf/c9ZzMyuXfd/UgONDf7ZJMISw8jDFVyr+WxbNqVxov3dCNto1C3C7HFMEOWxljqowV21N5zblb7vV2Wm6VZuFhjKkSDqRn8uD8WNo2DGHK4M5ul2OKYeFhjHFdTm4ef5q7nszsXF6/pQfVq9k4R1VnYx7GGNf9Y8l2Viem8cpN3TmvYS23yzElYHsexhhXLd+Wwpvf7+Dm3s0ZEnXmDblNVWXhYYxxzZ7Dp3hwXiztG4cw+Rp7KqAnsfAwxrji1Okcxs6OQVV5e1RPu57Dw9iYhzGm0qkqEz7exLaDx5l+Wy+7zboHsj0PY0yle++HXSzasI9HL29H/3YN3S7HnAMLD2NMpfpv/CGeXbyVKzo35h57nKzHsvAwxlSavWmnuH/uOlqH1eLFG7rZ8zk8mIWHMaZSZJzOZdzsteTkKe/cGk2tIBty9WT20zPGVDhV5c+fbmLrgWO8Pzqalg1sgNzT2Z6HMabCvfpdAp+uT+ahS9vyx/aN3C7HlAMLD2NMhfpkXRL/XLKdYVHNuP+P57ldjiknFh7GmArzU8IhHv94I/1a1ee567raALkXsfAwxlSI7QePM27OWiLr1+StUT2pFmB/bryJ/TSNMeUu5Vgmt09fQ3CgP9Nv70Wd6oFul2TKmYWHMaZcnczK4Y6Za0g7eZppo3sRXq+G2yWZCmDhYYwpN78+1GnLvmO8NiKKLuF13C7JVBC7zsMYUy7y8pT/+3gjy35J4ekhnRnQwU7J9WblsuchIoNEZJuIJIjIhEKWB4nIfGf5KhGJLLBsojN/m4gMLK5PEWnp9JHg9FmtPLbBGHPuVJXJi+L4ZF0yD1/WllF9W7hdkqlgZQ4PEfEHXgeuADoCN4tIxzOajQGOqOp5wMvA8866HYHhQCdgEPCGiPgX0+fzwMtOX0ecvo0xLlFVnv96G7N/3s24i1vZtRw+ojz2PHoDCaq6U1VPA/OAwWe0GQzMdN5/BAyQ/BO+BwPzVDVLVXcBCU5/hfbprPNHpw+cPoeUwzYYY87R68sTeOs/OxjZN4IJV7S3azl8RHmERzNgb4HpJGdeoW1UNQdIB+qfZd2i5tcHjjp9FPVZAIjIWBGJEZGY1NTUc9gsY0xxpv13Fy99m3/1+JRrO1tw+BCvPdtKVd9R1WhVjQ4LC3O7HGO8zoI1e5nyxRYGdmrEC9d3xc/PgsOXlEd4JAPNC0yHO/MKbSMiAUAd4PBZ1i1q/mGgrtNHUZ9ljKlgC2OTefyTjVzcNoypN0cR4O+1/w81RSiPn/gaoI1zFlQ18gfAF53RZhEw2nl/PfCdqqozf7hzNlZLoA2wuqg+nXWWO33g9LmwHLbBGFNCH61N4sH5sfRpGcrbI3sSFODvdknGBWW+zkNVc0TkPuAbwB+YpqpxIjIFiFHVRcD7wGwRSQDSyA8DnHYLgC1ADnCvquYCFNan85GPA/NE5G/AeqdvY0wlmLt6D3/+dBMXtG7Au7dGU72aBYevkvz/zHu36OhojYmJcbsMYzzarJWJTFoYR/92Ybw1sifBgRYc3k5E1qpqdGHL7ApzY0yx3vthJ3/7ciuXdmjE67dE2aEqY+FhjDm7N75P4IWvt3FF58b8a3iU3VrdABYexpgiqCovL41n6rJ4ru3WlH/e2M3OqjK/sfAwxvyPnNw8/rIwjrmr93B9z3Cev64r/nYdhynAwsMY8zuZ2bncP3c9S7Yc5J7+rXlsYDu7ctz8DwsPY8xvjp46zZiZMazbc4Snru3E6PMj3S7JVFEWHsYYAJKPZjB62mr2HD7F6yN6cGWXJm6XZKowCw9jDL8cOMboaas5lZXLzDt60691fbdLMlWchYcxPm7F9lTu/WAdNYL8WTC+Hx2a1Ha7JOMBLDyM8WFzft7N5EVxtGlYi/dv60WzutXdLsl4CAsPY3xQbp7yzJdbmfbjLv7QLoxXR/SgVpD9OTAlZ78txviYE1k5PDB3Pct+SeG28yN58qoOdvGfKTULD2N8yL6jGYyZGcO2A8eYMrgTt/aLdLsk46EsPIzxEev3HGHc7LWcOp3LtNt60b9dQ7dLMh7MwsMYH7AgZi9PfrqZRnWCmD2mD+0ah7hdkvFwFh7GeLGc3Dye+Wor039M5ILz6vPazT2oV7Oa22UZL2DhYYyXOnLyNPf+ex0/7TjMHRe05M9XtreBcVNuLDyM8UJb9x9j7OwYDh7L4qUbunF9z3C3SzJexsLDGC/z9eb9PLxgAyHBASwY14/uzeu6XZLxQhYexniJvDxl6nfxvLI0nu7N6/LOqJ40rB3sdlnGS1l4GOMFTmbl8OiHG1i8+QDX9QjnmaGdCQ6054ybimPhYYyH25t2irtmxbD94HGevKoDYy5saQ9vMhXOwsMYD7Zq52Hu/mAdObl5TL+9N5e0DXO7JOMjLDyM8VDz1+zhiU83E1G/Bu/dGk2rsFpul2R8iIWHMR4mL0958dttvPn9Di5uG8ZrI6KoHRzodlnGx5TpiiERCRWRJSIS73ytV0S70U6beBEZXWB+TxHZJCIJIjJVnAO1RfUrIv1FJF1EYp3XpLLUb4ynyczO5f6563nz+x3c0ieCaaOjLTiMK8p6uekEYJmqtgGWOdO/IyKhwGSgD9AbmFwgZN4E7gLaOK9BJej3B1Xt7rymlLF+YzzGoRNZ3Pzuz3y1eT9PXNmBvw3pbFeMG9eU9TdvMDDTeT8TGFJIm4HAElVNU9UjwBJgkIg0AWqr6s+qqsCsAuuXpF9jfEZCygmGvvEjW/cf481benDXxa3sjCrjqrKGRyNV3e+8PwA0KqRNM2BvgekkZ14z5/2Z84vrt5+IbBCRxSLSqYz1G1PlrdxxmGFv/EjG6Vzmje3HoM5N3C7JmOIHzEVkKdC4kEVPFJxQVRURLa/Ciuh3HdBCVU+IyJXAZ+Qf7vofIjIWGAsQERFR3mWVyumcPJZuPciuQyfp1LQ2XZrVoX6tIFdrMp7ho7VJTPxkIy3q12T6bb1oHlrD7ZKMAUoQHqp6aVHLROSgiDRR1f3OYaiUQpolA/0LTIcD3zvzw8+Yn+y8L7RfVT1WoK6vROQNEWmgqocKqfsd4B2A6Ojocg+1kkhIOcH8NXv4eF0yaSdP/25Zs7rV6Rpehy7hdYhuEUqvyHp2GML8RlV5eWk8U5fFc8F59Xnjlp7UqW4D46bqKOupuouA0cBzzteFhbT5Bvh7gUHyy4GJqpomIsdEpC+wCrgVePVs/YpIY+CgszfSm/zDbofLuA3lKuN0Ll9u2s/8NXtYk3iEAD/h0g6NGN67OVER9di6/xgbk46yMSmdjUnpLN58AIAruzTmb0O6EGrPWvB5WTm5PP7RRj6L3ccNPcN5ZmgXqgXYwLipWiR/rPocVxapDywAIoDdwI1OKEQD41X1TqfdHcCfndWeUdXpzvxoYAZQHVgM3O8EQ1H93gfcDeQAGcDDqvpTcXVGR0drTEzMOW9nSaWdPM2Nb68kIeUErRrU5KZezRnWI5ywkKIPUR09dZoPVu3hlaXbqVujGs9f14U/ti9s6Mj4giMnTzNu9lpWJ6bx2MB23NO/te2RGteIyFpVjS50WVnCw1NURniczMphxLs/88uB47w+ogcDOjQs1T/6uH3pPDx/A9sOHufm3hE8eVUHagbZNZy+JPHQSW6fsYbkIxm8eENXBndvVvxKxlSgs4WH7QuXg9M5eYyfs5bN+47x2ogeXNqxUan/t9ipaR0W3X8B4y5pxbw1e7jiXz8Qk5hWQRWbqmZNYhpD3/gxf0/0rj4WHKbKs/Aoo7w85ZEPN/BD/CGeHdaFyzqe+yGnoAB/Jl7Rgflj+6EoN7y9kgVr9ha/ovFon61P5pZ3V1GvRjU+vecCekWGul2SMcWy8CgDVeWpz+P4fMM+JlzRnhujm5dLv71bhrL4gYu58LwGTPhkI4s37S9+JeNxVJVXlm7nwfmxREXU5ZN7zieyQU23yzKmRCw8yuC17xKYuXI3d17YknEXtyrXvmsFBfD2qJ50b16XB+bF8kN8arn2b9yVlZPLwws28MrSeK7rEc7sMX2oW8POtDOew8LjHH2wajf/WLKdYVHN+POVHSrkjJga1QKYfltvWoXVZNzstazbc6TcP8NUviMnTzPqvdV8uj6ZRy9vy0s3dLVTcY3Hsd/Yc7Dr0EkmL4yjf7swnr++K35+FXcqZZ0agcy6ozdhIUHcPn0N2w4cr7DPMhVv9+GTDH3jR2KTjvLqzVHc98c2diqu8UgWHufgn0u2E+jvxwvXdyWwEu5q2rB2MHPG9CEowI9R769iz+FTFf6ZpvxtTk7nujd/Ij0jm7l39eWabk3dLsmYc2bhUUpx+9L5fMM+7rgwkoYhwZX2uc1DazDnzj6czs1j5PurSDmWWWmfbcruv/GHuOntlQQF+PPR3efTs0Whj74xxmNYeJTSS99so071QMZe3LrSP7ttoxCm39aLQyeyuPff+c+tNlXfog37uH3GapqH1uCTe86ntT0u1ngBC49SWJOYxvJtqYy/pLVrN6mLiqjH34d2YU3iEV5bnuBKDabk3v/vLv40dz1REfWYP64fjWpX3t6qMRXJwqOEVJXnF/9Cw5Agbjs/0tVahkQ1Y1hUM6Yui2f1LrsKvSpSVZ5dvJWnv9jCoE6NmXVHb7srrvEqFh4ltHxbCjG7j3D/gDZUr+bvdjlMGdKZ5qE1eHDeetJPZbtdjilAVfnLws28/Z+djOwbweu39CA40P3fGWPKk4VHCeTlKS9+s50W9WswvFf5XEVeVrWCApg6PIqU41lM/HQjvnCDS0+gqkxaGMecn/cw7pJWPD24M/4VeCq3MW6x8CiBzzfuY+v+Yzx8WdtKOTW3pLo1r8ujA9vx1aYDzLd7YLlOVZm8KI7ZP+9m3MWtmDCovV3DYbxW1flLWEVl5+bxzyXbad84hGu6Vr3z8sde1IoLz2vAU59vISHFLiB0S/59zrYwa+Vuxl7ciglXWHAY72bhUYwFMXvZffgUjw1sV6FXkp8rPz/hnzd2o3o1f+6fG0tmdq7bJfmcX4Njxk+J3HlhSyZacBgfYOFxFpnZuUxdFk/PFvX4Y/uGbpdTpIa1g3nx+q5s3X+MF77e5nY5PkVVmfJFfnCMubAlT1xVMfc5M6aqsfA4i03J6RzPzOH/Brar8n8QBnRoxKi+LZj+0y7W7rYbKFaWF7/ZxvQfE7n9gkietOAwPsTC4yx6RYaycsIA+rSq73YpJfL4Fe1pXDuYiZ9s5HSOXX1e0f69ag9vfL+DEX0imHR1RwsO41MsPIpRp4bnXNhVKyiApwd3ZvvBE7z9nx1ul+PVlm9L4S8LN/OHdmFMubaTBYfxORYeXubSjo24qmsTXv0ugR2pJ9wuxyvF7Uvnvg/W0b5xCK+N6EFAFTp925jKYr/1XmjyNR0JDvRj4iebyMuziwfL076jGdwxYw11qgcy7bZe1AwKcLskY1xh4eGFGoYE88RVHVi9K435MXbxYHk5lpnNHTPWcCorl2m397KbHBqfZuHhpW6Mbk7fVqH8/aut9uyPcpCdm8e9H6wjIeUEb47sSfvGtd0uyRhXWXh4KRHh2WFdycrJY/KiOLfL8XiTFm7mh/hD/H1YFy5s08DtcoxxnYWHF2vZoCYPDGjD4s0H+DbugNvleKz5a/Ywd/Ve7unfmhujq8aNMY1xW5nCQ0RCRWSJiMQ7Xwt9tqaIjHbaxIvI6ALze4rIJhFJEJGp4pzvKCI3iEiciOSJSPQZfU102m8TkYFlqd8XjL24Fe0bhzBpYRzHM+3W7aW1OTmdvyyM48LzGvDI5e3cLseYKqOsex4TgGWq2gZY5kz/joiEApOBPkBvYHKBkHkTuAto47wGOfM3A8OAFWf01REYDpf8NUEAAA3MSURBVHRy2r4hIvaghLMI9Pfj2WFdOHg8k5eXxLtdjkc5euo04+espUHNavxreHe7tboxBZQ1PAYDM533M4EhhbQZCCxR1TRVPQIsAQaJSBOgtqr+rPkPo5j16/qqulVVC7tJ02BgnqpmqeouIIH8QDJnERVRjxG9I5jx0y42J6e7XY5HyMtTHpofy8Fjmbx+Sw/q1wpyuyRjqpSyhkcjVd3vvD8ANCqkTTOg4PmiSc68Zs77M+efTVF9/Q8RGSsiMSISk5qaWky33u//BrYntGY1nvxss137UQKvLU9g+bZUJl3dkaiIQo/GGuPTig0PEVkqIpsLeQ0u2M7Ze6gyf5VU9R1VjVbV6LCwMLfLcV2dGoE8cVUHYvceZe6aPW6XU6Wt2J7Ky0u3MzSqGSP7tnC7HGOqpGIvj1XVS4taJiIHRaSJqu53DkOlFNIsGehfYDoc+N6ZH37G/ORiykkGCp7uUpJ1jGNI92YsWJPE84t/4fKOjQkLsUMxZ0o+msED89bTtmEIzwztbPesMqYIZT1stQj49eyp0cDCQtp8A1wuIvWcgfLLgW+cw13HRKSvc5bVrUWsf+bnDReRIBFpSf4g++oyboPPEBGeHtKZjOxcnv1qq9vlVDmnc/K454N15OQqb43qSY1qdusRY4pS1vB4DrhMROKBS51pRCRaRN4DUNU04GlgjfOa4swDuAd4j/yB7x3AYmf9oSKSBPQDvhSRb5y+4oAFwBbga+BeVbVH55XCeQ1rMf6S1nyyPpmfdhxyu5wq5fmvf2HD3qO8eENXWjao6XY5xlRpkj9U4d2io6M1JibG7TKqjMzsXC5/eQUB/sLiBy4iKMDOdl6y5SB3zYrhtvMj+eu1ndwux5gqQUTWqmp0YcvsCnMfFBzoz5TBndiZepJ3V+x0uxzXJR/N4NEPN9C5WW0mXtne7XKM8QgWHj6qf7uGXNmlMa9+l8DuwyfdLsc12bl53P/vdeTmKa/d3MP2wowpIQsPHzbp6k5U889/7ocvHL4szD++3c66PUd5dlgXIm2cw5gSs/DwYY3rBDPhyvb8tOMwH65NKn4FL7N8Wwpv/Sf/GeTXdGvqdjnGeBQLDx93c68IekeG8syXW0k57jvP/TiQnskjCzbk3zTy6o5ul2OMx7Hw8HF+fsKz13Uh43QuT32+xe1yKkVObh5/mreezOxcXr+lB8GBNs5hTGlZeBhah9XiTwPO48uN+1my5aDb5VS415YnsHpXGn8b0pnWYbXcLscYj2ThYQAYe3Fr2jcO4S+fbfbq536s3pXG1GXxDOvRjGE9wotfwRhTKAsPA0C1AD+eu64rB49n8sLXhd0N3/MdPXWaB+etJyK0BlMGd3a7HGM8moWH+U335nW5/fyWzP55NzGJacWv4EFUlQkfbyL1RBav3tyDWkF23ypjysLCw/zOI5e3pVnd6jz+8UaycrzntmEfrNrD13EH+L+B7ekSXsftcozxeBYe5ndqBgXw92Fd2JF6kn98u93tcsrFtgPHefqLLVzSNowxF7Z0uxxjvIKFh/kfl7QN45Y+EbyzYif/2e7ZT2HMzM7l/rnrCAkO5KUbuuFnzyE3plxYeJhC/eXqjrRrFMIjC2I9+uLBv325he0HT/DPG7vZw6+MKUcWHqZQwYH+vDoiiuOZOTyyYINHPvf8y437mfPzHsZd3IqL29qjiI0pTxYepkhtG4Uw6ZqO/BB/iHd/8Kxbt8ftS+fRDzfQI6Iuj1zezu1yjPE6Fh7mrEb0juCKzo158ZttxO496nY5JXLoRBZjZ62lbo1A3hrVk2oB9mtuTHmzf1XmrESE54Z1pVHtYP40d32Vv/r8dE4e98xZx6ETWbwzKpqGIcFul2SMV7LwMMWqUyOQqTd3J/loBk98urlKP/vjr5/HsToxjReu72rXcxhTgSw8TIn0bBHKQ5e2YdGGfcxdvdftcgo1++fd/HvVHu7u35rB3Zu5XY4xXs3Cw5TY3f3P4+K2YTz52SYWb9rvdjm/s3LHYZ5aFMcf2zfkURsgN6bCWXiYEvP3E94a2YOoiHr8ad56lm9LcbskAPamneKeD9bSon4NXhneHX+7ENCYCmfhYUqlRrUApt3Wi3aNQxg/ey0rdxx2tZ7Nyenc8NZKcvOU90b3onZwoKv1GOMrLDxMqdWpHsisO/oQEVqDMTPXsG7PEVfq+Hrzfm54ayV+AnPH9qVlg5qu1GGML7LwMOcktGY1PrizD2EhQdw2bTVx+9Ir7bNVlde+i2f8nHW0axzCZ/ddQKemdmaVMZWpTOEhIqEiskRE4p2v9YpoN9ppEy8iowvM7ykim0QkQUSmiog4828QkTgRyROR6ALtI0UkQ0RinddbZanflE3D2sF8cGcfagUFcOv7q0lIOV7hn5mZncuD82N56dvtDOnelHlj+9q1HMa4oKx7HhOAZaraBljmTP+OiIQCk4E+QG9gcoGQeRO4C2jjvAY58zcDw4AVhXzmDlXt7rzGl7F+U0bh9Wow584+iAjD3viJ+Wv2VNh1ICnHMxn+zs8sjN3HYwPb8fJN3QkO9K+QzzLGnF1Zw2MwMNN5PxMYUkibgcASVU1T1SPAEmCQiDQBaqvqz5r/12bWr+ur6lZV9c5noXqhVmG1+Gh8P9o3qc3jH29ixLurSDx0stz6Tz+VzStLt3PZP1ew7cBx3hrZg3v/cB7OjqoxxgVlDY9GqvrrCf8HgEaFtGkGFLyqLMmZ18x5f+b84rQUkfUi8h8RuaioRiIyVkRiRCQmNdWzn0nhCSIb1GTeXX35+9AubE5OZ+ArK3jz+x3k5Oadc5+HTmTx/Ne/cMHz3/HK0nh6twzls3svYFDnJuVYuTHmXBT7IGcRWQo0LmTREwUnVFVFpKLvW7EfiFDVwyLSE/hMRDqp6rEzG6rqO8A7ANHR0VX3fhpexM9PGNEnggEdGjJp4Wae//oXPt+wj6eHdKZHRN0S7ynsT8/g3RW7+Pfq3WTl5HFVlybc+4fz6NCkdgVvgTGmpIoND1W9tKhlInJQRJqo6n7nMFRhV40lA/0LTIcD3zvzw8+Yn1xMLVlAlvN+rYjsANoCMcVth6k8jWoH8/aoaL7evJ9JC+O47s2fqFM9kG7N69K9eV2inK/1alYjPSObuOR0NiansykpnU3J6exJO4W/nzCkezPu+UNrWofVcnuTjDFnKDY8irEIGA0853xdWEibb4C/FxgkvxyYqKppInJMRPoCq4BbgVfP9mEiEgakqWquiLQif5Ddsx404UMGdW5Cv9YN+HrzfmL3HmX9nqO89l08vz5XqkGtIA6dyPqtfXi96nQNr8Pw3s25pmtTmofWcKlyY0xxyhoezwELRGQMsBu4EcA5vXa8qt7phMTTwBpnnSmqmua8vweYAVQHFjsvRGQo+UESBnwpIrGqOhC4GJgiItlAnvMZv/ZlqqA61QO5qVcEN/WKAOBkVg4bk9KJ3XuUhJQTtGxQgy7hdenSrA6hNau5XK0xpqSkKt9eu7xER0drTIwd2TLGmNIQkbWqGl3YMrvC3BhjTKlZeBhjjCk1Cw9jjDGlZuFhjDGm1Cw8jDHGlJqFhzHGmFKz8DDGGFNqFh7GGGNKzScuEhSRVPKvgPdEDYBDbhfhAttu32LbXTW1UNWwwhb4RHh4MhGJKeoKT29m2+1bbLs9jx22MsYYU2oWHsYYY0rNwqPqe8ftAlxi2+1bbLs9jI15GGOMKTXb8zDGGFNqFh7GGGNKzcKjChCRQSKyTUQSRGRCIcsjRGS5iKwXkY0icqUbdZa3Emx3CxFZ5mzz9yISXlg/nkZEpolIiohsLmK5iMhU5/uyUUR6VHaNFaEE291eRFaKSJaIPFrZ9VWUEmz3Lc7PeZOI/CQi3Sq7xnNh4eEyEfEHXgeuADoCN4tIxzOaPQksUNUoYDjwRuVWWf5KuN0vAbNUtSswBXi2cqusMDOAQWdZfgXQxnmNBd6shJoqwwzOvt1pwJ/I/7l7kxmcfbt3AZeoahfgaTxkEN3Cw329gQRV3amqp4F5wOAz2ihQ23lfB9hXifVVlJJsd0fgO+f98kKWeyRVXUH+H8qiDCY/NFVVfwbqikiTyqmu4hS33aqaoqprgOzKq6rilWC7f1LVI87kz4BH7GFbeLivGbC3wHSSM6+gvwIjRSQJ+Aq4v3JKq1Al2e4NwDDn/VAgRETqV0JtbivJ98Z4pzHAYreLKAkLD89wMzBDVcOBK4HZIuILP7tHgUtEZD1wCZAM5LpbkjEVQ0T+QH54PO52LSUR4HYBhmSgeYHpcGdeQWNwjpmq6koRCSb/hmoplVJhxSh2u1V1H86eh4jUAq5T1aOVVqF7SvI7YbyIiHQF3gOuUNXDbtdTEr7wv9eqbg3QRkRaikg18gfEF53RZg8wAEBEOgDBQGqlVln+it1uEWlQYA9rIjCtkmt0yyLgVuesq75Auqrud7soUzFEJAL4BBilqtvdrqekbM/DZaqaIyL3Ad8A/sA0VY0TkSlAjKouAh4B3hWRh8gfPL9NPfzWACXc7v7AsyKiwArgXtcKLkciMpf8bWvgjGNNBgIBVPUt8se1rgQSgFPA7e5UWr6K224RaQzEkH9ySJ6IPAh0VNVjLpVcLkrw854E1AfeEBGAHE+4067dnsQYY0yp2WErY4wxpWbhYYwxptQsPIwxxpSahYcxxphSs/AwxhhTahYexhhjSs3CwxhjTKn9P1/HEkwiTqnTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "x6sybszTOo51",
        "outputId": "783755d3-d28d-4bfe-a696-b674290cdcf7"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "E0I0Kg81bxua",
        "outputId": "4764329e-ea4f-4531-c9c1-89494645f6de"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.75, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 0.75, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ShGjACICb4pL",
        "outputId": "f781e453-d24b-425a-ca51-a2a3492ed2a8"
      },
      "source": [
        "temp_k = 0.7865970\n",
        "temp_r = 0.376597\n",
        "temp_sigma = 0.2587698\n",
        "\n",
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, temp_k, S, temp_sigma, temp_r, temp_r]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, temp_k, 1, temp_r, temp_sigma))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEDlx88OgfCF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}