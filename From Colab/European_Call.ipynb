{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "European_Call.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "TY_9g3tbdLiY",
        "u2_89jOknwjH",
        "rXT4Bg0wdL7l"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/From%20Colab/European_Call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "77d8debc-dafe-43fc-d7c8-b0e121e40e41"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   5917      0 --:--:-- --:--:-- --:--:--  5895\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9 MB 41 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 56.4 MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "### Train(Erin Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxT9Eida-c_"
      },
      "source": [
        "# ################################# TEST ########################################\n",
        "# %%writefile cupy_dataset.py\n",
        "\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import random\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "#         paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           X = cupy.array([])\n",
        "#           K_rand = cupy.random.rand(1)[0]\n",
        "#           B_rand = cupy.random.rand(1)[0]\n",
        "#           r_rand = cupy.random.rand(1)[0]\n",
        "#           for i in range(0,self.N_STOCKS):\n",
        "#             X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "#           X = X.reshape(self.N_STOCKS,6)\n",
        "#           X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "#           #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "#           #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "#           # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#           #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "#           # make sure the Barrier is smaller than the Strike price\n",
        "#           # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#           for i in range(self.N_STOCKS):\n",
        "#             paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "#           stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "#           rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "#           #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "#           #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "#           #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#           stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "#           cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "#           num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "#           randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                                         num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "#           b1_r = randoms_gpu[:,0]\n",
        "#           b2_r = randoms_gpu[:,1]\n",
        "#           randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "#           for i in range(interval):\n",
        "#             if i % 2 == 0:\n",
        "#                 ind = int(i/2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "#             else:\n",
        "#                 ind = int(i//2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "#           randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "#           o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "#           Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# # ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# # for i in ds:\n",
        "# #     print(i[0])\n",
        "# ################################# TEST ########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dZnWTTfbf1"
      },
      "source": [
        "### Train (European Call option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeREuPw0fguQ",
        "outputId": "5a418161-e85c-4754-c4bf-fbde150497d4"
      },
      "source": [
        "################################# TEST ########################################\n",
        "#%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def European_call_option(d_s, T, K, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    #tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        h = T[batch_id] / N_STEPS\n",
        "        tmp1 = r[batch_id]*T[batch_id]/N_STEPS \n",
        "        tmp2 = math.exp(-r[batch_id]*T[batch_id]) # discount\n",
        "        tmp3 = math.sqrt(T[batch_id]/N_STEPS)\n",
        "        #running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "          s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "          #s_curr = s_curr * math.exp((r[batch_id] - (1/2)*sigma[batch_id]**2)*h + sigma[batch_id] * tmp3 * d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH])\n",
        "          #running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "          #if i==0 and batch_id == 2:\n",
        "          #    print(s_curr)\n",
        "          #if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "          #    break\n",
        "        #payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        payoff = s_curr - K[batch_id] if s_curr > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        #self.N_STEPS = 365\n",
        "        self.N_STEPS = 10000\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        #self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        #paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 5), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          X = cupy.array([])\n",
        "          #T_rand = cupy.random.rand(1)[0]\n",
        "          #K_rand = cupy.random.rand(1)[0]\n",
        "          #B_rand = cupy.random.rand(1)[0]\n",
        "          #r_rand = cupy.random.rand(1)[0]\n",
        "          for i in range(0, self.N_STOCKS):\n",
        "            #X =  cupy.concatenate((X, cupy.array([K_rand,B_rand]), cupy.random.rand(3), cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "            #X = cupy.concatenate((X, cupy.random.rand(2), cupy.array([200]), cupy.random.rand(3))) #[T, K, S0, sigma, mu, r]\n",
        "            #X = cupy.concatenate((X, cupy.random.rand(6))) #[T, K, S0, sigma, mu, r]\n",
        "            X = cupy.concatenate((X, cupy.array([1]), cupy.random.rand(5))) #[T, K, S0, sigma, mu, r]\n",
        "          \n",
        "          X = X.reshape(self.N_STOCKS, 6)\n",
        "          #X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #[T, K, S0, sigma, mu, r]\n",
        "          X =  X * ((cupy.array([1.0, 200.0, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "            #paras[op, i*5:(i+1)*5] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          if self.N_STOCKS != 1:\n",
        "            stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "            cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "            num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "            randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                          num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "            b1_r = randoms_gpu[:,0]\n",
        "            b2_r = randoms_gpu[:,1]\n",
        "            randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "            interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "            for i in range(interval):\n",
        "              if i % 2 == 0:\n",
        "                  ind = int(i/2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "              else:\n",
        "                  ind = int(i//2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          if self.N_STOCKS == 1:\n",
        "            randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          \n",
        "          European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "ds = NumbaOptionDataSet(2, number_path = 100000, batch = 2, seed = random.randint(0,100), stocks=1)\n",
        "for i in ds:\n",
        "    print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[1.0000e+00, 1.1197e+02, 1.5444e+02, 1.8564e-02, 1.5154e-01, 1.1312e-01],\n",
            "        [1.0000e+00, 1.2814e+02, 1.1048e+02, 1.8003e-01, 3.9067e-02, 7.4481e-02]],\n",
            "       device='cuda:0'), tensor([54.4637,  4.6592], device='cuda:0'))\n",
            "(tensor([[1.0000e+00, 8.6864e+01, 1.4655e+02, 3.9490e-01, 8.5624e-02, 1.7582e-01],\n",
            "        [1.0000e+00, 4.2803e+01, 1.5523e+02, 1.3902e-01, 1.3803e-01, 6.3116e-02]],\n",
            "       device='cuda:0'), tensor([ 74.3593, 114.9912], device='cuda:0'))\n",
            "(tensor([[1.0000e+00, 1.1898e+02, 1.2402e+02, 1.5232e-02, 1.7539e-01, 1.6806e-01],\n",
            "        [1.0000e+00, 5.0197e+01, 7.6913e-01, 3.7553e-02, 7.4393e-02, 1.7420e-01]],\n",
            "       device='cuda:0'), tensor([23.4381,  0.0000], device='cuda:0'))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e439fa-4192-46f1-de13-f2c065f5dd9c"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0, 0.1, 200.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([10, 200.0, 200.0, 0.4, 0.2, 0.2]*1)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85beba09-86aa-4f37-d4dd-4c4d26cf5fde"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.6-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 31.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 61 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 92 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 112 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 122 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 143 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 153 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 184 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 194 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 204 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 215 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 225 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 232 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "cab1afb8-3274-4e5f-b686-1fba43bb44ff"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 1)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=1000)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 4345.66943359375 average time 0.15903827270000193 iter num 20\n",
            "loss 3012.779296875 average time 0.08096135865000136 iter num 40\n",
            "loss 2574.60302734375 average time 0.05494608450000082 iter num 60\n",
            "loss 1909.2137451171875 average time 0.041934115712501455 iter num 80\n",
            "loss 2653.244873046875 average time 0.03413452260000128 iter num 100\n",
            "loss 464.98406982421875 average time 0.019771664149999424 iter num 20\n",
            "loss 193.920166015625 average time 0.0113261652499979 iter num 40\n",
            "loss 192.4547576904297 average time 0.008520631849998968 iter num 60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e3589be74a28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m           \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m           \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49141548-ef88-4b5d-b853-2c776a9b9d27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall_const_T_1_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0eb66e-3bc4-4d3f-c132-ae178ff42a1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0327fedb-1dc7-456d-b62d-bb6b23fcd98d"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall_1_5.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42034db-d7b8-4e3c-c851-5d4b14776e00"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "### Continue to train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "647594c8-de85-486a-dd26-c0dff3a4838c"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 1)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=500)\n",
        "\n",
        "model_save_name = 'EuCall_const_T_1_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 79.92724609375 average time 0.021596667799994407 iter num 20\n",
            "loss 83.46621704101562 average time 0.012242642349998788 iter num 40\n",
            "loss 117.10698699951172 average time 0.009127384633332743 iter num 60\n",
            "loss 86.61559295654297 average time 0.007562617999999332 iter num 80\n",
            "loss 187.11129760742188 average time 0.006602321600000778 iter num 100\n",
            "loss 150.06582641601562 average time 0.019505113350004422 iter num 20\n",
            "loss 126.90660858154297 average time 0.011191702350004107 iter num 40\n",
            "loss 133.57923889160156 average time 0.008366337666666368 iter num 60\n",
            "loss 78.5478515625 average time 0.006958825899999255 iter num 80\n",
            "loss 87.39934539794922 average time 0.006113160170000356 iter num 100\n",
            "loss 102.37899017333984 average time 0.019668237499999464 iter num 20\n",
            "loss 84.23907470703125 average time 0.011291790449998017 iter num 40\n",
            "loss 52.07353591918945 average time 0.008477129399998755 iter num 60\n",
            "loss 103.57672119140625 average time 0.0070620426374993885 iter num 80\n",
            "loss 76.71125793457031 average time 0.006192263660000208 iter num 100\n",
            "loss 53.28943634033203 average time 0.019559577699993724 iter num 20\n",
            "loss 32.1822509765625 average time 0.01113920077499415 iter num 40\n",
            "loss 59.78447341918945 average time 0.008328264949993999 iter num 60\n",
            "loss 138.63197326660156 average time 0.006937371362495526 iter num 80\n",
            "loss 49.47869110107422 average time 0.006094843399996535 iter num 100\n",
            "loss 91.12503051757812 average time 0.01979786925000724 iter num 20\n",
            "loss 125.29549407958984 average time 0.011373998024998855 iter num 40\n",
            "loss 50.58280944824219 average time 0.008534564399999075 iter num 60\n",
            "loss 48.9233283996582 average time 0.007178766212497578 iter num 80\n",
            "loss 164.7464141845703 average time 0.006294851229998244 iter num 100\n",
            "loss 59.26190185546875 average time 0.01961410149999665 iter num 20\n",
            "loss 78.40165710449219 average time 0.011175093675001335 iter num 40\n",
            "loss 84.42887115478516 average time 0.008457422133331723 iter num 60\n",
            "loss 103.29072570800781 average time 0.0070262334250010385 iter num 80\n",
            "loss 150.10653686523438 average time 0.006246673900000133 iter num 100\n",
            "loss 111.16619873046875 average time 0.020143770699993978 iter num 20\n",
            "loss 64.72418212890625 average time 0.011556235074996835 iter num 40\n",
            "loss 68.05352783203125 average time 0.008651800216659922 iter num 60\n",
            "loss 50.891632080078125 average time 0.007182269999989899 iter num 80\n",
            "loss 75.07247924804688 average time 0.006301490109989345 iter num 100\n",
            "loss 37.34264373779297 average time 0.019997208350019946 iter num 20\n",
            "loss 33.49562454223633 average time 0.011447199650012863 iter num 40\n",
            "loss 40.981956481933594 average time 0.008575947600009688 iter num 60\n",
            "loss 53.895965576171875 average time 0.0071229668375139 iter num 80\n",
            "loss 48.53364944458008 average time 0.006242435790013588 iter num 100\n",
            "loss 34.19879150390625 average time 0.019583215049993895 iter num 20\n",
            "loss 28.185184478759766 average time 0.011173242050000453 iter num 40\n",
            "loss 24.549928665161133 average time 0.008366414133335335 iter num 60\n",
            "loss 57.759979248046875 average time 0.006961248737496817 iter num 80\n",
            "loss 21.574275970458984 average time 0.0061360456799980055 iter num 100\n",
            "loss 46.407203674316406 average time 0.02029760164998038 iter num 20\n",
            "loss 26.22721290588379 average time 0.011578247599985048 iter num 40\n",
            "loss 17.659692764282227 average time 0.008701711999990872 iter num 60\n",
            "loss 18.91246223449707 average time 0.007221354524992308 iter num 80\n",
            "loss 14.43082046508789 average time 0.0063501672899940334 iter num 100\n",
            "loss 37.23778533935547 average time 0.019838339299997187 iter num 20\n",
            "loss 46.21848678588867 average time 0.011343978300001823 iter num 40\n",
            "loss 21.533527374267578 average time 0.008478580983328736 iter num 60\n",
            "loss 37.122371673583984 average time 0.007036943874996382 iter num 80\n",
            "loss 60.368438720703125 average time 0.006241023679999671 iter num 100\n",
            "loss 33.12882614135742 average time 0.020188722100010638 iter num 20\n",
            "loss 9.683818817138672 average time 0.011523042400008876 iter num 40\n",
            "loss 26.392498016357422 average time 0.008614355250006155 iter num 60\n",
            "loss 15.84203052520752 average time 0.007139302950008641 iter num 80\n",
            "loss 41.512962341308594 average time 0.0062586550100058955 iter num 100\n",
            "loss 56.87682342529297 average time 0.019493696000017734 iter num 20\n",
            "loss 27.9350528717041 average time 0.01115495510000244 iter num 40\n",
            "loss 23.971553802490234 average time 0.008361870899998772 iter num 60\n",
            "loss 17.37297821044922 average time 0.006952953712502108 iter num 80\n",
            "loss 33.61488342285156 average time 0.006144810120003967 iter num 100\n",
            "loss 27.63921356201172 average time 0.020072904000016933 iter num 20\n",
            "loss 40.153472900390625 average time 0.011461178525004811 iter num 40\n",
            "loss 22.982952117919922 average time 0.008596242700000782 iter num 60\n",
            "loss 29.675140380859375 average time 0.0071506153625023215 iter num 80\n",
            "loss 33.4056282043457 average time 0.006277854840001282 iter num 100\n",
            "loss 31.576290130615234 average time 0.01959368480000876 iter num 20\n",
            "loss 40.873443603515625 average time 0.011157768624994447 iter num 40\n",
            "loss 20.02275848388672 average time 0.00834149359999401 iter num 60\n",
            "loss 15.274381637573242 average time 0.00694770033749279 iter num 80\n",
            "loss 15.354877471923828 average time 0.006174944049995475 iter num 100\n",
            "loss 25.72818374633789 average time 0.020019504299995106 iter num 20\n",
            "loss 29.97509002685547 average time 0.011411214174992778 iter num 40\n",
            "loss 31.02081298828125 average time 0.008547856866670145 iter num 60\n",
            "loss 13.067264556884766 average time 0.007101736937507042 iter num 80\n",
            "loss 16.330429077148438 average time 0.00622986876000482 iter num 100\n",
            "loss 23.520984649658203 average time 0.01968271120001077 iter num 20\n",
            "loss 15.94769287109375 average time 0.01119787180000742 iter num 40\n",
            "loss 20.278461456298828 average time 0.008375550083330078 iter num 60\n",
            "loss 16.635223388671875 average time 0.0069802283249956075 iter num 80\n",
            "loss 27.369735717773438 average time 0.006149592579995442 iter num 100\n",
            "loss 16.47591209411621 average time 0.020023229699978627 iter num 20\n",
            "loss 32.484649658203125 average time 0.011435694574984723 iter num 40\n",
            "loss 21.802566528320312 average time 0.008532637033321558 iter num 60\n",
            "loss 33.7705078125 average time 0.007127313799986723 iter num 80\n",
            "loss 12.073498725891113 average time 0.006267686789985873 iter num 100\n",
            "loss 20.211868286132812 average time 0.0201705076999815 iter num 20\n",
            "loss 41.67218780517578 average time 0.01149484759999666 iter num 40\n",
            "loss 13.350692749023438 average time 0.008621234483333259 iter num 60\n",
            "loss 12.462533950805664 average time 0.0071920751875012455 iter num 80\n",
            "loss 13.44787883758545 average time 0.006344860860000381 iter num 100\n",
            "loss 16.435325622558594 average time 0.019859790100031204 iter num 20\n",
            "loss 46.05748748779297 average time 0.01134779985001444 iter num 40\n",
            "loss 29.096790313720703 average time 0.008516146000012744 iter num 60\n",
            "loss 33.081764221191406 average time 0.007099739862512422 iter num 80\n",
            "loss 31.522701263427734 average time 0.0062664563500106855 iter num 100\n",
            "loss 12.170581817626953 average time 0.02030221235001477 iter num 20\n",
            "loss 21.069499969482422 average time 0.011555565375013543 iter num 40\n",
            "loss 22.98419189453125 average time 0.008634317633349535 iter num 60\n",
            "loss 17.787654876708984 average time 0.0071993711000089885 iter num 80\n",
            "loss 10.69089412689209 average time 0.006316538860005494 iter num 100\n",
            "loss 36.63128662109375 average time 0.01993010685000627 iter num 20\n",
            "loss 20.707489013671875 average time 0.011326322500005403 iter num 40\n",
            "loss 40.32307815551758 average time 0.008476906216664777 iter num 60\n",
            "loss 24.33066749572754 average time 0.007069468175002669 iter num 80\n",
            "loss 39.20952606201172 average time 0.006227565670002377 iter num 100\n",
            "loss 16.437088012695312 average time 0.020006562450021192 iter num 20\n",
            "loss 17.429872512817383 average time 0.011389725825034702 iter num 40\n",
            "loss 6.013175010681152 average time 0.008515156333343536 iter num 60\n",
            "loss 4.462209701538086 average time 0.0070847970500039995 iter num 80\n",
            "loss 15.280608177185059 average time 0.006230111500001385 iter num 100\n",
            "loss 38.87394714355469 average time 0.019684650349995537 iter num 20\n",
            "loss 27.340499877929688 average time 0.011238266625002779 iter num 40\n",
            "loss 20.987600326538086 average time 0.008491365850003756 iter num 60\n",
            "loss 12.73383903503418 average time 0.007073405062507731 iter num 80\n",
            "loss 17.881961822509766 average time 0.0062257695300058915 iter num 100\n",
            "loss 36.3452033996582 average time 0.020150360000002365 iter num 20\n",
            "loss 9.40603256225586 average time 0.011502154975005396 iter num 40\n",
            "loss 16.536592483520508 average time 0.008700882583339838 iter num 60\n",
            "loss 12.410664558410645 average time 0.007280460162513691 iter num 80\n",
            "loss 11.701175689697266 average time 0.006465035360010915 iter num 100\n",
            "loss 15.146916389465332 average time 0.019941775999996025 iter num 20\n",
            "loss 13.532485961914062 average time 0.011380132475005666 iter num 40\n",
            "loss 11.925264358520508 average time 0.008619130733325164 iter num 60\n",
            "loss 19.13300323486328 average time 0.007178908462498157 iter num 80\n",
            "loss 13.11421012878418 average time 0.006332937729998775 iter num 100\n",
            "loss 14.724466323852539 average time 0.0199260810999931 iter num 20\n",
            "loss 24.18144989013672 average time 0.011371578049994469 iter num 40\n",
            "loss 14.321967124938965 average time 0.008564750566654311 iter num 60\n",
            "loss 8.122692108154297 average time 0.007204254124982867 iter num 80\n",
            "loss 19.93791389465332 average time 0.006336038979989098 iter num 100\n",
            "loss 19.940086364746094 average time 0.01997713349994683 iter num 20\n",
            "loss 23.64501953125 average time 0.011361383799976466 iter num 40\n",
            "loss 12.546339988708496 average time 0.008489137949978461 iter num 60\n",
            "loss 20.58353614807129 average time 0.007054782212489385 iter num 80\n",
            "loss 15.39796257019043 average time 0.006192939529992145 iter num 100\n",
            "loss 26.658458709716797 average time 0.019483443849981085 iter num 20\n",
            "loss 25.33993911743164 average time 0.011132811599992465 iter num 40\n",
            "loss 11.493071556091309 average time 0.0083684144000055 iter num 60\n",
            "loss 10.174676895141602 average time 0.007013952512508581 iter num 80\n",
            "loss 8.708869934082031 average time 0.006185493550008232 iter num 100\n",
            "loss 22.757957458496094 average time 0.02007938189996139 iter num 20\n",
            "loss 13.628929138183594 average time 0.011414429599977894 iter num 40\n",
            "loss 9.848800659179688 average time 0.008523862516646356 iter num 60\n",
            "loss 37.87065887451172 average time 0.0070754607749847764 iter num 80\n",
            "loss 10.853069305419922 average time 0.006229360229992835 iter num 100\n",
            "loss 22.07061195373535 average time 0.0195152676000248 iter num 20\n",
            "loss 18.697830200195312 average time 0.011189742400017622 iter num 40\n",
            "loss 8.840496063232422 average time 0.00839381593334186 iter num 60\n",
            "loss 7.412625312805176 average time 0.0070267598875148 iter num 80\n",
            "loss 10.19247055053711 average time 0.00618704713001307 iter num 100\n",
            "loss 9.966119766235352 average time 0.01976389514999255 iter num 20\n",
            "loss 12.642383575439453 average time 0.011262923599980467 iter num 40\n",
            "loss 14.455536842346191 average time 0.008425287333318465 iter num 60\n",
            "loss 16.955591201782227 average time 0.007003296774988144 iter num 80\n",
            "loss 19.011091232299805 average time 0.006145287829997415 iter num 100\n",
            "loss 5.933891296386719 average time 0.019707391750000625 iter num 20\n",
            "loss 7.357585906982422 average time 0.011317065499997624 iter num 40\n",
            "loss 12.970592498779297 average time 0.00848906271666389 iter num 60\n",
            "loss 8.655731201171875 average time 0.007078537612500213 iter num 80\n",
            "loss 3.851435899734497 average time 0.006248840850003034 iter num 100\n",
            "loss 12.708118438720703 average time 0.020393429499995362 iter num 20\n",
            "loss 15.834152221679688 average time 0.011584659049987067 iter num 40\n",
            "loss 8.201913833618164 average time 0.008752392283323236 iter num 60\n",
            "loss 16.739896774291992 average time 0.007340328299994781 iter num 80\n",
            "loss 18.48904800415039 average time 0.006462314339994463 iter num 100\n",
            "loss 14.604182243347168 average time 0.01971308475000342 iter num 20\n",
            "loss 27.12822723388672 average time 0.011292738125001733 iter num 40\n",
            "loss 26.019073486328125 average time 0.008480683066659367 iter num 60\n",
            "loss 9.999308586120605 average time 0.00710619716248857 iter num 80\n",
            "loss 20.18290138244629 average time 0.006273131349987579 iter num 100\n",
            "loss 8.075616836547852 average time 0.01966129010000941 iter num 20\n",
            "loss 16.328493118286133 average time 0.011196010900010833 iter num 40\n",
            "loss 14.476415634155273 average time 0.008381567483327974 iter num 60\n",
            "loss 11.065858840942383 average time 0.006989390712496402 iter num 80\n",
            "loss 3.437265396118164 average time 0.006142119349999575 iter num 100\n",
            "loss 27.37159538269043 average time 0.01978856000000633 iter num 20\n",
            "loss 13.077194213867188 average time 0.011329808400000729 iter num 40\n",
            "loss 20.178836822509766 average time 0.008543175383329072 iter num 60\n",
            "loss 10.251480102539062 average time 0.007118040475000953 iter num 80\n",
            "loss 12.900099754333496 average time 0.0062551188599968555 iter num 100\n",
            "loss 20.75198745727539 average time 0.01966518699999824 iter num 20\n",
            "loss 21.45360565185547 average time 0.01131803977498862 iter num 40\n",
            "loss 29.295801162719727 average time 0.00849465413332761 iter num 60\n",
            "loss 16.898468017578125 average time 0.007106867312501208 iter num 80\n",
            "loss 6.105133056640625 average time 0.006249303629999758 iter num 100\n",
            "loss 6.461296081542969 average time 0.019946983749980517 iter num 20\n",
            "loss 7.931818008422852 average time 0.011341441649994977 iter num 40\n",
            "loss 20.616588592529297 average time 0.008482793833331925 iter num 60\n",
            "loss 14.03634262084961 average time 0.007051087874992845 iter num 80\n",
            "loss 14.167621612548828 average time 0.0061911495099980125 iter num 100\n",
            "loss 4.3839006423950195 average time 0.020003497300001526 iter num 20\n",
            "loss 12.778234481811523 average time 0.01141312862500854 iter num 40\n",
            "loss 30.833541870117188 average time 0.008561049933348386 iter num 60\n",
            "loss 16.456623077392578 average time 0.007123252900015586 iter num 80\n",
            "loss 10.169332504272461 average time 0.006244438500009437 iter num 100\n",
            "loss 19.932209014892578 average time 0.019466752749974604 iter num 20\n",
            "loss 14.358074188232422 average time 0.011098387624980433 iter num 40\n",
            "loss 11.662251472473145 average time 0.00833235179998913 iter num 60\n",
            "loss 12.070186614990234 average time 0.006937803824993693 iter num 80\n",
            "loss 9.670038223266602 average time 0.006112461769992024 iter num 100\n",
            "loss 50.68445587158203 average time 0.02013451749999149 iter num 20\n",
            "loss 8.281312942504883 average time 0.011486275149997028 iter num 40\n",
            "loss 6.2478814125061035 average time 0.008589784233322462 iter num 60\n",
            "loss 9.669743537902832 average time 0.007130161000000612 iter num 80\n",
            "loss 20.290401458740234 average time 0.006256380030008586 iter num 100\n",
            "loss 9.236686706542969 average time 0.019545017700022527 iter num 20\n",
            "loss 10.532227516174316 average time 0.01113676850001184 iter num 40\n",
            "loss 9.193413734436035 average time 0.008328923066676453 iter num 60\n",
            "loss 15.4102783203125 average time 0.006940760362513743 iter num 80\n",
            "loss 19.285118103027344 average time 0.006118580790014221 iter num 100\n",
            "loss 5.824363708496094 average time 0.01999312124997914 iter num 20\n",
            "loss 15.047195434570312 average time 0.011396639224960836 iter num 40\n",
            "loss 11.602995872497559 average time 0.008534651466641207 iter num 60\n",
            "loss 18.573244094848633 average time 0.007114630374985609 iter num 80\n",
            "loss 11.127607345581055 average time 0.006241601129984247 iter num 100\n",
            "loss 19.454364776611328 average time 0.01994351625003219 iter num 20\n",
            "loss 18.006542205810547 average time 0.011432772325014185 iter num 40\n",
            "loss 7.544801712036133 average time 0.008557774583338566 iter num 60\n",
            "loss 10.234396934509277 average time 0.007092029875002482 iter num 80\n",
            "loss 10.36179256439209 average time 0.006236793350005883 iter num 100\n",
            "loss 7.78241491317749 average time 0.02015398194997715 iter num 20\n",
            "loss 9.526077270507812 average time 0.011502093049978157 iter num 40\n",
            "loss 14.708230018615723 average time 0.008603416416652484 iter num 60\n",
            "loss 21.722572326660156 average time 0.0071338948749883006 iter num 80\n",
            "loss 25.48142433166504 average time 0.006289649959987855 iter num 100\n",
            "loss 12.592926025390625 average time 0.02006299404999936 iter num 20\n",
            "loss 7.219844818115234 average time 0.011463783825007567 iter num 40\n",
            "loss 5.943899154663086 average time 0.00858098250000315 iter num 60\n",
            "loss 15.12161636352539 average time 0.007149599037498433 iter num 80\n",
            "loss 10.999327659606934 average time 0.006280103859994597 iter num 100\n",
            "loss 19.842756271362305 average time 0.019734822949999398 iter num 20\n",
            "loss 17.52306365966797 average time 0.011291377725018491 iter num 40\n",
            "loss 7.37222957611084 average time 0.008441159450004914 iter num 60\n",
            "loss 31.280864715576172 average time 0.007012936800003899 iter num 80\n",
            "loss 13.076532363891602 average time 0.006176915210007791 iter num 100\n",
            "loss 6.526675224304199 average time 0.019859999150003204 iter num 20\n",
            "loss 20.76239776611328 average time 0.011433561725010578 iter num 40\n",
            "loss 10.40733528137207 average time 0.008536472133350041 iter num 60\n",
            "loss 20.048782348632812 average time 0.007082670487511678 iter num 80\n",
            "loss 11.783927917480469 average time 0.006237163779999264 iter num 100\n",
            "loss 10.048104286193848 average time 0.01933636004999926 iter num 20\n",
            "loss 10.703150749206543 average time 0.011020789825005295 iter num 40\n",
            "loss 11.25987434387207 average time 0.008247460900004928 iter num 60\n",
            "loss 14.297261238098145 average time 0.006897924212501039 iter num 80\n",
            "loss 12.72573184967041 average time 0.006105117849999715 iter num 100\n",
            "loss 25.52216911315918 average time 0.02006700494997631 iter num 20\n",
            "loss 24.968505859375 average time 0.011470254099970134 iter num 40\n",
            "loss 21.54848861694336 average time 0.008567396749970157 iter num 60\n",
            "loss 18.99925994873047 average time 0.0071127390374670085 iter num 80\n",
            "loss 3.890876054763794 average time 0.006235916449973047 iter num 100\n",
            "loss 31.992036819458008 average time 0.01952375825001127 iter num 20\n",
            "loss 26.865535736083984 average time 0.011108218700013595 iter num 40\n",
            "loss 15.240208625793457 average time 0.008286958416662552 iter num 60\n",
            "loss 8.161819458007812 average time 0.006908167112496244 iter num 80\n",
            "loss 18.173789978027344 average time 0.00610286745998792 iter num 100\n",
            "loss 10.797966003417969 average time 0.019959816649975436 iter num 20\n",
            "loss 7.436498165130615 average time 0.011431666300006782 iter num 40\n",
            "loss 10.841999053955078 average time 0.008558564199999334 iter num 60\n",
            "loss 5.831426620483398 average time 0.00709590673750995 iter num 80\n",
            "loss 10.94047737121582 average time 0.006218640600009167 iter num 100\n",
            "loss 16.235410690307617 average time 0.019698390499979724 iter num 20\n",
            "loss 8.129150390625 average time 0.011324570075021256 iter num 40\n",
            "loss 10.297279357910156 average time 0.008571977500006748 iter num 60\n",
            "loss 7.4261698722839355 average time 0.00714290050003683 iter num 80\n",
            "loss 8.72671127319336 average time 0.006287190840030234 iter num 100\n",
            "loss 15.293724060058594 average time 0.020087251099948845 iter num 20\n",
            "loss 24.07964324951172 average time 0.011437643575004586 iter num 40\n",
            "loss 13.199287414550781 average time 0.00856756726667148 iter num 60\n",
            "loss 10.736474990844727 average time 0.00713385124998922 iter num 80\n",
            "loss 8.639871597290039 average time 0.0063181410499873894 iter num 100\n",
            "loss 11.56296443939209 average time 0.019901695199928328 iter num 20\n",
            "loss 7.272143363952637 average time 0.011340817450013673 iter num 40\n",
            "loss 23.10260581970215 average time 0.008493178416673193 iter num 60\n",
            "loss 13.665748596191406 average time 0.00705067201250813 iter num 80\n",
            "loss 9.133045196533203 average time 0.006234142640000755 iter num 100\n",
            "loss 11.537969589233398 average time 0.019704216149943932 iter num 20\n",
            "loss 6.3839335441589355 average time 0.011286882249964946 iter num 40\n",
            "loss 17.435955047607422 average time 0.00843858286663514 iter num 60\n",
            "loss 7.489830017089844 average time 0.0070366300874866285 iter num 80\n",
            "loss 7.328144550323486 average time 0.006189678649980124 iter num 100\n",
            "loss 7.444738388061523 average time 0.020069130449951445 iter num 20\n",
            "loss 24.641620635986328 average time 0.011403778249996322 iter num 40\n",
            "loss 5.7590532302856445 average time 0.008530618700009047 iter num 60\n",
            "loss 17.061845779418945 average time 0.007072517337502404 iter num 80\n",
            "loss 6.342922210693359 average time 0.0062088205500094775 iter num 100\n",
            "loss 32.263267517089844 average time 0.0195962272000088 iter num 20\n",
            "loss 14.86828899383545 average time 0.011146508950037059 iter num 40\n",
            "loss 18.474300384521484 average time 0.008345642533352777 iter num 60\n",
            "loss 12.987680435180664 average time 0.006953030700032059 iter num 80\n",
            "loss 14.942418098449707 average time 0.006176425390035547 iter num 100\n",
            "loss 28.161148071289062 average time 0.019970166150073965 iter num 20\n",
            "loss 16.15058135986328 average time 0.011343453525023505 iter num 40\n",
            "loss 12.275893211364746 average time 0.008465352600001096 iter num 60\n",
            "loss 14.807987213134766 average time 0.0070227406125127345 iter num 80\n",
            "loss 9.84541130065918 average time 0.00619649727000251 iter num 100\n",
            "loss 25.659713745117188 average time 0.019371754550070362 iter num 20\n",
            "loss 12.092133522033691 average time 0.011024515400026757 iter num 40\n",
            "loss 24.042551040649414 average time 0.008291422533367647 iter num 60\n",
            "loss 10.578451156616211 average time 0.006923072937513553 iter num 80\n",
            "loss 16.807594299316406 average time 0.006102637370004231 iter num 100\n",
            "loss 7.868132591247559 average time 0.019962796099957814 iter num 20\n",
            "loss 30.81006622314453 average time 0.011333493424967856 iter num 40\n",
            "loss 13.05974292755127 average time 0.008466767366659649 iter num 60\n",
            "loss 12.197443008422852 average time 0.007035034599988421 iter num 80\n",
            "loss 5.262033462524414 average time 0.006194552439992549 iter num 100\n",
            "loss 4.483463764190674 average time 0.019731792049969953 iter num 20\n",
            "loss 13.186809539794922 average time 0.011211599975001717 iter num 40\n",
            "loss 21.218830108642578 average time 0.008411863333359785 iter num 60\n",
            "loss 21.278705596923828 average time 0.007052542275016549 iter num 80\n",
            "loss 5.164889335632324 average time 0.006201385620006477 iter num 100\n",
            "loss 15.872726440429688 average time 0.01996540919999461 iter num 20\n",
            "loss 18.246809005737305 average time 0.011365840575058428 iter num 40\n",
            "loss 3.3428072929382324 average time 0.008482667600045108 iter num 60\n",
            "loss 15.213531494140625 average time 0.007034372500027075 iter num 80\n",
            "loss 4.542418479919434 average time 0.006168883460013604 iter num 100\n",
            "loss 9.9049654006958 average time 0.019768467649964806 iter num 20\n",
            "loss 32.97037887573242 average time 0.011383815024987597 iter num 40\n",
            "loss 12.02923583984375 average time 0.008518684050000048 iter num 60\n",
            "loss 7.168993949890137 average time 0.007091760962492799 iter num 80\n",
            "loss 14.068707466125488 average time 0.0062278140499938675 iter num 100\n",
            "loss 7.310967922210693 average time 0.019970749700019042 iter num 20\n",
            "loss 16.616792678833008 average time 0.011382406750010432 iter num 40\n",
            "loss 11.793790817260742 average time 0.00850877276667082 iter num 60\n",
            "loss 22.119098663330078 average time 0.007125475337511488 iter num 80\n",
            "loss 17.486587524414062 average time 0.0062651889999870035 iter num 100\n",
            "loss 28.694900512695312 average time 0.019947392300036882 iter num 20\n",
            "loss 8.38852596282959 average time 0.011343469450002886 iter num 40\n",
            "loss 16.09075927734375 average time 0.008454239966681598 iter num 60\n",
            "loss 26.57147979736328 average time 0.007034038900002315 iter num 80\n",
            "loss 9.656673431396484 average time 0.006201235780004026 iter num 100\n",
            "loss 23.789602279663086 average time 0.01963370119999581 iter num 20\n",
            "loss 12.809513092041016 average time 0.011264597199965466 iter num 40\n",
            "loss 31.92906379699707 average time 0.008470519899985145 iter num 60\n",
            "loss 22.317869186401367 average time 0.007062271099988493 iter num 80\n",
            "loss 7.76521110534668 average time 0.006215596589991037 iter num 100\n",
            "loss 11.393035888671875 average time 0.01996627605010417 iter num 20\n",
            "loss 13.843931198120117 average time 0.011367838900048355 iter num 40\n",
            "loss 9.441645622253418 average time 0.008478686933373562 iter num 60\n",
            "loss 5.9488043785095215 average time 0.007032475462528965 iter num 80\n",
            "loss 17.208045959472656 average time 0.006164831530022639 iter num 100\n",
            "loss 10.412311553955078 average time 0.019471108000038838 iter num 20\n",
            "loss 8.256507873535156 average time 0.011264630050004599 iter num 40\n",
            "loss 16.39160919189453 average time 0.008441989916673266 iter num 60\n",
            "loss 6.300443172454834 average time 0.007040204912522085 iter num 80\n",
            "loss 9.919346809387207 average time 0.00618433424001978 iter num 100\n",
            "loss 15.23564338684082 average time 0.019242335050034854 iter num 20\n",
            "loss 12.43547248840332 average time 0.01096596502503644 iter num 40\n",
            "loss 13.433215141296387 average time 0.008215568183360725 iter num 60\n",
            "loss 5.2782745361328125 average time 0.006880454075019316 iter num 80\n",
            "loss 18.985515594482422 average time 0.006043834830011292 iter num 100\n",
            "loss 25.343486785888672 average time 0.01968708744996093 iter num 20\n",
            "loss 23.46209144592285 average time 0.01124837947497781 iter num 40\n",
            "loss 10.815332412719727 average time 0.00842861000000236 iter num 60\n",
            "loss 17.89069366455078 average time 0.007030890112491761 iter num 80\n",
            "loss 12.790799140930176 average time 0.006220714499986571 iter num 100\n",
            "loss 31.331279754638672 average time 0.019940722099954657 iter num 20\n",
            "loss 28.781526565551758 average time 0.011363463299994691 iter num 40\n",
            "loss 27.604219436645508 average time 0.008497487300011623 iter num 60\n",
            "loss 22.76938247680664 average time 0.007065982037499907 iter num 80\n",
            "loss 6.4781317710876465 average time 0.006213607310000952 iter num 100\n",
            "loss 12.940947532653809 average time 0.019608004849965256 iter num 20\n",
            "loss 10.92501449584961 average time 0.011235757425015435 iter num 40\n",
            "loss 8.805660247802734 average time 0.008486097100004978 iter num 60\n",
            "loss 7.582579612731934 average time 0.0070742091749991685 iter num 80\n",
            "loss 7.003705978393555 average time 0.006225929659999565 iter num 100\n",
            "loss 10.618094444274902 average time 0.0195979253500127 iter num 20\n",
            "loss 33.45744705200195 average time 0.0112250905750102 iter num 40\n",
            "loss 5.420681953430176 average time 0.00841656374999881 iter num 60\n",
            "loss 32.389610290527344 average time 0.0070533093874757926 iter num 80\n",
            "loss 11.497893333435059 average time 0.006197679689980759 iter num 100\n",
            "loss 8.727631568908691 average time 0.01949211574999481 iter num 20\n",
            "loss 7.645968914031982 average time 0.011211399300009361 iter num 40\n",
            "loss 14.669957160949707 average time 0.008372855516662033 iter num 60\n",
            "loss 7.584855079650879 average time 0.006960785612517384 iter num 80\n",
            "loss 19.379487991333008 average time 0.006113732310013802 iter num 100\n",
            "loss 7.350861549377441 average time 0.019635990349979694 iter num 20\n",
            "loss 31.572288513183594 average time 0.011269267599959675 iter num 40\n",
            "loss 15.166971206665039 average time 0.008488821116642006 iter num 60\n",
            "loss 4.744114875793457 average time 0.007070828112495065 iter num 80\n",
            "loss 27.233139038085938 average time 0.006207999960006419 iter num 100\n",
            "loss 13.899215698242188 average time 0.019431997499964382 iter num 20\n",
            "loss 26.047807693481445 average time 0.011064031474995772 iter num 40\n",
            "loss 7.361648082733154 average time 0.008287769499997922 iter num 60\n",
            "loss 5.685549736022949 average time 0.006922750962485224 iter num 80\n",
            "loss 5.694675922393799 average time 0.006091289759988286 iter num 100\n",
            "loss 12.437873840332031 average time 0.019974820100037503 iter num 20\n",
            "loss 8.031549453735352 average time 0.011399784425054803 iter num 40\n",
            "loss 48.41781234741211 average time 0.008527317666676026 iter num 60\n",
            "loss 19.350059509277344 average time 0.007098047637515492 iter num 80\n",
            "loss 12.708575248718262 average time 0.006221843299999819 iter num 100\n",
            "loss 26.397415161132812 average time 0.01950950895002279 iter num 20\n",
            "loss 12.576086044311523 average time 0.011132437399987793 iter num 40\n",
            "loss 8.173073768615723 average time 0.008361790349999865 iter num 60\n",
            "loss 16.073457717895508 average time 0.006985240912507606 iter num 80\n",
            "loss 14.188451766967773 average time 0.006128857830003653 iter num 100\n",
            "loss 14.029186248779297 average time 0.01977219105006043 iter num 20\n",
            "loss 9.843547821044922 average time 0.011365478799996253 iter num 40\n",
            "loss 4.888608455657959 average time 0.00851709648331204 iter num 60\n",
            "loss 14.227638244628906 average time 0.007105370062481598 iter num 80\n",
            "loss 5.726680755615234 average time 0.006241307999976016 iter num 100\n",
            "loss 35.81626892089844 average time 0.019808351649999167 iter num 20\n",
            "loss 10.7078857421875 average time 0.01130679582497578 iter num 40\n",
            "loss 10.693160057067871 average time 0.008452378216657053 iter num 60\n",
            "loss 16.51354217529297 average time 0.007018209637487871 iter num 80\n",
            "loss 22.776145935058594 average time 0.006156622849994164 iter num 100\n",
            "loss 14.766926765441895 average time 0.019917918450028084 iter num 20\n",
            "loss 5.215123653411865 average time 0.011426046900032816 iter num 40\n",
            "loss 22.478477478027344 average time 0.008559873133352387 iter num 60\n",
            "loss 12.396499633789062 average time 0.007111761462499544 iter num 80\n",
            "loss 11.242034912109375 average time 0.006232497519999924 iter num 100\n",
            "loss 13.666170120239258 average time 0.01960464305000187 iter num 20\n",
            "loss 25.719207763671875 average time 0.01118391990003147 iter num 40\n",
            "loss 21.124670028686523 average time 0.00836422523333719 iter num 60\n",
            "loss 11.821008682250977 average time 0.007009314700002278 iter num 80\n",
            "loss 7.263874053955078 average time 0.006165879760005737 iter num 100\n",
            "loss 21.877037048339844 average time 0.020159360099955847 iter num 20\n",
            "loss 14.91952896118164 average time 0.011484835825012852 iter num 40\n",
            "loss 11.876710891723633 average time 0.00864328513334082 iter num 60\n",
            "loss 22.607460021972656 average time 0.007174111787503534 iter num 80\n",
            "loss 10.469310760498047 average time 0.006286150320015622 iter num 100\n",
            "loss 5.729031085968018 average time 0.01982256489993688 iter num 20\n",
            "loss 9.472589492797852 average time 0.011348475149941351 iter num 40\n",
            "loss 7.6212897300720215 average time 0.008554787449922212 iter num 60\n",
            "loss 12.014843940734863 average time 0.007114097587441392 iter num 80\n",
            "loss 8.773443222045898 average time 0.006250046239961194 iter num 100\n",
            "loss 14.541707992553711 average time 0.019434616150010697 iter num 20\n",
            "loss 11.694157600402832 average time 0.011077009025018469 iter num 40\n",
            "loss 10.591950416564941 average time 0.008321680366680084 iter num 60\n",
            "loss 9.927705764770508 average time 0.00699167390000639 iter num 80\n",
            "loss 11.158641815185547 average time 0.006171310120003 iter num 100\n",
            "loss 25.422544479370117 average time 0.020008643299979666 iter num 20\n",
            "loss 11.748464584350586 average time 0.011406950025002516 iter num 40\n",
            "loss 23.76428985595703 average time 0.008548308049989828 iter num 60\n",
            "loss 8.002915382385254 average time 0.00709062448748341 iter num 80\n",
            "loss 5.66546630859375 average time 0.006216453709971575 iter num 100\n",
            "loss 6.234302520751953 average time 0.019634598499897037 iter num 20\n",
            "loss 10.223371505737305 average time 0.011194442624946533 iter num 40\n",
            "loss 10.518556594848633 average time 0.008378410283307857 iter num 60\n",
            "loss 16.36767578125 average time 0.0069723508124809545 iter num 80\n",
            "loss 8.638988494873047 average time 0.0061410272799776065 iter num 100\n",
            "loss 6.808435440063477 average time 0.020006680949927615 iter num 20\n",
            "loss 25.789241790771484 average time 0.011418407724954704 iter num 40\n",
            "loss 11.313604354858398 average time 0.00862235229997926 iter num 60\n",
            "loss 7.47499942779541 average time 0.007195481949969463 iter num 80\n",
            "loss 8.534220695495605 average time 0.006302001679973728 iter num 100\n",
            "loss 11.803197860717773 average time 0.020001975199920707 iter num 20\n",
            "loss 1.8335771560668945 average time 0.011382189249957264 iter num 40\n",
            "loss 14.121394157409668 average time 0.00848811803330894 iter num 60\n",
            "loss 4.899081707000732 average time 0.007084194487492823 iter num 80\n",
            "loss 8.917352676391602 average time 0.0062309990899893815 iter num 100\n",
            "loss 41.45546340942383 average time 0.02000686539997787 iter num 20\n",
            "loss 22.570907592773438 average time 0.011414475299966398 iter num 40\n",
            "loss 10.523146629333496 average time 0.008519080616626222 iter num 60\n",
            "loss 18.605152130126953 average time 0.007071916874963335 iter num 80\n",
            "loss 22.95285415649414 average time 0.0062199368499614134 iter num 100\n",
            "loss 36.88450622558594 average time 0.019620339449966197 iter num 20\n",
            "loss 16.6414794921875 average time 0.011171224199983953 iter num 40\n",
            "loss 6.648189067840576 average time 0.008353975816612546 iter num 60\n",
            "loss 4.723149299621582 average time 0.006963801187481522 iter num 80\n",
            "loss 11.310264587402344 average time 0.00612643722998655 iter num 100\n",
            "loss 27.732830047607422 average time 0.020867358400028024 iter num 20\n",
            "loss 24.239879608154297 average time 0.011877331050004613 iter num 40\n",
            "loss 13.080551147460938 average time 0.008829226850010249 iter num 60\n",
            "loss 13.085996627807617 average time 0.0073129698000173 iter num 80\n",
            "loss 4.587098121643066 average time 0.006441587070003152 iter num 100\n",
            "loss 17.211143493652344 average time 0.020026373150017207 iter num 20\n",
            "loss 6.985198497772217 average time 0.011487092924994613 iter num 40\n",
            "loss 14.8419189453125 average time 0.008606393416645612 iter num 60\n",
            "loss 8.440593719482422 average time 0.007231888199993364 iter num 80\n",
            "loss 12.124960899353027 average time 0.006363828719991034 iter num 100\n",
            "loss 6.38295841217041 average time 0.01964899129995956 iter num 20\n",
            "loss 11.798095703125 average time 0.011250877824988948 iter num 40\n",
            "loss 8.113831520080566 average time 0.008420732683346917 iter num 60\n",
            "loss 11.677858352661133 average time 0.007025982237513518 iter num 80\n",
            "loss 31.13828468322754 average time 0.006230138500013709 iter num 100\n",
            "loss 4.61745023727417 average time 0.019981833949964313 iter num 20\n",
            "loss 22.495891571044922 average time 0.011383555749978314 iter num 40\n",
            "loss 20.630817413330078 average time 0.008485686266673535 iter num 60\n",
            "loss 7.860986709594727 average time 0.007101186850013619 iter num 80\n",
            "loss 16.566150665283203 average time 0.006239299650023895 iter num 100\n",
            "loss 4.12944221496582 average time 0.019680126050070613 iter num 20\n",
            "loss 34.365516662597656 average time 0.011206146675056062 iter num 40\n",
            "loss 3.32466983795166 average time 0.00839354103338034 iter num 60\n",
            "loss 24.44097328186035 average time 0.007005431175019794 iter num 80\n",
            "loss 15.516885757446289 average time 0.006210360020004373 iter num 100\n",
            "loss 5.488916397094727 average time 0.020084826400056953 iter num 20\n",
            "loss 13.457592010498047 average time 0.011562796500027162 iter num 40\n",
            "loss 8.89550495147705 average time 0.008659708849988117 iter num 60\n",
            "loss 12.416399955749512 average time 0.0072558887874834 iter num 80\n",
            "loss 5.7529425621032715 average time 0.006345862079983817 iter num 100\n",
            "loss 29.67073631286621 average time 0.01950840385002266 iter num 20\n",
            "loss 16.457979202270508 average time 0.011130033125016325 iter num 40\n",
            "loss 10.943668365478516 average time 0.008352054416673128 iter num 60\n",
            "loss 7.395450592041016 average time 0.006975193687503633 iter num 80\n",
            "loss 12.241802215576172 average time 0.00619463044000895 iter num 100\n",
            "loss 20.545135498046875 average time 0.019984180999949787 iter num 20\n",
            "loss 7.1424384117126465 average time 0.011387088699973447 iter num 40\n",
            "loss 3.236626625061035 average time 0.00849790531663075 iter num 60\n",
            "loss 8.972368240356445 average time 0.007071346124990896 iter num 80\n",
            "loss 2.118401527404785 average time 0.006246443709992491 iter num 100\n",
            "loss 7.806351661682129 average time 0.01967586480004684 iter num 20\n",
            "loss 13.1949462890625 average time 0.011178160050030783 iter num 40\n",
            "loss 6.760525703430176 average time 0.008428946049995526 iter num 60\n",
            "loss 7.337869644165039 average time 0.007026038262500834 iter num 80\n",
            "loss 4.4015374183654785 average time 0.0062037586899987215 iter num 100\n",
            "loss 15.680598258972168 average time 0.01998926279995885 iter num 20\n",
            "loss 16.288915634155273 average time 0.011471613074979814 iter num 40\n",
            "loss 6.948102951049805 average time 0.008564615466631646 iter num 60\n",
            "loss 9.530683517456055 average time 0.007105255974971669 iter num 80\n",
            "loss 13.879525184631348 average time 0.006225237489975371 iter num 100\n",
            "loss 15.661421775817871 average time 0.01947990695000499 iter num 20\n",
            "loss 12.6251220703125 average time 0.01112726667500965 iter num 40\n",
            "loss 20.333866119384766 average time 0.008351175533319596 iter num 60\n",
            "loss 21.672107696533203 average time 0.006961034149992429 iter num 80\n",
            "loss 8.900760650634766 average time 0.006149748380003075 iter num 100\n",
            "loss 14.563907623291016 average time 0.02004051105000144 iter num 20\n",
            "loss 28.472970962524414 average time 0.011396610450026401 iter num 40\n",
            "loss 8.53969955444336 average time 0.008536094533337747 iter num 60\n",
            "loss 30.29163360595703 average time 0.007133572750007033 iter num 80\n",
            "loss 7.001621246337891 average time 0.006269527539998308 iter num 100\n",
            "loss 17.70863914489746 average time 0.020071828399977676 iter num 20\n",
            "loss 18.736024856567383 average time 0.011536259799993332 iter num 40\n",
            "loss 4.780717849731445 average time 0.008599389216654648 iter num 60\n",
            "loss 8.95127010345459 average time 0.0071268328124858725 iter num 80\n",
            "loss 4.575901985168457 average time 0.006251021059997583 iter num 100\n",
            "loss 21.3407039642334 average time 0.019537158749994888 iter num 20\n",
            "loss 18.493385314941406 average time 0.011150333824980407 iter num 40\n",
            "loss 11.329255104064941 average time 0.008356148066673086 iter num 60\n",
            "loss 7.964245319366455 average time 0.00697905273749484 iter num 80\n",
            "loss 5.452368259429932 average time 0.006203142160006792 iter num 100\n",
            "loss 11.619162559509277 average time 0.019818126850032057 iter num 20\n",
            "loss 10.866889953613281 average time 0.01128434490002519 iter num 40\n",
            "loss 9.291580200195312 average time 0.00842969635003404 iter num 60\n",
            "loss 9.528640747070312 average time 0.00706472230002646 iter num 80\n",
            "loss 23.95849609375 average time 0.0062161428400213485 iter num 100\n",
            "loss 17.078014373779297 average time 0.019692554500056757 iter num 20\n",
            "loss 8.294864654541016 average time 0.011303746200042042 iter num 40\n",
            "loss 10.29287338256836 average time 0.008571094766656037 iter num 60\n",
            "loss 16.40445327758789 average time 0.0071593170999847185 iter num 80\n",
            "loss 9.863823890686035 average time 0.006288244669995038 iter num 100\n",
            "loss 20.193801879882812 average time 0.02012956284995653 iter num 20\n",
            "loss 20.886486053466797 average time 0.01142132349992835 iter num 40\n",
            "loss 12.201824188232422 average time 0.008534411699937057 iter num 60\n",
            "loss 11.048871040344238 average time 0.007090026124950554 iter num 80\n",
            "loss 16.092079162597656 average time 0.006209290329970827 iter num 100\n",
            "loss 8.440948486328125 average time 0.019429902000001675 iter num 20\n",
            "loss 12.008821487426758 average time 0.01123854027496236 iter num 40\n",
            "loss 4.660135746002197 average time 0.008447014733269498 iter num 60\n",
            "loss 30.0080509185791 average time 0.00703749181245712 iter num 80\n",
            "loss 13.741774559020996 average time 0.006205838359965128 iter num 100\n",
            "loss 44.93103790283203 average time 0.019787150199999815 iter num 20\n",
            "loss 22.48617935180664 average time 0.011320966625009986 iter num 40\n",
            "loss 4.219786167144775 average time 0.008464710300002783 iter num 60\n",
            "loss 16.444913864135742 average time 0.007042606124974782 iter num 80\n",
            "loss 25.105335235595703 average time 0.006182425849979154 iter num 100\n",
            "loss 30.89720344543457 average time 0.019561262200022612 iter num 20\n",
            "loss 18.241161346435547 average time 0.011236907075044655 iter num 40\n",
            "loss 7.09090518951416 average time 0.008485179133337321 iter num 60\n",
            "loss 17.981157302856445 average time 0.007067940349986657 iter num 80\n",
            "loss 9.102226257324219 average time 0.006216192249976302 iter num 100\n",
            "loss 23.150541305541992 average time 0.019815139900128998 iter num 20\n",
            "loss 13.895955085754395 average time 0.011356618800073192 iter num 40\n",
            "loss 11.298190116882324 average time 0.008508352966676588 iter num 60\n",
            "loss 17.312101364135742 average time 0.007088859412499459 iter num 80\n",
            "loss 11.979764938354492 average time 0.006233327749996533 iter num 100\n",
            "loss 9.985708236694336 average time 0.019759235799983798 iter num 20\n",
            "loss 21.864501953125 average time 0.011271398100052465 iter num 40\n",
            "loss 23.337438583374023 average time 0.00841570010002215 iter num 60\n",
            "loss 12.223820686340332 average time 0.006987458662513291 iter num 80\n",
            "loss 4.311710357666016 average time 0.006129826950027564 iter num 100\n",
            "loss 31.49545669555664 average time 0.019942004549920966 iter num 20\n",
            "loss 11.520509719848633 average time 0.01137928734997331 iter num 40\n",
            "loss 11.072681427001953 average time 0.008572319233265565 iter num 60\n",
            "loss 7.707480430603027 average time 0.007174831687450478 iter num 80\n",
            "loss 8.693596839904785 average time 0.0062831870499485375 iter num 100\n",
            "loss 17.671571731567383 average time 0.019427918799874534 iter num 20\n",
            "loss 21.532800674438477 average time 0.011071506874850456 iter num 40\n",
            "loss 23.774883270263672 average time 0.00831954913323898 iter num 60\n",
            "loss 21.00830841064453 average time 0.006989171424925189 iter num 80\n",
            "loss 17.443565368652344 average time 0.006141865929912456 iter num 100\n",
            "loss 10.342304229736328 average time 0.019701235350066782 iter num 20\n",
            "loss 10.459948539733887 average time 0.01126964702496025 iter num 40\n",
            "loss 14.730725288391113 average time 0.00850753666665393 iter num 60\n",
            "loss 24.154069900512695 average time 0.007097398062455795 iter num 80\n",
            "loss 8.24311351776123 average time 0.006278154189967609 iter num 100\n",
            "loss 18.59208869934082 average time 0.019631690249934764 iter num 20\n",
            "loss 20.257587432861328 average time 0.011172939924949787 iter num 40\n",
            "loss 9.517248153686523 average time 0.008375985849958549 iter num 60\n",
            "loss 6.86392879486084 average time 0.006952313387455433 iter num 80\n",
            "loss 10.716153144836426 average time 0.006105237509955259 iter num 100\n",
            "loss 12.157693862915039 average time 0.019796224800074924 iter num 20\n",
            "loss 16.05923843383789 average time 0.011348999175038444 iter num 40\n",
            "loss 4.628515720367432 average time 0.008505085833318542 iter num 60\n",
            "loss 5.860287666320801 average time 0.0071079000250165334 iter num 80\n",
            "loss 7.532476902008057 average time 0.006237814390024141 iter num 100\n",
            "loss 8.335192680358887 average time 0.019678873549901256 iter num 20\n",
            "loss 16.534404754638672 average time 0.011276963124942085 iter num 40\n",
            "loss 9.19810962677002 average time 0.008437480816564858 iter num 60\n",
            "loss 8.060113906860352 average time 0.0069963136998921985 iter num 80\n",
            "loss 11.14296817779541 average time 0.006156001719928099 iter num 100\n",
            "loss 7.5692644119262695 average time 0.01979877054991448 iter num 20\n",
            "loss 14.847274780273438 average time 0.011385673599943403 iter num 40\n",
            "loss 8.154354095458984 average time 0.008579024433295975 iter num 60\n",
            "loss 18.462974548339844 average time 0.007126972587457203 iter num 80\n",
            "loss 12.605417251586914 average time 0.006287402729958558 iter num 100\n",
            "loss 7.698907375335693 average time 0.019924333950029904 iter num 20\n",
            "loss 7.460598468780518 average time 0.011320120799996402 iter num 40\n",
            "loss 17.083755493164062 average time 0.008558077299994693 iter num 60\n",
            "loss 10.527673721313477 average time 0.0072239465749930785 iter num 80\n",
            "loss 6.686761379241943 average time 0.006334479409997584 iter num 100\n",
            "loss 10.357714653015137 average time 0.020208954749978147 iter num 20\n",
            "loss 26.995834350585938 average time 0.011571215899948584 iter num 40\n",
            "loss 11.90561580657959 average time 0.008646257983324782 iter num 60\n",
            "loss 15.129600524902344 average time 0.007175590362487582 iter num 80\n",
            "loss 8.604734420776367 average time 0.006293098939977426 iter num 100\n",
            "loss 4.745608329772949 average time 0.020089729049959714 iter num 20\n",
            "loss 14.78105354309082 average time 0.01154639117496572 iter num 40\n",
            "loss 5.971214771270752 average time 0.008642249683377182 iter num 60\n",
            "loss 12.601062774658203 average time 0.007161237824982436 iter num 80\n",
            "loss 18.862201690673828 average time 0.006269632499979707 iter num 100\n",
            "loss 8.356365203857422 average time 0.019972978050054734 iter num 20\n",
            "loss 9.552315711975098 average time 0.011335766925094503 iter num 40\n",
            "loss 10.358861923217773 average time 0.00846291441678962 iter num 60\n",
            "loss 5.472747802734375 average time 0.007039706387638489 iter num 80\n",
            "loss 12.798382759094238 average time 0.006186099620126697 iter num 100\n",
            "loss 9.659276962280273 average time 0.02002511914988645 iter num 20\n",
            "loss 15.542603492736816 average time 0.011435475274993223 iter num 40\n",
            "loss 6.975881576538086 average time 0.008557955616682496 iter num 60\n",
            "loss 13.858088493347168 average time 0.007132436225026595 iter num 80\n",
            "loss 14.164712905883789 average time 0.006252525190011511 iter num 100\n",
            "loss 8.682544708251953 average time 0.019888413850048893 iter num 20\n",
            "loss 32.19990921020508 average time 0.011304367300022022 iter num 40\n",
            "loss 24.656906127929688 average time 0.008444392683350089 iter num 60\n",
            "loss 6.168540954589844 average time 0.007014927212549082 iter num 80\n",
            "loss 7.515599727630615 average time 0.006174993740023638 iter num 100\n",
            "loss 12.03213882446289 average time 0.0199054202500065 iter num 20\n",
            "loss 8.263267517089844 average time 0.011371934624980895 iter num 40\n",
            "loss 13.472881317138672 average time 0.008519937649937977 iter num 60\n",
            "loss 37.412086486816406 average time 0.007070077812477394 iter num 80\n",
            "loss 7.1597900390625 average time 0.006204819809981927 iter num 100\n",
            "loss 8.003379821777344 average time 0.01943863140004396 iter num 20\n",
            "loss 13.68289566040039 average time 0.011072725600001831 iter num 40\n",
            "loss 6.937459945678711 average time 0.008343693266685175 iter num 60\n",
            "loss 13.034805297851562 average time 0.006951196650015845 iter num 80\n",
            "loss 15.26615047454834 average time 0.006124225050007226 iter num 100\n",
            "loss 7.137981414794922 average time 0.0202005488501527 iter num 20\n",
            "loss 21.196353912353516 average time 0.011586046125103167 iter num 40\n",
            "loss 18.08231544494629 average time 0.00865088570003536 iter num 60\n",
            "loss 8.150655746459961 average time 0.007172297387546678 iter num 80\n",
            "loss 7.418612480163574 average time 0.006284191830027339 iter num 100\n",
            "loss 9.203313827514648 average time 0.01988066570006595 iter num 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9725cefcad6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EuCall_const_T_1_2.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m           \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m           \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd11a0b-95c8-4f55-b7e9-cdd991000d45"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[18.5560]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0cc499-aa98-4d33-9b82-15b454e67a86"
      },
      "source": [
        "inputs = torch.tensor([[10, 110.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.9872e+00, -2.8651e-01,  8.5026e-01,  8.3555e+01, -1.6804e+00,\n",
              "          3.1912e+02]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwgeVDsA_Mr"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "7fd0eb38-304a-4255-d621-fb8896cae386"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    # inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs = torch.tensor([[10, 110.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcfdad86910>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGk5Hw64fMdh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5a8c224f-e626-4e7c-8ed7-25f69ed3e4f3"
      },
      "source": [
        "## Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.5\n",
        "    #inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    #inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs1 = torch.tensor([10, 110, S, 0.35, 0.1, 0.05]).cuda()\n",
        "    inputs2 = torch.tensor([10, 110, S + epsilon, 0.35, 0.1, 0.05]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcfdcb002d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLStvS2qCSjm"
      },
      "source": [
        "compute_delta(110)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1I8COnUxnz"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySrey9KzB0AF"
      },
      "source": [
        "compute_delta(110).item()  # It's not 0.5!! SOS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMyME_1WCGZz"
      },
      "source": [
        "compute_delta(102).item() # Close to 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB8LPwfBMHQ"
      },
      "source": [
        "# Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoaOyCDxQy0"
      },
      "source": [
        "##Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsXgOwWZ_ru"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lca2xrBh9a"
      },
      "source": [
        "# Vega"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muozc-hzhSGA"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KATxBCAdlFt"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a timev\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05]*3]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 0.0, 110.0, sigma, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEiAredqQGxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}