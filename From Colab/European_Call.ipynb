{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "European_Call.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "TY_9g3tbdLiY",
        "u2_89jOknwjH",
        "rXT4Bg0wdL7l"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/From%20Colab/European_Call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "6a9fc40a-7d02-4d35-8a57-3e0daf53b683"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   5873      0 --:--:-- --:--:-- --:--:--  5873\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9 MB 41 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 58.1 MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "### Train(Erin Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxT9Eida-c_"
      },
      "source": [
        "# ################################# TEST ########################################\n",
        "# %%writefile cupy_dataset.py\n",
        "\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import random\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "#         paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           X = cupy.array([])\n",
        "#           K_rand = cupy.random.rand(1)[0]\n",
        "#           B_rand = cupy.random.rand(1)[0]\n",
        "#           r_rand = cupy.random.rand(1)[0]\n",
        "#           for i in range(0,self.N_STOCKS):\n",
        "#             X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "#           X = X.reshape(self.N_STOCKS,6)\n",
        "#           X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "#           #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "#           #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "#           # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#           #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "#           # make sure the Barrier is smaller than the Strike price\n",
        "#           # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#           for i in range(self.N_STOCKS):\n",
        "#             paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "#           stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "#           rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "#           #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "#           #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "#           #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#           stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "#           cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "#           num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "#           randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                                         num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "#           b1_r = randoms_gpu[:,0]\n",
        "#           b2_r = randoms_gpu[:,1]\n",
        "#           randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "#           for i in range(interval):\n",
        "#             if i % 2 == 0:\n",
        "#                 ind = int(i/2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "#             else:\n",
        "#                 ind = int(i//2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "#           randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "#           o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "#           Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# # ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# # for i in ds:\n",
        "# #     print(i[0])\n",
        "# ################################# TEST ########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dZnWTTfbf1"
      },
      "source": [
        "### Train (European Call option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeREuPw0fguQ",
        "outputId": "7d4554e9-9e73-4431-d5e3-c88eb2827074"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def European_call_option(d_s, T, K, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    #tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        h = T[batch_id] / N_STEPS\n",
        "        tmp1 = mu[batch_id]*T[batch_id]/N_STEPS \n",
        "        tmp2 = math.exp(-r[batch_id]*T[batch_id]) # discount\n",
        "        tmp3 = math.sqrt(T[batch_id]/N_STEPS)\n",
        "        #running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "          #s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "          s_curr = s_curr * math.exp((r[batch_id] - (1/2)*sigma[batch_id]**2)*h + sigma[batch_id] * tmp3 * d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH])\n",
        "          #running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "          #if i==0 and batch_id == 2:\n",
        "          #    print(s_curr)\n",
        "          #if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "          #    break\n",
        "        #payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        payoff = s_curr - K[batch_id] if s_curr > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        #self.N_STEPS = 365\n",
        "        self.N_STEPS = 10000\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        #self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        #paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 5), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          X = cupy.array([])\n",
        "          #T_rand = cupy.random.rand(1)[0]\n",
        "          #K_rand = cupy.random.rand(1)[0]\n",
        "          #B_rand = cupy.random.rand(1)[0]\n",
        "          #r_rand = cupy.random.rand(1)[0]\n",
        "          for i in range(0, self.N_STOCKS):\n",
        "            #X =  cupy.concatenate((X, cupy.array([K_rand,B_rand]), cupy.random.rand(3), cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "            #X = cupy.concatenate((X, cupy.random.rand(2), cupy.array([200]), cupy.random.rand(3))) #[T, K, S0, sigma, mu, r]\n",
        "            X = cupy.concatenate((X, cupy.random.rand(6))) #[T, K, S0, sigma, mu, r]\n",
        "          \n",
        "          X = X.reshape(self.N_STOCKS, 6)\n",
        "          #X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #[T, K, S0, sigma, mu, r]\n",
        "          X = X * ((cupy.array([10.0, 200.0, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "            #paras[op, i*5:(i+1)*5] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          if self.N_STOCKS != 1:\n",
        "            stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "            cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "            num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "            randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                          num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "            b1_r = randoms_gpu[:,0]\n",
        "            b2_r = randoms_gpu[:,1]\n",
        "            randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "            interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "            for i in range(interval):\n",
        "              if i % 2 == 0:\n",
        "                  ind = int(i/2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "              else:\n",
        "                  ind = int(i//2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          if self.N_STOCKS == 1:\n",
        "            randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          \n",
        "          European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "#ds = NumbaOptionDataSet(2, number_path = 100000, batch = 2, seed = random.randint(0,100), stocks=1)\n",
        "#for i in ds:\n",
        "#    print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ac3dc8-9d7a-43b9-9dd7-63c06f73c7e8"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0, 0.1, 200.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([5, 200.0, 200.0, 0.4, 0.2, 0.2]*1)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d193a59c-b13f-494c-fd26-8981f568d73d"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.6-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 92 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 112 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 122 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 143 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 153 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 184 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 194 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 204 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 215 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 225 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 232 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "0d3d85c9-ef3c-4980-ee69-c9efb13896d3"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 1)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=1000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 2563.40087890625 average time 0.04123284975000132 iter num 20\n",
            "loss 2519.291748046875 average time 0.022019344975002752 iter num 40\n",
            "loss 2737.483154296875 average time 0.015677246733335436 iter num 60\n",
            "loss 2217.03173828125 average time 0.012496549562501614 iter num 80\n",
            "loss 1050.57861328125 average time 0.01052187313000104 iter num 100\n",
            "loss 382.244873046875 average time 0.019852679699998533 iter num 20\n",
            "loss 275.40509033203125 average time 0.011248531749999558 iter num 40\n",
            "loss 208.04864501953125 average time 0.00845738604999866 iter num 60\n",
            "loss 172.51266479492188 average time 0.007033391162498859 iter num 80\n",
            "loss 139.39344787597656 average time 0.0061781544999999485 iter num 100\n",
            "loss 136.45681762695312 average time 0.019586651299997017 iter num 20\n",
            "loss 92.70323181152344 average time 0.011223272774995508 iter num 40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e3589be74a28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mrandoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m           European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n\u001b[0m\u001b[1;32m    128\u001b[0m                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf095724-9491-4af4-d5c4-b69b8fcd3689"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall_1_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596ad73a-3fed-4361-9ea1-078903eb9483"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edecd5bb-8f4f-49d3-a78c-48cc086527b0"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall_1_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08d3eb8-4ada-442f-9545-44b604cdb158"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "### Continue to train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T",
        "outputId": "19aed370-cfc9-4542-c217-32cfe09d5703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 1)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=500)\n",
        "\n",
        "model_save_name = 'EuCall_1_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 5.48129940032959 average time 0.021832092750264563 iter num 20\n",
            "loss 2.501391887664795 average time 0.0122969582249425 iter num 40\n",
            "loss 4.964184761047363 average time 0.009116517066710609 iter num 60\n",
            "loss 2.845534324645996 average time 0.007576494750173879 iter num 80\n",
            "loss 3.367083787918091 average time 0.006605306710152945 iter num 100\n",
            "loss 20.06426239013672 average time 0.019364986299660814 iter num 20\n",
            "loss 2.8096232414245605 average time 0.011064768824871863 iter num 40\n",
            "loss 2.3230819702148438 average time 0.008372192316649792 iter num 60\n",
            "loss 4.927371025085449 average time 0.007069782037478945 iter num 80\n",
            "loss 5.626785755157471 average time 0.00625421359007305 iter num 100\n",
            "loss 9.765752792358398 average time 0.01950306195030862 iter num 20\n",
            "loss 2.588192939758301 average time 0.011162854924987187 iter num 40\n",
            "loss 4.329535007476807 average time 0.008391532583300432 iter num 60\n",
            "loss 3.451312780380249 average time 0.007006650499852185 iter num 80\n",
            "loss 5.017573356628418 average time 0.006227525539943599 iter num 100\n",
            "loss 3.263014078140259 average time 0.019649885900616938 iter num 20\n",
            "loss 2.760225772857666 average time 0.011218685600124446 iter num 40\n",
            "loss 2.592623710632324 average time 0.008380523100155794 iter num 60\n",
            "loss 5.862109184265137 average time 0.006957832612579295 iter num 80\n",
            "loss 3.480402708053589 average time 0.006126526910120447 iter num 100\n",
            "loss 1.9255495071411133 average time 0.019280799750049482 iter num 20\n",
            "loss 15.633362770080566 average time 0.011024999974961248 iter num 40\n",
            "loss 2.7148232460021973 average time 0.008296023633253451 iter num 60\n",
            "loss 3.721221446990967 average time 0.006935725987523256 iter num 80\n",
            "loss 6.129107475280762 average time 0.006121108389888832 iter num 100\n",
            "loss 6.165797233581543 average time 0.019672924550286554 iter num 20\n",
            "loss 8.617997169494629 average time 0.011194948824959283 iter num 40\n",
            "loss 4.561302661895752 average time 0.008366860849734318 iter num 60\n",
            "loss 2.1596689224243164 average time 0.007001851174845797 iter num 80\n",
            "loss 3.85782527923584 average time 0.006196190609880432 iter num 100\n",
            "loss 5.8322014808654785 average time 0.019390985399695637 iter num 20\n",
            "loss 4.275524616241455 average time 0.011216444299861905 iter num 40\n",
            "loss 10.443353652954102 average time 0.008466163316613044 iter num 60\n",
            "loss 6.4116339683532715 average time 0.0070581791623681054 iter num 80\n",
            "loss 16.661582946777344 average time 0.006223383069991542 iter num 100\n",
            "loss 6.338642120361328 average time 0.01973303950016998 iter num 20\n",
            "loss 5.882795333862305 average time 0.011345435225121037 iter num 40\n",
            "loss 4.046800136566162 average time 0.008495795833490168 iter num 60\n",
            "loss 2.801085948944092 average time 0.007057307862669404 iter num 80\n",
            "loss 4.9028000831604 average time 0.006200644460113836 iter num 100\n",
            "loss 1.1615458726882935 average time 0.01931725574995653 iter num 20\n",
            "loss 3.9357194900512695 average time 0.011127175500041631 iter num 40\n",
            "loss 3.0310845375061035 average time 0.008360834099948988 iter num 60\n",
            "loss 2.5585875511169434 average time 0.006971033787249325 iter num 80\n",
            "loss 5.883520126342773 average time 0.00614410999991378 iter num 100\n",
            "loss 10.329530715942383 average time 0.019490541750019474 iter num 20\n",
            "loss 4.065627574920654 average time 0.011122529100066458 iter num 40\n",
            "loss 3.8847358226776123 average time 0.00832581304997196 iter num 60\n",
            "loss 4.582735061645508 average time 0.006951651124927594 iter num 80\n",
            "loss 4.740167617797852 average time 0.0061231972598397985 iter num 100\n",
            "loss 13.42213249206543 average time 0.019732239550285156 iter num 20\n",
            "loss 1.2353811264038086 average time 0.011292632574986783 iter num 40\n",
            "loss 5.530265808105469 average time 0.008468819433376969 iter num 60\n",
            "loss 1.0856692790985107 average time 0.00705792308758646 iter num 80\n",
            "loss 5.086226463317871 average time 0.006211160540151468 iter num 100\n",
            "loss 11.668779373168945 average time 0.019927452349838858 iter num 20\n",
            "loss 4.572358131408691 average time 0.01143933367502541 iter num 40\n",
            "loss 5.772402763366699 average time 0.00859154333350792 iter num 60\n",
            "loss 7.44877815246582 average time 0.007171973199956483 iter num 80\n",
            "loss 4.444809913635254 average time 0.006321616220047872 iter num 100\n",
            "loss 12.507993698120117 average time 0.019255282250196615 iter num 20\n",
            "loss 3.6062612533569336 average time 0.010991355550322624 iter num 40\n",
            "loss 6.8602294921875 average time 0.008277680100218277 iter num 60\n",
            "loss 8.100495338439941 average time 0.006909968787749676 iter num 80\n",
            "loss 2.0963616371154785 average time 0.006065925030197832 iter num 100\n",
            "loss 3.4222254753112793 average time 0.01933381400012877 iter num 20\n",
            "loss 8.258546829223633 average time 0.011082298625024124 iter num 40\n",
            "loss 7.069362640380859 average time 0.008334444466709102 iter num 60\n",
            "loss 6.628526210784912 average time 0.006964739812565313 iter num 80\n",
            "loss 4.426444053649902 average time 0.006135453700007929 iter num 100\n",
            "loss 3.2168784141540527 average time 0.019367313549810205 iter num 20\n",
            "loss 4.417318344116211 average time 0.011065059400061727 iter num 40\n",
            "loss 1.6383490562438965 average time 0.008295584533334477 iter num 60\n",
            "loss 1.5995426177978516 average time 0.006983059699996375 iter num 80\n",
            "loss 14.57416820526123 average time 0.00613840010984859 iter num 100\n",
            "loss 1.761634111404419 average time 0.019628596400616517 iter num 20\n",
            "loss 3.740699052810669 average time 0.011229641325280681 iter num 40\n",
            "loss 3.572873592376709 average time 0.008423526316801145 iter num 60\n",
            "loss 1.3371578454971313 average time 0.0070227802124463775 iter num 80\n",
            "loss 3.2736053466796875 average time 0.006182465539968689 iter num 100\n",
            "loss 3.3216259479522705 average time 0.01929311490021064 iter num 20\n",
            "loss 7.337790489196777 average time 0.011007246149620186 iter num 40\n",
            "loss 3.548452854156494 average time 0.008253370299765568 iter num 60\n",
            "loss 9.514322280883789 average time 0.006885420462322145 iter num 80\n",
            "loss 4.329076766967773 average time 0.00606281438984297 iter num 100\n",
            "loss 6.43296480178833 average time 0.01949677514967334 iter num 20\n",
            "loss 11.83488655090332 average time 0.011165887899824155 iter num 40\n",
            "loss 5.876769542694092 average time 0.008377139149949168 iter num 60\n",
            "loss 2.8085732460021973 average time 0.006986703925076654 iter num 80\n",
            "loss 4.497005462646484 average time 0.006142556010090629 iter num 100\n",
            "loss 7.277566909790039 average time 0.019413489650105475 iter num 20\n",
            "loss 5.405300617218018 average time 0.01114115097479953 iter num 40\n",
            "loss 2.9883296489715576 average time 0.008344600416724765 iter num 60\n",
            "loss 7.615167617797852 average time 0.006936107187630114 iter num 80\n",
            "loss 4.995780944824219 average time 0.006095912609926018 iter num 100\n",
            "loss 3.980268955230713 average time 0.01919831365084974 iter num 20\n",
            "loss 3.9776017665863037 average time 0.011054799100293167 iter num 40\n",
            "loss 4.371798038482666 average time 0.008325133800220404 iter num 60\n",
            "loss 3.0235886573791504 average time 0.006958450987667675 iter num 80\n",
            "loss 6.8660783767700195 average time 0.006128611020176322 iter num 100\n",
            "loss 3.9952340126037598 average time 0.019360090149893948 iter num 20\n",
            "loss 1.7218294143676758 average time 0.011027860224930918 iter num 40\n",
            "loss 8.240687370300293 average time 0.008248616216466569 iter num 60\n",
            "loss 4.6868391036987305 average time 0.006862645437331593 iter num 80\n",
            "loss 1.614870309829712 average time 0.006029357639890804 iter num 100\n",
            "loss 4.888522148132324 average time 0.01943606785025622 iter num 20\n",
            "loss 7.448208808898926 average time 0.01112927717522325 iter num 40\n",
            "loss 1.3567631244659424 average time 0.008435568066852283 iter num 60\n",
            "loss 2.1362524032592773 average time 0.0070254931501494864 iter num 80\n",
            "loss 7.077211380004883 average time 0.00616977448011312 iter num 100\n",
            "loss 8.957880020141602 average time 0.01940605620002316 iter num 20\n",
            "loss 5.264501571655273 average time 0.011135174500122958 iter num 40\n",
            "loss 5.266468524932861 average time 0.008357682899926052 iter num 60\n",
            "loss 3.9457778930664062 average time 0.006965804937453868 iter num 80\n",
            "loss 3.769554853439331 average time 0.006124006189929787 iter num 100\n",
            "loss 3.1967527866363525 average time 0.019201480100491608 iter num 20\n",
            "loss 10.927881240844727 average time 0.010946167400106788 iter num 40\n",
            "loss 1.9618135690689087 average time 0.008218597400324748 iter num 60\n",
            "loss 6.630207061767578 average time 0.006874922950237306 iter num 80\n",
            "loss 4.800094127655029 average time 0.006051946520201455 iter num 100\n",
            "loss 5.428585052490234 average time 0.01944386735012813 iter num 20\n",
            "loss 5.544347286224365 average time 0.011230590550167108 iter num 40\n",
            "loss 1.5925829410552979 average time 0.008521358450040376 iter num 60\n",
            "loss 4.813755035400391 average time 0.0071916635248271635 iter num 80\n",
            "loss 10.287473678588867 average time 0.006292336039878137 iter num 100\n",
            "loss 3.994513511657715 average time 0.019339488949663065 iter num 20\n",
            "loss 3.6429452896118164 average time 0.011110290499891562 iter num 40\n",
            "loss 10.732369422912598 average time 0.008300630383503933 iter num 60\n",
            "loss 3.8902764320373535 average time 0.006903477887726694 iter num 80\n",
            "loss 5.226274490356445 average time 0.006061906160175568 iter num 100\n",
            "loss 3.9407103061676025 average time 0.019398866449955678 iter num 20\n",
            "loss 3.7286014556884766 average time 0.01115711949978504 iter num 40\n",
            "loss 3.093252420425415 average time 0.00838580233327472 iter num 60\n",
            "loss 2.816161632537842 average time 0.006985230499958561 iter num 80\n",
            "loss 8.025768280029297 average time 0.006135066009992442 iter num 100\n",
            "loss 8.256494522094727 average time 0.019082767050531402 iter num 20\n",
            "loss 3.9885828495025635 average time 0.01090731095027877 iter num 40\n",
            "loss 0.9918271899223328 average time 0.008188599933479661 iter num 60\n",
            "loss 4.349331378936768 average time 0.006809596425227937 iter num 80\n",
            "loss 2.3334336280822754 average time 0.005992802790096903 iter num 100\n",
            "loss 8.022425651550293 average time 0.01970701519967406 iter num 20\n",
            "loss 11.370570182800293 average time 0.011269341200113558 iter num 40\n",
            "loss 2.095313549041748 average time 0.008547609316883609 iter num 60\n",
            "loss 5.252429485321045 average time 0.007190728237719668 iter num 80\n",
            "loss 8.305548667907715 average time 0.0063279381400934654 iter num 100\n",
            "loss 13.227620124816895 average time 0.019207189900043886 iter num 20\n",
            "loss 3.9836859703063965 average time 0.01101197959987985 iter num 40\n",
            "loss 2.1905722618103027 average time 0.008233735533273527 iter num 60\n",
            "loss 5.079856872558594 average time 0.0068442743250670905 iter num 80\n",
            "loss 5.399570465087891 average time 0.006051829400130373 iter num 100\n",
            "loss 4.158858299255371 average time 0.019869319900499248 iter num 20\n",
            "loss 4.51113748550415 average time 0.01132112430050256 iter num 40\n",
            "loss 3.0725035667419434 average time 0.00850067418365749 iter num 60\n",
            "loss 20.058467864990234 average time 0.007093265375351621 iter num 80\n",
            "loss 3.3984196186065674 average time 0.006217110920297273 iter num 100\n",
            "loss 6.840775489807129 average time 0.019610298250154302 iter num 20\n",
            "loss 9.026578903198242 average time 0.011247687799732375 iter num 40\n",
            "loss 3.7675209045410156 average time 0.008484808599678217 iter num 60\n",
            "loss 1.7837516069412231 average time 0.007048189912302405 iter num 80\n",
            "loss 7.923001289367676 average time 0.0061765223698239426 iter num 100\n",
            "loss 2.4257407188415527 average time 0.019337582899970583 iter num 20\n",
            "loss 3.2166495323181152 average time 0.011013533575078328 iter num 40\n",
            "loss 1.7442593574523926 average time 0.008243489200018909 iter num 60\n",
            "loss 11.472896575927734 average time 0.006879689075094575 iter num 80\n",
            "loss 0.5649200677871704 average time 0.006065048170094087 iter num 100\n",
            "loss 17.777904510498047 average time 0.019834517799426975 iter num 20\n",
            "loss 4.150505542755127 average time 0.011308890949658234 iter num 40\n",
            "loss 4.363297939300537 average time 0.00846838521635315 iter num 60\n",
            "loss 1.7712316513061523 average time 0.007042104962283702 iter num 80\n",
            "loss 2.0946497917175293 average time 0.0061699921499894115 iter num 100\n",
            "loss 4.94666051864624 average time 0.019601194499955454 iter num 20\n",
            "loss 1.276440978050232 average time 0.011184536400014622 iter num 40\n",
            "loss 5.717428684234619 average time 0.008360109633334408 iter num 60\n",
            "loss 5.711236000061035 average time 0.00693683253753079 iter num 80\n",
            "loss 1.1063828468322754 average time 0.006095083210020676 iter num 100\n",
            "loss 9.527114868164062 average time 0.020237896649996402 iter num 20\n",
            "loss 7.330479621887207 average time 0.011820835775142768 iter num 40\n",
            "loss 8.69876480102539 average time 0.0088604591002877 iter num 60\n",
            "loss 1.3763951063156128 average time 0.00731651758778753 iter num 80\n",
            "loss 6.35206413269043 average time 0.006389146820220048 iter num 100\n",
            "loss 5.846660137176514 average time 0.019381950350543777 iter num 20\n",
            "loss 6.255411148071289 average time 0.01108295407566402 iter num 40\n",
            "loss 6.633659839630127 average time 0.00832684971701383 iter num 60\n",
            "loss 7.114940643310547 average time 0.006949673275266832 iter num 80\n",
            "loss 15.816423416137695 average time 0.006109404130111216 iter num 100\n",
            "loss 5.925142288208008 average time 0.01971139964971371 iter num 20\n",
            "loss 4.319648742675781 average time 0.011266506199990544 iter num 40\n",
            "loss 0.4447769224643707 average time 0.008451692683350606 iter num 60\n",
            "loss 3.594695568084717 average time 0.007050790850007616 iter num 80\n",
            "loss 2.52809476852417 average time 0.006186933419958223 iter num 100\n",
            "loss 5.096458435058594 average time 0.019423257950074914 iter num 20\n",
            "loss 1.8929674625396729 average time 0.01106264842537712 iter num 40\n",
            "loss 3.542083740234375 average time 0.008290263300053387 iter num 60\n",
            "loss 1.6407387256622314 average time 0.006899198362407333 iter num 80\n",
            "loss 2.9983084201812744 average time 0.006074042069885763 iter num 100\n",
            "loss 5.473233699798584 average time 0.019650007599739183 iter num 20\n",
            "loss 6.325048923492432 average time 0.011256402524850273 iter num 40\n",
            "loss 1.9584741592407227 average time 0.008431227249699684 iter num 60\n",
            "loss 2.1658995151519775 average time 0.007022888737310495 iter num 80\n",
            "loss 3.387693405151367 average time 0.006165928639820777 iter num 100\n",
            "loss 3.781200408935547 average time 0.019250597350219324 iter num 20\n",
            "loss 4.520614147186279 average time 0.010986107575172354 iter num 40\n",
            "loss 7.358236789703369 average time 0.008227809533430748 iter num 60\n",
            "loss 11.641861915588379 average time 0.00683871818746411 iter num 80\n",
            "loss 1.0747921466827393 average time 0.006027784350044385 iter num 100\n",
            "loss 5.442913055419922 average time 0.01973843065024994 iter num 20\n",
            "loss 4.97580099105835 average time 0.011296216624941735 iter num 40\n",
            "loss 4.172882080078125 average time 0.00849609246658171 iter num 60\n",
            "loss 0.8899755477905273 average time 0.007046954000043115 iter num 80\n",
            "loss 5.86113166809082 average time 0.006226294150110334 iter num 100\n",
            "loss 9.54602336883545 average time 0.019687642299140863 iter num 20\n",
            "loss 4.70247745513916 average time 0.01126297677437833 iter num 40\n",
            "loss 5.7110395431518555 average time 0.008436976216398762 iter num 60\n",
            "loss 1.6411542892456055 average time 0.006997261849846836 iter num 80\n",
            "loss 1.5200271606445312 average time 0.006144433509944065 iter num 100\n",
            "loss 11.018335342407227 average time 0.01912424504989758 iter num 20\n",
            "loss 2.460843563079834 average time 0.010948907900183259 iter num 40\n",
            "loss 6.998856544494629 average time 0.008205424166771991 iter num 60\n",
            "loss 1.9017720222473145 average time 0.0068287592000160656 iter num 80\n",
            "loss 3.333616256713867 average time 0.006036340589998872 iter num 100\n",
            "loss 5.625984191894531 average time 0.019672167249700577 iter num 20\n",
            "loss 18.6539306640625 average time 0.011209415799839917 iter num 40\n",
            "loss 8.160431861877441 average time 0.008398084816568977 iter num 60\n",
            "loss 1.8540470600128174 average time 0.0069825948623929435 iter num 80\n",
            "loss 2.058187484741211 average time 0.006116069629970297 iter num 100\n",
            "loss 7.710569858551025 average time 0.019368596599997547 iter num 20\n",
            "loss 1.523457407951355 average time 0.011029783550111461 iter num 40\n",
            "loss 1.6555328369140625 average time 0.008288242133373085 iter num 60\n",
            "loss 1.3666274547576904 average time 0.006893868562610805 iter num 80\n",
            "loss 4.00726318359375 average time 0.006077467590075685 iter num 100\n",
            "loss 7.389292240142822 average time 0.01985155114962254 iter num 20\n",
            "loss 1.5160458087921143 average time 0.011335805099952268 iter num 40\n",
            "loss 1.179418683052063 average time 0.008475545833486345 iter num 60\n",
            "loss 4.539806365966797 average time 0.007038850462640767 iter num 80\n",
            "loss 5.671309947967529 average time 0.0061645383100767505 iter num 100\n",
            "loss 4.677063941955566 average time 0.019257597999785502 iter num 20\n",
            "loss 13.722092628479004 average time 0.011073071699865978 iter num 40\n",
            "loss 8.28216552734375 average time 0.00827726611654119 iter num 60\n",
            "loss 5.471825122833252 average time 0.006885339912378186 iter num 80\n",
            "loss 10.320005416870117 average time 0.006097794619927299 iter num 100\n",
            "loss 8.509491920471191 average time 0.020031655550701542 iter num 20\n",
            "loss 5.340266227722168 average time 0.011409970450404216 iter num 40\n",
            "loss 2.1672327518463135 average time 0.008524350666933363 iter num 60\n",
            "loss 3.7000412940979004 average time 0.007061027487679894 iter num 80\n",
            "loss 18.81381607055664 average time 0.006182451080057945 iter num 100\n",
            "loss 6.252702236175537 average time 0.019221660449693444 iter num 20\n",
            "loss 5.809941291809082 average time 0.010983444999783386 iter num 40\n",
            "loss 3.0866737365722656 average time 0.00819721764970988 iter num 60\n",
            "loss 8.084914207458496 average time 0.0068254649496793714 iter num 80\n",
            "loss 5.130859375 average time 0.006020733429613756 iter num 100\n",
            "loss 1.6090008020401 average time 0.019716261950088666 iter num 20\n",
            "loss 2.228168487548828 average time 0.011225360324988286 iter num 40\n",
            "loss 10.79896068572998 average time 0.008384697316553986 iter num 60\n",
            "loss 4.3902387619018555 average time 0.006993731149850646 iter num 80\n",
            "loss 5.714807033538818 average time 0.006153231509924808 iter num 100\n",
            "loss 3.53041934967041 average time 0.01965714339985425 iter num 20\n",
            "loss 7.064363479614258 average time 0.011224692175255768 iter num 40\n",
            "loss 8.088502883911133 average time 0.008392996483416936 iter num 60\n",
            "loss 3.753321409225464 average time 0.006983026800162407 iter num 80\n",
            "loss 2.234391212463379 average time 0.006132304230195586 iter num 100\n",
            "loss 6.807833671569824 average time 0.01919866339976579 iter num 20\n",
            "loss 2.848069667816162 average time 0.011101268750007876 iter num 40\n",
            "loss 8.234823226928711 average time 0.0083768542001053 iter num 60\n",
            "loss 3.4676570892333984 average time 0.007008846962617099 iter num 80\n",
            "loss 6.643247604370117 average time 0.006175116080012231 iter num 100\n",
            "loss 9.84439468383789 average time 0.01997057955031778 iter num 20\n",
            "loss 6.568307876586914 average time 0.011401473950172658 iter num 40\n",
            "loss 8.219345092773438 average time 0.008553091466880384 iter num 60\n",
            "loss 1.5887858867645264 average time 0.0071142506876185506 iter num 80\n",
            "loss 8.447708129882812 average time 0.0062224017200787785 iter num 100\n",
            "loss 19.63770294189453 average time 0.019311533600193796 iter num 20\n",
            "loss 2.4027419090270996 average time 0.011029153775052692 iter num 40\n",
            "loss 4.98954963684082 average time 0.008255714183239131 iter num 60\n",
            "loss 3.9180614948272705 average time 0.006899686424821994 iter num 80\n",
            "loss 5.321078300476074 average time 0.006074835649851593 iter num 100\n",
            "loss 2.410210609436035 average time 0.019612997650074248 iter num 20\n",
            "loss 4.600190162658691 average time 0.011200419049964693 iter num 40\n",
            "loss 6.445553779602051 average time 0.008389640133282228 iter num 60\n",
            "loss 1.7963652610778809 average time 0.006964636674911162 iter num 80\n",
            "loss 2.3628687858581543 average time 0.006109649569953035 iter num 100\n",
            "loss 9.604411125183105 average time 0.01933087110010092 iter num 20\n",
            "loss 4.401970863342285 average time 0.011004920225150271 iter num 40\n",
            "loss 11.220154762268066 average time 0.00827637511665671 iter num 60\n",
            "loss 14.122547149658203 average time 0.00690912078753172 iter num 80\n",
            "loss 3.5880651473999023 average time 0.006096653070017055 iter num 100\n",
            "loss 12.403221130371094 average time 0.019669049399817597 iter num 20\n",
            "loss 3.5718019008636475 average time 0.01121712904987362 iter num 40\n",
            "loss 6.201686859130859 average time 0.008378221233275933 iter num 60\n",
            "loss 9.447959899902344 average time 0.006953811562516421 iter num 80\n",
            "loss 2.1729342937469482 average time 0.006120266910074861 iter num 100\n",
            "loss 3.372267246246338 average time 0.01921703779971722 iter num 20\n",
            "loss 4.355051040649414 average time 0.011004613099612471 iter num 40\n",
            "loss 7.525491714477539 average time 0.008236806749664538 iter num 60\n",
            "loss 3.011657476425171 average time 0.00687288193716995 iter num 80\n",
            "loss 2.5244884490966797 average time 0.006051239609623735 iter num 100\n",
            "loss 2.054324150085449 average time 0.019902056700448156 iter num 20\n",
            "loss 5.418936252593994 average time 0.011329018125252333 iter num 40\n",
            "loss 8.033024787902832 average time 0.008467604133268953 iter num 60\n",
            "loss 6.293752193450928 average time 0.007026075962539835 iter num 80\n",
            "loss 5.372666358947754 average time 0.006154196690076787 iter num 100\n",
            "loss 6.466566562652588 average time 0.019219966799391842 iter num 20\n",
            "loss 8.746124267578125 average time 0.010957249624880205 iter num 40\n",
            "loss 4.762443542480469 average time 0.00827300791667464 iter num 60\n",
            "loss 4.120475769042969 average time 0.00693394828754208 iter num 80\n",
            "loss 3.5367672443389893 average time 0.0060984751299838534 iter num 100\n",
            "loss 4.3978190422058105 average time 0.01987682209983177 iter num 20\n",
            "loss 4.809417724609375 average time 0.011320379699736805 iter num 40\n",
            "loss 2.4641811847686768 average time 0.008457918883323146 iter num 60\n",
            "loss 3.6625850200653076 average time 0.0070898344250508675 iter num 80\n",
            "loss 7.474282264709473 average time 0.006314170500190812 iter num 100\n",
            "loss 15.334115982055664 average time 0.019923753550392574 iter num 20\n",
            "loss 7.116413116455078 average time 0.011322577874852868 iter num 40\n",
            "loss 2.8627448081970215 average time 0.008470063350068812 iter num 60\n",
            "loss 1.6914067268371582 average time 0.0070286972249505196 iter num 80\n",
            "loss 2.5416066646575928 average time 0.006161449099927267 iter num 100\n",
            "loss 7.58876895904541 average time 0.019282672199915397 iter num 20\n",
            "loss 2.8700549602508545 average time 0.01100209085007009 iter num 40\n",
            "loss 5.510059833526611 average time 0.00823847268341827 iter num 60\n",
            "loss 2.549063205718994 average time 0.006874045612403279 iter num 80\n",
            "loss 1.5784629583358765 average time 0.006071959280016017 iter num 100\n",
            "loss 6.684471607208252 average time 0.01982885140041617 iter num 20\n",
            "loss 3.8922111988067627 average time 0.011279493125039152 iter num 40\n",
            "loss 14.505290031433105 average time 0.008407090750066952 iter num 60\n",
            "loss 2.92726731300354 average time 0.006974948325205332 iter num 80\n",
            "loss 4.763895034790039 average time 0.006118907760137518 iter num 100\n",
            "loss 4.826081275939941 average time 0.019277263049661996 iter num 20\n",
            "loss 4.991898536682129 average time 0.011001649574609473 iter num 40\n",
            "loss 37.37380599975586 average time 0.008292926399614467 iter num 60\n",
            "loss 8.847046852111816 average time 0.006938839112217465 iter num 80\n",
            "loss 7.979441165924072 average time 0.006119515629652596 iter num 100\n",
            "loss 4.377450942993164 average time 0.01965833570029645 iter num 20\n",
            "loss 7.897727966308594 average time 0.011194792625246919 iter num 40\n",
            "loss 4.728026866912842 average time 0.008364577133397689 iter num 60\n",
            "loss 2.3695483207702637 average time 0.006941531899883557 iter num 80\n",
            "loss 3.466449737548828 average time 0.006087231959972996 iter num 100\n",
            "loss 6.669852256774902 average time 0.01930741520009178 iter num 20\n",
            "loss 1.9472869634628296 average time 0.010996860800150898 iter num 40\n",
            "loss 1.816382646560669 average time 0.008226947650049018 iter num 60\n",
            "loss 5.282767295837402 average time 0.006869138412457687 iter num 80\n",
            "loss 2.9168314933776855 average time 0.006092348480015062 iter num 100\n",
            "loss 6.04375696182251 average time 0.02038162784956512 iter num 20\n",
            "loss 8.565563201904297 average time 0.011558066475026862 iter num 40\n",
            "loss 1.9430859088897705 average time 0.00859016290002425 iter num 60\n",
            "loss 1.4080559015274048 average time 0.0071208368374300335 iter num 80\n",
            "loss 6.491340637207031 average time 0.00622060239988059 iter num 100\n",
            "loss 5.491652488708496 average time 0.019130197100457734 iter num 20\n",
            "loss 5.171363353729248 average time 0.010932013300225663 iter num 40\n",
            "loss 8.10566520690918 average time 0.008202688400403228 iter num 60\n",
            "loss 3.2027933597564697 average time 0.006846170125299977 iter num 80\n",
            "loss 4.614417552947998 average time 0.006045972800238815 iter num 100\n",
            "loss 3.9498820304870605 average time 0.019819914299478113 iter num 20\n",
            "loss 0.8379675149917603 average time 0.011262940324832016 iter num 40\n",
            "loss 2.303699493408203 average time 0.008434839783209707 iter num 60\n",
            "loss 4.677380561828613 average time 0.007016610637401754 iter num 80\n",
            "loss 3.7919929027557373 average time 0.006170062950004649 iter num 100\n",
            "loss 4.548193454742432 average time 0.019722443200043925 iter num 20\n",
            "loss 1.3670713901519775 average time 0.011198345225056982 iter num 40\n",
            "loss 7.048530578613281 average time 0.008407175466588038 iter num 60\n",
            "loss 12.807573318481445 average time 0.006989174537420695 iter num 80\n",
            "loss 5.0999956130981445 average time 0.0061288675099422105 iter num 100\n",
            "loss 11.063787460327148 average time 0.019358960149656924 iter num 20\n",
            "loss 5.780484199523926 average time 0.01103785102477559 iter num 40\n",
            "loss 3.3272533416748047 average time 0.008292966499963465 iter num 60\n",
            "loss 2.4033000469207764 average time 0.006931311812513741 iter num 80\n",
            "loss 1.0424220561981201 average time 0.006130737760067859 iter num 100\n",
            "loss 2.7902235984802246 average time 0.01964875804969779 iter num 20\n",
            "loss 3.4877171516418457 average time 0.011148663774929447 iter num 40\n",
            "loss 1.0109446048736572 average time 0.00835137046675906 iter num 60\n",
            "loss 8.745620727539062 average time 0.006937064700014162 iter num 80\n",
            "loss 7.077524185180664 average time 0.006082266689991229 iter num 100\n",
            "loss 14.724824905395508 average time 0.019894049950198677 iter num 20\n",
            "loss 5.006910800933838 average time 0.011271930825296294 iter num 40\n",
            "loss 16.589582443237305 average time 0.008449535733598168 iter num 60\n",
            "loss 11.539840698242188 average time 0.0070355421626572935 iter num 80\n",
            "loss 13.672943115234375 average time 0.00618123831012781 iter num 100\n",
            "loss 6.187919616699219 average time 0.019843266549287364 iter num 20\n",
            "loss 7.046445846557617 average time 0.011309804199481733 iter num 40\n",
            "loss 2.7412352561950684 average time 0.008438596949539108 iter num 60\n",
            "loss 1.6677062511444092 average time 0.0069976950620457504 iter num 80\n",
            "loss 7.169392108917236 average time 0.006160664659619215 iter num 100\n",
            "loss 8.842756271362305 average time 0.019275856100648525 iter num 20\n",
            "loss 3.130627393722534 average time 0.010977997650388715 iter num 40\n",
            "loss 3.2407562732696533 average time 0.008256099600293965 iter num 60\n",
            "loss 3.200089454650879 average time 0.006939742387703518 iter num 80\n",
            "loss 9.12666130065918 average time 0.006146145070197236 iter num 100\n",
            "loss 2.905531406402588 average time 0.019787869099855016 iter num 20\n",
            "loss 10.014752388000488 average time 0.011243992324671126 iter num 40\n",
            "loss 3.5528132915496826 average time 0.008393841699762561 iter num 60\n",
            "loss 2.255582094192505 average time 0.0069599202248809885 iter num 80\n",
            "loss 0.9928727746009827 average time 0.0061145496399331025 iter num 100\n",
            "loss 2.611847400665283 average time 0.019267728000340868 iter num 20\n",
            "loss 4.361034393310547 average time 0.011028042650286807 iter num 40\n",
            "loss 1.054247498512268 average time 0.008295943367011204 iter num 60\n",
            "loss 2.8266806602478027 average time 0.006919107525300205 iter num 80\n",
            "loss 3.2886924743652344 average time 0.006103499330238265 iter num 100\n",
            "loss 4.145256042480469 average time 0.019711508550062717 iter num 20\n",
            "loss 2.256126642227173 average time 0.011235419875083608 iter num 40\n",
            "loss 4.843034267425537 average time 0.008670035666546028 iter num 60\n",
            "loss 7.413737773895264 average time 0.007212647224923785 iter num 80\n",
            "loss 3.286257266998291 average time 0.006320712919841753 iter num 100\n",
            "loss 1.7561869621276855 average time 0.019572045450149744 iter num 20\n",
            "loss 2.99546480178833 average time 0.011221843625207839 iter num 40\n",
            "loss 2.0942816734313965 average time 0.00836919878347544 iter num 60\n",
            "loss 23.924638748168945 average time 0.006944397087636389 iter num 80\n",
            "loss 4.69452428817749 average time 0.00611168916009774 iter num 100\n",
            "loss 9.756202697753906 average time 0.019423920099870883 iter num 20\n",
            "loss 19.084449768066406 average time 0.011076935725031944 iter num 40\n",
            "loss 1.752030611038208 average time 0.008423355249942688 iter num 60\n",
            "loss 3.4890313148498535 average time 0.007019126974955725 iter num 80\n",
            "loss 3.951978921890259 average time 0.006185508129965456 iter num 100\n",
            "loss 5.58544921875 average time 0.019474419300058797 iter num 20\n",
            "loss 5.581842422485352 average time 0.011080519899951468 iter num 40\n",
            "loss 4.1636786460876465 average time 0.008328909916478248 iter num 60\n",
            "loss 9.295254707336426 average time 0.006914825512421885 iter num 80\n",
            "loss 1.669581413269043 average time 0.006076702119862602 iter num 100\n",
            "loss 7.693575859069824 average time 0.019257470249431207 iter num 20\n",
            "loss 6.700201988220215 average time 0.011006302024998148 iter num 40\n",
            "loss 6.395975112915039 average time 0.008300283083372051 iter num 60\n",
            "loss 2.703448534011841 average time 0.006922368537425427 iter num 80\n",
            "loss 1.8117246627807617 average time 0.0061003179799809 iter num 100\n",
            "loss 9.174878120422363 average time 0.019765878350699494 iter num 20\n",
            "loss 1.2971833944320679 average time 0.01138330167532331 iter num 40\n",
            "loss 5.180332660675049 average time 0.008501069650143715 iter num 60\n",
            "loss 10.691340446472168 average time 0.007095690062669746 iter num 80\n",
            "loss 27.436283111572266 average time 0.006220636620055302 iter num 100\n",
            "loss 6.134286403656006 average time 0.019372752549861615 iter num 20\n",
            "loss 4.129924774169922 average time 0.011133403050007474 iter num 40\n",
            "loss 4.497000217437744 average time 0.008338283866760322 iter num 60\n",
            "loss 7.428638935089111 average time 0.00695217667516772 iter num 80\n",
            "loss 4.307620048522949 average time 0.006113261840182531 iter num 100\n",
            "loss 3.724640369415283 average time 0.019888495649684045 iter num 20\n",
            "loss 7.30991268157959 average time 0.01135148217481401 iter num 40\n",
            "loss 4.385744571685791 average time 0.008499015866679354 iter num 60\n",
            "loss 7.384977340698242 average time 0.007050208787677548 iter num 80\n",
            "loss 3.8266959190368652 average time 0.006179321420058841 iter num 100\n",
            "loss 3.8194010257720947 average time 0.019110661349805012 iter num 20\n",
            "loss 4.156152248382568 average time 0.010933974150248105 iter num 40\n",
            "loss 3.364224433898926 average time 0.00825096873347017 iter num 60\n",
            "loss 5.137383460998535 average time 0.0068858581001222776 iter num 80\n",
            "loss 8.982054710388184 average time 0.00607967637010006 iter num 100\n",
            "loss 5.907485008239746 average time 0.01961277934969985 iter num 20\n",
            "loss 9.581948280334473 average time 0.011210287524954765 iter num 40\n",
            "loss 5.210742950439453 average time 0.008389735099869237 iter num 60\n",
            "loss 5.357781410217285 average time 0.006978361649908038 iter num 80\n",
            "loss 3.227114200592041 average time 0.006134616079943953 iter num 100\n",
            "loss 3.478677749633789 average time 0.01925106799972127 iter num 20\n",
            "loss 11.069299697875977 average time 0.011026105800101504 iter num 40\n",
            "loss 5.3679070472717285 average time 0.008301920849953603 iter num 60\n",
            "loss 2.671351194381714 average time 0.00692459122501532 iter num 80\n",
            "loss 1.862856388092041 average time 0.006098207870090846 iter num 100\n",
            "loss 6.045406341552734 average time 0.01925926139956573 iter num 20\n",
            "loss 3.1055550575256348 average time 0.011050915449595777 iter num 40\n",
            "loss 2.8433666229248047 average time 0.008301095316467885 iter num 60\n",
            "loss 2.5764808654785156 average time 0.006980771724829538 iter num 80\n",
            "loss 1.247982382774353 average time 0.006134541389910737 iter num 100\n",
            "loss 26.698894500732422 average time 0.019149699349873116 iter num 20\n",
            "loss 6.443541526794434 average time 0.010914905349727633 iter num 40\n",
            "loss 3.032954216003418 average time 0.008222775833201013 iter num 60\n",
            "loss 4.636290550231934 average time 0.0068413776623401645 iter num 80\n",
            "loss 9.247649192810059 average time 0.006031533429923002 iter num 100\n",
            "loss 3.6700286865234375 average time 0.01915093105053529 iter num 20\n",
            "loss 3.6703922748565674 average time 0.010968036000122083 iter num 40\n",
            "loss 5.893714904785156 average time 0.008243939383404116 iter num 60\n",
            "loss 2.1921892166137695 average time 0.006880797175062981 iter num 80\n",
            "loss 4.810564994812012 average time 0.006056762390035147 iter num 100\n",
            "loss 4.221684455871582 average time 0.01914024414945743 iter num 20\n",
            "loss 1.700329303741455 average time 0.010925585124641656 iter num 40\n",
            "loss 2.3031201362609863 average time 0.008260095116202137 iter num 60\n",
            "loss 2.216883897781372 average time 0.006910276299549878 iter num 80\n",
            "loss 1.8400145769119263 average time 0.006080317209634814 iter num 100\n",
            "loss 5.657269477844238 average time 0.019258413150055276 iter num 20\n",
            "loss 4.981357574462891 average time 0.011184577499807346 iter num 40\n",
            "loss 2.9539601802825928 average time 0.00838097263325229 iter num 60\n",
            "loss 1.881906509399414 average time 0.007005041074944529 iter num 80\n",
            "loss 3.3308844566345215 average time 0.006155058549993555 iter num 100\n",
            "loss 7.743950843811035 average time 0.019312120850554493 iter num 20\n",
            "loss 4.8898024559021 average time 0.011011877200053278 iter num 40\n",
            "loss 8.011894226074219 average time 0.008226877033242394 iter num 60\n",
            "loss 3.9491822719573975 average time 0.006854560075089467 iter num 80\n",
            "loss 7.418306350708008 average time 0.0060217054199893025 iter num 100\n",
            "loss 4.0869340896606445 average time 0.01934759910000139 iter num 20\n",
            "loss 4.78325891494751 average time 0.011112285625040385 iter num 40\n",
            "loss 2.255338191986084 average time 0.008335988933383002 iter num 60\n",
            "loss 3.8734326362609863 average time 0.006960322974964584 iter num 80\n",
            "loss 6.351578712463379 average time 0.006126026589954563 iter num 100\n",
            "loss 1.6265640258789062 average time 0.019425331649836154 iter num 20\n",
            "loss 23.033117294311523 average time 0.011108501625130884 iter num 40\n",
            "loss 2.721480369567871 average time 0.008298729150131598 iter num 60\n",
            "loss 5.2859883308410645 average time 0.006890223587697619 iter num 80\n",
            "loss 3.5116477012634277 average time 0.006061105120206776 iter num 100\n",
            "loss 5.066773414611816 average time 0.01925733910047711 iter num 20\n",
            "loss 9.89074420928955 average time 0.011048070675224153 iter num 40\n",
            "loss 8.900676727294922 average time 0.008297332733491203 iter num 60\n",
            "loss 6.913270473480225 average time 0.0069144429126936306 iter num 80\n",
            "loss 4.403013229370117 average time 0.0060744071101726145 iter num 100\n",
            "loss 8.331018447875977 average time 0.01940851625004143 iter num 20\n",
            "loss 4.4902753829956055 average time 0.011077284075327043 iter num 40\n",
            "loss 9.830042839050293 average time 0.008309007533656161 iter num 60\n",
            "loss 4.23988151550293 average time 0.0069237791876730626 iter num 80\n",
            "loss 2.4036154747009277 average time 0.006119874470205105 iter num 100\n",
            "loss 3.355003595352173 average time 0.01932954300027632 iter num 20\n",
            "loss 4.285441875457764 average time 0.011023583025144034 iter num 40\n",
            "loss 17.34281349182129 average time 0.008273946700076825 iter num 60\n",
            "loss 2.3578574657440186 average time 0.006904879550120313 iter num 80\n",
            "loss 3.404707908630371 average time 0.0060817123801462005 iter num 100\n",
            "loss 3.877479076385498 average time 0.019240823749896663 iter num 20\n",
            "loss 3.0179789066314697 average time 0.01100147922479664 iter num 40\n",
            "loss 6.311678409576416 average time 0.008273528749850812 iter num 60\n",
            "loss 0.890783429145813 average time 0.006969569299963041 iter num 80\n",
            "loss 6.310184478759766 average time 0.006114440429919341 iter num 100\n",
            "loss 4.562386512756348 average time 0.019305639099729887 iter num 20\n",
            "loss 5.346194267272949 average time 0.011003582700050174 iter num 40\n",
            "loss 8.190454483032227 average time 0.008212086816820374 iter num 60\n",
            "loss 3.7852582931518555 average time 0.006826227437568377 iter num 80\n",
            "loss 5.191729545593262 average time 0.006024008720123675 iter num 100\n",
            "loss 4.658315181732178 average time 0.019283408149749447 iter num 20\n",
            "loss 11.098503112792969 average time 0.011025093474927417 iter num 40\n",
            "loss 6.027946949005127 average time 0.00826766216669057 iter num 60\n",
            "loss 12.294713973999023 average time 0.006892476099983469 iter num 80\n",
            "loss 1.8810420036315918 average time 0.006075940629998513 iter num 100\n",
            "loss 10.825601577758789 average time 0.019211505250132176 iter num 20\n",
            "loss 6.770735740661621 average time 0.010944555399782984 iter num 40\n",
            "loss 22.429431915283203 average time 0.008201588316721124 iter num 60\n",
            "loss 10.374104499816895 average time 0.006814744725079436 iter num 80\n",
            "loss 18.442811965942383 average time 0.005979707260121358 iter num 100\n",
            "loss 6.4025468826293945 average time 0.01964945460040326 iter num 20\n",
            "loss 3.6616806983947754 average time 0.01152028742544644 iter num 40\n",
            "loss 5.004245281219482 average time 0.008615694083467436 iter num 60\n",
            "loss 5.947846412658691 average time 0.007148177737644801 iter num 80\n",
            "loss 2.90419864654541 average time 0.0062500196201290235 iter num 100\n",
            "loss 8.111104965209961 average time 0.019180507750024844 iter num 20\n",
            "loss 1.5816214084625244 average time 0.010926590700000816 iter num 40\n",
            "loss 3.323239803314209 average time 0.008174256633234714 iter num 60\n",
            "loss 5.84224271774292 average time 0.006845893287390936 iter num 80\n",
            "loss 1.6833484172821045 average time 0.006014388299990969 iter num 100\n",
            "loss 6.144177436828613 average time 0.019563509099498333 iter num 20\n",
            "loss 5.285335540771484 average time 0.011162887699902058 iter num 40\n",
            "loss 3.902212619781494 average time 0.008376880749953368 iter num 60\n",
            "loss 7.4471540451049805 average time 0.0069815462625228975 iter num 80\n",
            "loss 13.194195747375488 average time 0.006117990919992735 iter num 100\n",
            "loss 8.318516731262207 average time 0.019404446699445542 iter num 20\n",
            "loss 18.01569366455078 average time 0.01105062252454445 iter num 40\n",
            "loss 2.3828227519989014 average time 0.008252025366467327 iter num 60\n",
            "loss 6.959536075592041 average time 0.0068515606248183755 iter num 80\n",
            "loss 5.7387213706970215 average time 0.0060119110398227345 iter num 100\n",
            "loss 8.268489837646484 average time 0.01980842860011762 iter num 20\n",
            "loss 5.108670711517334 average time 0.011299562025214982 iter num 40\n",
            "loss 7.01663064956665 average time 0.0084634857169173 iter num 60\n",
            "loss 1.6132144927978516 average time 0.007028311262729403 iter num 80\n",
            "loss 10.678384780883789 average time 0.006157846470232471 iter num 100\n",
            "loss 2.5010197162628174 average time 0.019499556999835476 iter num 20\n",
            "loss 2.136072874069214 average time 0.011118653224821173 iter num 40\n",
            "loss 4.338903903961182 average time 0.008327360133262119 iter num 60\n",
            "loss 13.131031036376953 average time 0.006921888499982742 iter num 80\n",
            "loss 4.674017906188965 average time 0.006097037340041424 iter num 100\n",
            "loss 4.541622161865234 average time 0.0192201453499365 iter num 20\n",
            "loss 4.785266876220703 average time 0.0109867715747896 iter num 40\n",
            "loss 14.351123809814453 average time 0.008226988066417107 iter num 60\n",
            "loss 1.1969788074493408 average time 0.006835509999837086 iter num 80\n",
            "loss 4.9609456062316895 average time 0.00600041549983871 iter num 100\n",
            "loss 32.25038146972656 average time 0.020076334650366335 iter num 20\n",
            "loss 9.929973602294922 average time 0.011710084224978346 iter num 40\n",
            "loss 3.4598236083984375 average time 0.008841710633168987 iter num 60\n",
            "loss 4.87461519241333 average time 0.00732174093723188 iter num 80\n",
            "loss 19.317514419555664 average time 0.00638088154966681 iter num 100\n",
            "loss 5.914593696594238 average time 0.019137698850136074 iter num 20\n",
            "loss 5.1366376876831055 average time 0.010903015424992191 iter num 40\n",
            "loss 2.8361997604370117 average time 0.008164067616720179 iter num 60\n",
            "loss 8.077985763549805 average time 0.006854982075174121 iter num 80\n",
            "loss 7.28680944442749 average time 0.0060246407402155455 iter num 100\n",
            "loss 4.914552688598633 average time 0.019900486100232227 iter num 20\n",
            "loss 12.596653938293457 average time 0.011383482675137202 iter num 40\n",
            "loss 6.107807159423828 average time 0.008539373633599705 iter num 60\n",
            "loss 2.750247001647949 average time 0.007097565137610218 iter num 80\n",
            "loss 4.638700008392334 average time 0.006209663670160808 iter num 100\n",
            "loss 2.309598445892334 average time 0.019406901150068732 iter num 20\n",
            "loss 2.861607313156128 average time 0.011073134324851708 iter num 40\n",
            "loss 1.98455011844635 average time 0.008255751299960442 iter num 60\n",
            "loss 1.5575189590454102 average time 0.006863337800086811 iter num 80\n",
            "loss 1.9460010528564453 average time 0.006047219520078215 iter num 100\n",
            "loss 2.5079824924468994 average time 0.0197812630000044 iter num 20\n",
            "loss 5.144367218017578 average time 0.011320069499925011 iter num 40\n",
            "loss 4.581687927246094 average time 0.008497507033340904 iter num 60\n",
            "loss 11.471420288085938 average time 0.007061838925119445 iter num 80\n",
            "loss 4.325081825256348 average time 0.006210717419999128 iter num 100\n",
            "loss 6.629952907562256 average time 0.019430311200812865 iter num 20\n",
            "loss 4.762862205505371 average time 0.01111498130048858 iter num 40\n",
            "loss 4.0340895652771 average time 0.008290130183619718 iter num 60\n",
            "loss 11.770818710327148 average time 0.006893912250052381 iter num 80\n",
            "loss 4.350752353668213 average time 0.006061163170088548 iter num 100\n",
            "loss 5.661791801452637 average time 0.019986858250013027 iter num 20\n",
            "loss 3.856689691543579 average time 0.011392530024932058 iter num 40\n",
            "loss 6.494617938995361 average time 0.008588607816515529 iter num 60\n",
            "loss 2.6177873611450195 average time 0.007121020474914985 iter num 80\n",
            "loss 3.778334379196167 average time 0.006255705169896828 iter num 100\n",
            "loss 7.998388767242432 average time 0.019876074800413333 iter num 20\n",
            "loss 8.958871841430664 average time 0.01145453240014831 iter num 40\n",
            "loss 3.0426602363586426 average time 0.00856269645022015 iter num 60\n",
            "loss 2.9530327320098877 average time 0.007088555250038553 iter num 80\n",
            "loss 1.4258573055267334 average time 0.006286320359940874 iter num 100\n",
            "loss 3.540501594543457 average time 0.019162626399884176 iter num 20\n",
            "loss 7.789363384246826 average time 0.01103770794961747 iter num 40\n",
            "loss 13.300153732299805 average time 0.008267106032993373 iter num 60\n",
            "loss 0.9040380716323853 average time 0.0069320094996783155 iter num 80\n",
            "loss 3.5145998001098633 average time 0.006109281439676124 iter num 100\n",
            "loss 2.864419937133789 average time 0.019811781049611454 iter num 20\n",
            "loss 13.110424041748047 average time 0.011292522700023256 iter num 40\n",
            "loss 3.624545097351074 average time 0.008461837933343002 iter num 60\n",
            "loss 5.112325191497803 average time 0.0070436761375731296 iter num 80\n",
            "loss 2.8873090744018555 average time 0.0061688427599438 iter num 100\n",
            "loss 6.054032802581787 average time 0.01931565054983366 iter num 20\n",
            "loss 4.158454895019531 average time 0.01109838159964056 iter num 40\n",
            "loss 12.220317840576172 average time 0.008287655899645566 iter num 60\n",
            "loss 2.223564624786377 average time 0.006870861674633489 iter num 80\n",
            "loss 2.680006980895996 average time 0.00611487865968229 iter num 100\n",
            "loss 5.440649032592773 average time 0.019828925450019595 iter num 20\n",
            "loss 13.87637710571289 average time 0.011307155550275639 iter num 40\n",
            "loss 2.8817811012268066 average time 0.008459126400218035 iter num 60\n",
            "loss 3.4768075942993164 average time 0.007012533637634988 iter num 80\n",
            "loss 7.022169589996338 average time 0.006165793390064209 iter num 100\n",
            "loss 5.735875129699707 average time 0.019409203749819425 iter num 20\n",
            "loss 2.963613986968994 average time 0.011077177600054711 iter num 40\n",
            "loss 3.195528984069824 average time 0.008278908966834328 iter num 60\n",
            "loss 1.032555341720581 average time 0.006898855000099502 iter num 80\n",
            "loss 0.8610105514526367 average time 0.006075142530135053 iter num 100\n",
            "loss 3.3851356506347656 average time 0.01968219880054676 iter num 20\n",
            "loss 2.8608055114746094 average time 0.01125963469994531 iter num 40\n",
            "loss 3.62491512298584 average time 0.008399541933249566 iter num 60\n",
            "loss 13.842382431030273 average time 0.006968186137328303 iter num 80\n",
            "loss 2.2125706672668457 average time 0.0061084995798955785 iter num 100\n",
            "loss 5.978565216064453 average time 0.019613470199874427 iter num 20\n",
            "loss 7.157055854797363 average time 0.011203941074836622 iter num 40\n",
            "loss 1.7632291316986084 average time 0.008392992549973617 iter num 60\n",
            "loss 1.8433489799499512 average time 0.0070169742250072885 iter num 80\n",
            "loss 1.7142212390899658 average time 0.006193311249917315 iter num 100\n",
            "loss 7.786741256713867 average time 0.01944801909976377 iter num 20\n",
            "loss 6.67919397354126 average time 0.011128018324689038 iter num 40\n",
            "loss 12.548479080200195 average time 0.008326792533019519 iter num 60\n",
            "loss 1.6130911111831665 average time 0.006913078199613665 iter num 80\n",
            "loss 4.732760906219482 average time 0.006105382139758149 iter num 100\n",
            "loss 7.629641532897949 average time 0.019980715200290433 iter num 20\n",
            "loss 2.9471781253814697 average time 0.011377320175324713 iter num 40\n",
            "loss 3.7891902923583984 average time 0.008534811183623484 iter num 60\n",
            "loss 8.805044174194336 average time 0.007113454675436515 iter num 80\n",
            "loss 14.739980697631836 average time 0.006232623190335289 iter num 100\n",
            "loss 10.184146881103516 average time 0.019262836299458286 iter num 20\n",
            "loss 1.470024585723877 average time 0.010982591649462848 iter num 40\n",
            "loss 2.0961391925811768 average time 0.008235578699714096 iter num 60\n",
            "loss 1.0243384838104248 average time 0.00687400493725363 iter num 80\n",
            "loss 4.892213821411133 average time 0.00610034099976474 iter num 100\n",
            "loss 11.19710922241211 average time 0.019773324949346714 iter num 20\n",
            "loss 3.1745033264160156 average time 0.011312875574731152 iter num 40\n",
            "loss 10.53361701965332 average time 0.008471659383333947 iter num 60\n",
            "loss 4.67680549621582 average time 0.00707332033757666 iter num 80\n",
            "loss 7.819304943084717 average time 0.006223878169948876 iter num 100\n",
            "loss 26.81060791015625 average time 0.01931588730039948 iter num 20\n",
            "loss 10.105310440063477 average time 0.011003722025088791 iter num 40\n",
            "loss 2.338726758956909 average time 0.008281358600106615 iter num 60\n",
            "loss 2.878753185272217 average time 0.006902184900127395 iter num 80\n",
            "loss 4.441615104675293 average time 0.006145345990080387 iter num 100\n",
            "loss 2.157062530517578 average time 0.01967329044982762 iter num 20\n",
            "loss 1.2688376903533936 average time 0.011215679075030493 iter num 40\n",
            "loss 5.423478126525879 average time 0.008438698833257756 iter num 60\n",
            "loss 2.2240161895751953 average time 0.007000879724819243 iter num 80\n",
            "loss 1.9692883491516113 average time 0.006132119789835997 iter num 100\n",
            "loss 4.257859230041504 average time 0.01931232275055663 iter num 20\n",
            "loss 2.9843406677246094 average time 0.011003229375091905 iter num 40\n",
            "loss 7.127941131591797 average time 0.008277683466743232 iter num 60\n",
            "loss 3.64907169342041 average time 0.006990557724930113 iter num 80\n",
            "loss 3.2301831245422363 average time 0.006272750999924029 iter num 100\n",
            "loss 4.91276216506958 average time 0.019734438250088714 iter num 20\n",
            "loss 4.619519233703613 average time 0.011257679975096834 iter num 40\n",
            "loss 2.679023265838623 average time 0.008415351566751876 iter num 60\n",
            "loss 3.6653871536254883 average time 0.006996335662506681 iter num 80\n",
            "loss 4.042983055114746 average time 0.006143650619997061 iter num 100\n",
            "loss 2.165438413619995 average time 0.019456721850110624 iter num 20\n",
            "loss 5.580473899841309 average time 0.011060593600177526 iter num 40\n",
            "loss 1.5540729761123657 average time 0.008265729966781994 iter num 60\n",
            "loss 4.127363204956055 average time 0.0068836042501516205 iter num 80\n",
            "loss 6.554284572601318 average time 0.006070296190227964 iter num 100\n",
            "loss 5.861789703369141 average time 0.01967090204980195 iter num 20\n",
            "loss 7.044500350952148 average time 0.011230226749830763 iter num 40\n",
            "loss 2.4355835914611816 average time 0.008384562233247076 iter num 60\n",
            "loss 2.9650909900665283 average time 0.0069596729249497 iter num 80\n",
            "loss 1.9703396558761597 average time 0.006105974050042278 iter num 100\n",
            "loss 14.217523574829102 average time 0.019454762249552005 iter num 20\n",
            "loss 10.157888412475586 average time 0.011077422474772902 iter num 40\n",
            "loss 1.594918131828308 average time 0.008312072266623243 iter num 60\n",
            "loss 4.921275615692139 average time 0.006938073912442633 iter num 80\n",
            "loss 1.2366926670074463 average time 0.006106262529938249 iter num 100\n",
            "loss 6.781953811645508 average time 0.019770720350061312 iter num 20\n",
            "loss 5.038046836853027 average time 0.011318665874841826 iter num 40\n",
            "loss 5.131784439086914 average time 0.008446451283210385 iter num 60\n",
            "loss 6.763519287109375 average time 0.0070541408623739695 iter num 80\n",
            "loss 3.8056447505950928 average time 0.006197124269783671 iter num 100\n",
            "loss 5.962738513946533 average time 0.01970914875073504 iter num 20\n",
            "loss 1.9859046936035156 average time 0.011299584800417506 iter num 40\n",
            "loss 20.405256271362305 average time 0.008484383366946228 iter num 60\n",
            "loss 5.072978496551514 average time 0.0070347490252061105 iter num 80\n",
            "loss 3.076246738433838 average time 0.006169090840121499 iter num 100\n",
            "loss 5.0603227615356445 average time 0.01935700340072799 iter num 20\n",
            "loss 7.206212043762207 average time 0.01103562425059863 iter num 40\n",
            "loss 12.56257438659668 average time 0.008265574600288043 iter num 60\n",
            "loss 12.231851577758789 average time 0.007006892512617924 iter num 80\n",
            "loss 12.328912734985352 average time 0.006255281950107019 iter num 100\n",
            "loss 3.551687240600586 average time 0.02015221304991428 iter num 20\n",
            "loss 1.443585991859436 average time 0.011470270774771052 iter num 40\n",
            "loss 3.6477713584899902 average time 0.008591115266547907 iter num 60\n",
            "loss 10.007662773132324 average time 0.007116789175051963 iter num 80\n",
            "loss 1.8542237281799316 average time 0.006235241330032295 iter num 100\n",
            "loss 3.826223850250244 average time 0.019365657800153712 iter num 20\n",
            "loss 1.5327552556991577 average time 0.011060162150079123 iter num 40\n",
            "loss 3.943528890609741 average time 0.008270173450000584 iter num 60\n",
            "loss 3.013702392578125 average time 0.006906189387473205 iter num 80\n",
            "loss 10.631391525268555 average time 0.006117562410036043 iter num 100\n",
            "loss 7.4673991203308105 average time 0.019819131599797403 iter num 20\n",
            "loss 7.707153797149658 average time 0.011412462724911166 iter num 40\n",
            "loss 3.654740810394287 average time 0.008522186316561904 iter num 60\n",
            "loss 4.557168483734131 average time 0.00706707007498153 iter num 80\n",
            "loss 1.9798994064331055 average time 0.0061907526698269065 iter num 100\n",
            "loss 2.837437629699707 average time 0.019338420600251992 iter num 20\n",
            "loss 6.224353790283203 average time 0.011004898749706626 iter num 40\n",
            "loss 2.399181842803955 average time 0.00822956518313731 iter num 60\n",
            "loss 10.588332176208496 average time 0.006865032124778736 iter num 80\n",
            "loss 5.791553974151611 average time 0.006044711119793647 iter num 100\n",
            "loss 4.751739978790283 average time 0.019929992699690046 iter num 20\n",
            "loss 14.20247745513916 average time 0.011440517774826731 iter num 40\n",
            "loss 11.60104751586914 average time 0.008535617366578662 iter num 60\n",
            "loss 3.680607795715332 average time 0.007080541062305201 iter num 80\n",
            "loss 2.157456159591675 average time 0.006201700759775121 iter num 100\n",
            "loss 9.913379669189453 average time 0.01938923435027391 iter num 20\n",
            "loss 9.337474822998047 average time 0.011085025650027091 iter num 40\n",
            "loss 9.594707489013672 average time 0.008285168816595008 iter num 60\n",
            "loss 7.92302942276001 average time 0.006911704512549477 iter num 80\n",
            "loss 3.2208900451660156 average time 0.006103449290021672 iter num 100\n",
            "loss 4.497380256652832 average time 0.019981454050684987 iter num 20\n",
            "loss 8.377314567565918 average time 0.011352883525432844 iter num 40\n",
            "loss 1.8198682069778442 average time 0.008559765600269505 iter num 60\n",
            "loss 5.5492329597473145 average time 0.007115957700307263 iter num 80\n",
            "loss 1.1936428546905518 average time 0.006272442920235335 iter num 100\n",
            "loss 4.6912641525268555 average time 0.019738044050063762 iter num 20\n",
            "loss 7.2082695960998535 average time 0.01140146405014093 iter num 40\n",
            "loss 5.8407368659973145 average time 0.008488273750178148 iter num 60\n",
            "loss 0.9627392292022705 average time 0.007052567562595868 iter num 80\n",
            "loss 5.876981258392334 average time 0.00620928124004422 iter num 100\n",
            "loss 3.859933376312256 average time 0.019268180599829064 iter num 20\n",
            "loss 5.699392795562744 average time 0.011058951199811418 iter num 40\n",
            "loss 12.37813949584961 average time 0.008319004916241586 iter num 60\n",
            "loss 2.0459959506988525 average time 0.006947917037086882 iter num 80\n",
            "loss 1.1743662357330322 average time 0.006122424459608738 iter num 100\n",
            "loss 6.7433013916015625 average time 0.019706117699752213 iter num 20\n",
            "loss 5.4513654708862305 average time 0.011250044249845815 iter num 40\n",
            "loss 3.3274412155151367 average time 0.008416146683399954 iter num 60\n",
            "loss 3.865325450897217 average time 0.006998572550082826 iter num 80\n",
            "loss 1.3973132371902466 average time 0.00618943155015586 iter num 100\n",
            "loss 6.911630630493164 average time 0.01924935179977183 iter num 20\n",
            "loss 26.31577491760254 average time 0.010975175749717892 iter num 40\n",
            "loss 4.678460121154785 average time 0.00824840239971915 iter num 60\n",
            "loss 5.870105743408203 average time 0.006887554799686768 iter num 80\n",
            "loss 10.06956672668457 average time 0.006084731339651626 iter num 100\n",
            "loss 3.8327083587646484 average time 0.020020150050186203 iter num 20\n",
            "loss 7.260127544403076 average time 0.011495042625210772 iter num 40\n",
            "loss 5.17364501953125 average time 0.00857923580018299 iter num 60\n",
            "loss 5.389213562011719 average time 0.007186804137745639 iter num 80\n",
            "loss 2.535681962966919 average time 0.006294895190148963 iter num 100\n",
            "loss 2.5239970684051514 average time 0.01926366780007811 iter num 20\n",
            "loss 1.9083051681518555 average time 0.01097175087506912 iter num 40\n",
            "loss 1.6542662382125854 average time 0.008242865866729213 iter num 60\n",
            "loss 6.89525032043457 average time 0.006876906000161398 iter num 80\n",
            "loss 4.752155780792236 average time 0.006059419960038213 iter num 100\n",
            "loss 7.268418312072754 average time 0.020571772849871196 iter num 20\n",
            "loss 4.929637908935547 average time 0.01163974467499429 iter num 40\n",
            "loss 1.3777812719345093 average time 0.008647787116569817 iter num 60\n",
            "loss 2.044987440109253 average time 0.007169080024914365 iter num 80\n",
            "loss 7.15630578994751 average time 0.006294010619894834 iter num 100\n",
            "loss 4.646317005157471 average time 0.019236443449335637 iter num 20\n",
            "loss 4.396327495574951 average time 0.010955686099805462 iter num 40\n",
            "loss 6.822013854980469 average time 0.008234703199741488 iter num 60\n",
            "loss 4.795241832733154 average time 0.00685886322476108 iter num 80\n",
            "loss 13.255623817443848 average time 0.006040274679617141 iter num 100\n",
            "loss 4.7634172439575195 average time 0.019622737700228755 iter num 20\n",
            "loss 3.3773608207702637 average time 0.011192359225151449 iter num 40\n",
            "loss 1.4273439645767212 average time 0.00835710933364074 iter num 60\n",
            "loss 7.829381465911865 average time 0.00694721600025332 iter num 80\n",
            "loss 8.749218940734863 average time 0.006110829710196413 iter num 100\n",
            "loss 16.345998764038086 average time 0.019354404200021234 iter num 20\n",
            "loss 10.13451099395752 average time 0.011028728975179546 iter num 40\n",
            "loss 2.4210145473480225 average time 0.008305236750129553 iter num 60\n",
            "loss 3.8621177673339844 average time 0.007018574012636236 iter num 80\n",
            "loss 4.708793640136719 average time 0.006204684510266816 iter num 100\n",
            "loss 1.7580209970474243 average time 0.019867655399684735 iter num 20\n",
            "loss 20.28350830078125 average time 0.011305605650068173 iter num 40\n",
            "loss 13.587294578552246 average time 0.00847405198328488 iter num 60\n",
            "loss 3.8249940872192383 average time 0.007065891887395992 iter num 80\n",
            "loss 2.8145430088043213 average time 0.006223245739893173 iter num 100\n",
            "loss 6.083634376525879 average time 0.01983906815021328 iter num 20\n",
            "loss 3.8488500118255615 average time 0.011271098350152896 iter num 40\n",
            "loss 18.64984130859375 average time 0.008402334116726706 iter num 60\n",
            "loss 3.3189663887023926 average time 0.006972151962554562 iter num 80\n",
            "loss 3.036363124847412 average time 0.0061420537101002995 iter num 100\n",
            "loss 2.4076132774353027 average time 0.01962448080030299 iter num 20\n",
            "loss 6.825841426849365 average time 0.011189696350265876 iter num 40\n",
            "loss 2.1373109817504883 average time 0.008390005217006547 iter num 60\n",
            "loss 2.6606979370117188 average time 0.006993370725285786 iter num 80\n",
            "loss 5.750924110412598 average time 0.0061508774902176815 iter num 100\n",
            "loss 3.910032272338867 average time 0.019629138949858314 iter num 20\n",
            "loss 2.3761484622955322 average time 0.011145058824968145 iter num 40\n",
            "loss 4.838168144226074 average time 0.008324911999989127 iter num 60\n",
            "loss 2.1303279399871826 average time 0.0069414339374816334 iter num 80\n",
            "loss 9.034595489501953 average time 0.006100252029973489 iter num 100\n",
            "loss 7.81959867477417 average time 0.019451454800218926 iter num 20\n",
            "loss 5.906683921813965 average time 0.011090939125006116 iter num 40\n",
            "loss 6.1172685623168945 average time 0.008379016483255933 iter num 60\n",
            "loss 6.163717746734619 average time 0.006983939362362435 iter num 80\n",
            "loss 5.691056251525879 average time 0.006150706149855978 iter num 100\n",
            "loss 5.608222007751465 average time 0.019704318400181366 iter num 20\n",
            "loss 5.278389930725098 average time 0.011199630249939219 iter num 40\n",
            "loss 3.7875277996063232 average time 0.008369306433451129 iter num 60\n",
            "loss 5.837313652038574 average time 0.0069426371124336585 iter num 80\n",
            "loss 4.236230373382568 average time 0.006101310399972135 iter num 100\n",
            "loss 6.028486728668213 average time 0.019186870249541244 iter num 20\n",
            "loss 4.575042247772217 average time 0.010968669249632513 iter num 40\n",
            "loss 1.967495322227478 average time 0.008256397000028908 iter num 60\n",
            "loss 7.512735366821289 average time 0.006902677075049724 iter num 80\n",
            "loss 4.621047019958496 average time 0.006103582030191319 iter num 100\n",
            "loss 3.725646495819092 average time 0.019817903749753896 iter num 20\n",
            "loss 2.311960220336914 average time 0.011253508649770084 iter num 40\n",
            "loss 12.529151916503906 average time 0.008409011049904318 iter num 60\n",
            "loss 4.8853373527526855 average time 0.0070084537872389776 iter num 80\n",
            "loss 2.40081787109375 average time 0.006140181359733106 iter num 100\n",
            "loss 3.9541733264923096 average time 0.01938884155024425 iter num 20\n",
            "loss 14.428954124450684 average time 0.011061868550132203 iter num 40\n",
            "loss 6.021784782409668 average time 0.008321131750259761 iter num 60\n",
            "loss 1.5568106174468994 average time 0.006963025800314426 iter num 80\n",
            "loss 4.083399772644043 average time 0.006129293070262065 iter num 100\n",
            "loss 4.832017421722412 average time 0.019663948749985137 iter num 20\n",
            "loss 2.7578930854797363 average time 0.011214252700119687 iter num 40\n",
            "loss 6.804068565368652 average time 0.008408572883308808 iter num 60\n",
            "loss 2.661452531814575 average time 0.006998760499936907 iter num 80\n",
            "loss 5.939293384552002 average time 0.006172528510032862 iter num 100\n",
            "loss 5.177114486694336 average time 0.019874069999787025 iter num 20\n",
            "loss 5.187960147857666 average time 0.011268247149837407 iter num 40\n",
            "loss 4.174067497253418 average time 0.008399444099874623 iter num 60\n",
            "loss 3.559760570526123 average time 0.007032273199956762 iter num 80\n",
            "loss 2.9869465827941895 average time 0.00616026369996689 iter num 100\n",
            "loss 17.828582763671875 average time 0.019395188399903417 iter num 20\n",
            "loss 9.854880332946777 average time 0.011141795175080915 iter num 40\n",
            "loss 4.919917106628418 average time 0.008402102683370079 iter num 60\n",
            "loss 8.757140159606934 average time 0.007098235812509302 iter num 80\n",
            "loss 17.27738380432129 average time 0.006230665520015464 iter num 100\n",
            "loss 2.5887675285339355 average time 0.019214018000275244 iter num 20\n",
            "loss 4.246337890625 average time 0.010945669924785762 iter num 40\n",
            "loss 1.8499243259429932 average time 0.008189569666440851 iter num 60\n",
            "loss 12.779714584350586 average time 0.00686760677476741 iter num 80\n",
            "loss 7.805380821228027 average time 0.006064835939760087 iter num 100\n",
            "loss 23.887537002563477 average time 0.019233633950534568 iter num 20\n",
            "loss 4.583518981933594 average time 0.011039590325162862 iter num 40\n",
            "loss 2.2980072498321533 average time 0.008313054983470161 iter num 60\n",
            "loss 2.2503254413604736 average time 0.0069267175250843135 iter num 80\n",
            "loss 2.155613660812378 average time 0.006100277610057674 iter num 100\n",
            "loss 4.1056084632873535 average time 0.019347278450004522 iter num 20\n",
            "loss 2.9525833129882812 average time 0.0110239148501023 iter num 40\n",
            "loss 4.200308799743652 average time 0.008263223550238763 iter num 60\n",
            "loss 5.637271404266357 average time 0.006881389162708728 iter num 80\n",
            "loss 2.7940714359283447 average time 0.00603462617007608 iter num 100\n",
            "loss 3.111453056335449 average time 0.019257875700714067 iter num 20\n",
            "loss 2.4914848804473877 average time 0.011049309175450616 iter num 40\n",
            "loss 3.5940628051757812 average time 0.008362560533714713 iter num 60\n",
            "loss 2.496032238006592 average time 0.00697318697521041 iter num 80\n",
            "loss 1.00519597530365 average time 0.006179717800041545 iter num 100\n",
            "loss 8.426661491394043 average time 0.019653098700109693 iter num 20\n",
            "loss 5.419939041137695 average time 0.011183136325053055 iter num 40\n",
            "loss 6.340888500213623 average time 0.008338840966644057 iter num 60\n",
            "loss 1.832664966583252 average time 0.006921719412548555 iter num 80\n",
            "loss 5.151079177856445 average time 0.006084057920015766 iter num 100\n",
            "loss 3.5025811195373535 average time 0.01935979390036664 iter num 20\n",
            "loss 3.6401987075805664 average time 0.011067800325145071 iter num 40\n",
            "loss 2.7751975059509277 average time 0.008298325166955086 iter num 60\n",
            "loss 5.326277256011963 average time 0.006936369250115604 iter num 80\n",
            "loss 2.9755642414093018 average time 0.006135404090164229 iter num 100\n",
            "loss 7.224984169006348 average time 0.01921531204970961 iter num 20\n",
            "loss 4.657846927642822 average time 0.011008143024719174 iter num 40\n",
            "loss 2.9377126693725586 average time 0.008215308399788531 iter num 60\n",
            "loss 1.4209084510803223 average time 0.006847902024719588 iter num 80\n",
            "loss 7.532551288604736 average time 0.006015155489913013 iter num 100\n",
            "loss 10.217409133911133 average time 0.019463193200499516 iter num 20\n",
            "loss 6.687872886657715 average time 0.01113033665014882 iter num 40\n",
            "loss 2.845899820327759 average time 0.00834547516672804 iter num 60\n",
            "loss 6.650508880615234 average time 0.006930443425153499 iter num 80\n",
            "loss 2.428706645965576 average time 0.006103088860254502 iter num 100\n",
            "loss 13.275651931762695 average time 0.01942223624955659 iter num 20\n",
            "loss 12.475381851196289 average time 0.011109775099976104 iter num 40\n",
            "loss 4.361965656280518 average time 0.008327069916534431 iter num 60\n",
            "loss 3.7564563751220703 average time 0.006957289449928794 iter num 80\n",
            "loss 3.4524154663085938 average time 0.006108369440007664 iter num 100\n",
            "loss 2.077843189239502 average time 0.019283163249747305 iter num 20\n",
            "loss 15.684816360473633 average time 0.011090916374996596 iter num 40\n",
            "loss 2.0345582962036133 average time 0.008309645183423224 iter num 60\n",
            "loss 12.52934455871582 average time 0.006924911600071937 iter num 80\n",
            "loss 7.196924209594727 average time 0.006087562130160222 iter num 100\n",
            "loss 2.804192304611206 average time 0.01946942834983929 iter num 20\n",
            "loss 6.91196346282959 average time 0.011177946849875298 iter num 40\n",
            "loss 7.1620588302612305 average time 0.008403833733245847 iter num 60\n",
            "loss 3.521446943283081 average time 0.007003038187531274 iter num 80\n",
            "loss 8.66097354888916 average time 0.006187271350027004 iter num 100\n",
            "loss 9.048723220825195 average time 0.019474492449990066 iter num 20\n",
            "loss 4.148410797119141 average time 0.01110877180008174 iter num 40\n",
            "loss 7.692351341247559 average time 0.008327071449942498 iter num 60\n",
            "loss 6.258694171905518 average time 0.006938068974932321 iter num 80\n",
            "loss 12.96564769744873 average time 0.0061002604200257335 iter num 100\n",
            "loss 1.110216736793518 average time 0.019458505349757616 iter num 20\n",
            "loss 4.1493449211120605 average time 0.01116456339996148 iter num 40\n",
            "loss 2.107485294342041 average time 0.008391636399998485 iter num 60\n",
            "loss 3.470069408416748 average time 0.0070027809874318335 iter num 80\n",
            "loss 3.179849624633789 average time 0.0061497855500783774 iter num 100\n",
            "loss 2.478484869003296 average time 0.019563923100213286 iter num 20\n",
            "loss 3.071598529815674 average time 0.01115553332510899 iter num 40\n",
            "loss 6.36572265625 average time 0.008346359166656233 iter num 60\n",
            "loss 2.3543829917907715 average time 0.006942360762468525 iter num 80\n",
            "loss 3.028257369995117 average time 0.006088548340012494 iter num 100\n",
            "loss 5.260691165924072 average time 0.019355611949868033 iter num 20\n",
            "loss 0.8145120143890381 average time 0.011096791924865101 iter num 40\n",
            "loss 6.767855644226074 average time 0.008323955666613377 iter num 60\n",
            "loss 5.7127203941345215 average time 0.006933385250158608 iter num 80\n",
            "loss 4.217315196990967 average time 0.006083237270213431 iter num 100\n",
            "loss 3.222543239593506 average time 0.019447421249788023 iter num 20\n",
            "loss 5.891449928283691 average time 0.01105557492492153 iter num 40\n",
            "loss 6.107100486755371 average time 0.008289964533226642 iter num 60\n",
            "loss 3.9173154830932617 average time 0.0068777059125750386 iter num 80\n",
            "loss 1.449590802192688 average time 0.0060311363701839584 iter num 100\n",
            "loss 3.8164401054382324 average time 0.019557005250135262 iter num 20\n",
            "loss 7.975780010223389 average time 0.011164516225107946 iter num 40\n",
            "loss 2.906785011291504 average time 0.00840296418343011 iter num 60\n",
            "loss 3.934217929840088 average time 0.007008132762621244 iter num 80\n",
            "loss 9.333724975585938 average time 0.006138133910171746 iter num 100\n",
            "loss 6.834721565246582 average time 0.01976397094949789 iter num 20\n",
            "loss 5.483177661895752 average time 0.011290572574944235 iter num 40\n",
            "loss 1.65996253490448 average time 0.008482775933225639 iter num 60\n",
            "loss 10.415814399719238 average time 0.0070628483373639025 iter num 80\n",
            "loss 11.974021911621094 average time 0.006182117319913232 iter num 100\n",
            "loss 7.413904666900635 average time 0.019829218300401408 iter num 20\n",
            "loss 1.7370010614395142 average time 0.011277877725387953 iter num 40\n",
            "loss 1.2054893970489502 average time 0.008392815733714087 iter num 60\n",
            "loss 7.031588554382324 average time 0.006981001400299646 iter num 80\n",
            "loss 5.396676063537598 average time 0.006116793210312607 iter num 100\n",
            "loss 8.713164329528809 average time 0.01961250209951686 iter num 20\n",
            "loss 10.06729793548584 average time 0.01118666182483139 iter num 40\n",
            "loss 4.154775619506836 average time 0.008381927049837638 iter num 60\n",
            "loss 2.151564598083496 average time 0.006959843762342643 iter num 80\n",
            "loss 6.355344295501709 average time 0.006104580030005309 iter num 100\n",
            "loss 4.600696563720703 average time 0.0192613770999742 iter num 20\n",
            "loss 1.786787509918213 average time 0.010941542699947604 iter num 40\n",
            "loss 3.727161169052124 average time 0.00820268429997668 iter num 60\n",
            "loss 2.920732021331787 average time 0.0068123892000130585 iter num 80\n",
            "loss 5.1529364585876465 average time 0.005985677540047618 iter num 100\n",
            "loss 3.7559540271759033 average time 0.019846094450076634 iter num 20\n",
            "loss 8.850130081176758 average time 0.011370353374968546 iter num 40\n",
            "loss 21.124265670776367 average time 0.008495812566737489 iter num 60\n",
            "loss 8.422906875610352 average time 0.00705220008762808 iter num 80\n",
            "loss 6.794232368469238 average time 0.00618525973008218 iter num 100\n",
            "loss 3.90090274810791 average time 0.01922621115027141 iter num 20\n",
            "loss 6.264122486114502 average time 0.011005406400090578 iter num 40\n",
            "loss 2.312438488006592 average time 0.008237796483566246 iter num 60\n",
            "loss 1.690648078918457 average time 0.006844377425204584 iter num 80\n",
            "loss 4.346588134765625 average time 0.0060201170602158524 iter num 100\n",
            "loss 4.019686698913574 average time 0.019575698849985203 iter num 20\n",
            "loss 10.841029167175293 average time 0.011154094200173858 iter num 40\n",
            "loss 2.812840461730957 average time 0.008357030833455308 iter num 60\n",
            "loss 5.9796671867370605 average time 0.006943890800039298 iter num 80\n",
            "loss 3.871790885925293 average time 0.0060962347800159475 iter num 100\n",
            "loss 5.64142370223999 average time 0.019319956099934643 iter num 20\n",
            "loss 4.798099517822266 average time 0.011003877324947097 iter num 40\n",
            "loss 2.4150009155273438 average time 0.008223523816620096 iter num 60\n",
            "loss 7.510303497314453 average time 0.006818736512468604 iter num 80\n",
            "loss 9.206486701965332 average time 0.006022780799976317 iter num 100\n",
            "loss 3.4582157135009766 average time 0.019541021199802344 iter num 20\n",
            "loss 2.3790221214294434 average time 0.011161234949850041 iter num 40\n",
            "loss 11.418338775634766 average time 0.00835752604980371 iter num 60\n",
            "loss 3.1672372817993164 average time 0.006938995437394624 iter num 80\n",
            "loss 9.120647430419922 average time 0.006102802579844138 iter num 100\n",
            "loss 2.7993597984313965 average time 0.019602731099439552 iter num 20\n",
            "loss 8.14984130859375 average time 0.01118865772477875 iter num 40\n",
            "loss 1.8820674419403076 average time 0.008390733949757609 iter num 60\n",
            "loss 2.203277826309204 average time 0.006987415062485525 iter num 80\n",
            "loss 1.3173339366912842 average time 0.006120821919976152 iter num 100\n",
            "loss 5.437648296356201 average time 0.019481291000192868 iter num 20\n",
            "loss 6.248522758483887 average time 0.011128576299779525 iter num 40\n",
            "loss 5.7314958572387695 average time 0.008311491516239281 iter num 60\n",
            "loss 3.4541842937469482 average time 0.0069135889997596674 iter num 80\n",
            "loss 3.5812482833862305 average time 0.006077024429905578 iter num 100\n",
            "loss 4.4345173835754395 average time 0.01992053034991841 iter num 20\n",
            "loss 10.767438888549805 average time 0.011359485950197267 iter num 40\n",
            "loss 3.150364398956299 average time 0.008531518832751318 iter num 60\n",
            "loss 3.8943581581115723 average time 0.007058790811970539 iter num 80\n",
            "loss 4.1696553230285645 average time 0.006186950239571161 iter num 100\n",
            "loss 13.805089950561523 average time 0.019296667300659466 iter num 20\n",
            "loss 4.643247604370117 average time 0.010983154676068807 iter num 40\n",
            "loss 2.257089614868164 average time 0.008237600150581177 iter num 60\n",
            "loss 5.482662200927734 average time 0.006838913550291181 iter num 80\n",
            "loss 3.466888427734375 average time 0.006029649080373929 iter num 100\n",
            "loss 2.9250309467315674 average time 0.019807163800214765 iter num 20\n",
            "loss 3.9758167266845703 average time 0.011300986275637115 iter num 40\n",
            "loss 17.964950561523438 average time 0.008452757500344887 iter num 60\n",
            "loss 4.861555099487305 average time 0.007012743300219881 iter num 80\n",
            "loss 3.091191291809082 average time 0.006180132040026365 iter num 100\n",
            "loss 4.296642303466797 average time 0.019418995699379594 iter num 20\n",
            "loss 5.091033458709717 average time 0.01104376412476995 iter num 40\n",
            "loss 3.2518470287323 average time 0.008246337400123593 iter num 60\n",
            "loss 3.504786491394043 average time 0.006882732875055808 iter num 80\n",
            "loss 6.161046028137207 average time 0.006056872749904869 iter num 100\n",
            "loss 8.703320503234863 average time 0.019821323950236548 iter num 20\n",
            "loss 2.3401193618774414 average time 0.011307041774671234 iter num 40\n",
            "loss 2.457629919052124 average time 0.008473579033428299 iter num 60\n",
            "loss 2.7705514430999756 average time 0.007024837012704665 iter num 80\n",
            "loss 3.4689483642578125 average time 0.006204560120240785 iter num 100\n",
            "loss 10.450794219970703 average time 0.019186866199743236 iter num 20\n",
            "loss 3.293842315673828 average time 0.010929366299751563 iter num 40\n",
            "loss 3.2611427307128906 average time 0.008266829566249119 iter num 60\n",
            "loss 2.9935362339019775 average time 0.0068671653747514935 iter num 80\n",
            "loss 5.737464904785156 average time 0.0060516301398456564 iter num 100\n",
            "loss 7.819250583648682 average time 0.019660485050553688 iter num 20\n",
            "loss 5.425806999206543 average time 0.011228240925447608 iter num 40\n",
            "loss 2.232855796813965 average time 0.008408096567169803 iter num 60\n",
            "loss 12.48416805267334 average time 0.006989180350501556 iter num 80\n",
            "loss 1.1640396118164062 average time 0.0061456028703833 iter num 100\n",
            "loss 2.4830198287963867 average time 0.019518331750077778 iter num 20\n",
            "loss 5.5412397384643555 average time 0.01112877027480863 iter num 40\n",
            "loss 5.5737409591674805 average time 0.008300746683259301 iter num 60\n",
            "loss 4.386612892150879 average time 0.0069230219121891425 iter num 80\n",
            "loss 4.110860347747803 average time 0.006095101609607809 iter num 100\n",
            "loss 3.7918314933776855 average time 0.019535360849477 iter num 20\n",
            "loss 8.223577499389648 average time 0.011172673275177658 iter num 40\n",
            "loss 4.272256851196289 average time 0.008348407049925299 iter num 60\n",
            "loss 1.581984043121338 average time 0.0069458478125852706 iter num 80\n",
            "loss 9.255782127380371 average time 0.006125849000090966 iter num 100\n",
            "loss 4.238364219665527 average time 0.01974454115079425 iter num 20\n",
            "loss 2.5729100704193115 average time 0.011343908375420143 iter num 40\n",
            "loss 3.053316593170166 average time 0.008496662683561832 iter num 60\n",
            "loss 2.105307102203369 average time 0.007046590699974331 iter num 80\n",
            "loss 2.682408332824707 average time 0.006169414580072043 iter num 100\n",
            "loss 7.002495288848877 average time 0.01945165225006349 iter num 20\n",
            "loss 4.562705039978027 average time 0.011052167774869304 iter num 40\n",
            "loss 8.306456565856934 average time 0.008286933349760753 iter num 60\n",
            "loss 0.839440643787384 average time 0.006934535862183111 iter num 80\n",
            "loss 1.283221960067749 average time 0.006110903379740193 iter num 100\n",
            "loss 2.5510504245758057 average time 0.01970509464990755 iter num 20\n",
            "loss 1.1506253480911255 average time 0.01130562322523474 iter num 40\n",
            "loss 2.9594340324401855 average time 0.00845814381694557 iter num 60\n",
            "loss 11.467889785766602 average time 0.007034342899987678 iter num 80\n",
            "loss 2.5750412940979004 average time 0.00619879387995752 iter num 100\n",
            "loss 9.910929679870605 average time 0.019337452499894426 iter num 20\n",
            "loss 7.803377151489258 average time 0.011054855324437085 iter num 40\n",
            "loss 4.137931823730469 average time 0.008295507382960447 iter num 60\n",
            "loss 3.3244831562042236 average time 0.006946596424313611 iter num 80\n",
            "loss 3.0037810802459717 average time 0.006125600519662839 iter num 100\n",
            "loss 6.19685697555542 average time 0.019714015100180404 iter num 20\n",
            "loss 2.1915550231933594 average time 0.01128513252515404 iter num 40\n",
            "loss 4.497885704040527 average time 0.008442237667137913 iter num 60\n",
            "loss 3.2161834239959717 average time 0.007020960962927347 iter num 80\n",
            "loss 4.441037178039551 average time 0.00616855510037567 iter num 100\n",
            "loss 2.7133800983428955 average time 0.019382534300530097 iter num 20\n",
            "loss 12.634215354919434 average time 0.011057396324940783 iter num 40\n",
            "loss 8.72397232055664 average time 0.008278073683201607 iter num 60\n",
            "loss 2.3702950477600098 average time 0.006909264437399543 iter num 80\n",
            "loss 2.4791219234466553 average time 0.006102079589982168 iter num 100\n",
            "loss 16.648616790771484 average time 0.02003066770012083 iter num 20\n",
            "loss 2.272456407546997 average time 0.011710321450300399 iter num 40\n",
            "loss 4.680639266967773 average time 0.008764060416676026 iter num 60\n",
            "loss 2.705845832824707 average time 0.007250137212577101 iter num 80\n",
            "loss 6.373379707336426 average time 0.00636202190988115 iter num 100\n",
            "loss 3.7247376441955566 average time 0.01944233090071066 iter num 20\n",
            "loss 5.478450775146484 average time 0.011111254200659459 iter num 40\n",
            "loss 10.401747703552246 average time 0.008330629150320117 iter num 60\n",
            "loss 2.5379860401153564 average time 0.006931259425437019 iter num 80\n",
            "loss 1.1735334396362305 average time 0.006104524190304801 iter num 100\n",
            "loss 6.477016925811768 average time 0.020016421300169895 iter num 20\n",
            "loss 10.905302047729492 average time 0.011416637299953436 iter num 40\n",
            "loss 8.063506126403809 average time 0.008526860949859837 iter num 60\n",
            "loss 3.8699936866760254 average time 0.007111524900028599 iter num 80\n",
            "loss 3.7783355712890625 average time 0.006242107750003925 iter num 100\n",
            "loss 4.213404655456543 average time 0.01986586525054008 iter num 20\n",
            "loss 2.103682518005371 average time 0.011320247874937194 iter num 40\n",
            "loss 10.102226257324219 average time 0.008432082966707336 iter num 60\n",
            "loss 5.958486557006836 average time 0.007015741925351903 iter num 80\n",
            "loss 0.4981882572174072 average time 0.006175893910549349 iter num 100\n",
            "loss 2.634415626525879 average time 0.019464697950388653 iter num 20\n",
            "loss 6.73411750793457 average time 0.011110604225177668 iter num 40\n",
            "loss 4.40626335144043 average time 0.008325206217098943 iter num 60\n",
            "loss 5.73225212097168 average time 0.006956789550531539 iter num 80\n",
            "loss 1.574407935142517 average time 0.006188395600256627 iter num 100\n",
            "loss 31.384254455566406 average time 0.019835549349954816 iter num 20\n",
            "loss 6.580836772918701 average time 0.011329077200207394 iter num 40\n",
            "loss 5.481097221374512 average time 0.00847388478347663 iter num 60\n",
            "loss 5.951796531677246 average time 0.0070441690374536845 iter num 80\n",
            "loss 3.144113540649414 average time 0.0062052457100071476 iter num 100\n",
            "loss 5.50532341003418 average time 0.019372598498375738 iter num 20\n",
            "loss 11.141837120056152 average time 0.011165178524242947 iter num 40\n",
            "loss 3.6506507396698 average time 0.008335246516071492 iter num 60\n",
            "loss 5.212404727935791 average time 0.00698590136189523 iter num 80\n",
            "loss 1.024845838546753 average time 0.006141910069636651 iter num 100\n",
            "loss 13.503347396850586 average time 0.019771980900986818 iter num 20\n",
            "loss 3.2934494018554688 average time 0.011265378900861833 iter num 40\n",
            "loss 6.495070457458496 average time 0.008642887167297886 iter num 60\n",
            "loss 3.9084088802337646 average time 0.007165091325532558 iter num 80\n",
            "loss 3.8345181941986084 average time 0.006262584030264406 iter num 100\n",
            "loss 3.1735358238220215 average time 0.019579023850019438 iter num 20\n",
            "loss 8.834847450256348 average time 0.011108809374309202 iter num 40\n",
            "loss 1.9185545444488525 average time 0.008398242382827447 iter num 60\n",
            "loss 1.6158748865127563 average time 0.00699810078695009 iter num 80\n",
            "loss 5.882436275482178 average time 0.006183401069501997 iter num 100\n",
            "loss 3.9190309047698975 average time 0.019779175799703806 iter num 20\n",
            "loss 12.263895988464355 average time 0.011336833374298295 iter num 40\n",
            "loss 4.200894355773926 average time 0.008510534683227888 iter num 60\n",
            "loss 7.621070384979248 average time 0.007070327774999896 iter num 80\n",
            "loss 2.718580722808838 average time 0.006238001639940194 iter num 100\n",
            "loss 1.2740339040756226 average time 0.019498568201015587 iter num 20\n",
            "loss 2.1948912143707275 average time 0.011235690599642112 iter num 40\n",
            "loss 13.57833194732666 average time 0.008430955999331975 iter num 60\n",
            "loss 7.0475969314575195 average time 0.007097348674324167 iter num 80\n",
            "loss 3.9658761024475098 average time 0.006294642409484368 iter num 100\n",
            "loss 11.82608699798584 average time 0.02059235339911538 iter num 20\n",
            "loss 7.778258323669434 average time 0.011889860049268463 iter num 40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-3dfcc4a6c3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EuCall_1_3.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m           \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m           \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87791f6a-eb0c-4803-ef70-e6436271920e"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[5, 110.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[42.6336]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3366903d-5a36-4cf3-877f-5d07f5dc78e0"
      },
      "source": [
        "inputs = torch.tensor([[5, 110.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  4.7450,  -0.3646,   0.7592,  73.5362,  -0.8495, 210.4249]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwgeVDsA_Mr"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "49e4c537-055d-45a7-e76d-08b66561f66b"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    # inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs = torch.tensor([[5, 110.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdf31214910>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGk5Hw64fMdh",
        "outputId": "6215936f-29ea-4095-8a00-9c88a097f903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "## Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-63026e7fa71e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdeltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_delta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prices'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-63026e7fa71e>\u001b[0m in \u001b[0;36mcompute_delta\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minputs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# normalize the parameter to range [0-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (18) must match the size of tensor b (6) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLStvS2qCSjm"
      },
      "source": [
        "compute_delta(110)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1I8COnUxnz"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySrey9KzB0AF"
      },
      "source": [
        "compute_delta(110).item()  # It's not 0.5!! SOS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMyME_1WCGZz"
      },
      "source": [
        "compute_delta(102).item() # Close to 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB8LPwfBMHQ"
      },
      "source": [
        "# Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoaOyCDxQy0"
      },
      "source": [
        "##Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsXgOwWZ_ru"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lca2xrBh9a"
      },
      "source": [
        "# Vega"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muozc-hzhSGA"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KATxBCAdlFt"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a timev\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05]*3]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 0.0, 110.0, sigma, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEiAredqQGxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}