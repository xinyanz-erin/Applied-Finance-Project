{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "European_Call.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "TY_9g3tbdLiY",
        "u2_89jOknwjH",
        "rXT4Bg0wdL7l"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/From%20Colab/European_Call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "e59e5ec5-f227-4a37-ace2-25f9eb1d69ef"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0   8144      0 --:--:-- --:--:-- --:--:--  8144\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9 MB 33 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 49.6 MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "### Train(Erin Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxT9Eida-c_"
      },
      "source": [
        "# ################################# TEST ########################################\n",
        "# %%writefile cupy_dataset.py\n",
        "\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import random\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "#         paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           X = cupy.array([])\n",
        "#           K_rand = cupy.random.rand(1)[0]\n",
        "#           B_rand = cupy.random.rand(1)[0]\n",
        "#           r_rand = cupy.random.rand(1)[0]\n",
        "#           for i in range(0,self.N_STOCKS):\n",
        "#             X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "#           X = X.reshape(self.N_STOCKS,6)\n",
        "#           X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "#           #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "#           #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "#           # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#           #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "#           # make sure the Barrier is smaller than the Strike price\n",
        "#           # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#           for i in range(self.N_STOCKS):\n",
        "#             paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "#           stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "#           rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "#           #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "#           #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "#           #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#           stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "#           cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "#           num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "#           randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                                         num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "#           b1_r = randoms_gpu[:,0]\n",
        "#           b2_r = randoms_gpu[:,1]\n",
        "#           randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "#           for i in range(interval):\n",
        "#             if i % 2 == 0:\n",
        "#                 ind = int(i/2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "#             else:\n",
        "#                 ind = int(i//2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "#           randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "#           o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "#           Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# # ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# # for i in ds:\n",
        "# #     print(i[0])\n",
        "# ################################# TEST ########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dZnWTTfbf1"
      },
      "source": [
        "### Train (European Call option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeREuPw0fguQ",
        "outputId": "c6dc03ce-c641-4bda-edf9-4574645081d3"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def European_call_option(d_s, T, K, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    #tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        h = T[batch_id] / N_STEPS\n",
        "        tmp1 = r[batch_id]*T[batch_id]/N_STEPS \n",
        "        tmp2 = math.exp(-r[batch_id]*T[batch_id]) # discount\n",
        "        tmp3 = math.sqrt(T[batch_id]/N_STEPS)\n",
        "        #running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "          s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price #save2\n",
        "          #s_curr = s_curr * math.exp((r[batch_id] - (1/2)*sigma[batch_id]**2)*h + sigma[batch_id] * tmp3 * d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]) #save1\n",
        "          #running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "          #if i==0 and batch_id == 2:\n",
        "          #    print(s_curr)\n",
        "          #if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "          #    break\n",
        "        #payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        payoff = s_curr - K[batch_id] if s_curr > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        #self.N_STEPS = 365\n",
        "        self.N_STEPS = 10000\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        #self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        #paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 5), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          X = cupy.array([])\n",
        "          #T_rand = cupy.random.rand(1)[0]\n",
        "          #K_rand = cupy.random.rand(1)[0]\n",
        "          #B_rand = cupy.random.rand(1)[0]\n",
        "          #r_rand = cupy.random.rand(1)[0]\n",
        "          for i in range(0, self.N_STOCKS):\n",
        "            #X =  cupy.concatenate((X, cupy.array([K_rand,B_rand]), cupy.random.rand(3), cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "            #X = cupy.concatenate((X, cupy.random.rand(2), cupy.array([200]), cupy.random.rand(3))) #[T, K, S0, sigma, mu, r]\n",
        "            #X = cupy.concatenate((X, cupy.random.rand(6))) #[T, K, S0, sigma, mu, r]\n",
        "            X = cupy.concatenate((X, cupy.array([1]), cupy.random.rand(5))) #[T, K, S0, sigma, mu, r]\n",
        "          \n",
        "          X = X.reshape(self.N_STOCKS, 6)\n",
        "          #X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #[T, K, S0, sigma, mu, r]\n",
        "          X = X * ((cupy.array([1.0, 150.0, 150.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "            #paras[op, i*5:(i+1)*5] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          if self.N_STOCKS != 1:\n",
        "            stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "            cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "            num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "            randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                          num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "            b1_r = randoms_gpu[:,0]\n",
        "            b2_r = randoms_gpu[:,1]\n",
        "            randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "            interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "            for i in range(interval):\n",
        "              if i % 2 == 0:\n",
        "                  ind = int(i/2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "              else:\n",
        "                  ind = int(i//2)\n",
        "                  randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          if self.N_STOCKS == 1:\n",
        "            randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          \n",
        "          European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "#ds = NumbaOptionDataSet(2, number_path = 100000, batch = 2, seed = random.randint(0,100), stocks=1)\n",
        "#for i in ds:\n",
        "#    print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49dc3856-4df2-4be1-de80-e3433a57a568"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0, 0.1, 200.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 150.0, 150.0, 0.4, 0.2, 0.2]*1)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53090b1e-b87e-426c-a121-16aade5c5ced"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.6-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30 kB 41.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 61 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 92 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 112 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 122 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 143 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 153 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 184 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 194 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 204 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 215 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 225 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 232 kB 16.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "0dfbc2e8-2c91-4db2-fdf6-7a16bb727f07"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024*4, batch = 32, stocks = 1)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=1000)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 1524.402587890625 average time 0.0195295595500113 iter num 20\n",
            "loss 1077.0811767578125 average time 0.011225993750008456 iter num 40\n",
            "loss 988.1787109375 average time 0.008436475933342536 iter num 60\n",
            "loss 663.1386108398438 average time 0.007055909300004259 iter num 80\n",
            "loss 779.9702758789062 average time 0.006208462930003406 iter num 100\n",
            "loss 46.67201232910156 average time 0.01940017870001043 iter num 20\n",
            "loss 22.54370880126953 average time 0.011123689875000764 iter num 40\n",
            "loss 19.555679321289062 average time 0.008377782550002166 iter num 60\n",
            "loss 12.625123977661133 average time 0.006992404450001289 iter num 80\n",
            "loss 13.249013900756836 average time 0.006201642740002171 iter num 100\n",
            "loss 10.894521713256836 average time 0.019117149949994427 iter num 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5b23730e7b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m           \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m           \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a1853d-773c-4d89-8e77-a0ecca2e1511"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall2_const_T_1_4.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0eb66e-3bc4-4d3f-c132-ae178ff42a1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7f67d1-cba8-4826-e6e1-5a975617a0d6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'EuCall2_const_T_1_3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d65e7f-1658-4ea8-9ed0-fa021f81c562"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "### Continue to train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1009692c-7d5b-4402-b5e5-fa1ec137587d"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 100, number_path = 51200, batch = 32, stocks = 1)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=20)\n",
        "\n",
        "model_save_name = 'EuCall2_const_T_1_5.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 3.739781379699707 average time 0.12451430550036094 iter num 20\n",
            "loss 0.25414562225341797 average time 0.06368929530017339 iter num 40\n",
            "loss 0.0736747533082962 average time 0.04341023393350649 iter num 60\n",
            "loss 0.02601216360926628 average time 0.03326455397518657 iter num 80\n",
            "loss 0.015108730643987656 average time 0.02718651799015788 iter num 100\n",
            "loss 0.06354039162397385 average time 0.12845637885002362 iter num 20\n",
            "loss 0.06600115448236465 average time 0.06567673607505639 iter num 40\n",
            "loss 0.020860284566879272 average time 0.04474277910009429 iter num 60\n",
            "loss 0.01636497862637043 average time 0.034273154650145445 iter num 80\n",
            "loss 0.017406577244400978 average time 0.027985578600109876 iter num 100\n",
            "loss 0.023038573563098907 average time 0.12883953089994976 iter num 20\n",
            "loss 0.04388163611292839 average time 0.06584414339977229 iter num 40\n",
            "loss 0.030836032703518867 average time 0.04486050916645278 iter num 60\n",
            "loss 0.0171180572360754 average time 0.03437326083728749 iter num 80\n",
            "loss 0.05121181905269623 average time 0.02806737984987194 iter num 100\n",
            "loss 0.03877648711204529 average time 0.12856772830009505 iter num 20\n",
            "loss 0.03062572330236435 average time 0.06571608197500609 iter num 40\n",
            "loss 0.022143784910440445 average time 0.044771413583324225 iter num 60\n",
            "loss 0.01303704734891653 average time 0.03429625622493404 iter num 80\n",
            "loss 0.012503995560109615 average time 0.028001974879916815 iter num 100\n",
            "loss 0.022748714312911034 average time 0.1290993565001372 iter num 20\n",
            "loss 0.028023410588502884 average time 0.06596208597516125 iter num 40\n",
            "loss 0.013596422970294952 average time 0.044949155000055425 iter num 60\n",
            "loss 0.018277116119861603 average time 0.03442710502506543 iter num 80\n",
            "loss 0.0222991481423378 average time 0.028136434920106694 iter num 100\n",
            "loss 0.06754269450902939 average time 0.12856647239987068 iter num 20\n",
            "loss 0.026747841387987137 average time 0.06578122689993507 iter num 40\n",
            "loss 0.030260492116212845 average time 0.044812035666594356 iter num 60\n",
            "loss 0.015357468277215958 average time 0.034315688474930536 iter num 80\n",
            "loss 0.016390172764658928 average time 0.028023891419998108 iter num 100\n",
            "loss 0.0154084712266922 average time 0.12853801335004392 iter num 20\n",
            "loss 0.010480832308530807 average time 0.06571066827500545 iter num 40\n",
            "loss 0.036318644881248474 average time 0.044755930350008084 iter num 60\n",
            "loss 0.026585552841424942 average time 0.03429826851236158 iter num 80\n",
            "loss 0.02223633974790573 average time 0.028011485539882416 iter num 100\n",
            "loss 0.1477663516998291 average time 0.12910483224986818 iter num 20\n",
            "loss 0.0554761178791523 average time 0.06601274939994255 iter num 40\n",
            "loss 0.038797855377197266 average time 0.04496451751662486 iter num 60\n",
            "loss 0.015126812271773815 average time 0.034427053612466806 iter num 80\n",
            "loss 0.01865576207637787 average time 0.028114506369984155 iter num 100\n",
            "loss 0.12296918779611588 average time 0.12853582310017372 iter num 20\n",
            "loss 0.0418642982840538 average time 0.06568286430015177 iter num 40\n",
            "loss 0.039670877158641815 average time 0.04475007250008881 iter num 60\n",
            "loss 0.019768714904785156 average time 0.034274760987523224 iter num 80\n",
            "loss 0.03179421275854111 average time 0.02798948116000247 iter num 100\n",
            "loss 0.04382338374853134 average time 0.1292621312501069 iter num 20\n",
            "loss 0.275496244430542 average time 0.06614566015009586 iter num 40\n",
            "loss 0.10910443216562271 average time 0.045057721800094444 iter num 60\n",
            "loss 0.018527355045080185 average time 0.03450526206265749 iter num 80\n",
            "loss 0.017979029566049576 average time 0.02818358149015694 iter num 100\n",
            "loss 0.05067797005176544 average time 0.12872092200013868 iter num 20\n",
            "loss 0.06317603588104248 average time 0.06580961732515789 iter num 40\n",
            "loss 0.038599662482738495 average time 0.04494678818346074 iter num 60\n",
            "loss 0.017292849719524384 average time 0.03441887786252664 iter num 80\n",
            "loss 0.023687763139605522 average time 0.028114043969944758 iter num 100\n",
            "loss 0.022830704227089882 average time 0.12839752549998593 iter num 20\n",
            "loss 0.024620845913887024 average time 0.06570933799994236 iter num 40\n",
            "loss 0.018113091588020325 average time 0.04476192101668251 iter num 60\n",
            "loss 0.022156044840812683 average time 0.034287002800010666 iter num 80\n",
            "loss 0.025540489703416824 average time 0.028040032939970844 iter num 100\n",
            "loss 0.28646397590637207 average time 0.1290320453500499 iter num 20\n",
            "loss 0.2844439744949341 average time 0.06595767659991907 iter num 40\n",
            "loss 0.34789663553237915 average time 0.04493012021660737 iter num 60\n",
            "loss 0.05005231872200966 average time 0.03442186511242653 iter num 80\n",
            "loss 0.030153848230838776 average time 0.02810670413988191 iter num 100\n",
            "loss 0.038795746862888336 average time 0.129048511900055 iter num 20\n",
            "loss 0.009362712502479553 average time 0.06598011552505341 iter num 40\n",
            "loss 0.035720422863960266 average time 0.04497008240011079 iter num 60\n",
            "loss 0.02382553368806839 average time 0.03445361661254083 iter num 80\n",
            "loss 0.01628524251282215 average time 0.02813403570000446 iter num 100\n",
            "loss 0.0802735835313797 average time 0.12898706400010268 iter num 20\n",
            "loss 0.050619035959243774 average time 0.0659493632000249 iter num 40\n",
            "loss 0.04002436622977257 average time 0.04492861283336727 iter num 60\n",
            "loss 0.03246791288256645 average time 0.03454501023752528 iter num 80\n",
            "loss 0.01068244781345129 average time 0.028214437259975966 iter num 100\n",
            "loss 0.04742417857050896 average time 0.12849659024986976 iter num 20\n",
            "loss 0.0191362202167511 average time 0.06567986927502716 iter num 40\n",
            "loss 0.03119211457669735 average time 0.04475818856669018 iter num 60\n",
            "loss 0.02191813476383686 average time 0.03430644872496487 iter num 80\n",
            "loss 0.013837523758411407 average time 0.028015298079935747 iter num 100\n",
            "loss 0.05882081761956215 average time 0.12906207109981552 iter num 20\n",
            "loss 0.028188753873109818 average time 0.06605000304994064 iter num 40\n",
            "loss 0.04715922474861145 average time 0.04501975646659654 iter num 60\n",
            "loss 0.019762283191084862 average time 0.03450500159999592 iter num 80\n",
            "loss 0.01674669049680233 average time 0.028198560009968787 iter num 100\n",
            "loss 0.07712892442941666 average time 0.128788999399967 iter num 20\n",
            "loss 0.046721674501895905 average time 0.06589385965003203 iter num 40\n",
            "loss 0.13493937253952026 average time 0.04489345453336379 iter num 60\n",
            "loss 0.0628805160522461 average time 0.03440257220008789 iter num 80\n",
            "loss 0.011698950082063675 average time 0.02810202410006241 iter num 100\n",
            "loss 0.12630444765090942 average time 0.12929565765007284 iter num 20\n",
            "loss 0.06433742493391037 average time 0.06618202800004838 iter num 40\n",
            "loss 0.03189800679683685 average time 0.045136240316727104 iter num 60\n",
            "loss 0.020157035440206528 average time 0.034595977862522886 iter num 80\n",
            "loss 0.018298640847206116 average time 0.028259927989947756 iter num 100\n",
            "loss 0.3143196702003479 average time 0.12867101620049654 iter num 20\n",
            "loss 0.4215439558029175 average time 0.06582599690045754 iter num 40\n",
            "loss 0.04097738862037659 average time 0.04487746305039764 iter num 60\n",
            "loss 0.03630460426211357 average time 0.03436907077525575 iter num 80\n",
            "loss 0.03228038549423218 average time 0.028072068730289175 iter num 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f88fef-fc03-4faa-b152-5656ab2c5aa3"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[18.0840]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb9f07c-7bc1-44d8-de75-7c9ea2f9b581"
      },
      "source": [
        "inputs = torch.tensor([[10, 110.0, 110.0, 0.35, 0.1, 0.05]*1]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  7.2341,  -0.5060,   0.5023,  -9.1928, -18.1935,  -5.4768]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwgeVDsA_Mr"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "0a052411-4ade-4a7a-9d94-19aef910156c"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    # inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f094810f690>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGk5Hw64fMdh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "af157c3c-b0ee-46b7-9aff-bf40ea062ab2"
      },
      "source": [
        "## Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.5\n",
        "    #inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    #inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs1 = torch.tensor([1, 110, S, 0.35, 0.1, 0.05]).cuda()\n",
        "    inputs2 = torch.tensor([1, 110, S + epsilon, 0.35, 0.1, 0.05]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0b38d12f50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3G8c+XLCSBELbIHgIIAoJsEbRabau2Ila0rRWtVtGKXWyt2tvaaq3X9tba3apXpS2iVgW1tdKKYl0qrVYlyCI7AVkCARKWbGTP9/4xgzdiwhJyciaZ5/16zSszZ07Ck5Nhnjnb75i7IyIi8atD2AFERCRcKgIRkTinIhARiXMqAhGROKciEBGJcyoCEZE4F1gRmNksM9tlZiuaeN7M7Hdmlmdmy81sfFBZRESkaYkB/uzZwH3Ao008PxkYGr1NAh6Ifj2knj17enZ2dsskFBGJE4sXLy5y98zGngusCNx9oZllH2KWqcCjHjmj7S0z62pmfdy94FA/Nzs7m9zc3BZMKiLS/pnZ5qaeC3MfQT9ga4PH+dFpH2FmM8ws18xyCwsLWyWciEi8aBM7i919prvnuHtOZmajazYiItJMYRbBNmBAg8f9o9NERKQVhVkE84AvR48eOgUoPtz+ARERaXmB7Sw2syeBTwA9zSwf+BGQBODuDwLzgfOAPGA/MD2oLCIi0rQgjxq69DDPO/CNoP59ERE5Mm1iZ7GIiAQnyBPKRETkKNTW1bO7vJpdJVXs3V9NWVUtpZU1lFbWUlpZy6eGH8eYAV1b/N9VEYiItJK95dW8v7uczbvLyd9TwfbiCrbvq2RnSSVFZVXsLq/mUBeNzEzvqCIQEYl1xRU1vF9UzvtFZWwq2s+m3eVsKipn0+79FFfUfGjeHp2S6dM1hf7dUhmX1Y3j0juSGb1175RMekoi6SlJpKck0ik5kYQOFkhmFYGISDPU1tXzflE5qwpKWLOjlNUFJawuKGFnSdUH85hB34xUBvXsxGfH9CG7R6fIrWcn+ndLJSUpIcTf4P+pCEREGuHulFXVUlhaRWFpFUVl1WzeU07erjLW7Sxl3c4yqmvrAUhKMIZkdua0IT0Z1judwT07MTizEwO6p9ExMTbe7A9FRSAicaemrp5dpVXsiG6j31FcyfbiCnaWVLKzpIpdpZUUllZRWVP/ke/t3SWFob06c+WpAxnRpwsj+nRhSGZnkhPb7kGYKgIRaZcqa+rYUBj59L5hVzlb9uxny579FBRXUFhaRf1BO2XTkhPonZFCr/QUJmR1+2Bb/XHpKfTs3JGe6cn07ZpKl5SkcH6hAKkIRKRdKCqr4vW1hfxzXSErtxWzaXf5B2/2CR2Mvl1TGNAtjTOGZtInI4U+XVPpnZFC34zI1y4piZgFszM21qkIRKTNyt+7nxdX7GD+ewW8u2UfEDnEcnxWV84/qQ/DeqdzQq90snt2Iimh7W66CZqKQETalNLKGuYt285Tufks2xp58z+xbxduOmcYnxp+HCP7dKFDQIdZtlcqAhGJee7O8vxinnxnC/OWbWd/dR3De6fzvXOHM3lUb7J7dgo7YpumIhCRmFVZU8e8pduZ/eYmVhWUkJqUwAVj+nLppCzG9M+I2236LU1FICIxp6q2jrmLtnLfq3nsKq1ieO90fnLhKKaO7Ut6OzxqJ2wqAhGJGbV19fxlyTbueXk92/ZVMDG7O7+9ZCynDumhT/8BUhGISOjcnRdW7OBXL61lQ2E5J/XP4K7PjebjQ3uqAFqBikBEQuPu/Gt9Eb9YsJb3thVz/HGdefDyCXzmxF4qgFakIhCRVufuvLpmF/e+msfSrfvo1zWVX148hovG9QtshE1pmopARFpNXb3z4ood3PdaHqsLSujXNZUfXziKL+b0bxODs7VXKgIRCVxNXT3zlm7n/n/msbGwnME9O/HLi8cwdWxfnfEbA1QEIhKYqto6nlmcz4Ovb2DrngqG907nvsvGMXlUH20CiiEqAhFpcVW1dTz59hYefH0jO0oqGTOgKz86/0TOGnGcdgLHIBWBiLSYj5wHMKg7v7x4DKcdr/MAYpmKQESOmbvz8upd/OyF1WwoLGdM/wzu/vxJKoA2QkUgIsdk8+5y7pi3ktfWFuo8gDZKRSAizVJf7zy0cCO/eXkdyQkduG3KCK78WLaOAmqDVAQictR2FFdy01NLeXPDbiaP6s0dF5xIry4pYceSZlIRiMhRWbByB9/783Kqaur5+edP4uKc/toM1MapCETkiFRU1/Hj51fxxNtbGN0vg3umjWVwZuewY0kLUBGIyGGt3F7MDXOWkrerjOvOHMzN55xAcqL2BbQXKgIRaZK7M+uNTdz9whq6piXxp2smcfrQnmHHkhamIhCRRu0uq+I7Ty/jtbWFnD2iFz//wkl075QcdiwJQKDrdmZ2rpmtNbM8M7ulkeezzOw1M1tiZsvN7Lwg84jIkXkzr4jJ9/yLNzbs5s6pJ/L7L09QCbRjga0RmFkCcD9wDpAPLDKzee6+qsFstwFPufsDZjYSmA9kB5VJRA6turaeX720lpn/2signp2YPX0iI/t2CTuWBCzITUMTgTx33whgZnOAqUDDInDgwKssA9geYB4ROYS8XWXcMGcJK7eXcNmkLG6bMoK0ZG09jgdB/pX7AVsbPM4HJh00zx3AS2b2TaATcHZjP8jMZgAzALKyslo8qEg8c3eeeGcLP/77KlKTEph5xQQ+fWLvsGNJKwr7+K9Lgdnu3h84D3jMzD6Syd1nunuOu+dkZma2ekiR9qqorIoZjy3m1mdXcHJ2dxZ8+wyVQBwKco1gGzCgweP+0WkNXQOcC+Du/zGzFKAnsCvAXCICPL+8gB8+t4KyylpumzKCq08bRAddLCYuBVkEi4ChZjaISAFMAy47aJ4twFnAbDMbAaQAhQFmEol7e8qruf25Ffx9eQEn9c/gVxePYWiv9LBjSYgCKwJ3rzWz64EFQAIwy91XmtmdQK67zwNuBn5vZjcS2XF8lbt7UJlE4t0/1+7iO08vp7iimu98ehhfPXMIiRotNO4FekiAu88nckhow2m3N7i/CjgtyAwiErl4/C8XrOWhhRsZ3judR6/WYaHy/3RsmEg7t3XPfr41ZwlLtuzjS5Oy+OH5I0lJSgg7lsQQFYFIO/biigK++8xy3OH+y8Yz5aQ+YUeSGKQiEGmHKmvq+On81Tz6n82M6Z/BvZeOJ6tHWtixJEapCETamdUFJdwwZwnrdpZxzemD+N65wzVktBySikCknaivdx5+MzJkdEZaEo9cPZEzh+kETDk8FYFIO7CzpJLvPL2Mf60v4uwRvbj786Pp0blj2LGkjVARiLRxL67YwS1/WU5lTR0/uXAUX5qUpWsIy1FREYi0UXX1zi8WrOXB1zcwul8Gv502liG6hrA0g4pApA0q3l/DN+csYeG6Qi6blMUdnz1RO4Sl2VQEIm3M2h2lzHgsl+37KvjpRaO5bJKGZpdjoyIQaUMWb97LVbPeISU5gTkzTmHCwO5hR5J2QEUg0kYs2rSHq2a9Q2Z6R5649hT6dk0NO5K0EyoCkTbg7Y27mT57Eb27pPDEtafQOyMl7EjSjqgIRGLcok17uOrhRfTtmsKT157CcV1UAtKyVAQiMWxDYRlfeSSXPhkpPDnjFI5LVwlIy9PxZiIxqqisiukPLyKxgzF7+kSVgARGawQiMaiypo6vPJLLzpJK5sw4RSOHSqBUBCIxpr7e+facpSzL38cDX5rAuKxuYUeSdk6bhkRizF0vrObFlTu4bcpIzh3VO+w4EgdUBCIx5JE3N/H7f73PVR/L5urTssOOI3FCRSASI55fXsAdf1vJOSN78cPzR2oEUWk1KgKRGPCfDbu5ce5SJmR1495Lx5HQQSUgrUdFIBKyNTtKmPFoLlk90vjDlTmkJCWEHUnijIpAJEQFxRVMf3gRqckJPHL1RLqmJYcdSeKQikAkJKWVNUx/eBElFTU8PP1k+mkQOQmJziMQCUFVbR3XPbaYvF1lzLrqZE7smxF2JIljKgKRVlZf79z01DLe3LCbX39xDGcMyww7ksQ5bRoSaWU/eX41zy8v4PuTh/O58f3DjiOiIhBpTU++s4VZb7zP9NOymXHG4LDjiAAqApFWs3jzHm5/bgVnDMvktik6YUxih4pApBXsKq3ka396lz4Zqdw7TSeMSWxREYgErKaunuufWEJJZQ0PXTGBjLSksCOJfEigRWBm55rZWjPLM7Nbmpjni2a2ysxWmtkTQeYRCcPPXljDO+/v4WefO4kRfbqEHUfkIwI7fNTMEoD7gXOAfGCRmc1z91UN5hkKfB84zd33mtlxQeURCcO8Zdv5478jo4leOK5f2HFEGhXkGsFEIM/dN7p7NTAHmHrQPNcC97v7XgB33xVgHpFWtTx/H997Zjk5A7vxg/NGhB1HpElBFkE/YGuDx/nRaQ0NA4aZ2Rtm9paZnRtgHpFWs21fBdc8kkv3Tsk8cPkEkhO1O05iV9hnFicCQ4FPAP2BhWY22t33NZzJzGYAMwCysrJaO6PIUSmtrOHqhxdRWV3H41+fRGZ6x7AjiRxSkB9TtgEDGjzuH53WUD4wz91r3P19YB2RYvgQd5/p7jnunpOZqdPxJXbVRo8Qyiss438vH8+wXulhRxI5rCCLYBEw1MwGmVkyMA2Yd9A8fyWyNoCZ9SSyqWhjgJlEAvWLBWt5fV0hP546io8P1YcWaRsCKwJ3rwWuBxYAq4Gn3H2lmd1pZhdEZ1sA7DazVcBrwH+5++6gMokE6fnlBTy0cCOXn5LFZZO0CVPaDnP3sDMclZycHM/NzQ07hsiHrN1RykX/+wbDe6czZ8ap2jksMcfMFrt7TmPP6dUqcoyKK2q47rFc0pITdYSQtEl6xYocg/p656a5S8nfW8EDl4+nV5eUsCOJHDUVgcgxmJu7lVfW7OLWKSM4Obt72HFEmkVFINJMu0oruWv+aiYN6s5VH8sOO45Is6kIRJrpJ39fTWVNPT/93GhdW0DaNBWBSDO8mVfEvGXb+donhjAks3PYcUSOiYpA5ChV19bzw+dWkNU9ja99YkjYcUSOWdhjDYm0ObPeeJ8NheXMuiqHlKSEsOOIHDOtEYgche37Krjn5fWcM7IXnxreK+w4Ii3iiNYIoheQuQsYCXxwoLS7Dw4ol0hM+snzq3Cc288fGXYUkRZzpGsEDwMPALXAJ4FHgT8FFUokFi1cV8j893Zw/SePZ0D3tLDjiLSYIy2CVHd/hcjYRJvd/Q5gSnCxRGJLaWUNP3j2PQb17MS1Z2hFWNqXI91ZXGVmHYD1ZnY9kesK6Jg5iRu3P7eSguJKnrruVDomagextC9HukZwA5AGfAuYAFwOfDmoUCKx5Lml23h2yTa++anjmTCwW9hxRFrckRZBtruXuXu+u093988DGnBd2r2te/Zz27MryBnYjes/eXzYcUQCcaRF8P0jnCbSbtTW1XPj3KUA/OaSsSQm6GhraZ8OuY/AzCYD5wH9zOx3DZ7qQuQIIpF26/7XNpC7eS+/vWSsjhKSdu1wO4u3A4uBC6JfDygFbgwqlEjY3t64m3teWcdF4/px4bh+YccRCdQhi8DdlwHLzOxP0WsQi7R7e8uruWHOUrK6p/HjC0eFHUckcIfbNPQe4NH7H3ne3U8KJpZIeG75y3J2l1fx7NdPo3NHDccl7d/hXuXnt0oKkRjx6pqdLFi5k++eewKj+mWEHUekVRxu09DmA/fNbCAw1N1fNrPUw32vSFtTWVPHnX9bxeDMTnzldJ09LPHjiI6HM7NrgWeAh6KT+gN/DSqUSBh+v3Ajm3bv578vOJHkRB0qKvHjSF/t3wBOA0oA3H09cFxQoURa29Y9+7nvtTymjO7Dx4dmhh1HpFUdaRFUuXv1gQdmlkh0J7JIe/Dff1tFQgfjtvNHhB1FpNUdaRG8bmY/AFLN7BzgaeBvwcUSaT2vrd3Fy6t3csNZQ+mTkRp2HJFWd6RFcAtQCLwHXAfMB24LKpRIa6mrd342fw3ZPdKYftqgsOOIhOKIjvxx93oz+yvwV3cvDDiTSKv587v5rN1Zyv2XjdcOYolbh3zlW8QdZlYErAXWmlmhmd3eOvFEglNRXcevX1rHmAFdOW9077DjiITmcB+BbiRytNDJ7t7d3bsDk4DTzExjDUmb9vCb77OjpJLvTx7e6JnzIvHicEVwBXCpu79/YIK7b0QXppE2bk95NQ/8cwNnDT+OUwb3CDuOSKgOVwRJ7l508MTofoKkYCKJBO83/1jH/uo6bpk8POwoIqE7XBFUN/M5kZi1fmcpT7yzhcsmZjG0V3rYcURCd7giGGNmJY3cSoHRh/vhZnauma01szwzu+UQ833ezNzMco72FxA5Gu7OnX9fRVpyAjeeMyzsOCIx4XCDziU09webWQJwP3AOkA8sMrN57r7qoPnSgRuAt5v7b4kcqcff3sK/1hdx59QT6d4pOew4IjEhyAOnJwJ57r4xOjzFHGBqI/P9GLgbqAwwiwjvF5XzP8+v5uNDe3L5pIFhxxGJGUEWQT9ga4PH+dFpHzCz8cAAd3/+UD/IzGaYWa6Z5RYW6nw2aZ47/7aSpATjF18YQ4cOOlxU5IDQTqU0sw7Ar4GbDzevu8909xx3z8nM1MiQcvSWbNnLa2sL+eonhtA7IyXsOCIxJcgi2AYMaPC4f3TaAenAKOCfZrYJOAWYpx3GEoTfvryebmlJXHlqdthRRGJOkEWwCBhqZoPMLBmYBsw78KS7F7t7T3fPdvds4C3gAnfPDTCTxKElW/by+rpCZpwxhE66BrHIRwRWBO5eC1wPLABWA0+5+0ozu9PMLgjq3xU52O9eiawNfPlU7SAWaUygH4/cfT6RIasbTmt0wDp3/0SQWSQ+Lc/fx2trC/mvz5ygtQGRJmjcXWnX7n01jy4piVobEDkEFYG0Wyu2FfOPVTu55vTBpKdoaCyRpqgIpF06MJRE907JXHVadthxRGKaikDapfnv7eCd9/dw86eHkZGqtQGRQ1ERSLtTWVPHT+evZkSfLkw7OSvsOCIxT0Ug7c49r6xn274KfvTZkSRoKAmRw1IRSLvy7pa9PPT6BqadPEBXHhM5QioCaTf27a/mxrlL6d0lhVunjAg7jkiboTNspF2oravn+ieWsH1fBXNmnKLDRUWOgopA2oX7Xsvj33lF/PzzJzFhYPew44i0Kdo0JG3e0q37uPfVPC4c25cvnjzg8N8gIh+iIpA2raK6jpvmLqVXekf+e+qosOOItEnaNCRt2t0vrmFjUTlPfGWSThwTaSatEUib9eaGIma/uYmrPpbNx47vGXYckTZLRSBtUvH+Gr77zHIG9ezE984dHnYckTZNm4akzampq+frTyxmV0kVc647hdTkhLAjibRpKgJpc+782yreyNvNLy8ew/isbmHHEWnztGlI2pTH/rOJx97azHVnDOYLE/qHHUekXVARSJvx7/VF3PG3VZw1/Di+q/0CIi1GRSBtwsbCMr7++GKOz+zMPZeO06iiIi1IRSAxr7yqlmsfzSUxoQN/uDKHzroIvUiL0v8oiXl3zFvJxqJyHv/KJAZ0Tws7jki7ozUCiWnPLd3G04vzuf6Tx/OxITppTCQIKgKJWZt3l3PrsyvIGdiNG84aGnYckXZLRSAxqbq2nm89uYQOBr+dNpbEBL1URYKifQQSk379j3Usyy/mgS+Np3837RcQCZI+ZknMWbZ1HzMXbuCSnAFMHt0n7Dgi7Z6KQGJKTV093/vzcjLTO3Lr+brusEhr0KYhiSkPvb6BNTtKmXnFBLrousMirUJrBBIz8naV8btX8pgyug+fPrF32HFE4oaKQGJCfb3z/b8sJzU5gTsuODHsOCJxRUUgMeHxtzezaNNebpsygsz0jmHHEYkrgRaBmZ1rZmvNLM/Mbmnk+ZvMbJWZLTezV8xsYJB5JDYVFFdw94trOf34nhpaWiQEgRWBmSUA9wOTgZHApWY28qDZlgA57n4S8Azw86DySGxyd3741xXU1tfz04tGY6ZRRUVaW5BrBBOBPHff6O7VwBxgasMZ3P01d98fffgWoI+DcWb+ezt4efUubj7nBLJ66MQxkTAEWQT9gK0NHudHpzXlGuCFxp4wsxlmlmtmuYWFhS0YUcJUvL+GH81byeh+GUw/LTvsOCJxKyZ2FpvZ5UAO8IvGnnf3me6e4+45mZmZrRtOAvPT+avZu7+auz43WmMJiYQoyBPKtgEDGjzuH532IWZ2NnArcKa7VwWYR2LImxuKmJu7la+eOYRR/TLCjiMS14L8GLYIGGpmg8wsGZgGzGs4g5mNAx4CLnD3XQFmkRhSWVPHD/7yHgN7pPHtszW8tEjYAisCd68FrgcWAKuBp9x9pZndaWYXRGf7BdAZeNrMlprZvCZ+nLQj97yynk2793PXRaNJSUoIO45I3At0rCF3nw/MP2ja7Q3unx3kvy+xZ9X2EmYu3MjFE/rzseN1xTGRWKA9dNJqauvqueUvy+mWlsStUzSyqEisUBFIq5n95iaW5xfzo8+eSNe05LDjiEiUikBaxaaicn710jrOGn4c55+ki82IxBIVgQSurt65+ellJCUY/6NhJERiji5MI4GbuXAjizfv5beXjKV3RkrYcUTkIFojkECt2VHCb/6xjsmjejN1bN+w44hII1QEEpjq2npumruMLqmJ/OTCUdokJBKjtGlIAvPT+atZVVDCzCsm0KOzLjYjEqu0RiCBmLdsO7Pf3MQ1pw/S9YdFYpyKQFpc3q5SbvnzciYM7MYtk4eHHUdEDkNFIC1qf3UtX/vTu6QmJXD/ZeNJ0vDSIjFP+wikxbg733l6GXmFZTx29SQdKirSRujjmrSYe1/NY/57O/j+5OGcPlQDyom0FSoCaRELVu7g1/9Yx0Xj+nHtxweHHUdEjoKKQI7Zmh0l3DR3KWP6Z3DX5zSEhEhboyKQY7J9XwXTH15Ep46JPHRFji40I9IGqQik2fbtr+bLs96hrLKW2dMnauewSBulo4akWcqrarl69iK27NnPo1dPZGTfLmFHEpFm0hqBHLWSyhqu+OPbLMsv5nfTxnLK4B5hRxKRY6A1Ajkqu8uqmD57EasLSrjv0nGcO0oXmRFp61QEcsTW7Szl6tmLKCyt4sHLJ3DWiF5hRxKRFqAikCOyYOUObn5qGanJCcy97lTGDugadiQRaSEqAjmkiuo6fvz8Kp54ewuj+2Xw0BUT6Ns1NexYItKCVATSpH+vL+L251awsaic684czM3nnEByoo4vEGlvVATyEdv2VXDX/NX8fXkBA3uk8fhXJnHa8Ro7SKS9UhHIB3YUV3L/a3nMWbQFM+PGs4dx3ZmDdbawSDunIhBWbCvm0f9s4q9Lt1Nf73zx5AF845PH00/7AkTigoogThXvr+HFlQU8nZtP7ua9pCYl8IUJ/fnamUMY0D0t7Hgi0opUBHGksLSKhesKeWFFAa+vK6SmzhncsxO3TRnBxTkDyEhNCjuiiIRARdCOlVbWsHTrPt7auJvX1xWyYlsJAL27pHDlqdlMHduPUf26aNhokTinImgnyqpqWb+zlLU7SlmWv493N+9j3a5S3CGhgzFhYDf+6zMncOawTEb26UKHDnrzF5EIFUEbUllTR/7eCrbu3U/+nv1s3VvBhl1lrN1ZSv7eig/m65KSyLisbpw3ug/jB3Zl7ICupKdos4+INC7QIjCzc4F7gATgD+7+s4Oe7wg8CkwAdgOXuPumIDPFEnenrKqW0spaSipr2FNeTVFZNUWlVRSVHbhVU1RWRUFxJYWlVR/6/uSEDgzO7MT4rG5cOjGLYb3SGdarMwO6pekTv4gcscCKwMwSgPuBc4B8YJGZzXP3VQ1muwbY6+7Hm9k04G7gkqAyHY67U1vv1NY5NfX11NY5tXX11NRHv9Y5tdHpVbV1VFTXU1lTR0X0VhX9WllTH/0auVVU11FeXUdJRc0Hb/qllbWUVtZQ741nSexg9OicTM/OHenZuSMn9EpnQPc0BnRPZUC3NAZ0TyOzc0e94YvIMQtyjWAikOfuGwHMbA4wFWhYBFOBO6L3nwHuMzNz9ybeHptv7qItPLRw40fe3D/0pt/Uu3IzJCd0oGNSB1KTEkhNTiA1KYEuqUn07ZrC8JR00lMS6ZKaFPmakkR6ShLdOiWRGX3jz0hN0pu8iLSKIIugH7C1weN8YFJT87h7rZkVAz2AooYzmdkMYAZAVlZWs8J079SREX26kNTBSEzoQFKCkdihA4kJRlJCBxIPTP/Q803Pm5KUQEpS5A0+JalD5H5yZFpKYgcSEzQmj4i0DW1iZ7G7zwRmAuTk5DTrY/s5I3txzkiNny8icrAgP7ZuAwY0eNw/Oq3RecwsEcggstNYRERaSZBFsAgYamaDzCwZmAbMO2ieecCV0ftfAF4NYv+AiIg0LbBNQ9Ft/tcDC4gcPjrL3Vea2Z1ArrvPA/4IPGZmecAeImUhIiKtKNB9BO4+H5h/0LTbG9yvBC4OMoOIiByaDm0REYlzKgIRkTinIhARiXMqAhGROGdt7WhNMysENoedowk9Oeis6BijfMcu1jMq37Fpz/kGuntmY0+0uSKIZWaW6+45YedoivIdu1jPqHzHJl7zadOQiEicUxGIiMQ5FUHLmhl2gMNQvmMX6xmV79jEZT7tIxARiXNaIxARiXMqAhGROKciaCYzG2Bmr5nZKjNbaWY3RKffYWbbzGxp9HZeiBk3mdl70Ry50WndzewfZrY++rVbSNlOaLCMlppZiZl9O8zlZ2azzGyXma1oMK3R5WURvzOzPDNbbmbjQ8r3CzNbE83wrJl1jU7PNrOKBsvxwZDyNfn3NLPvR5ffWjP7TEj55jbItsnMlkanh7H8mnpPCf416O66NeMG9AHGR++nA+uAkUSuwfydsPNFc20Ceh407efALdH7twB3x0DOBGAHMDDM5QecAYwHVhxueQHnAS8ABpwCvB1Svk8DidH7dzfIl91wvhCXX6N/z+j/lWVAR2AQsAFIaO18Bz3/K+D2EJdfU+8pgb8GtUbQTO5e4O7vRu+XAquJXIM51k0FHonefwS4MMQsB5wFbHD3UM8Yd/eFRLuyvYAAAASLSURBVK6L0VBTy2sq8KhHvAV0NbM+rZ3P3V9y99row7eIXAkwFE0sv6ZMBea4e5W7vw/kARMDC8eh85mZAV8Engwyw6Ec4j0l8NegiqAFmFk2MA54Ozrp+uiq2qywNr1EOfCSmS02sxnRab3cvSB6fwcQCxdynsaH/wPGyvKDppdXP2Brg/nyCf+DwNVEPiEeMMjMlpjZ62b28bBC0fjfM9aW38eBne6+vsG00JbfQe8pgb8GVQTHyMw6A38Gvu3uJcADwBBgLFBAZHUzLKe7+3hgMvANMzuj4ZMeWb8M9fhhi1zG9ALg6eikWFp+HxILy6spZnYrUAs8Hp1UAGS5+zjgJuAJM+sSQrSY/Xse5FI+/GEktOXXyHvKB4J6DaoIjoGZJRH5gz3u7n8BcPed7l7n7vXA7wl4dfdQ3H1b9Osu4Nlolp0HVh+jX3eFlS9qMvCuu++E2Fp+UU0tr23AgAbz9Y9Oa3VmdhVwPvCl6BsF0U0uu6P3FxPZBj+stbMd4u8ZS8svEfgcMPfAtLCWX2PvKbTCa1BF0EzRbYp/BFa7+68bTG+4je4iYMXB39sazKyTmaUfuE9kp+IKYB5wZXS2K4HnwsjXwIc+icXK8mugqeU1D/hy9MiNU4DiBqvvrcbMzgW+C1zg7vsbTM80s4To/cHAUGBjCPma+nvOA6aZWUczGxTN905r54s6G1jj7vkHJoSx/Jp6T6E1XoOtuVe8Pd2A04msoi0HlkZv5wGPAe9Fp88D+oSUbzCRozKWASuBW6PTewCvAOuBl4HuIS7DTsBuIKPBtNCWH5FCKgBqiGxvvaap5UXkSI37iXxSfA/ICSlfHpHtxAdegw9G5/189O++FHgX+GxI+Zr8ewK3RpffWmByGPmi02cDXz1o3jCWX1PvKYG/BjXEhIhInNOmIRGROKciEBGJcyoCEZE4pyIQEYlzKgIRkTinIhBpJjO708zODjuHyLHS4aMizWBmCe5eF3YOkZagNQKRg0THol9jZo+b2Woze8bM0qLj1d9tZu8CF5vZbDP7QvR7TjazN81smZm9Y2bpZpZgkesFLIoOunZddN4+ZrYwOs79ipAHhBMhMewAIjHqBCJnnr5hZrOAr0en7/bIQH4Hhnc4MHDeXOASd18UHZysgsiZtcXufrKZdQTeMLOXiIxrs8Dd/yc6jEFa6/5qIh+mIhBp3FZ3fyN6/0/At6L35zYy7wlAgbsvAvDoiJFm9mngpANrDUAGkTFrFgGzogOM/dXdlwb0O4gcERWBSOMO3nl24HH5UfwMA77p7gs+8kRkSPApwGwz+7W7P9q8mCLHTvsIRBqXZWanRu9fBvz7EPOuBfqY2ckA0f0DicAC4GvRT/6Y2bDoqLADiVwE5ffAH4hcPlEkNCoCkcatJXIxn9VANyIXWGmUu1cDlwD3mtky4B9ACpE3+VXAuxa5YPpDRNbCPwEsM7Ml0e+7J8DfQ+SwdPioyEGilwn8u7uPCjmKSKvQGoGISJzTGoGISJzTGoGISJxTEYiIxDkVgYhInFMRiIjEORWBiEic+z86yVmOFYF1wAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLStvS2qCSjm"
      },
      "source": [
        "compute_delta(110)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1I8COnUxnz"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySrey9KzB0AF"
      },
      "source": [
        "compute_delta(110).item()  # It's not 0.5!! SOS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMyME_1WCGZz"
      },
      "source": [
        "compute_delta(102).item() # Close to 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB8LPwfBMHQ"
      },
      "source": [
        "# Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoaOyCDxQy0"
      },
      "source": [
        "##Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsXgOwWZ_ru"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lca2xrBh9a"
      },
      "source": [
        "# Vega"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muozc-hzhSGA"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KATxBCAdlFt"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a timev\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05]*3]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 0.0, 110.0, sigma, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEiAredqQGxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}