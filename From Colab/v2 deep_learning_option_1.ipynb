{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"v2 deep_learning_option_1.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uEOzBi66EYup"},"source":["### Deep Learning Barrier Option\n","\n","We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n","\n","```\n","T - Maturity (yrs.)\n","S - Spot (usd)\n","K - Strike (usd)\n","sigma - Volatility (per.)\n","r - Risk Free Rate (per.)\n","mu - Stock Drift Rate (per.)\n","B - Barrier (usd)\n","```\n","\n","### Batched Data generation\n","\n","The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. \n","\n","Loading all the necessary libraries:-"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rmLieKk5M0wC","executionInfo":{"status":"ok","timestamp":1624321847813,"user_tz":420,"elapsed":18031,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"98973e92-ddd3-434f-863e-e1f3e7dede98"},"source":["!curl https://colab.chainer.org/install |sh -\n","import cupy"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0  10063      0 --:--:-- --:--:-- --:--:-- 10063\n","+ apt -y -q install cuda-libraries-dev-10-0\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n","0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n","+ pip install -q cupy-cuda100  chainer \n","\u001b[K     |████████████████████████████████| 58.9MB 82kB/s \n","\u001b[K     |████████████████████████████████| 1.0MB 36.7MB/s \n","\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","+ set +ex\n","Installation succeeded!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6lRKL5JjWvX-"},"source":["import numba\n","from numba import cuda\n","import cupy\n","import numpy as np\n","import math\n","import time\n","import torch\n","cupy.cuda.set_allocator(None)\n","from torch.utils.dlpack import from_dlpack\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbbUoS6jWxes"},"source":["cupy_barrier_option = cupy.RawKernel(r'''\n","extern \"C\" __global__ void barrier_option(\n","    float *d_s,\n","    const float T,\n","    const float K,\n","    const float B,\n","    const float S0,\n","    const float sigma,\n","    const float mu,\n","    const float r,\n","    const float * d_normals,\n","    const long N_STEPS,\n","    const long N_PATHS)\n","{\n","  unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned stride = blockDim.x * gridDim.x;\n","  unsigned tid = threadIdx.x;\n"," \n","  const float tmp1 = mu*T/N_STEPS;\n","  const float tmp2 = exp(-r*T);\n","  const float tmp3 = sqrt(T/N_STEPS);\n","  double running_average = 0.0;\n"," \n","  for (unsigned i = idx; i<N_PATHS; i+=stride)\n","  {\n","    float s_curr = S0;\n","    unsigned n=0;\n","    for(unsigned n = 0; n < N_STEPS; n++){\n","       s_curr += tmp1 * s_curr + sigma*s_curr*tmp3*d_normals[i + n * N_PATHS];\n","       running_average += (s_curr - running_average) / (n + 1.0) ;\n","       if (running_average <= B){\n","           break;\n","       }\n","    }\n"," \n","    float payoff = (running_average>K ? running_average-K : 0.f);\n","    d_s[i] = tmp2 * payoff;\n","  }\n","}\n"," \n","''', 'barrier_option')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4J6wlot0W0NW"},"source":["def get_option_price(T, K, B, S0, sigma, mu, r, N_PATHS = 8192000, N_STEPS = 365, seed=3):\n","    number_of_threads = 256\n","    number_of_blocks = (N_PATHS-1) // number_of_threads + 1\n","    cupy.random.seed(seed)\n","    randoms_gpu = cupy.random.normal(0, 1, N_PATHS * N_STEPS, dtype=cupy.float32)\n","    output =  cupy.zeros(N_PATHS, dtype=cupy.float32)\n","    cupy_barrier_option((number_of_blocks,), (number_of_threads,),\n","                   (output, np.float32(T), np.float32(K), \n","                    np.float32(B), np.float32(S0), \n","                    np.float32(sigma), np.float32(mu), \n","                    np.float32(r),  randoms_gpu, N_STEPS, N_PATHS))\n","    v = output.mean()\n","    out_df = cudf.DataFrame()\n","    out_df['p'] = cudf.Series([v.item()])\n","    return out_df "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77},"id":"0a8ZQ573W4ey","executionInfo":{"status":"ok","timestamp":1622128042629,"user_tz":420,"elapsed":337,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"a9cd2547-3628-4ae9-c7cd-0ed3e9c98a90"},"source":["import pandas as cudf\n","get_option_price(1, 100, 95, 100, 0.2, 0, 0, N_PATHS = 819200, N_STEPS = 365, seed=3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>p</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.411485</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          p\n","0  4.411485"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jYo1xKvTX9tG","executionInfo":{"status":"ok","timestamp":1622128323443,"user_tz":420,"elapsed":7817,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"ef1f1d0c-b9c0-4ae6-98b4-e9272815d22f"},"source":["import numba\n","from numba import cuda\n","\n","@cuda.jit\n","def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n","    # ii - overall thread index\n","    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n","    stride = cuda.gridDim.x * cuda.blockDim.x\n","    tmp3 = math.sqrt(T/N_STEPS)\n","    for i in range(ii, N_PATHS * N_BATCH, stride):\n","        batch_id = i // N_PATHS\n","        path_id = i % N_PATHS\n","        tmp1 = mu[batch_id]*T/N_STEPS\n","        tmp2 = math.exp(-r[batch_id]*T)\n","        running_average = 0.0\n","        s_curr = S0[batch_id]\n","        for n in range(N_STEPS):\n","\n","            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]\n","            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average)\n","            if i==0 and batch_id == 2:\n","                print(s_curr)\n","            if running_average <= B[batch_id]:\n","                break\n","        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n","        d_s[i] = tmp2 * payoff\n","\n","class NumbaOptionDataSet(object):\n","    \n","    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15):\n","        self.num = 0\n","        self.max_length = max_len\n","        self.N_PATHS = number_path\n","        self.N_STEPS = 365\n","        self.N_BATCH = batch\n","        self.T = np.float32(1.0)\n","        self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n","        self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n","        self.number_of_threads = threads\n","        cupy.random.seed(seed)\n","        \n","    def __len__(self):\n","        return self.max_length\n","        \n","    def __iter__(self):\n","        self.num = 0\n","        return self\n","    \n","    def __next__(self):\n","        if self.num > self.max_length:\n","            raise StopIteration\n","        #X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n","        # scale the [0, 1) random numbers to the correct range for each of the option parameters\n","        X = cupy.array([100.0, 95, 100, 0.2, 0, 0], dtype=cupy.float32)\n","        # make sure the Barrier is smaller than the Strike price\n","        #X[1] = X[0] * X[1]\n","        randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n","        batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[0], \n","                              X[1], X[2], X[3], X[4], X[5], randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH)\n","        o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n","        Y = o.mean(axis = 1) \n","        self.num += 1\n","        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n","ds = NumbaOptionDataSet(10, number_path=100000, batch=16, seed=15)\n","for i in ds:\n","    print(i[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([4.3987, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.4298, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.3900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.4235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.4219, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.3761, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.4012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.4142, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.4370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.4050, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n","tensor([4.4364, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","       device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UW1hbp9wEYu1"},"source":["We can implement the same code by using Numba to accelerate the calculation in GPU:-"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKlTmRoFEYu1","executionInfo":{"status":"ok","timestamp":1622129045720,"user_tz":420,"elapsed":1114,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"ddc2b2ec-af2b-4a93-b334-674faf999168"},"source":["#%%writefile cupy_dataset.py\n","import numba\n","from numba import cuda\n","import cupy\n","import numpy as np\n","import math\n","import time\n","import torch\n","cupy.cuda.set_allocator(None)\n","from torch.utils.dlpack import from_dlpack\n","\n","@cuda.jit\n","def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n","\n","    # ii - overall thread index\n","    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n","    stride = cuda.gridDim.x * cuda.blockDim.x\n","    tmp2 = math.exp(-r*T)\n","    tmp3 = math.sqrt(T/N_STEPS)    \n","\n","    for i in range(ii, N_PATHS, stride): # for each path          \n","        running_average = 0.0\n","\n","        for j in range(N_STOCKS): # initialize S0\n","            s_curr[j] = S0[j]\n","\n","        for n in range(N_STEPS): # for each step\n","            s_curr_avg = 0.0\n","\n","            for j in range(N_STOCKS): # for each stock\n","                tmp1 = mu[j]*T/N_STEPS  \n","                s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n","                s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n","\n","            # add stock average to running average\n","            running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n","\n","            # compare to barrier\n","            if running_average <= B:\n","                break\n","\n","        payoff = running_average - K if running_average > K else 0\n","        d_s[i] = tmp2 * payoff\n","    \n","\n","class NumbaOptionDataSet(object):\n","    \n","    def __init__(self, max_len=10, number_path = 1000, number_stocks = 3, batch=1, threads=512, seed=15, T=1):\n","        self.num = 0\n","        self.max_length = max_len\n","        self.N_PATHS = number_path\n","        self.N_STEPS = 365\n","        self.N_STOCKS = number_stocks\n","        self.N_BATCH = batch\n","        self.T = np.float32(T)\n","        self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n","        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n","        self.number_of_threads = threads\n","        cupy.random.seed(seed)\n","\n","        ############ <new\n","        self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n","        self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n","        cupy.fill_diagonal(self.Z_cov, 1)\n","        ############ new>\n","\n","    def __len__(self):\n","        return self.max_length\n","        \n","    def __iter__(self):\n","        self.num = 0\n","        return self\n","    \n","    def __next__(self):\n","        if self.num > self.max_length:\n","            raise StopIteration\n","\n","        X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n","        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n","\n","        for i in range(self.N_BATCH): # for each batch\n","          self.S0 = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 200\n","          self.K = 110.0\n","          self.B = 100.0\n","          self.sigma = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n","          self.mu = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n","          self.r = 0.05\n","          self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n","\n","          ############ <new - add correlation between stocks\n","          all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n","          ############ new>\n","          \n","          single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n","                                                                                    self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n","          Y[i] = self.output.mean()\n","\n","          ############ <new - combine to get X matrix\n","          X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n","                                self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n","          ############ new>\n","        \n","        self.num += 1\n","        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n","\n","ds = NumbaOptionDataSet(max_len=10, number_path=100, batch=16, seed=15)\n","for i in ds:\n","  print(i[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py:476: FutureWarning: cupy.random.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.multivariate_normal')\n","/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py:337: FutureWarning: cupy.random.RandomState.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.RandomState.multivariate_normal')\n","/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py:476: FutureWarning: cupy.random.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.multivariate_normal')\n","/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py:337: FutureWarning: cupy.random.RandomState.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.RandomState.multivariate_normal')\n"],"name":"stderr"},{"output_type":"stream","text":["tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n","tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n","       device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V7THCTUAnHXi","executionInfo":{"status":"ok","timestamp":1624321931322,"user_tz":420,"elapsed":832,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"fa8b9819-953a-4da6-b099-bd6cc4b27b6d"},"source":["#%%writefile cupy_dataset.py\n","import numba\n","from numba import cuda\n","import cupy\n","import numpy as np\n","import math\n","import time\n","import torch\n","cupy.cuda.set_allocator(None)\n","from torch.utils.dlpack import from_dlpack\n","\n","@cuda.jit\n","def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n","\n","    # ii - overall thread index\n","    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n","    stride = cuda.gridDim.x * cuda.blockDim.x\n","    tmp2 = math.exp(-r*T)\n","    tmp3 = math.sqrt(T/N_STEPS)\n","\n","    for i in range(ii, N_PATHS, stride): # for each path          \n","        running_average = 0.0\n","\n","        for j in range(N_STOCKS): # initialize S0\n","            s_curr[j] = S0[j]\n","\n","        for n in range(N_STEPS): # for each step\n","            s_curr_avg = 0.0\n","\n","            for j in range(N_STOCKS): # for each stock\n","                tmp1 = mu[j]*T/N_STEPS  \n","                s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n","                s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n","\n","            # add stock average to running average\n","            running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n","\n","            # compare to barrier\n","            if running_average <= B:\n","                break\n","\n","        payoff = running_average - K if running_average > K else 0\n","        d_s[i] = tmp2 * payoff\n","    \n","\n","class NumbaOptionDataSet(object):\n","    \n","    def __init__(self, max_len=10, number_path = 1000, number_stocks = 2, batch=1, threads=512, seed=15, T=1):\n","        self.num = 0\n","        self.max_length = max_len\n","        self.N_PATHS = number_path\n","        self.N_STEPS = 365\n","        self.N_STOCKS = number_stocks\n","        self.N_BATCH = batch\n","        self.T = np.float32(T)\n","        self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n","        self.number_of_blocks = (self.N_PATHS - 1) // threads + 1\n","        self.number_of_threads = threads\n","        cupy.random.seed(seed)\n","\n","        ############ <new\n","        self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n","        #self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n","        #cupy.fill_diagonal(self.Z_cov, 1)\n","        #self.Z_cov = cupy.ones([self.N_STOCKS,self.N_STOCKS], dtype=cupy.float32)\n","        self.Z_cov = cupy.array([[1,0.99999],[0.99999,1]], dtype=cupy.float32)\n","        ############ new>\n","\n","    def __len__(self):\n","        return self.max_length\n","        \n","    def __iter__(self):\n","        self.num = 0\n","        return self\n","    \n","    def __next__(self):\n","        if self.num > self.max_length:\n","            raise StopIteration\n","\n","        X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n","        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n","\n","        for i in range(self.N_BATCH): # for each batch\n","            self.S0 = cupy.array([100]*self.N_STOCKS, dtype=cupy.float32)\n","            self.K = 100.0\n","            self.B = 95.0\n","            self.sigma = cupy.array([0.2]*self.N_STOCKS, dtype=cupy.float32)\n","            self.mu = cupy.array([0]*self.N_STOCKS, dtype=cupy.float32)\n","            self.r = 0.05\n","            self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n","\n","            ############ <new - add correlation between stocks\n","            all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n","            ############ new>\n","            \n","            single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n","                                                                                      self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n","            Y[i] = self.output.mean()\n","\n","            ############ <new - combine to get X matrix\n","            X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n","                                  self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n","            ############ new>\n","          \n","        self.num += 1\n","        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n","\n","ds = NumbaOptionDataSet(max_len=10, number_path=10000, batch=3, seed=15)\n","for i in ds:\n","  print(i[0])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py:476: FutureWarning: cupy.random.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.multivariate_normal')\n","/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py:337: FutureWarning: cupy.random.RandomState.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.RandomState.multivariate_normal')\n"],"name":"stderr"},{"output_type":"stream","text":["tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n","tensor([[1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02],\n","        [1.0000e+02, 9.5000e+01, 1.0000e+02, 1.0000e+02, 2.0000e-01, 2.0000e-01,\n","         0.0000e+00, 0.0000e+00, 5.0000e-02]], device='cuda:0')\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py:476: FutureWarning: cupy.random.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.multivariate_normal')\n","/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py:337: FutureWarning: cupy.random.RandomState.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.RandomState.multivariate_normal')\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3wCf20PHdlXw"},"source":["#%%writefile cupy_dataset.py\n","import numba\n","from numba import cuda\n","import cupy\n","import numpy as np\n","import math\n","import time\n","import torch\n","cupy.cuda.set_allocator(None)\n","from torch.utils.dlpack import from_dlpack\n","\n","@cuda.jit\n","def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n","\n","    # ii - overall thread index\n","    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n","    stride = cuda.gridDim.x * cuda.blockDim.x\n","    tmp2 = math.exp(-r*T)\n","    tmp3 = math.sqrt(T/N_STEPS)    \n","\n","    for i in range(ii, N_PATHS, stride): # for each path          \n","        running_average = 0.0\n","\n","        for j in range(N_STOCKS): # initialize S0\n","            s_curr[j] = S0[j]\n","\n","        for n in range(N_STEPS): # for each step\n","            s_curr_avg = 0.0\n","\n","            for j in range(N_STOCKS): # for each stock\n","                tmp1 = mu[j]*T/N_STEPS  \n","                s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n","                s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n","\n","            # add stock average to running average\n","            running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n","\n","            # compare to barrier\n","            if running_average <= B:\n","                break\n","\n","        payoff = running_average - K if running_average > K else 0\n","        #d_s[i] = tmp2 * payoff\n","        d_s[i] = running_average\n","    \n","\n","class NumbaOptionDataSet(object):\n","    \n","    def __init__(self, max_len=10, number_path = 1000, number_stocks = 3, batch=1, threads=512, seed=15, T=1):\n","        self.num = 0\n","        self.max_length = max_len\n","        self.N_PATHS = number_path\n","        self.N_STEPS = 365\n","        self.N_STOCKS = number_stocks\n","        self.N_BATCH = batch\n","        self.T = np.float32(T)\n","        self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n","        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n","        self.number_of_threads = threads\n","        cupy.random.seed(seed)\n","\n","        ############ <new\n","        self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n","        #self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n","        #cupy.fill_diagonal(self.Z_cov, 1)\n","        self.Z_cov = cupy.ones([self.N_STOCKS,self.N_STOCKS], dtype=cupy.float32)\n","        ############ new>\n","\n","    def __len__(self):\n","        return self.max_length\n","        \n","    def __iter__(self):\n","        self.num = 0\n","        return self\n","    \n","    def __next__(self):\n","        if self.num > self.max_length:\n","            raise StopIteration\n","\n","        X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n","        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n","\n","        for i in range(self.N_BATCH): # for each batch\n","          self.S0 = cupy.array([100]*self.N_STOCKS, dtype=cupy.float32)\n","          self.K = 100.0\n","          self.B = 95.0\n","          self.sigma = cupy.array([0.2]*self.N_STOCKS, dtype=cupy.float32)\n","          self.mu = cupy.array([0]*self.N_STOCKS, dtype=cupy.float32)\n","          self.r = 0.05\n","          self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n","\n","          ############ <new - add correlation between stocks\n","          all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n","          ############ new>\n","          \n","          single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n","                                                                                    self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n","         # Y[i] = self.output.mean()\n","          Y[i] = self.output\n","\n","          ############ <new - combine to get X matrix\n","          X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n","                                self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n","          ############ new>\n","        \n","        self.num += 1\n","        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n","\n","ds = NumbaOptionDataSet(max_len=10, number_path=100, batch=1, seed=15)\n","for i in ds:\n","  print(i[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_WsyJp7px2w","executionInfo":{"status":"ok","timestamp":1622136691763,"user_tz":420,"elapsed":565,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"2dfa8ae5-68e2-4779-f28a-7defef4c852e"},"source":["N_STOCKS = 2\n","N_PATHS = 100\n","N_STEPS = 365\n","\n","Z_mean = cupy.zeros(N_STOCKS, dtype=cupy.float32)\n","# Z_cov = (-0.2 + cupy.random.rand(N_STOCKS*N_STOCKS, dtype=cupy.float32)*0.4).reshape(N_STOCKS,N_STOCKS)\n","# cupy.fill_diagonal(Z_cov, 1)\n","#Z_cov = cupy.ones([N_STOCKS,N_STOCKS], dtype=cupy.float32)\n","Z_cov = cupy.array([[1,0.99999],[0.99999,1]], dtype=cupy.float32)\n","\n","test = cupy.random.multivariate_normal(Z_mean, Z_cov, (N_PATHS, N_STEPS), dtype=cupy.float32)\n","np.cov(test[0,:,0], test[0,:,1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py:476: FutureWarning: cupy.random.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.multivariate_normal')\n","/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py:337: FutureWarning: cupy.random.RandomState.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.RandomState.multivariate_normal')\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([[1.01319218, 1.0129091 ],\n","       [1.0129091 , 1.01264464]])"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRpOWA3rsei2","executionInfo":{"status":"ok","timestamp":1622133576058,"user_tz":420,"elapsed":8,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"f8d20a82-f9a2-4ddd-83aa-cd65925f6f7f"},"source":["mean = (1, 2)\n","cov = [[1, 0], [0, 1]]\n","x = np.random.multivariate_normal(mean, cov, (3, 3))\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 2.82741622,  2.44091164],\n","        [ 1.21183596,  2.93504818],\n","        [ 1.32137184,  1.39742252]],\n","\n","       [[ 2.63743642,  1.22083863],\n","        [ 2.6331421 ,  1.35025378],\n","        [ 0.10744089,  3.06378457]],\n","\n","       [[ 2.3769185 ,  2.06274757],\n","        [ 1.07737134,  2.89589748],\n","        [-0.14065766, -0.25167839]]])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SN87MzwtsMIo","executionInfo":{"status":"ok","timestamp":1622133502755,"user_tz":420,"elapsed":271,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"a45672de-51b4-4235-b082-37328e810a91"},"source":["cupy.zeros(N_STOCKS, dtype=cupy.float32)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0.], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjRpklzsquvx","executionInfo":{"status":"ok","timestamp":1622133136604,"user_tz":420,"elapsed":555,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"c5f89c42-6438-495e-e5e1-b86e5d0e7287"},"source":["cov[:,:,0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.0983905 , -0.32467306, -0.00850703, ..., -0.69119817,\n","         1.1081166 ,  1.4816111 ],\n","       [ 0.13093112,  0.49157935,  1.2215294 , ..., -1.7402995 ,\n","        -1.4249837 ,  0.14142117],\n","       [ 0.5419938 , -2.4765553 ,  0.37870562, ..., -0.6660495 ,\n","         0.49496147,  1.0302433 ],\n","       ...,\n","       [ 0.5332466 ,  0.9650368 ,  0.04600096, ..., -1.4403468 ,\n","        -0.34186473,  0.24416114],\n","       [ 1.3939893 ,  0.4824665 ,  1.0664046 , ..., -1.3792837 ,\n","        -0.04063462,  0.80182713],\n","       [ 1.0238372 , -0.91995674,  0.6804266 , ...,  0.32282877,\n","         0.27119708,  0.6155007 ]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nKs-Wn-Pi5bR","executionInfo":{"status":"ok","timestamp":1622131084546,"user_tz":420,"elapsed":9,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"41149253-4aa2-42cd-b2bd-f3a0f3052ee8"},"source":["cupy.ones([10,5])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"tnNHRhu4H64m"},"source":["@cuda.jit\n","def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n","    # ii - overall thread index\n","    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n","    stride = cuda.gridDim.x * cuda.blockDim.x\n","    tmp3 = math.sqrt(T/N_STEPS)\n","    tmp2 = math.exp(-r*T)\n","    for i in range(ii, N_PATHS, stride):              \n","        running_average = 0.0\n","        for j in range(N_STOCKS): # initialize S0\n","            s_curr[j] = S0[j]\n","        for n in range(N_STEPS):\n","            s_curr_avg = 0.0\n","            for j in range(N_STOCKS):\n","                tmp1 = mu[j]*T/N_STEPS  \n","                s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n","                s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n","            running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n","            if running_average <= B:\n","                break\n","        payoff = running_average - K if running_average > K else 0\n","        d_s[i] = tmp2 * payoff"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrtK-yaKIKAO"},"source":["max_len=10\n","number_path = 1000\n","number_stocks = 5\n","batch=1\n","threads=512\n","seed=15\n","T=1\n","\n","num = 0\n","max_length = max_len\n","N_PATHS = number_path\n","N_STEPS = 365\n","N_STOCKS = number_stocks\n","N_BATCH = batch\n","T = np.float32(T)\n","output = cupy.zeros(N_PATHS, dtype=cupy.float32) \n","number_of_blocks = (N_PATHS * N_STOCKS - 1) // threads + 1\n","number_of_threads = threads\n","cupy.random.seed(seed)\n","\n","Z_mean = cupy.zeros(N_STOCKS, dtype=cupy.float32)\n","Z_cov = (-0.3 + cupy.random.rand(N_STOCKS*N_STOCKS, dtype=cupy.float32)*0.6).reshape(N_STOCKS,N_STOCKS)\n","cupy.fill_diagonal(Z_cov, 1)\n","\n","S0 = cupy.random.rand(N_STOCKS, dtype=cupy.float32) * 200\n","K = 110.0\n","B = 100.0\n","sigma = cupy.random.rand(N_STOCKS, dtype=cupy.float32) * 0.4\n","mu = cupy.random.rand(N_STOCKS, dtype=cupy.float32) * 0.2\n","r = 0.05\n","s_curr = cupy.zeros(N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n","\n","all_normals = cupy.random.multivariate_normal(Z_mean, Z_cov, (N_PATHS, N_STEPS), dtype=cupy.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Q5g0aAAI_NA"},"source":["batch_barrier_option[(number_of_blocks,), (number_of_threads,)](output, T, K, B, S0, sigma, mu, r, all_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr)\n","output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qfLKExmaEYu2"},"source":["### Model\n","To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7OjEPNsEYu2","executionInfo":{"status":"ok","timestamp":1621541356678,"user_tz":420,"elapsed":306,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"f49efac5-ffd0-4ded-fcaa-6f66cc4eb2a0"},"source":["%%writefile model.py\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self, hidden=1024):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(12, hidden)\n","        self.fc2 = nn.Linear(hidden, hidden)\n","        self.fc3 = nn.Linear(hidden, hidden)\n","        self.fc4 = nn.Linear(hidden, hidden)\n","        self.fc5 = nn.Linear(hidden, hidden)\n","        self.fc6 = nn.Linear(hidden, 1)\n","        self.register_buffer('norm',\n","                             torch.tensor([110.0, #K\n","                                           100.0, #B\n","                                           200.0, 200.0, 200.0, #S0\n","                                           0.2, 0.2, 0.2, #sigma\n","                                           0.2, 0.2, 0.2, #mu\n","                                           0.05])) #r\n","\n","    def forward(self, x):\n","        # normalize the parameter to range [0-1] \n","        x = x / self.norm\n","        x = F.elu(self.fc1(x))\n","        x = F.elu(self.fc2(x))\n","        x = F.elu(self.fc3(x))\n","        x = F.elu(self.fc4(x))\n","        x = F.elu(self.fc5(x))\n","        return self.fc6(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing model.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bOwESpF5EYu2"},"source":["As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "]},{"cell_type":"markdown","metadata":{"id":"fi2qIvUvEYu2"},"source":["For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hq5daEvHwj9p","executionInfo":{"status":"ok","timestamp":1621541366802,"user_tz":420,"elapsed":3215,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"110a3e25-a556-4e3a-c8aa-06b10cc6724c"},"source":["!pip install pytorch-ignite"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytorch-ignite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/d3/640f70d69393b415e6a29b27c735047ad86267921ad62682d1d756556d48/pytorch_ignite-0.4.4-py3-none-any.whl (200kB)\n","\r\u001b[K     |█▋                              | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 102kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 112kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 122kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 133kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 153kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 163kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 174kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 184kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 194kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 4.2MB/s \n","\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.8.1+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (1.19.5)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.4.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":525},"id":"n1BiCZP5EYu3","executionInfo":{"status":"error","timestamp":1621541835792,"user_tz":420,"elapsed":95969,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"858cafd8-39b5-461b-f1ec-9b9830c61fe8"},"source":["from ignite.engine import Engine, Events\n","from ignite.handlers import Timer\n","from torch.nn import MSELoss\n","from torch.optim import Adam\n","from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n","from ignite.handlers import ModelCheckpoint\n","from model import Net\n","from cupy_dataset import NumbaOptionDataSet\n","timer = Timer(average=True)\n","model = Net().cuda()\n","loss_fn = MSELoss()\n","optimizer = Adam(model.parameters(), lr=1e-3)\n","dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2)\n","#dataset = OptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n","\n","def train_update(engine, batch):\n","    model.train()\n","    optimizer.zero_grad()\n","    x = batch[0]\n","    y = batch[1]\n","    y_pred = model(x)\n","    loss = loss_fn(y_pred[:,0], y)\n","    loss.backward()\n","    optimizer.step()\n","    return loss.item()\n","\n","trainer = Engine(train_update)\n","log_interval = 100\n","\n","scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n","trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n","timer.attach(trainer,\n","             start=Events.EPOCH_STARTED,\n","             resume=Events.ITERATION_STARTED,\n","             pause=Events.ITERATION_COMPLETED,\n","             step=Events.ITERATION_COMPLETED)    \n","@trainer.on(Events.ITERATION_COMPLETED)\n","def log_training_loss(engine):\n","    iter = (engine.state.iteration - 1) % len(dataset) + 1\n","    if iter % log_interval == 0:\n","        print('loss', engine.state.output, 'average time', timer.value())\n","        \n","trainer.run(dataset, max_epochs=100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py:476: FutureWarning: cupy.random.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.multivariate_normal')\n","/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py:337: FutureWarning: cupy.random.RandomState.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.RandomState.multivariate_normal')\n"],"name":"stderr"},{"output_type":"stream","text":["loss 114.11465454101562 average time 0.004820869999996376\n","loss 103.97425842285156 average time 0.00477378225999928\n","loss 65.08654022216797 average time 0.004628413440002532\n","loss 1.4812583923339844 average time 0.004637326459992437\n","loss 0.5752648115158081 average time 0.0046728680700061885\n"],"name":"stdout"},{"output_type":"stream","text":["Engine run is terminating due to exception: \n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a6274739361d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m           \u001b[0;31m############ <new - add correlation between stocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m           \u001b[0mall_normals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m           \u001b[0;31m############ new>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py\u001b[0m in \u001b[0;36mmultivariate_normal\u001b[0;34m(mean, cov, size, check_valid, tol, method, dtype)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     return rs.multivariate_normal(\n\u001b[0;32m--> 479\u001b[0;31m         mean, cov, size, check_valid, tol, method, dtype)\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py\u001b[0m in \u001b[0;36mmultivariate_normal\u001b[0;34m(self, mean, cov, size, check_valid, tol, method, dtype)\u001b[0m\n\u001b[1;32m    410\u001b[0m                                  dtype=dtype).reshape(-1, mean.shape[0])\n\u001b[1;32m    411\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecomp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"qDl7nWhoEYu3"},"source":["The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."]},{"cell_type":"markdown","metadata":{"id":"Emqy1rBsEYu3"},"source":["### TensorCore mixed precision training\n","\n","The V100 GPUs have 640 tensor cores that can accelerate half precision matrix multiplication calculation which is the core computation done by the DL model. [Apex library](https://github.com/NVIDIA/apex) developed by NVIDIA makes mixed precision and distributed training in Pytorch easy. By changing 3 lines of code, it can use the tensor cores to accelerate the training. "]},{"cell_type":"code","metadata":{"id":"13dB1NeN4Dcv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621541869843,"user_tz":420,"elapsed":3151,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"9ba68dcf-45dc-4801-c688-f9b659d3277c"},"source":["!git clone https://github.com/NVIDIA/apex"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'apex'...\n","remote: Enumerating objects: 8042, done.\u001b[K\n","remote: Counting objects: 100% (129/129), done.\u001b[K\n","remote: Compressing objects: 100% (94/94), done.\u001b[K\n","remote: Total 8042 (delta 61), reused 69 (delta 30), pack-reused 7913\u001b[K\n","Receiving objects: 100% (8042/8042), 14.11 MiB | 11.67 MiB/s, done.\n","Resolving deltas: 100% (5460/5460), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81KC3i9x7AjS","executionInfo":{"status":"ok","timestamp":1621541875018,"user_tz":420,"elapsed":877,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"e431ebf1-0076-4a6a-83d8-8773bff0f4de"},"source":["cd apex"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/apex\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5tIR6ix7CfS","executionInfo":{"status":"ok","timestamp":1621541892416,"user_tz":420,"elapsed":5901,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"8cb4f8d7-6e40-40b8-c4a8-04f8fca905dc"},"source":["!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-ai1_gori\n","Created temporary directory: /tmp/pip-req-tracker-vtnptd92\n","Created requirements tracker '/tmp/pip-req-tracker-vtnptd92'\n","Created temporary directory: /tmp/pip-install-657rtgx2\n","Processing /content/apex\n","  Created temporary directory: /tmp/pip-req-build-v0dzb7mt\n","  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-vtnptd92'\n","    Running setup.py (path:/tmp/pip-req-build-v0dzb7mt/setup.py) egg_info for package from file:///content/apex\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.8.1+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-v0dzb7mt/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-v0dzb7mt/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-v0dzb7mt/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-v0dzb7mt/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-v0dzb7mt/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n","    writing manifest file '/tmp/pip-req-build-v0dzb7mt/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-v0dzb7mt/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-v0dzb7mt has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n","  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-vtnptd92'\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Created temporary directory: /tmp/pip-record-ipoy12vf\n","    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-v0dzb7mt/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-v0dzb7mt/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ipoy12vf/install-record.txt --single-version-externally-managed --compile\n","\n","\n","    torch.__version__  = 1.8.1+cu101\n","\n","\n","    /tmp/pip-req-build-v0dzb7mt/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2020 NVIDIA Corporation\n","    Built on Wed_Jul_22_19:09:09_PDT_2020\n","    Cuda compilation tools, release 11.0, V11.0.221\n","    Build cuda_11.0_bu.TC445_37.28845127_0\n","    from /usr/local/cuda/bin\n","\n","    Traceback (most recent call last):\n","      File \"<string>\", line 1, in <module>\n","      File \"/tmp/pip-req-build-v0dzb7mt/setup.py\", line 171, in <module>\n","        check_cuda_torch_binary_vs_bare_metal(torch.utils.cpp_extension.CUDA_HOME)\n","      File \"/tmp/pip-req-build-v0dzb7mt/setup.py\", line 106, in check_cuda_torch_binary_vs_bare_metal\n","        \"https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  \"\n","    RuntimeError: Cuda extensions are being compiled with a version of Cuda that does not match the version used to compile Pytorch binaries.  Pytorch binaries were compiled with Cuda 10.1.\n","    In some cases, a minor-version mismatch will not cause later errors:  https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  You can try commenting out this check (at your own risk).\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25herror\n","Cleaning up...\n","  Removing source in /tmp/pip-req-build-v0dzb7mt\n","Removed build tracker '/tmp/pip-req-tracker-vtnptd92'\n","\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-v0dzb7mt/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-v0dzb7mt/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ipoy12vf/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n","Exception information:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n","    status = self.run(options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n","    use_user_site=options.use_user_site,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/__init__.py\", line 62, in install_given_reqs\n","    **kwargs\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py\", line 888, in install\n","    cwd=self.unpacked_source_directory,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 275, in runner\n","    spinner=spinner,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 242, in call_subprocess\n","    raise InstallationError(exc_msg)\n","pip._internal.exceptions.InstallationError: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-v0dzb7mt/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-v0dzb7mt/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ipoy12vf/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kIkv55m8EYu3","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1621542356238,"user_tz":420,"elapsed":146972,"user":{"displayName":"Pui Ieng Lei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUxB62djaXyekhwBWrALyZjUPepPF_29I1tV5i=s64","userId":"03446030301916229832"}},"outputId":"cdb4a533-8028-4562-9947-7385eebd1e05"},"source":["from apex import amp\n","from ignite.engine import Engine, Events\n","from torch.nn import MSELoss\n","from ignite.handlers import Timer\n","from torch.optim import Adam\n","from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n","from ignite.handlers import ModelCheckpoint\n","from model import Net\n","from cupy_dataset import NumbaOptionDataSet\n","timer = Timer(average=True)\n","model = Net().cuda()\n","loss_fn = MSELoss()\n","optimizer = Adam(model.parameters(), lr=1e-3)\n","# set the AMP optimization level to O1\n","opt_level = 'O1'\n","# wrap the optimizer and model\n","model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n","dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2)\n","\n","def train_update(engine, batch):\n","    model.train()\n","    optimizer.zero_grad()\n","    x = batch[0]\n","    y = batch[1]\n","    y_pred = model(x)\n","    loss = loss_fn(y_pred[:,0], y)\n","    # amp handles the auto loss scaling\n","    with amp.scale_loss(loss, optimizer) as scaled_loss:\n","        scaled_loss.backward()\n","    optimizer.step()\n","    return loss.item()\n","\n","trainer = Engine(train_update)\n","log_interval = 100\n","timer.attach(trainer,\n","             start=Events.EPOCH_STARTED,\n","             resume=Events.ITERATION_STARTED,\n","             pause=Events.ITERATION_COMPLETED,\n","             step=Events.ITERATION_COMPLETED)    \n","scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n","trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n","    \n","@trainer.on(Events.ITERATION_COMPLETED)\n","def log_training_loss(engine):\n","    iter = (engine.state.iteration - 1) % len(dataset) + 1\n","    if iter % log_interval == 0:\n","        print('loss', engine.state.output, 'average time', timer.value())\n","        \n","trainer.run(dataset, max_epochs=100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py:476: FutureWarning: cupy.random.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.multivariate_normal')\n","/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py:337: FutureWarning: cupy.random.RandomState.multivariate_normal is experimental. The interface can change in the future.\n","  _util.experimental('cupy.random.RandomState.multivariate_normal')\n"],"name":"stderr"},{"output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n","loss 198.99514770507812 average time 0.007484014839999418\n","loss 77.05516815185547 average time 0.007719316529999105\n","loss 46.65948486328125 average time 0.007572146340005474\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0\n","loss 0.5194532871246338 average time 0.007530288619989278\n","loss 1.6039482355117798 average time 0.007623035989990967\n","loss 415.6108703613281 average time 0.007764328390010178\n","loss 4.144603729248047 average time 0.007359686510008032\n"],"name":"stdout"},{"output_type":"stream","text":["Engine run is terminating due to exception: \n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-61536acc41c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m           \u001b[0;31m############ <new - add correlation between stocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m           \u001b[0mall_normals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m           \u001b[0;31m############ new>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/random/_distributions.py\u001b[0m in \u001b[0;36mmultivariate_normal\u001b[0;34m(mean, cov, size, check_valid, tol, method, dtype)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     return rs.multivariate_normal(\n\u001b[0;32m--> 479\u001b[0;31m         mean, cov, size, check_valid, tol, method, dtype)\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/random/_generator.py\u001b[0m in \u001b[0;36mmultivariate_normal\u001b[0;34m(self, mean, cov, size, check_valid, tol, method, dtype)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cholesky'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                         \u001b[0mdecomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eigh'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/linalg/_decomposition.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg_common_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cusolver_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"POgAafQrEYu3"},"source":["It improves to compute each mini-batch in $8ms$. As we reduce the model weights to half precision for better performance, the loss need to be scaled to make sure the half precision dynamic range aligns with the computation. It is guessing what is the correct loss scaling factor and adjust it automatically if the gradient overflows. In the end, we will get the best hardware acceleration while maintaining the accuracy of model prediction."]},{"cell_type":"markdown","metadata":{"id":"4HICOmAVEYu4"},"source":["### Multiple GPU training"]},{"cell_type":"markdown","metadata":{"id":"xYzWTw_SEYu4"},"source":["Apex makes multiple GPU training easy. Working on the same training script, we need to take care of a few extra steps:\n","\n","1. Add the argument `--local_rank` which will be automatically set by the distributed launcher\n","2. Initialize the process group\n","2. Generate independent batched data based on process id in the dataset.\n","3. Wrap the model and optimizer to handle distributed computation. \n","4. Scale the loss and optimizer\n","\n","To launch distributed training, we need to put everything into a python file. Following is an example:-"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"c7w-ch7jC3h9","executionInfo":{"status":"ok","timestamp":1619699660098,"user_tz":-480,"elapsed":1014,"user":{"displayName":"YILIAN ZHANG","photoUrl":"","userId":"00778400246289834299"}},"outputId":"85fd8893-5c79-46cd-cf35-106fd05aeb10"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/apex'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Er21_TT_C4wB","executionInfo":{"status":"ok","timestamp":1619699670522,"user_tz":-480,"elapsed":912,"user":{"displayName":"YILIAN ZHANG","photoUrl":"","userId":"00778400246289834299"}},"outputId":"558cafec-45d8-4bea-a5fd-ec7969dd09cd"},"source":["cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FVTX2kXxEYu4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619699684810,"user_tz":-480,"elapsed":1172,"user":{"displayName":"YILIAN ZHANG","photoUrl":"","userId":"00778400246289834299"}},"outputId":"c1da6484-3064-4513-fa37-7ecb499066b3"},"source":["%%writefile distributed_train.py \n","import cupy\n","import numpy as np\n","import math\n","import time\n","import os\n","import torch\n","from torch.utils.dlpack import from_dlpack\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","from apex import amp\n","from ignite.engine import Engine, Events\n","from torch.nn import MSELoss\n","from torch.optim import Adam\n","from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n","from ignite.handlers import ModelCheckpoint\n","from apex.parallel import DistributedDataParallel \n","import argparse\n","from model import Net\n","from cupy_dataset import OptionDataSet\n","\n","parser = argparse.ArgumentParser()\n","parser = argparse.ArgumentParser()\n","# this local_rank arg is automaticall set by distributed launch\n","parser.add_argument(\"--local_rank\", default=0, type=int)\n","args = parser.parse_args()\n","\n","args.distributed = False\n","if 'WORLD_SIZE' in os.environ:\n","    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n","\n","if args.distributed:\n","    torch.cuda.set_device(args.local_rank)\n","    torch.distributed.init_process_group(backend='nccl',\n","                                         init_method='env://')\n","\n","torch.backends.cudnn.benchmark = True\n","\n","\n","model = Net().cuda()\n","loss_fn = MSELoss()\n","optimizer = Adam(model.parameters(), lr=1e-3)\n","opt_level = 'O1'\n","model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n","if args.distributed:\n","    model = DistributedDataParallel(model)\n","dataset = OptionDataSet(max_len=10000, number_path = 1024, batch=10240, seed=args.local_rank)\n","\n","def train_update(engine, batch):\n","    model.train()\n","    optimizer.zero_grad()\n","    x = batch[0]\n","    y = batch[1]\n","    y_pred = model(x)\n","    loss = loss_fn(y_pred[:,0], y)\n","    with amp.scale_loss(loss, optimizer) as scaled_loss:\n","        scaled_loss.backward()\n","    optimizer.step()\n","    return loss.item()\n","\n","trainer = Engine(train_update)\n","log_interval = 100\n","\n","scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n","trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n","    \n","@trainer.on(Events.ITERATION_COMPLETED)\n","def log_training_loss(engine):\n","    iter = (engine.state.iteration - 1) % len(dataset) + 1\n","    if iter % log_interval == 0:\n","        print('loss', engine.state.output)\n","        \n","trainer.run(dataset, max_epochs=100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting distributed_train.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W45P_nauEYu4"},"source":["To launch multiple processes training, we need to run the following command:-"]},{"cell_type":"code","metadata":{"id":"vnPyq4bBEYu4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619699700908,"user_tz":-480,"elapsed":6272,"user":{"displayName":"YILIAN ZHANG","photoUrl":"","userId":"00778400246289834299"}},"outputId":"ed380b51-5f4a-46b6-c181-deadfa92ecb1"},"source":["%reset -f\n","\n","!python -m torch.distributed.launch --nproc_per_node=4 distributed_train.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["*****************************************\n","Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n","*****************************************\n","Traceback (most recent call last):\n","  File \"distributed_train.py\", line 11, in <module>\n","    from apex import amp\n","ModuleNotFoundError: No module named 'apex'\n","Traceback (most recent call last):\n","  File \"distributed_train.py\", line 11, in <module>\n","    from apex import amp\n","ModuleNotFoundError: No module named 'apex'\n","Traceback (most recent call last):\n","  File \"distributed_train.py\", line 11, in <module>\n","    from apex import amp\n","ModuleNotFoundError: No module named 'apex'\n","Traceback (most recent call last):\n","  File \"distributed_train.py\", line 11, in <module>\n","    from apex import amp\n","ModuleNotFoundError: No module named 'apex'\n","Killing subprocess 290\n","Killing subprocess 291\n","Killing subprocess 292\n","Killing subprocess 293\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 340, in <module>\n","    main()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 326, in main\n","    sigkill_handler(signal.SIGTERM, None)  # not coming back\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 301, in sigkill_handler\n","    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)\n","subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', 'distributed_train.py', '--local_rank=3']' returned non-zero exit status 1.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3OkLCotmEYu5"},"source":["It works and all the GPUs are busy to train this network. However, it has a few problems:-\n","   \n","    1. There is no model serialization so the trained model is not saved\n","    2. There is no validation dataset to check the training progress\n","    3. Most of the time is spent in Monte Carlo simulation hence the training is slow\n","    4. We use a few paths(1024) for each option parameter set which is noise and the model cannot converge to a low cost value.\n","We will address these problems in the next notebook"]},{"cell_type":"code","metadata":{"id":"_mYY70ewEYu5"},"source":[""],"execution_count":null,"outputs":[]}]}