{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "6dcf25e3-dd87-4edf-9b3b-32f7e1d87296"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   5663      0 --:--:-- --:--:-- --:--:--  5642\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9MB 86kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.9MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. \n",
        "\n",
        "Loading all the necessary libraries:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6no5JzH-B6"
      },
      "source": [
        "# !pip install cupy-cuda101"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbkx3hXWnwi8"
      },
      "source": [
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBN3YFOnwi-"
      },
      "source": [
        "The CuPy version of batched barrier option pricing simulation is as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhj4DtLnwi-"
      },
      "source": [
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRjmX5zcnwi_"
      },
      "source": [
        "Note, the parameters (K, B, S0, sigma, mu, r) are passed in as an array with length of batch size. The output array is a two dimensional array flatten to 1-D. The first dimension is for Batch and the second dimension is for Path. \n",
        "\n",
        "Testing it out by entering two sets of option parameters:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4PMo7Inwi_"
      },
      "source": [
        "# N_PATHS = 2048000\n",
        "# N_STEPS = 365\n",
        "# N_BATCH = 2\n",
        "# T = 1.0\n",
        "\n",
        "# K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
        "# B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
        "# S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
        "# sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
        "# mu = cupy.array([0.15, 0.1], dtype=cupy.float32)\n",
        "# r =cupy.array([0.05, 0.05], dtype=cupy.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpWK3wcEnwjA"
      },
      "source": [
        "Put everything into a simple function to launch this GPU kernel. The option prices for each batch is the average of the corresponding path terminal values. This can be computed easily by Cupy function `mean(axis=1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAb34NTnwjA"
      },
      "source": [
        "# def batch_run():\n",
        "#     number_of_threads = 256\n",
        "#     number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
        "#     randoms_gpu = cupy.random.normal(0, 1, N_BATCH*N_PATHS * N_STEPS, dtype=cupy.float32)\n",
        "#     output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     s = time.time()\n",
        "#     cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
        "#                        (output, np.float32(T), K, B, S0, sigma, mu, r,\n",
        "#                         randoms_gpu, N_STEPS, N_PATHS, N_BATCH))\n",
        "#     v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
        "#     cupy.cuda.stream.get_current_stream().synchronize()\n",
        "#     e = time.time()\n",
        "#     print('time', e-s, 'v',v)\n",
        "# batch_run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRgQCelnwjC"
      },
      "source": [
        "This produces the option prices $21.22$ and $0.848$ for these two sets of option parameters in $66ms$.\n",
        "\n",
        "It works efficiently hence we will construct an `OptionDataSet` class to wrap the above code so we can use it in Pytorch. For every `next` element, it generates uniform distributed random option parameters in the specified range, launches the GPU kernel to compute the option prices, convert the CuPy array to Pytorch tensors with zero copy via the DLPack. Note how we implemented the iterable Dataset interface:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1KUra7ZnwjC"
      },
      "source": [
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo46Vf4XnwjD"
      },
      "source": [
        "Put everything related to Pytorch dataset into a file `cupy_dataset.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQUGMBlnwjE"
      },
      "source": [
        "# #%%writefile cupy_dataset.py \n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "# cupy.cuda.set_allocator(None)\n",
        "\n",
        "# cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "# extern \"C\" __global__ void batched_barrier_option(\n",
        "#     float *d_s,\n",
        "#     const float T,\n",
        "#     const float * K,\n",
        "#     const float * B,\n",
        "#     const float * S0,\n",
        "#     const float * sigma,\n",
        "#     const float * mu,\n",
        "#     const float * r,\n",
        "#     const float * d_normals,\n",
        "#     const long N_STEPS,\n",
        "#     const long N_PATHS,\n",
        "#     const long N_BATCH)\n",
        "# {\n",
        "#   unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "#   unsigned stride = blockDim.x * gridDim.x;\n",
        "#   unsigned tid = threadIdx.x;\n",
        "#   const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "#   for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "#   {\n",
        "#     int batch_id = i / N_PATHS;\n",
        "#     int path_id = i % N_PATHS;\n",
        "#     float s_curr = S0[batch_id];\n",
        "#     float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "#     float tmp2 = exp(-r[batch_id]*T);\n",
        "#     unsigned n=0;\n",
        "#     double running_average = 0.0;\n",
        "#     for(unsigned n = 0; n < N_STEPS; n++){\n",
        "#        s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "#        running_average += (s_curr - running_average) / (n + 1.0);\n",
        "#        if (running_average <= B[batch_id]){\n",
        "#            break;\n",
        "#        }\n",
        "#     }\n",
        "\n",
        "#     float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
        "#     d_s[i] = tmp2 * payoff;\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# ''', 'batched_barrier_option')\n",
        "\n",
        "# class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
        "#                               cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "#         Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPAsh7JnwjF"
      },
      "source": [
        "Here is a test code to sample 10 data points with batch size 16:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLKxMF05nwjF"
      },
      "source": [
        "# from cupy_dataset import OptionDataSet\n",
        "# ds = OptionDataSet(10, number_path=100000, batch=16, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlzRTD0nwjG"
      },
      "source": [
        "We can implement the same code by using Numba to accelerate the calculation in GPU:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IsfSwVwnwjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb49636-2adf-46fe-b7f6-c1e6d12a4aa5"
      },
      "source": [
        "# import numba\n",
        "# from numba import cuda\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average)\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]:\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "#         X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "#         # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#         X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "#         # make sure the Barrier is smaller than the Strike price\n",
        "#         X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#         randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#         batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                               X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
        "#         o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
        "#         Y = o.mean(axis = 1) \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=1, seed=15)\n",
        "# for i in ds:\n",
        "#     print(i[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([165.6941], device='cuda:0')\n",
            "tensor([51.4442], device='cuda:0')\n",
            "tensor([31.0613], device='cuda:0')\n",
            "tensor([21.1520], device='cuda:0')\n",
            "tensor([32.8344], device='cuda:0')\n",
            "tensor([21.4236], device='cuda:0')\n",
            "tensor([0.], device='cuda:0')\n",
            "tensor([22.7325], device='cuda:0')\n",
            "tensor([0.0027], device='cuda:0')\n",
            "tensor([66.4288], device='cuda:0')\n",
            "tensor([39.2807], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "727ee689-f6cf-4ca0-8de8-5321c1bd98c5"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "          # X = cupy.random.rand(self.N_STOCKS, 6, dtype=cupy.float32)\n",
        "          X = cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS, 6)\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          # X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32) # parameters\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "          stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i[1])\n",
        "################################# TEST ########################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBOv_RiBsCWa"
      },
      "source": [
        "### PUI TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BME87CgGsFrd"
      },
      "source": [
        "# %%writefile cupy_dataset.py\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def single_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS, s_curr):\n",
        "\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp2 = math.exp(-r*T)\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)    \n",
        "\n",
        "#     for i in range(ii, N_PATHS, stride): # for each path          \n",
        "#         running_average = 0.0\n",
        "\n",
        "#         for j in range(N_STOCKS): # initialize S0\n",
        "#             s_curr[j] = S0[j]\n",
        "\n",
        "#         for n in range(N_STEPS): # for each step\n",
        "#             s_curr_avg = 0.0\n",
        "\n",
        "#             for j in range(N_STOCKS): # for each stock\n",
        "#                 tmp1 = mu[j]*T/N_STEPS  \n",
        "#                 s_curr[j] += tmp1 * s_curr[j] + sigma[j]*s_curr[j]*tmp3*d_normals[i,n,j]\n",
        "#                 s_curr_avg = s_curr_avg + 1.0/(j + 1.0) * (s_curr[j] - s_curr_avg) # S average in this step\n",
        "\n",
        "#             # add stock average to running average\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr_avg - running_average)\n",
        "\n",
        "#             # compare to barrier\n",
        "#             if running_average <= B:\n",
        "#                 break\n",
        "\n",
        "#         payoff = running_average - K if running_average > K else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "    \n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, number_stocks = 3, batch=1, threads=512, seed=15, T=1):\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_STOCKS = number_stocks\n",
        "#         self.N_BATCH = batch\n",
        "#         self.T = np.float32(T)\n",
        "#         self.output = cupy.zeros(self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "\n",
        "#         ############ <new\n",
        "#         self.Z_mean = cupy.zeros(self.N_STOCKS, dtype=cupy.float32)\n",
        "#         self.Z_cov = (-0.2 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*0.4).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#         cupy.fill_diagonal(self.Z_cov, 1)\n",
        "#         ############ new>\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "\n",
        "#         X = cupy.zeros((self.N_BATCH, 3 + self.N_STOCKS * 3), dtype=cupy.float32)\n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "\n",
        "#         for i in range(self.N_BATCH): # for each batch\n",
        "#           self.S0 = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 200\n",
        "#           self.K = 110.0\n",
        "#           self.B = 100.0\n",
        "#           self.sigma = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.mu = cupy.random.rand(self.N_STOCKS, dtype=cupy.float32) * 0.2\n",
        "#           self.r = 0.05\n",
        "#           self.s_curr = cupy.zeros(self.N_STOCKS, dtype=cupy.float32) # used to store s_curr in kernel\n",
        "\n",
        "#           ############ <new - add correlation between stocks\n",
        "#           all_normals = cupy.random.multivariate_normal(self.Z_mean, self.Z_cov, (self.N_PATHS, self.N_STEPS), dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "          \n",
        "#           single_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, self.K, self.B, self.S0, \n",
        "#                                                                                     self.sigma, self.mu, self.r, all_normals, self.N_STEPS, self.N_PATHS, self.N_STOCKS, self.s_curr)\n",
        "#           Y[i] = self.output.mean()\n",
        "\n",
        "#           ############ <new - combine to get X matrix\n",
        "#           X[i,:] = cupy.array([self.K, self.B] + self.S0.tolist() +\n",
        "#                                 self.sigma.tolist() + self.mu.tolist() + [self.r], dtype=cupy.float32)\n",
        "#           ############ new>\n",
        "        \n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "# ds = NumbaOptionDataSet(max_len=10, number_path=100, batch=2, seed=15)\n",
        "# for i in ds:\n",
        "#   print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQt8PqinwjI"
      },
      "source": [
        "# %%writefile model.py\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden=1024):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.fc1 = nn.Linear(6, hidden)\n",
        "#         self.fc2 = nn.Linear(hidden, hidden)\n",
        "#         self.fc3 = nn.Linear(hidden, hidden)\n",
        "#         self.fc4 = nn.Linear(hidden, hidden)\n",
        "#         self.fc5 = nn.Linear(hidden, hidden)\n",
        "#         self.fc6 = nn.Linear(hidden, 1)\n",
        "#         self.register_buffer('norm',\n",
        "#                              torch.tensor([200.0,\n",
        "#                                            198.0,\n",
        "#                                            200.0,\n",
        "#                                            0.4,\n",
        "#                                            0.2,\n",
        "#                                            0.2,]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # normalize the parameter to range [0-1] \n",
        "#         x = x / self.norm\n",
        "#         x = F.elu(self.fc1(x))\n",
        "#         x = F.elu(self.fc2(x))\n",
        "#         x = F.elu(self.fc3(x))\n",
        "#         x = F.elu(self.fc4(x))\n",
        "#         x = F.elu(self.fc5(x))\n",
        "#         return self.fc6(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3c7309-24e6-4ae1-fcce-6b919d8b0e35"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "                                           200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "                                           200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87674ae-09ff-4702-9307-c516a10d2aa9"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/c3/f472843797b5ccbb2f0e806a6927f52c7c9522bfcea8e7e881d39258368b/pytorch_ignite-0.4.5-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 24.8MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 28.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78a2254-fdf3-4d8b-d916-2f3edb0ed7ad"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=2, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.0038585129659622908 average time 0.05689646920001678\n",
            "loss 0.0017782573122531176 average time 0.00486341141000139\n",
            "loss 0.20000244677066803 average time 0.004686843499994211\n",
            "loss 0.5028747320175171 average time 0.004823656369997025\n",
            "loss 0.24546341598033905 average time 0.004941243999999187\n",
            "loss 0.2537771761417389 average time 0.004796870490008587\n",
            "loss 0.6996762752532959 average time 0.004677115920003416\n",
            "loss 0.11817583441734314 average time 0.004744044249998751\n",
            "loss 0.05672599747776985 average time 0.00476704006999853\n",
            "loss 0.2679877281188965 average time 0.004743635239990453\n",
            "loss 0.024315420538187027 average time 0.004603567749991271\n",
            "loss 0.9470547437667847 average time 0.00470041277999826\n",
            "loss 0.6844637989997864 average time 0.004733355819996632\n",
            "loss 0.10903322696685791 average time 0.00478906875002167\n",
            "loss 0.23783883452415466 average time 0.0046038681500022\n",
            "loss 0.0571712888777256 average time 0.004739948049998475\n",
            "loss 0.38784685730934143 average time 0.004668669330001194\n",
            "loss 0.22025611996650696 average time 0.0047856193299799085\n",
            "loss 0.44745370745658875 average time 0.0047576327100114215\n",
            "loss 0.5700281858444214 average time 0.004701397720032219\n",
            "loss 1.1032849550247192 average time 0.004765433809984643\n",
            "loss 0.03922648727893829 average time 0.00463688957001068\n",
            "loss 0.415818452835083 average time 0.004800303469992286\n",
            "loss 0.06619839370250702 average time 0.004684997179983838\n",
            "loss 0.9091931581497192 average time 0.00473808368997652\n",
            "loss 0.06425514817237854 average time 0.00469158565001635\n",
            "loss 0.08789884299039841 average time 0.004700355940012741\n",
            "loss 0.6399361491203308 average time 0.004665284759976202\n",
            "loss 1.7568528652191162 average time 0.004672357010008454\n",
            "loss 0.6927801370620728 average time 0.0046705188200030535\n",
            "loss 0.2734472155570984 average time 0.004632249740002408\n",
            "loss 0.015270335599780083 average time 0.004676004939979066\n",
            "loss 0.60392165184021 average time 0.0045789309400288405\n",
            "loss 0.28910428285598755 average time 0.004598629300012362\n",
            "loss 0.2513757050037384 average time 0.0046626577800179805\n",
            "loss 0.12217944115400314 average time 0.004713238909976099\n",
            "loss 1.0516345500946045 average time 0.0046829870399915305\n",
            "loss 0.01499935518950224 average time 0.00463543583999126\n",
            "loss 0.39171820878982544 average time 0.004672571450032592\n",
            "loss 0.007436634041368961 average time 0.00466660469000999\n",
            "loss 0.8542459011077881 average time 0.004597204160018009\n",
            "loss 0.4970620572566986 average time 0.004686887420007224\n",
            "loss 0.19346633553504944 average time 0.004654851880004572\n",
            "loss 0.01595914363861084 average time 0.004607680500007518\n",
            "loss 0.4130702614784241 average time 0.004804002680025406\n",
            "loss 0.03306752070784569 average time 0.004766120479989695\n",
            "loss 0.9605734944343567 average time 0.004757400930011499\n",
            "loss 0.11304017901420593 average time 0.004676208790042438\n",
            "loss 0.7079121470451355 average time 0.00465615516001435\n",
            "loss 0.39140447974205017 average time 0.0046365636299879045\n",
            "loss 0.0932857021689415 average time 0.004730483399985132\n",
            "loss 0.6637581586837769 average time 0.0046069826799976\n",
            "loss 0.4687536060810089 average time 0.004669702339997457\n",
            "loss 0.17044734954833984 average time 0.004603030210000724\n",
            "loss 0.6137639880180359 average time 0.004547968199976822\n",
            "loss 0.4363258183002472 average time 0.004576542079994397\n",
            "loss 0.3830256164073944 average time 0.004521978709967698\n",
            "loss 0.006598767824470997 average time 0.004634577960027855\n",
            "loss 0.15324275195598602 average time 0.004579095810008767\n",
            "loss 0.28004276752471924 average time 0.004524082780026219\n",
            "loss 0.21267646551132202 average time 0.004573558979982409\n",
            "loss 0.016450367867946625 average time 0.0044953010099879976\n",
            "loss 1.1028155088424683 average time 0.004550050799994096\n",
            "loss 0.16721603274345398 average time 0.004714432890027638\n",
            "loss 0.06623734533786774 average time 0.004588582869982929\n",
            "loss 0.17958904802799225 average time 0.004550792860013644\n",
            "loss 0.8510806560516357 average time 0.00459337797002263\n",
            "loss 0.021831590682268143 average time 0.004592049020020568\n",
            "loss 0.306341290473938 average time 0.004581654379990141\n",
            "loss 0.5076912641525269 average time 0.004671102260012958\n",
            "loss 0.06640646606683731 average time 0.004674652600006084\n",
            "loss 0.0809798389673233 average time 0.004700842600013857\n",
            "loss 1.1688700914382935 average time 0.0046917902000041065\n",
            "loss 0.35136646032333374 average time 0.004805932480016963\n",
            "loss 1.2638314962387085 average time 0.004746647920014766\n",
            "loss 0.038213394582271576 average time 0.00464073017003102\n",
            "loss 0.09332775324583054 average time 0.004543251820009573\n",
            "loss 0.1576586365699768 average time 0.004469301649969566\n",
            "loss 0.3063320517539978 average time 0.004534019519983303\n",
            "loss 0.15937772393226624 average time 0.0047098491599854245\n",
            "loss 0.39834272861480713 average time 0.004665508660023079\n",
            "loss 0.09052032232284546 average time 0.004754826010002944\n",
            "loss 0.10560834407806396 average time 0.00467413428999862\n",
            "loss 0.1829802244901657 average time 0.004935681949991704\n",
            "loss 1.0396734476089478 average time 0.005031777640001564\n",
            "loss 0.2023925483226776 average time 0.005065248999981123\n",
            "loss 0.17357009649276733 average time 0.00491343001999212\n",
            "loss 0.0971159040927887 average time 0.0047525895899843814\n",
            "loss 0.30475369095802307 average time 0.004888440499998978\n",
            "loss 0.2908500134944916 average time 0.0048459795000007945\n",
            "loss 0.41278672218322754 average time 0.004643243100013024\n",
            "loss 0.0015091608511283994 average time 0.004568187029994988\n",
            "loss 0.04149109497666359 average time 0.004665839550020792\n",
            "loss 0.3950432538986206 average time 0.00489510908002103\n",
            "loss 0.08065977692604065 average time 0.0047569323100060505\n",
            "loss 0.7246423363685608 average time 0.004742964399988523\n",
            "loss 0.46198880672454834 average time 0.0047686044299734935\n",
            "loss 0.022738277912139893 average time 0.004701286320000691\n",
            "loss 0.06429030746221542 average time 0.004629467599975214\n",
            "loss 0.08833403885364532 average time 0.004534451369991075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 10000\n",
              "\tepoch: 100\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 100\n",
              "\toutput: 0.08833403885364532\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41cb713-382b-4a0e-88ed-b06ddd89cd64"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[26.0681]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f72ea9-173f-4e50-c8d0-fb943dacdcd5"
      },
      "source": [
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.5394e-04, -1.0371e-04,  4.8766e-04,  1.1956e-01,  3.9489e-01,\n",
              "          3.5838e-01,  3.6811e-04, -7.6997e-05,  3.6901e-04,  1.4604e-01,\n",
              "          4.2417e-01,  3.9377e-01,  3.8934e-04,  6.0267e-06,  4.0937e-04,\n",
              "          1.7997e-01,  3.2281e-01,  4.1249e-01]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "63338e55-d139-407f-a164-a042fb424066"
      },
      "source": [
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2f4b4bde10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be2ac57-fc3e-44a7-ba61-f7a0e95d4444"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-6.5058e-07,  2.0320e-07, -1.2868e-06, -2.2664e-04, -7.3022e-04,\n",
              "          -6.6320e-04, -6.7666e-07,  1.7641e-07, -6.7138e-07, -2.5476e-04,\n",
              "          -7.4334e-04, -7.2711e-04, -7.3867e-07, -1.4147e-08, -7.3973e-07,\n",
              "          -3.5808e-04, -6.3159e-04, -7.4822e-04]], device='cuda:0'),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "4bcc2383-72d0-4787-9a2f-735cfa2ef3db"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, 0.35, 0.1, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2f4af3c210>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAERCAYAAABVU/GxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAW0UlEQVR4nO3df7RdZX3n8fdnElQUMFqi/DYoaoeh/NCLK2itiqiY6Uihpq3tYrRqcSwy0uJi2mEtrV1rukYZ6apTOzYOjDpDLdNCgAI1QJua4mDgBhNIuKDYWg0ECToIVAcL+c4fZ996CPfHyb733H3Cfb/WOiv7PM+z9/nefW/u5z5777NPqgpJkvbUv+i6AEnS3skAkSS1YoBIkloxQCRJrRggkqRWDBBJUiuLLkCSXJLkgSRb52l7RyS5PslEkjuTrJiP7UrSqFt0AQJ8Fjh1Hrf3eeDCqvqXwKuAB+Zx25I0shZdgFTVBuB7/W1JXpLki0k2JfnbJD85yLaSHA0sraobmm0/WlU/mP+qJWn0LLoAmcYa4JyqeiXwIeCPBlzvZcBDSa5I8tUkFyZZMrQqJWmELO26gK4l2Q94NfBnSSabn9n0nQH87hSr3VtVb6G3/14LnAB8C7gMeBdw8XCrlqTuLfoAoTcLe6iqjt+9o6quAK6YYd3twOaq+juAJFcCKzFAJC0Ci/4QVlU9DPx9ktUA6TluwNVvBZYlWd48Pxm4cwhlStLIWXQBkuQLwM3Ay5NsT/Ie4FeA9yTZAmwDThtkW1X1BL1zJn+V5A4gwGeGU7kkjZZ4O3dJUhuLbgYiSZofi+ok+oEHHlgrVqzougxJ2qts2rTpwapavnv7ogqQFStWMD4+3nUZkrRXSfIPU7V7CEuS1EonAZJkdZJtSXYlGZtmzOFJ1jc3KNyW5IN9fRcmuSvJ7UnWJlm2cNVLkqC7GchW4AxgwwxjHgfOq6qj6b057+zm3lMANwDHVNWxwNeA3x5msZKkp+okQKpqoqrunmXMjqq6rVl+BJgADm2eX19VjzdDvwIcNsx6JUlPtVecA2k+Y+MEYOMU3e8G/nKGdc9KMp5kfOfOncMpUJIWoaFdhZXkRuCgKbouqKqr9mA7+wGXA+c2tx3p77uA3qGuS6dbv6rW0LvbLmNjY75rUpLmydACpKpOmes2kuxDLzwubW5s2N/3LuBngTeWb6eXpAU3su8DSe/e6hcDE1V10W59pwLnA6/zA5wkqRtdXcZ7epLtwEnAtUnWNe2HJLmuGfYa4Ezg5CSbm8eqpu8Pgf2BG5r2Ty/01yBJi10nM5CqWgusnaL9PmBVs3wTvbvbTrX+UUMtUJI0q73iKixJ0ugxQCRJrRggkqRWDBBJUisGiCSpFQNEktSKASJJasUAkSS1YoBIkloxQCRJrRggkqRWDBBJUisGiCSpFQNEktSKASJJasUAkSS1YoBIkloxQCRJrRggkqRWDBBJUisGiCSpFQNEktRKJwGSZHWSbUl2JRmbZszhSdYnubMZ+8EpxpyXpJIcOPyqJUn9upqBbAXOADbMMOZx4LyqOhpYCZyd5OjJziSHA28GvjXMQiVJU+skQKpqoqrunmXMjqq6rVl+BJgADu0b8vvA+UANrVBJ0rT2inMgSVYAJwAbm+enAfdW1ZYB1j0ryXiS8Z07dw61TklaTJYOa8NJbgQOmqLrgqq6ag+2sx9wOXBuVT2c5NnAf6R3+GpWVbUGWAMwNjbmbEWS5snQAqSqTpnrNpLsQy88Lq2qK5rmlwBHAluSABwG3JbkVVV1/1xfU5I0mKEFyFyllw4XAxNVddFke1XdAbygb9w3gbGqenDBi5SkRayry3hPT7IdOAm4Nsm6pv2QJNc1w14DnAmcnGRz81jVRb2SpKfqZAZSVWuBtVO03wesapZvAjLAtlbMd32SpNntFVdhSZJGjwEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVjoJkCSrk2xLsivJ2DRjDk+yPsmdzdgP7tZ/TpK7mr6PL0zlkqRJSzt63a3AGcAfzzDmceC8qrotyf7ApiQ3VNWdSd4AnAYcV1WPJXnBAtQsSerTSYBU1QRAkpnG7AB2NMuPJJkADgXuBN4P/Oeqeqzpf2DYNUuSnmyvOAeSZAVwArCxaXoZ8NokG5N8KcmJM6x7VpLxJOM7d+4cfrGStEgMbQaS5EbgoCm6Lqiqq/ZgO/sBlwPnVtXDTfNS4PnASuBE4H8neXFV1e7rV9UaYA3A2NjYU/olSe0MLUCq6pS5biPJPvTC49KquqKvaztwRRMYtyTZBRwIOMWQpAUysoew0jtBcjEwUVUX7dZ9JfCGZtzLgGcADy5shZK0uHV1Ge/pSbYDJwHXJlnXtB+S5Lpm2GuAM4GTk2xuHquavkuAFyfZCvwp8M6pDl9JkoYni+n37tjYWI2Pj3ddhiTtVZJsqqqnvGdvZA9hSZJGmwEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktbJ00IFJjgGOBp412VZVnx9GUZKk0TdQgCT5CPB6egFyHfBW4CbAAJGkRWrQQ1hvB94I3F9VvwocBzy37YsmWZ1kW5JdScamGXN4kvVJ7mzGfrCv7/gkX0myOcl4kle1rUWS1M6gAfLDqtoFPJ7kAOAB4PA5vO5W4AxgwwxjHgfOq6qjgZXA2UmObvo+Dny0qo4HPtw8lyQtoEHPgYwnWQZ8BtgEPArc3PZFq2oCIMlMY3YAO5rlR5JMAIcCdwIFHNAMfS5wX9taJEntDBQgVfXrzeKnk3wROKCqbh9eWU+WZAVwArCxaToXWJfkv9CbRb16oWqRJPXsyVVYxwIrJtdJclRVXTHD+BuBg6bouqCqrtqD190PuBw4t6oebprfD/xGVV2e5BeAi4FTpln/LOAsgCOOOGLQl5UkzSJVNfug5BLgWGAbsKtprqp695xePPkb4ENVNT5N/z7ANcC6qrqor/37wLKqqvSOg32/qg6Yahv9xsbGanx8ypeSJE0jyaaqesoFT4POQFY2J7MXTBMMFwMT/eHRuA94HfA3wMnA1xeyNknS4Fdh3dx3BdScJTk9yXbgJODaJOua9kOSXNcMew1wJnByc7nu5iSrmr5fAz6RZAvwezSHqCRJC2fQQ1ivA64G7gceA0LvENaxwy1vfnkIS5L23FwPYV1MbzZwBz8+ByJJWsQGDZCdVXX1UCuRJO1VBg2Qryb5E+Av6B3CAmCmy3glSU9vgwbIvvSC4819bQUYIJK0SA36TvRfHXYho+wLt3yLDV/b2XUZktTa2W84imMObX0P3CkNejv3I4Fz6HsnOkBVvW1eqxlRDz7yGN/Y+WjXZUhSaz/8pyfmfZuDHsK6kt6VWH/BIrwK65w3vpRz3vjSrsuQpJEyaID8v6r65FArkSTtVQYNkD9oPpXwep58FdZtQ6lKkjTyBg2Qn6K5rQh9N1NsnkuSFqFBA2Q18OKq+tEwi5Ek7T0GvZniVmDZMAuRJO1dBp2BLAPuSnIrTz4Hsigu45UkPdWgAfKRoVYhSdrrDPpO9C8NuxBJ0t5loHMgSVYmuTXJo0l+lOSJJA/PvqYk6elq0JPofwi8g95Hx+4LvBf41LCKkiSNvkEDhKq6B1hSVU9U1f8ATh1eWZKkUTfoSfQfJHkGsCXJx4Ed7EH4SJKefgYNgTObsWcD/wgcBvz8sIqSJI2+GWcgSU4DDquqTzXPvwS8gN5tTG4G7hl6hZKkkTTbDOR8oP+z0J8JvBJ4PfD+IdUkSdoLzBYgz6iqb/c9v6mqvldV3wKeM5cXTrI6ybYku5KMTTPmWUluSbKlGfvRvr4jk2xMck+Sy5pzNJKkBTJbgDyv/0lVfaDv6fI5vvZW4AxgwwxjHgNOrqrjgOOBU5OsbPo+Bvx+VR0F/F/gPXOsR5K0B2YLkI1Jfm33xiTvA26ZywtX1URV3T3LmKqqyc+S3ad5VJLQu5X8nzd9nwN+bi71SJL2zGyX8f4GcGWSXwYmPzzqlfTOhSzIL+wkS4BNwFHAp6pqY5IDgYeq6vFm2Hbg0GnWPws4C+CII45YgIolaXGYMUCq6gHg1UlOBv5V03xtVf31IBtPciNw0BRdF1TVVYNso6qeAI5PsgxYm+QY4P5B1m3WXwOsARgbG6tB15MkzWzQmyn+NTBQaOy23il7XNH023ooyXp674D/BLAsydJmFnIYcO98vZYkaXYj/W7yJMubmQdJ9gXeBNxVVQWsB97eDH0nMNCMRpI0PzoLkCSnJ9kOnARcm2Rd035IkuuaYQcD65PcDtwK3FBV1zR9/wH4zST3AD8BXLywX4EkLW7p/TG/OIyNjdX4+HjXZUjSXiXJpqp6yvv1RvoQliRpdBkgkqRWDBBJUisGiCSpFQNEktSKASJJasUAkSS1YoBIkloxQCRJrRggkqRWDBBJUisGiCSpFQNEktSKASJJasUAkSS1YoBIkloxQCRJrRggkqRWDBBJUisGiCSpFQNEktSKASJJaqWTAEmyOsm2JLuSjE0z5llJbkmypRn70b6+S5PcnWRrkkuS7LNw1UuSoLsZyFbgDGDDDGMeA06uquOA44FTk6xs+i4FfhL4KWBf4L1DrFWSNIWlXbxoVU0AJJlpTAGPNk/3aR7V9F03OS7JLcBhw6pVkjS1kT4HkmRJks3AA8ANVbVxt/59gDOBL86wjbOSjCcZ37lz53ALlqRFZGgBkuTG5hzF7o/TBt1GVT1RVcfTm2G8Kskxuw35I2BDVf3tDNtYU1VjVTW2fPnydl+MJOkphnYIq6pOmcdtPZRkPXAqvfMnJPkIsBx433y9jiRpcCN7CCvJ8iTLmuV9gTcBdzXP3wu8BXhHVe3qrkpJWry6uoz39CTbgZOAa5Osa9oPSTJ5gvxgYH2S24Fb6Z0Duabp+zTwQuDmJJuTfHiBvwRJWvS6ugprLbB2ivb7gFXN8u3ACdOs30ndkqQfG9lDWJKk0WaASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqZXOAiTJ6iTbkuxKMjbNmGcluSXJlmbsR6cY88kkjw6/YklSvy5nIFuBM4ANM4x5DDi5qo4DjgdOTbJysrMJnucNtUpJ0pQ6C5Cqmqiqu2cZU1U1ObvYp3kUQJIlwIXA+UMtVJI0pZE/B5JkSZLNwAPADVW1sen6AHB1Ve2YZf2zkownGd+5c+ewy5WkRWPpMDee5EbgoCm6LqiqqwbZRlU9ARyfZBmwNskxwPeA1cDrB1h/DbAGYGxsrAYsXZI0i6EGSFWdMo/beijJeuBUYAI4CrgnCcCzk9xTVUfN1+tJkmY20oewkixvZh4k2Rd4E3BXVV1bVQdV1YqqWgH8wPCQpIXV5WW8pyfZDpwEXJtkXdN+SJLrmmEHA+uT3A7cSu8cyDXdVCxJ6jfUQ1gzqaq1wNop2u8DVjXLtwMnDLCt/ea9QEnSjEb6EJYkaXQZIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUSqqq6xoWTJKdwD90Xcc0DgQe7LqIGVjf3Fjf3I16jU/n+l5UVct3b1xUATLKkoxX1VjXdUzH+ubG+uZu1GtcjPV5CEuS1IoBIklqxQAZHWu6LmAW1jc31jd3o17joqvPcyCSpFacgUiSWjFAJEmtGCAdSHJ4kvVJ7kyyLckHm/bfSXJvks3NY1WHNX4zyR1NHeNN2/OT3JDk682/z+uotpf37aPNSR5Ocm6X+y/JJUkeSLK1r23K/ZWeTya5J8ntSV7RUX0XJrmrqWFtkmVN+4okP+zbj5/uqL5pv59JfrvZf3cneUtH9V3WV9s3k2xu2rvYf9P9Thnuz2BV+VjgB3Aw8IpmeX/ga8DRwO8AH+q6vqaubwIH7tb2ceC3muXfAj42AnUuAe4HXtTl/gN+BngFsHW2/QWsAv4SCLAS2NhRfW8GljbLH+urb0X/uA7335Tfz+b/yhbgmcCRwDeAJQtd3279nwA+3OH+m+53ylB/Bp2BdKCqdlTVbc3yI8AEcGi3VQ3kNOBzzfLngJ/rsJZJbwS+UVWd3mGgqjYA39utebr9dRrw+er5CrAsycELXV9VXV9VjzdPvwIcNswaZjLN/pvOacCfVtVjVfX3wD3Aq4ZWHDPXlyTALwBfGGYNM5nhd8pQfwYNkI4lWQGcAGxsmj7QTCkv6eoQUaOA65NsSnJW0/bCqtrRLN8PvLCb0p7kl3jyf9xR2X8w/f46FPh237jtdP8HxLvp/UU66cgkX03ypSSv7aoopv5+jtr+ey3wnar6el9bZ/tvt98pQ/0ZNEA6lGQ/4HLg3Kp6GPhvwEuA44Ed9KbFXfnpqnoF8Fbg7CQ/099ZvXlwp9eAJ3kG8Dbgz5qmUdp/TzIK+2s6SS4AHgcubZp2AEdU1QnAbwJ/kuSADkob2e/nbt7Bk/+I6Wz/TfE75Z8N42fQAOlIkn3ofaMvraorAKrqO1X1RFXtAj7DkKflM6mqe5t/HwDWNrV8Z3Ka2/z7QFf1Nd4K3FZV34HR2n+N6fbXvcDhfeMOa9oWXJJ3AT8L/ErzC4bm0NB3m+VN9M4xvGyha5vh+zlK+28pcAZw2WRbV/tvqt8pDPln0ADpQHPM9GJgoqou6mvvPwZ5OrB193UXQpLnJNl/cpneydatwNXAO5th7wSu6qK+Pk/6y29U9l+f6fbX1cC/ba6EWQl8v+8ww4JJcipwPvC2qvpBX/vyJEua5RcDLwX+roP6pvt+Xg38UpJnJjmyqe+Wha6vcQpwV1Vtn2zoYv9N9zuFYf8MLuSVAj7++YqJn6Y3lbwd2Nw8VgH/E7ijab8aOLij+l5M7yqXLcA24IKm/SeAvwK+DtwIPL/Dffgc4LvAc/vaOtt/9IJsB/BP9I4nv2e6/UXvypdP0fvL9A5grKP67qF3HHzyZ/DTzdifb77vm4HbgH/TUX3Tfj+BC5r9dzfw1i7qa9o/C/y73cZ2sf+m+50y1J9Bb2UiSWrFQ1iSpFYMEElSKwaIJKkVA0SS1IoBIklqxQCROpDkd5Oc0nUd0lx4Ga+0wJIsqaonuq5DmitnINI8aj4L4q4klyaZSPLnSZ7dfF7Ex5LcBqxO8tkkb2/WOTHJ/0myJcktSfZPsiS9z+u4tbmZ4PuasQcn2dB8zsTWjm90qEVuadcFSE9DL6f3TuUvJ7kE+PWm/bvVu0Hl5G1EJm8IeRnwi1V1a3PTvR/Seyf296vqxCTPBL6c5Hp6911aV1X/qbldxrMX9kuTfswAkebft6vqy83y/wL+fbN82RRjXw7sqKpbAaq5g2qSNwPHTs5SgOfSu6fSrcAlzY3zrqyqzUP6GqRZGSDS/Nv9xOLk83/cg20EOKeq1j2lo3dr/X8NfDbJRVX1+XZlSnPjORBp/h2R5KRm+ZeBm2YYezdwcJITAZrzH0uBdcD7m5kGSV7W3CX5RfQ+vOgzwH+n9zGrUicMEGn+3U3vQ7gmgOfR+2CkKVXVj4BfBP5rki3ADcCz6IXDncBtSbYCf0zviMHrgS1Jvtqs9wdD/DqkGXkZrzSPmo8Tvaaqjum4FGnonIFIklpxBiJJasUZiCSpFQNEktSKASJJasUAkSS1YoBIklr5/5nCZusIhDzZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "7092ea89-8070-42cf-889f-c39c5d292218"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 100.0, 120.0, sigma, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, sigma, 0.1, 0.05,\n",
        "                                           110.0, 100.0, 120.0, sigma, 0.1, 0.05]]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2f4ae4aad0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "ba1a1630-df39-44d3-cee1-fee32038e84e"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lower bound is too large\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ade689496c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mquoted_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbisection_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoted_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'implied volativity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    }
  ]
}