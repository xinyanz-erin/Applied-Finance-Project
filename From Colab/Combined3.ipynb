{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/From%20Colab/Combined3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "e93e6f77-aafb-45bf-d4a4-ffb570041f65"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0   8272      0 --:--:-- --:--:-- --:--:--  8272\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9 MB 43 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 64.2 MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "ac958b42-0bd5-4600-d2b8-246276885438"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 1.        , 0.05103263, 0.0071633 , 0.52167781,\n",
              "        1.        ],\n",
              "       [1.        , 1.        , 0.64857557, 0.32324551, 0.39745689,\n",
              "        1.        ],\n",
              "       [1.        , 1.        , 0.82301291, 0.46666519, 0.8391176 ,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "264cef2d-1ead-43b8-e5fd-d8a131f38fda"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.00000000e+02, 0.00000000e+00, 1.02065252e+01, 2.86532070e-03,\n",
              "        1.04335564e-01, 2.00000003e-01],\n",
              "       [2.00000000e+02, 0.00000000e+00, 1.29715113e+02, 1.29298207e-01,\n",
              "        7.94913799e-02, 2.00000003e-01],\n",
              "       [2.00000000e+02, 0.00000000e+00, 1.64602581e+02, 1.86666078e-01,\n",
              "        1.67823523e-01, 2.00000003e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "f2076fb5-71df-48ac-d7b9-22ddc2b4e760"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "          #[K,B,S0,sigma,mu,r]\n",
        "          X = cupy.array([])\n",
        "          K_rand = cupy.random.rand(1)[0]\n",
        "          B_rand = cupy.random.rand(1)[0]\n",
        "          r_rand = cupy.random.rand(1)[0]\n",
        "          for i in range(0,self.N_STOCKS):\n",
        "            X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand])))\n",
        "          X = X.reshape(self.N_STOCKS,6)\n",
        "          X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i[0])\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d0f71d-f974-4230-b62e-fb45b5cf5f91"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        # self.register_buffer('norm',\n",
        "        #                      torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([200.0, 0.1, 200.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9eb83b8-995a-489a-f2b9-6cdd72fce1b9"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.5-py3-none-any.whl (221 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30 kB 40.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 40 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 221 kB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "117f1147-e396-492b-e77c-0ee96bc6d607"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 1348.211181640625 average time 0.16685850134999783\n",
            "loss 1128.984375 average time 0.08487756330000593\n",
            "loss 940.9929809570312 average time 0.057521459633339114\n",
            "loss 125.07563781738281 average time 0.04385662178750636\n",
            "loss 72.34493255615234 average time 0.035652158290006356\n",
            "loss 76.38220977783203 average time 0.030184201966676483\n",
            "loss 27.641807556152344 average time 0.026275256678578832\n",
            "loss 31.383182525634766 average time 0.02334492896875844\n",
            "loss 23.642253875732422 average time 0.021068233811120586\n",
            "loss 24.765731811523438 average time 0.019250153810008896\n",
            "loss 27.127243041992188 average time 0.017756466418185565\n",
            "loss 27.962196350097656 average time 0.016511562258339534\n",
            "loss 30.81705665588379 average time 0.015457891973077354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-328a3a63f484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m           \u001b[0mb1_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandoms_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m           \u001b[0mb2_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandoms_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m           \u001b[0mrandoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m           \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/_creation/basic.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemset_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020310b7-76bb-4b4c-de91-165ec82df560"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "model_save_name = 'checkpoint9.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c7f579-08a8-4bf4-bf68-a8a4c98cb1f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22ffb45-fe76-45f0-a475-695474e43333"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'checkpoint9.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6644ffa2-46ce-4857-dea2-9c73907dc3fc"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=18, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564b7a76-287a-4297-a3d7-149ab1b0fa41"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=500, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=50)\n",
        "\n",
        "model_save_name = 'checkpoint10.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 1.1725244522094727 average time 0.4323034850000056 iter num 20\n",
            "loss 0.13988642394542694 average time 0.21764373019999822 iter num 40\n",
            "loss 0.282889187335968 average time 0.14612536759999936 iter num 60\n",
            "loss 0.22838425636291504 average time 0.1103057727874969 iter num 80\n",
            "loss 0.16920830309391022 average time 0.0888204752499962 iter num 100\n",
            "loss 0.1928844153881073 average time 0.07451979626666419 iter num 120\n",
            "loss 0.10348860174417496 average time 0.06428024114285173 iter num 140\n",
            "loss 0.1696300357580185 average time 0.05659896219374616 iter num 160\n",
            "loss 0.17314733564853668 average time 0.050624802322219394 iter num 180\n",
            "loss 0.1723569631576538 average time 0.04584875596999609 iter num 200\n",
            "loss 0.09975221008062363 average time 0.04194215399999476 iter num 220\n",
            "loss 0.14578139781951904 average time 0.038692223483329256 iter num 240\n",
            "loss 0.17541144788265228 average time 0.03593491178845975 iter num 260\n",
            "loss 0.1732379049062729 average time 0.033573120407139966 iter num 280\n",
            "loss 0.15464916825294495 average time 0.0315225837899978 iter num 300\n",
            "loss 0.1017194464802742 average time 0.029730472412497023 iter num 320\n",
            "loss 0.176366925239563 average time 0.028146256688231622 iter num 340\n",
            "loss 0.10251648724079132 average time 0.026740373902776407 iter num 360\n",
            "loss 0.15575680136680603 average time 0.02548089562368372 iter num 380\n",
            "loss 0.15619884431362152 average time 0.024348841949998246 iter num 400\n",
            "loss 0.08026155829429626 average time 0.0233248512595207 iter num 420\n",
            "loss 0.060240548104047775 average time 0.022397566575001932 iter num 440\n",
            "loss 0.0990242213010788 average time 0.021545207806522998 iter num 460\n",
            "loss 0.09665223956108093 average time 0.0207675962583347 iter num 480\n",
            "loss 0.13735240697860718 average time 0.020048716929999957 iter num 500\n",
            "loss 0.23656272888183594 average time 0.16791606725003022 iter num 20\n",
            "loss 0.8294192552566528 average time 0.08534876390003773 iter num 40\n",
            "loss 0.739423394203186 average time 0.05787896006668234 iter num 60\n",
            "loss 0.19335360825061798 average time 0.04412072375002367 iter num 80\n",
            "loss 0.27661773562431335 average time 0.03586151481002617 iter num 100\n",
            "loss 0.1501910388469696 average time 0.030355482816689042 iter num 120\n",
            "loss 0.0899299830198288 average time 0.02641063202145233 iter num 140\n",
            "loss 0.24905912578105927 average time 0.02346949360627093 iter num 160\n",
            "loss 0.299672931432724 average time 0.021170965427801194 iter num 180\n",
            "loss 0.24242638051509857 average time 0.019333830330019737 iter num 200\n",
            "loss 0.33476051688194275 average time 0.01783035075456031 iter num 220\n",
            "loss 0.22592024505138397 average time 0.016577768479176788 iter num 240\n",
            "loss 0.2217099517583847 average time 0.015524329350011192 iter num 260\n",
            "loss 0.3375847339630127 average time 0.01461977969286506 iter num 280\n",
            "loss 0.10429944097995758 average time 0.013832473803342207 iter num 300\n",
            "loss 0.23216266930103302 average time 0.013143753109383738 iter num 320\n",
            "loss 0.08476445078849792 average time 0.012537425388242863 iter num 340\n",
            "loss 0.0740097165107727 average time 0.01199454342223084 iter num 360\n",
            "loss 0.14138692617416382 average time 0.011511985521061481 iter num 380\n",
            "loss 0.08961553126573563 average time 0.011076047717507436 iter num 400\n",
            "loss 0.10143586993217468 average time 0.010681168911912124 iter num 420\n",
            "loss 0.09285852313041687 average time 0.010324278827278538 iter num 440\n",
            "loss 0.1051395833492279 average time 0.00999794240435182 iter num 460\n",
            "loss 0.07160324603319168 average time 0.009706426943756697 iter num 480\n",
            "loss 0.06409528851509094 average time 0.009433020368009238 iter num 500\n",
            "loss 0.10365010052919388 average time 0.1674612766999644 iter num 20\n",
            "loss 1.1135140657424927 average time 0.08512395342501122 iter num 40\n",
            "loss 0.2837657332420349 average time 0.05772387786671516 iter num 60\n",
            "loss 0.2027004063129425 average time 0.0440070087500601 iter num 80\n",
            "loss 0.7246109247207642 average time 0.03578493781005363 iter num 100\n",
            "loss 0.5481668710708618 average time 0.030291620691707523 iter num 120\n",
            "loss 0.5359918475151062 average time 0.026365658621453545 iter num 140\n",
            "loss 0.9636030197143555 average time 0.02343834429376841 iter num 160\n",
            "loss 0.4558601975440979 average time 0.021147334844466868 iter num 180\n",
            "loss 0.2216058075428009 average time 0.01931446812002605 iter num 200\n",
            "loss 0.344849169254303 average time 0.01783949865910844 iter num 220\n",
            "loss 0.21561716496944427 average time 0.016586984533343715 iter num 240\n",
            "loss 0.33159199357032776 average time 0.015528454242312015 iter num 260\n",
            "loss 0.164971262216568 average time 0.014624206196417942 iter num 280\n",
            "loss 0.09253305196762085 average time 0.013834565509998053 iter num 300\n",
            "loss 0.09447838366031647 average time 0.013144989540636231 iter num 320\n",
            "loss 0.07234273850917816 average time 0.012536254035302698 iter num 340\n",
            "loss 0.16778568923473358 average time 0.011996764427793651 iter num 360\n",
            "loss 0.07237565517425537 average time 0.01151580707633361 iter num 380\n",
            "loss 0.15781544148921967 average time 0.01108290463002163 iter num 400\n",
            "loss 0.09350211918354034 average time 0.010698231673832414 iter num 420\n",
            "loss 0.09954997897148132 average time 0.010339728572753094 iter num 440\n",
            "loss 0.12710033357143402 average time 0.010012251471764854 iter num 460\n",
            "loss 0.07374505698680878 average time 0.009711320087527042 iter num 480\n",
            "loss 0.09274225682020187 average time 0.009434047206025753 iter num 500\n",
            "loss 0.17842289805412292 average time 0.16734713039982124 iter num 20\n",
            "loss 0.34497010707855225 average time 0.08512313537491992 iter num 40\n",
            "loss 0.18231341242790222 average time 0.05779859151665126 iter num 60\n",
            "loss 0.21658317744731903 average time 0.04407507036246443 iter num 80\n",
            "loss 0.17213258147239685 average time 0.03582259075996262 iter num 100\n",
            "loss 0.22983209788799286 average time 0.03035122884997084 iter num 120\n",
            "loss 0.14532607793807983 average time 0.026414536971398256 iter num 140\n",
            "loss 0.21541330218315125 average time 0.02346119125622863 iter num 160\n",
            "loss 0.7215626239776611 average time 0.02116984106665364 iter num 180\n",
            "loss 0.14273202419281006 average time 0.019338160739989688 iter num 200\n",
            "loss 0.12899184226989746 average time 0.01783775445455004 iter num 220\n",
            "loss 0.0845571905374527 average time 0.016596233825002096 iter num 240\n",
            "loss 0.14135968685150146 average time 0.015539368661540301 iter num 260\n",
            "loss 0.1597726047039032 average time 0.014628323499999039 iter num 280\n",
            "loss 0.10905808210372925 average time 0.013842290426673572 iter num 300\n",
            "loss 0.09071145206689835 average time 0.01316940234374897 iter num 320\n",
            "loss 0.06497698277235031 average time 0.012559022555877378 iter num 340\n",
            "loss 0.1321195513010025 average time 0.01201865292221353 iter num 360\n",
            "loss 0.17652961611747742 average time 0.011531101036833111 iter num 380\n",
            "loss 0.13691037893295288 average time 0.011095709024989447 iter num 400\n",
            "loss 0.0913483053445816 average time 0.010700276335709143 iter num 420\n",
            "loss 0.0660901814699173 average time 0.010341710822721822 iter num 440\n",
            "loss 0.13059014081954956 average time 0.010016503741294598 iter num 460\n",
            "loss 0.08555281162261963 average time 0.009718821839569122 iter num 480\n",
            "loss 0.06664268672466278 average time 0.009444327931989393 iter num 500\n",
            "loss 0.20711465179920197 average time 0.1671966820999387 iter num 20\n",
            "loss 0.2871805429458618 average time 0.08502332202499474 iter num 40\n",
            "loss 0.2657255232334137 average time 0.05761948611667928 iter num 60\n",
            "loss 0.6770208477973938 average time 0.043914171950018496 iter num 80\n",
            "loss 0.61906898021698 average time 0.035692033850009464 iter num 100\n",
            "loss 0.6847019195556641 average time 0.03020623493335582 iter num 120\n",
            "loss 0.3457803726196289 average time 0.02629919529286521 iter num 140\n",
            "loss 0.12833639979362488 average time 0.023365956050002978 iter num 160\n",
            "loss 0.3038272559642792 average time 0.02107827628334336 iter num 180\n",
            "loss 0.29119622707366943 average time 0.019248827145001996 iter num 200\n",
            "loss 0.1318397969007492 average time 0.017773298613647882 iter num 220\n",
            "loss 0.10723179578781128 average time 0.016526302479174623 iter num 240\n",
            "loss 0.1775815635919571 average time 0.0154757717153958 iter num 260\n",
            "loss 0.10824491083621979 average time 0.014581689514287583 iter num 280\n",
            "loss 0.19254612922668457 average time 0.013799326916659993 iter num 300\n",
            "loss 0.0798163115978241 average time 0.013119431124991365 iter num 320\n",
            "loss 0.16131055355072021 average time 0.012516128399984689 iter num 340\n",
            "loss 0.1024336889386177 average time 0.01199940684720736 iter num 360\n",
            "loss 0.16101673245429993 average time 0.011515976513163351 iter num 380\n",
            "loss 0.059996724128723145 average time 0.01108254350749803 iter num 400\n",
            "loss 0.1001947820186615 average time 0.010691594014278528 iter num 420\n",
            "loss 0.10104189068078995 average time 0.01033649043636987 iter num 440\n",
            "loss 0.0886797308921814 average time 0.010012153382615947 iter num 460\n",
            "loss 0.06677812337875366 average time 0.009713593570846038 iter num 480\n",
            "loss 0.08942659199237823 average time 0.009446836894019725 iter num 500\n",
            "loss 0.43322205543518066 average time 0.16786321500003396 iter num 20\n",
            "loss 0.18585118651390076 average time 0.08535112924987516 iter num 40\n",
            "loss 0.19715817272663116 average time 0.057877793116616276 iter num 60\n",
            "loss 0.1608165204524994 average time 0.044115539699964755 iter num 80\n",
            "loss 0.23828232288360596 average time 0.035869496189970956 iter num 100\n",
            "loss 0.13462772965431213 average time 0.030383552774992495 iter num 120\n",
            "loss 0.28735852241516113 average time 0.026465233971417288 iter num 140\n",
            "loss 0.14829537272453308 average time 0.023515809674972844 iter num 160\n",
            "loss 0.6265444159507751 average time 0.021218356755505537 iter num 180\n",
            "loss 0.3267555832862854 average time 0.019378063244939767 iter num 200\n",
            "loss 0.15977810323238373 average time 0.017870387749967034 iter num 220\n",
            "loss 0.10474968701601028 average time 0.016619375029146493 iter num 240\n",
            "loss 0.09902837127447128 average time 0.0155707888615185 iter num 260\n",
            "loss 0.1603112816810608 average time 0.01465843138213521 iter num 280\n",
            "loss 0.2017795443534851 average time 0.013870456639997428 iter num 300\n",
            "loss 0.11746068298816681 average time 0.013182793815610694 iter num 320\n",
            "loss 0.09565922617912292 average time 0.012577019770571282 iter num 340\n",
            "loss 0.11417581886053085 average time 0.012042889405534475 iter num 360\n",
            "loss 0.09463812410831451 average time 0.01156198632628921 iter num 380\n",
            "loss 0.14920461177825928 average time 0.011126698142479654 iter num 400\n",
            "loss 0.15574955940246582 average time 0.010743937585703817 iter num 420\n",
            "loss 0.16469508409500122 average time 0.01038501025453694 iter num 440\n",
            "loss 0.05561825633049011 average time 0.010063330963033008 iter num 460\n",
            "loss 0.1413695365190506 average time 0.009768188149985235 iter num 480\n",
            "loss 0.09940046072006226 average time 0.009496118959981686 iter num 500\n",
            "loss 0.2345089614391327 average time 0.16771053170032246 iter num 20\n",
            "loss 0.14793053269386292 average time 0.08525478447522801 iter num 40\n",
            "loss 0.23332534730434418 average time 0.05777055686685344 iter num 60\n",
            "loss 0.2548791170120239 average time 0.044048466800200005 iter num 80\n",
            "loss 0.24976688623428345 average time 0.0358311179501834 iter num 100\n",
            "loss 0.17201320827007294 average time 0.030325255808505366 iter num 120\n",
            "loss 0.3868184983730316 average time 0.026415600978777678 iter num 140\n",
            "loss 0.710967481136322 average time 0.023478669693906794 iter num 160\n",
            "loss 0.6705000996589661 average time 0.02118581752236221 iter num 180\n",
            "loss 0.8446608185768127 average time 0.019349060800113876 iter num 200\n",
            "loss 0.1320168673992157 average time 0.017851072190991215 iter num 220\n",
            "loss 0.28462618589401245 average time 0.016606687800079574 iter num 240\n",
            "loss 0.17334187030792236 average time 0.015554986046236782 iter num 260\n",
            "loss 0.10617434978485107 average time 0.014645921885806145 iter num 280\n",
            "loss 0.19882018864154816 average time 0.013858880536754442 iter num 300\n",
            "loss 0.09710031747817993 average time 0.013168740209459883 iter num 320\n",
            "loss 0.06003328785300255 average time 0.012563811258929919 iter num 340\n",
            "loss 0.11115820705890656 average time 0.012023072519544561 iter num 360\n",
            "loss 0.10544009506702423 average time 0.01153897353167275 iter num 380\n",
            "loss 0.14603719115257263 average time 0.011104897442587571 iter num 400\n",
            "loss 0.07359249889850616 average time 0.01071277659770463 iter num 420\n",
            "loss 0.07670674473047256 average time 0.010354925636441047 iter num 440\n",
            "loss 0.15199014544487 average time 0.01002765434356226 iter num 460\n",
            "loss 0.16946564614772797 average time 0.009730008764650695 iter num 480\n",
            "loss 0.11199095845222473 average time 0.009452635486057261 iter num 500\n",
            "loss 0.21715456247329712 average time 0.16894538840006135 iter num 20\n",
            "loss 0.43265578150749207 average time 0.08593699262514747 iter num 40\n",
            "loss 1.242600679397583 average time 0.058237897449998854 iter num 60\n",
            "loss 1.437411904335022 average time 0.044390429350005436 iter num 80\n",
            "loss 0.4521212875843048 average time 0.03608591830001387 iter num 100\n",
            "loss 0.2380119264125824 average time 0.030543831825025337 iter num 120\n",
            "loss 0.1545235514640808 average time 0.026577762621452297 iter num 140\n",
            "loss 0.12551584839820862 average time 0.023622222281267112 iter num 160\n",
            "loss 0.12706130743026733 average time 0.021313846338903708 iter num 180\n",
            "loss 0.18727871775627136 average time 0.01948822396999276 iter num 200\n",
            "loss 0.2407853901386261 average time 0.01797075529998851 iter num 220\n",
            "loss 0.17078201472759247 average time 0.01672374214165302 iter num 240\n",
            "loss 0.17114928364753723 average time 0.015659594646135352 iter num 260\n",
            "loss 0.09101724624633789 average time 0.014744658353564383 iter num 280\n",
            "loss 0.16636544466018677 average time 0.0139490339066712 iter num 300\n",
            "loss 0.08635103702545166 average time 0.013254397353125568 iter num 320\n",
            "loss 0.09575941413640976 average time 0.012646464547057857 iter num 340\n",
            "loss 0.18658486008644104 average time 0.012099493855556729 iter num 360\n",
            "loss 0.13248386979103088 average time 0.01162177271579295 iter num 380\n",
            "loss 0.11538764834403992 average time 0.01118651568500809 iter num 400\n",
            "loss 0.09229478240013123 average time 0.010788597661905356 iter num 420\n",
            "loss 0.09455844759941101 average time 0.010434000806824108 iter num 440\n",
            "loss 0.10458192229270935 average time 0.01010125375218511 iter num 460\n",
            "loss 0.06578566879034042 average time 0.009797891318756531 iter num 480\n",
            "loss 0.14500901103019714 average time 0.009520374605999677 iter num 500\n",
            "loss 0.2763579189777374 average time 0.16809465219985215 iter num 20\n",
            "loss 0.13877183198928833 average time 0.08547779989989976 iter num 40\n",
            "loss 0.1939009130001068 average time 0.05792449893324374 iter num 60\n",
            "loss 0.17786771059036255 average time 0.0441936406124114 iter num 80\n",
            "loss 0.2892407774925232 average time 0.03592772304993559 iter num 100\n",
            "loss 0.5136467218399048 average time 0.030425674816630513 iter num 120\n",
            "loss 0.7337142825126648 average time 0.026501467378485747 iter num 140\n",
            "loss 0.2996332347393036 average time 0.02354472773741918 iter num 160\n",
            "loss 0.1678389310836792 average time 0.021253326899901972 iter num 180\n",
            "loss 0.16510474681854248 average time 0.01941416525488421 iter num 200\n",
            "loss 0.11110056191682816 average time 0.017913074054434798 iter num 220\n",
            "loss 0.316403329372406 average time 0.016657401795722157 iter num 240\n",
            "loss 0.2038244605064392 average time 0.015597293880651146 iter num 260\n",
            "loss 0.3932928442955017 average time 0.01468469048917151 iter num 280\n",
            "loss 0.14562784135341644 average time 0.013898244039895265 iter num 300\n",
            "loss 0.12346819043159485 average time 0.01320974683114855 iter num 320\n",
            "loss 0.08176247775554657 average time 0.012602438994021855 iter num 340\n",
            "loss 0.14717964828014374 average time 0.012063136263805063 iter num 360\n",
            "loss 0.17225083708763123 average time 0.01157993431306271 iter num 380\n",
            "loss 0.127998948097229 average time 0.01114420512992183 iter num 400\n",
            "loss 0.06750670075416565 average time 0.010753270488025702 iter num 420\n",
            "loss 0.14875340461730957 average time 0.010405699022658154 iter num 440\n",
            "loss 0.06878544390201569 average time 0.010080856606478134 iter num 460\n",
            "loss 0.09959160536527634 average time 0.009777145152035397 iter num 480\n",
            "loss 0.11624177545309067 average time 0.009506497789960123 iter num 500\n",
            "loss 0.1375632882118225 average time 0.17061893360005342 iter num 20\n",
            "loss 0.23232507705688477 average time 0.08674549225006559 iter num 40\n",
            "loss 0.36439046263694763 average time 0.05878449145005409 iter num 60\n",
            "loss 0.3171302080154419 average time 0.04481705908758613 iter num 80\n",
            "loss 0.7764749526977539 average time 0.03641545665012018 iter num 100\n",
            "loss 0.20682868361473083 average time 0.03081922370843131 iter num 120\n",
            "loss 0.12502551078796387 average time 0.026846998421504915 iter num 140\n",
            "loss 0.14953307807445526 average time 0.023858838775106507 iter num 160\n",
            "loss 0.18052716553211212 average time 0.021543569388955398 iter num 180\n",
            "loss 0.19668418169021606 average time 0.019673917645059193 iter num 200\n",
            "loss 0.13872721791267395 average time 0.018146694654611068 iter num 220\n",
            "loss 0.1141800731420517 average time 0.01686966041254436 iter num 240\n",
            "loss 0.1506194770336151 average time 0.015791116392336754 iter num 260\n",
            "loss 0.10861443728208542 average time 0.01486532107502561 iter num 280\n",
            "loss 0.13507826626300812 average time 0.014074004603329134 iter num 300\n",
            "loss 0.07580381631851196 average time 0.013386932543721742 iter num 320\n",
            "loss 0.11223557591438293 average time 0.012768597961718007 iter num 340\n",
            "loss 0.1355244219303131 average time 0.012218116322168902 iter num 360\n",
            "loss 0.10822391510009766 average time 0.011728607168381706 iter num 380\n",
            "loss 0.1048269122838974 average time 0.011281822109958739 iter num 400\n",
            "loss 0.09769519418478012 average time 0.01087989338090362 iter num 420\n",
            "loss 0.08913083374500275 average time 0.010513376756791166 iter num 440\n",
            "loss 0.11815974116325378 average time 0.010178538332595211 iter num 460\n",
            "loss 0.09743281453847885 average time 0.009880158345837723 iter num 480\n",
            "loss 0.10669482499361038 average time 0.009600374734014622 iter num 500\n",
            "loss 0.2507040202617645 average time 0.16911476789991867 iter num 20\n",
            "loss 0.48289453983306885 average time 0.08596662879999713 iter num 40\n",
            "loss 0.5182895660400391 average time 0.05826121431679591 iter num 60\n",
            "loss 0.3040875792503357 average time 0.044409439612627465 iter num 80\n",
            "loss 0.18033568561077118 average time 0.0360948812001152 iter num 100\n",
            "loss 0.284237802028656 average time 0.030601949908517175 iter num 120\n",
            "loss 0.24328042566776276 average time 0.026632581957268746 iter num 140\n",
            "loss 0.17709261178970337 average time 0.02366776715641663 iter num 160\n",
            "loss 0.11711043119430542 average time 0.021361715411270174 iter num 180\n",
            "loss 0.24555012583732605 average time 0.019514509145137707 iter num 200\n",
            "loss 0.2563457489013672 average time 0.018017068854715036 iter num 220\n",
            "loss 0.08434596657752991 average time 0.016751904350121548 iter num 240\n",
            "loss 0.1761852353811264 average time 0.015682206646264463 iter num 260\n",
            "loss 0.1037326231598854 average time 0.014767971739365749 iter num 280\n",
            "loss 0.18623846769332886 average time 0.013978318003391905 iter num 300\n",
            "loss 0.2068292200565338 average time 0.013284156253189395 iter num 320\n",
            "loss 0.12284384667873383 average time 0.012681165964750107 iter num 340\n",
            "loss 0.15151309967041016 average time 0.012132536147247366 iter num 360\n",
            "loss 0.13076572120189667 average time 0.011642624781617135 iter num 380\n",
            "loss 0.04919031634926796 average time 0.01120245641503061 iter num 400\n",
            "loss 0.040391452610492706 average time 0.01080381875718948 iter num 420\n",
            "loss 0.09260790795087814 average time 0.010444927061400284 iter num 440\n",
            "loss 0.10837734490633011 average time 0.010113641306563058 iter num 460\n",
            "loss 0.135703906416893 average time 0.009812799785436255 iter num 480\n",
            "loss 0.1131180077791214 average time 0.009535717358012334 iter num 500\n",
            "loss 0.11156287044286728 average time 0.1692039688497971 iter num 20\n",
            "loss 0.6317819356918335 average time 0.08605341519996727 iter num 40\n",
            "loss 0.21363165974617004 average time 0.05833126139993207 iter num 60\n",
            "loss 0.7657220363616943 average time 0.04445508267508558 iter num 80\n",
            "loss 1.0069339275360107 average time 0.036131057270140446 iter num 100\n",
            "loss 0.8083545565605164 average time 0.030581255433450374 iter num 120\n",
            "loss 0.23172535002231598 average time 0.026620735414378163 iter num 140\n",
            "loss 0.2710644006729126 average time 0.023662702018827985 iter num 160\n",
            "loss 0.20222076773643494 average time 0.021357946144538193 iter num 180\n",
            "loss 0.14689412713050842 average time 0.019507860530102335 iter num 200\n",
            "loss 0.11240896582603455 average time 0.017993797722787846 iter num 220\n",
            "loss 0.18047046661376953 average time 0.016733754700029142 iter num 240\n",
            "loss 0.17032074928283691 average time 0.01566655151927989 iter num 260\n",
            "loss 0.15787023305892944 average time 0.014750688757223024 iter num 280\n",
            "loss 0.05246780067682266 average time 0.013959694170104437 iter num 300\n",
            "loss 0.07431989908218384 average time 0.01326519590946873 iter num 320\n",
            "loss 0.12033750116825104 average time 0.012685132417712285 iter num 340\n",
            "loss 0.10322584956884384 average time 0.01213829096117883 iter num 360\n",
            "loss 0.12794390320777893 average time 0.011647594697416552 iter num 380\n",
            "loss 0.15339787304401398 average time 0.01120483126003819 iter num 400\n",
            "loss 0.06992244720458984 average time 0.010803324854809728 iter num 420\n",
            "loss 0.11544279009103775 average time 0.01044124251824211 iter num 440\n",
            "loss 0.08495321869850159 average time 0.010109034691373855 iter num 460\n",
            "loss 0.06588029861450195 average time 0.009805751245911173 iter num 480\n",
            "loss 0.042859431356191635 average time 0.009529852446088626 iter num 500\n",
            "loss 0.2932313084602356 average time 0.16936859420020484 iter num 20\n",
            "loss 0.2936083972454071 average time 0.0861088073503197 iter num 40\n",
            "loss 0.209974467754364 average time 0.05836033313341128 iter num 60\n",
            "loss 0.3531673550605774 average time 0.04447412046260979 iter num 80\n",
            "loss 0.34292587637901306 average time 0.03620191647009051 iter num 100\n",
            "loss 0.4278727173805237 average time 0.030642110083408624 iter num 120\n",
            "loss 0.4597037136554718 average time 0.026676853521530576 iter num 140\n",
            "loss 0.23753339052200317 average time 0.023703808943810144 iter num 160\n",
            "loss 0.12450861930847168 average time 0.021383411861158188 iter num 180\n",
            "loss 0.3042970895767212 average time 0.01953187725504904 iter num 200\n",
            "loss 0.4342137575149536 average time 0.018014971859156504 iter num 220\n",
            "loss 0.15210284292697906 average time 0.016750239870907534 iter num 240\n",
            "loss 0.1601737141609192 average time 0.0156788793846807 iter num 260\n",
            "loss 0.1807999461889267 average time 0.014759587164345639 iter num 280\n",
            "loss 0.13077020645141602 average time 0.013973068196707268 iter num 300\n",
            "loss 0.22190913558006287 average time 0.013294945712505068 iter num 320\n",
            "loss 0.07912220060825348 average time 0.012679061958817114 iter num 340\n",
            "loss 0.13776777684688568 average time 0.01213739497776866 iter num 360\n",
            "loss 0.06410147249698639 average time 0.011652198707882857 iter num 380\n",
            "loss 0.06157046556472778 average time 0.011214049167501799 iter num 400\n",
            "loss 0.15890896320343018 average time 0.010818044957124498 iter num 420\n",
            "loss 0.11490941047668457 average time 0.010458352495449866 iter num 440\n",
            "loss 0.05003512650728226 average time 0.010128285319573795 iter num 460\n",
            "loss 0.04835290461778641 average time 0.009823483156249797 iter num 480\n",
            "loss 0.07277563214302063 average time 0.00954449577799096 iter num 500\n",
            "loss 0.18870477378368378 average time 0.16749234219969367 iter num 20\n",
            "loss 0.45031434297561646 average time 0.0851705709246744 iter num 40\n",
            "loss 1.1089218854904175 average time 0.057727191599891134 iter num 60\n",
            "loss 0.3778928816318512 average time 0.04402043056243201 iter num 80\n",
            "loss 0.23802438378334045 average time 0.0357727392599918 iter num 100\n",
            "loss 0.21874749660491943 average time 0.030276258058347594 iter num 120\n",
            "loss 0.23148494958877563 average time 0.02636318237143444 iter num 140\n",
            "loss 0.15611353516578674 average time 0.023422644931270043 iter num 160\n",
            "loss 0.17763769626617432 average time 0.021148194750094263 iter num 180\n",
            "loss 0.14647036790847778 average time 0.01931964045506902 iter num 200\n",
            "loss 0.37531521916389465 average time 0.01782333309096099 iter num 220\n",
            "loss 0.11178113520145416 average time 0.01657075055009045 iter num 240\n",
            "loss 0.2556791305541992 average time 0.01552544473859318 iter num 260\n",
            "loss 0.1919749528169632 average time 0.01462157149298946 iter num 280\n",
            "loss 0.09388083219528198 average time 0.013842041646809472 iter num 300\n",
            "loss 0.07850697636604309 average time 0.01316669013763203 iter num 320\n",
            "loss 0.20534637570381165 average time 0.012558227014832321 iter num 340\n",
            "loss 0.06569018959999084 average time 0.012026020208435916 iter num 360\n",
            "loss 0.12116091698408127 average time 0.011549454031697487 iter num 380\n",
            "loss 0.1587456315755844 average time 0.011113109175148565 iter num 400\n",
            "loss 0.09574316442012787 average time 0.010716709878718013 iter num 420\n",
            "loss 0.06151491031050682 average time 0.010365038300113321 iter num 440\n",
            "loss 0.08690927922725677 average time 0.010036308515317734 iter num 460\n",
            "loss 0.07950359582901001 average time 0.009747515923011936 iter num 480\n",
            "loss 0.07440260797739029 average time 0.009469270574089023 iter num 500\n",
            "loss 0.37205955386161804 average time 0.16785522179943654 iter num 20\n",
            "loss 0.1771460771560669 average time 0.08538457377480882 iter num 40\n",
            "loss 0.4786251187324524 average time 0.057885891716675054 iter num 60\n",
            "loss 0.17522898316383362 average time 0.044164571149940456 iter num 80\n",
            "loss 0.16516508162021637 average time 0.035896339339851695 iter num 100\n",
            "loss 0.1445339173078537 average time 0.030394965491602 iter num 120\n",
            "loss 0.14914126694202423 average time 0.026461787357064687 iter num 140\n",
            "loss 0.24368737637996674 average time 0.02352503643737691 iter num 160\n",
            "loss 0.11580027639865875 average time 0.021229786922130088 iter num 180\n",
            "loss 0.1405361145734787 average time 0.01938774258489502 iter num 200\n",
            "loss 0.1786089837551117 average time 0.01788678139991217 iter num 220\n",
            "loss 0.20708054304122925 average time 0.01662957447908108 iter num 240\n",
            "loss 0.11361286789178848 average time 0.015572532049918663 iter num 260\n",
            "loss 0.23411084711551666 average time 0.014667455717738318 iter num 280\n",
            "loss 0.15196731686592102 average time 0.013878806226530286 iter num 300\n",
            "loss 0.13231433928012848 average time 0.013190965128001154 iter num 320\n",
            "loss 0.11160802841186523 average time 0.01258269179107789 iter num 340\n",
            "loss 0.0626722052693367 average time 0.012045263316586998 iter num 360\n",
            "loss 0.04734477400779724 average time 0.011561835026249787 iter num 380\n",
            "loss 0.1167244166135788 average time 0.011127986942428834 iter num 400\n",
            "loss 0.0917230173945427 average time 0.010733219745158012 iter num 420\n",
            "loss 0.12760034203529358 average time 0.010373869870371973 iter num 440\n",
            "loss 0.05797959864139557 average time 0.010046551941195503 iter num 460\n",
            "loss 0.09305182099342346 average time 0.009745514108233995 iter num 480\n",
            "loss 0.08647090196609497 average time 0.009468411881905923 iter num 500\n",
            "loss 0.2947444021701813 average time 0.16826313805031531 iter num 20\n",
            "loss 0.6077579259872437 average time 0.0855549351501395 iter num 40\n",
            "loss 1.1194335222244263 average time 0.057977066816602016 iter num 60\n",
            "loss 0.2117728888988495 average time 0.04421563864993914 iter num 80\n",
            "loss 0.7280293703079224 average time 0.035940080269938335 iter num 100\n",
            "loss 0.3586639165878296 average time 0.030421296641664715 iter num 120\n",
            "loss 0.13485711812973022 average time 0.02649662584286229 iter num 140\n",
            "loss 0.20336344838142395 average time 0.02355649601256573 iter num 160\n",
            "loss 0.1503862887620926 average time 0.02125355633341112 iter num 180\n",
            "loss 0.12465749680995941 average time 0.01943145405504765 iter num 200\n",
            "loss 0.10781805217266083 average time 0.01792472397277875 iter num 220\n",
            "loss 0.20343247056007385 average time 0.016669719083347447 iter num 240\n",
            "loss 0.21909981966018677 average time 0.015606656176938627 iter num 260\n",
            "loss 0.14037005603313446 average time 0.014702651460668546 iter num 280\n",
            "loss 0.18867185711860657 average time 0.013912636733305892 iter num 300\n",
            "loss 0.08065608143806458 average time 0.013215207543714769 iter num 320\n",
            "loss 0.12736180424690247 average time 0.012605071623468522 iter num 340\n",
            "loss 0.10710635036230087 average time 0.012062395563831766 iter num 360\n",
            "loss 0.09949558973312378 average time 0.011579483505221632 iter num 380\n",
            "loss 0.07256108522415161 average time 0.011139719332468302 iter num 400\n",
            "loss 0.09905215352773666 average time 0.010752148049946949 iter num 420\n",
            "loss 0.0848558247089386 average time 0.010393441872648427 iter num 440\n",
            "loss 0.10544788837432861 average time 0.010073313330355197 iter num 460\n",
            "loss 0.096096470952034 average time 0.009778394531167578 iter num 480\n",
            "loss 0.1024191677570343 average time 0.009501172295924334 iter num 500\n",
            "loss 0.21138903498649597 average time 0.16788447180006189 iter num 20\n",
            "loss 0.2055467665195465 average time 0.08538993342526738 iter num 40\n",
            "loss 0.1728442907333374 average time 0.057867620683524974 iter num 60\n",
            "loss 0.9991908669471741 average time 0.04412451607513503 iter num 80\n",
            "loss 0.3567044734954834 average time 0.03586947621017316 iter num 100\n",
            "loss 0.5897589325904846 average time 0.03037024976686856 iter num 120\n",
            "loss 0.5077229142189026 average time 0.026443592364505872 iter num 140\n",
            "loss 0.11332075297832489 average time 0.02349245783761944 iter num 160\n",
            "loss 0.3915570378303528 average time 0.02119794863336008 iter num 180\n",
            "loss 0.291238009929657 average time 0.019369831815020007 iter num 200\n",
            "loss 0.09064628183841705 average time 0.017864012618205874 iter num 220\n",
            "loss 0.20015978813171387 average time 0.016617257762527517 iter num 240\n",
            "loss 0.10193860530853271 average time 0.015561206376920522 iter num 260\n",
            "loss 0.08403556048870087 average time 0.014651354057150456 iter num 280\n",
            "loss 0.05951664596796036 average time 0.013864404760000374 iter num 300\n",
            "loss 0.110783651471138 average time 0.013185150456251904 iter num 320\n",
            "loss 0.059057481586933136 average time 0.012580018770548871 iter num 340\n",
            "loss 0.15720245242118835 average time 0.012042983138841615 iter num 360\n",
            "loss 0.1397605687379837 average time 0.01156243716572805 iter num 380\n",
            "loss 0.1262834370136261 average time 0.011125594194918449 iter num 400\n",
            "loss 0.11100266873836517 average time 0.010732037497516667 iter num 420\n",
            "loss 0.07493722438812256 average time 0.010371853647616676 iter num 440\n",
            "loss 0.048785679042339325 average time 0.010043680528155085 iter num 460\n",
            "loss 0.07766179740428925 average time 0.009741651479059026 iter num 480\n",
            "loss 0.1143522560596466 average time 0.009465454287914326 iter num 500\n",
            "loss 0.3179134726524353 average time 0.1693752842998947 iter num 20\n",
            "loss 0.41753488779067993 average time 0.08611554017479647 iter num 40\n",
            "loss 0.3917519450187683 average time 0.05837438718317571 iter num 60\n",
            "loss 0.22624416649341583 average time 0.04447945404967868 iter num 80\n",
            "loss 0.9439241886138916 average time 0.03616498862986191 iter num 100\n",
            "loss 0.6812164783477783 average time 0.030607634724886642 iter num 120\n",
            "loss 0.6072751879692078 average time 0.026644824985617432 iter num 140\n",
            "loss 1.110072135925293 average time 0.023667218793593747 iter num 160\n",
            "loss 0.3407813310623169 average time 0.021371970399882735 iter num 180\n",
            "loss 0.22035399079322815 average time 0.019516716654870835 iter num 200\n",
            "loss 0.06577048450708389 average time 0.018013845649776074 iter num 220\n",
            "loss 0.12793783843517303 average time 0.01674712383313211 iter num 240\n",
            "loss 0.13664057850837708 average time 0.01567971030368566 iter num 260\n",
            "loss 0.09910120069980621 average time 0.014760346478409962 iter num 280\n",
            "loss 0.12343952059745789 average time 0.013962528966512764 iter num 300\n",
            "loss 0.12472842633724213 average time 0.013287278765494648 iter num 320\n",
            "loss 0.11491593718528748 average time 0.012675737267540278 iter num 340\n",
            "loss 0.17738837003707886 average time 0.012131904272155224 iter num 360\n",
            "loss 0.14773330092430115 average time 0.011646746660450222 iter num 380\n",
            "loss 0.059217359870672226 average time 0.011204545264927219 iter num 400\n",
            "loss 0.08926406502723694 average time 0.010809220857057274 iter num 420\n",
            "loss 0.11123409867286682 average time 0.010446975715827152 iter num 440\n",
            "loss 0.08999806642532349 average time 0.010118768965137703 iter num 460\n",
            "loss 0.14395183324813843 average time 0.009818843899931077 iter num 480\n",
            "loss 0.14576517045497894 average time 0.00954477590793249 iter num 500\n",
            "loss 0.14864712953567505 average time 0.16760428589932416 iter num 20\n",
            "loss 0.1393466293811798 average time 0.08521462862445332 iter num 40\n",
            "loss 0.2441406100988388 average time 0.057745792399631075 iter num 60\n",
            "loss 0.29757267236709595 average time 0.04406769099973644 iter num 80\n",
            "loss 0.2623143494129181 average time 0.03582125993983937 iter num 100\n",
            "loss 0.09804657101631165 average time 0.03034649274156133 iter num 120\n",
            "loss 0.21299047768115997 average time 0.026423473621356868 iter num 140\n",
            "loss 0.2206462323665619 average time 0.02348673608748868 iter num 160\n",
            "loss 0.09949029982089996 average time 0.021190362599938655 iter num 180\n",
            "loss 0.13879282772541046 average time 0.019351973029970396 iter num 200\n",
            "loss 0.2155923843383789 average time 0.017851125954472413 iter num 220\n",
            "loss 0.11852098256349564 average time 0.016612320708251596 iter num 240\n",
            "loss 0.07624655216932297 average time 0.015568857388405797 iter num 260\n",
            "loss 0.06343929469585419 average time 0.014657667832047342 iter num 280\n",
            "loss 0.08050324022769928 average time 0.013866842039933545 iter num 300\n",
            "loss 0.0846797525882721 average time 0.013179683334340098 iter num 320\n",
            "loss 0.13548174500465393 average time 0.01257078594117047 iter num 340\n",
            "loss 0.10658981651067734 average time 0.01202734600276219 iter num 360\n",
            "loss 0.08646730333566666 average time 0.011544263728932205 iter num 380\n",
            "loss 0.0505966879427433 average time 0.01111085309250484 iter num 400\n",
            "loss 0.08881531655788422 average time 0.010716529442842807 iter num 420\n",
            "loss 0.16384372115135193 average time 0.010357183070399597 iter num 440\n",
            "loss 0.08076965808868408 average time 0.010027641758666583 iter num 460\n",
            "loss 0.07707621902227402 average time 0.009729207854108305 iter num 480\n",
            "loss 0.07336921989917755 average time 0.009452947229961865 iter num 500\n",
            "loss 0.14719566702842712 average time 0.16932987025065813 iter num 20\n",
            "loss 0.3462650179862976 average time 0.086128863200247 iter num 40\n",
            "loss 0.2823347747325897 average time 0.05838004725022377 iter num 60\n",
            "loss 0.21123723685741425 average time 0.04451094688761259 iter num 80\n",
            "loss 0.21522530913352966 average time 0.0362206391300424 iter num 100\n",
            "loss 0.30783218145370483 average time 0.030657292758375358 iter num 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7aec2f-64bb-4671-958a-ce054bbb723b"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10.7121]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b18e95c-9af0-40b8-a0e3-dcb90ef668c5"
      },
      "source": [
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -0.1527,  -7.3532,   0.2001,   7.1619,   8.7874, -16.5572,  -0.1754,\n",
              "         -27.3704,   0.1899,   7.7943,  11.3682,  -2.8901,  -0.1771,  29.7061,\n",
              "           0.2247,   8.1989,  12.2822,   7.0741]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "898aff81-20aa-4e7d-b3d8-9837ea2046df"
      },
      "source": [
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 300, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4893b83a90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23e9371-646e-4767-bc60-df2683daca01"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.4180e-03, -2.0633e-01,  4.0882e-03, -1.0803e-01,  3.5117e-01,\n",
              "          -3.2388e-01, -1.4516e-03, -3.2864e-01,  2.3022e-04,  6.0619e-03,\n",
              "           3.8800e-02,  7.9051e-02, -1.4529e-03,  4.3756e-01,  7.8982e-05,\n",
              "           9.8479e-03,  3.1283e-03,  8.0015e-03]], device='cuda:0'),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "8f403c2d-baa3-481a-dedc-4ea0327df52d"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 250, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f48936bb450>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5642d24a-6324-4ae2-89f4-1545353d85c1"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 0.0, 110.0, sigma, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7feb895b9490>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "3b564d6a-8e79-48a5-d27e-80d7550f8e72"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "upper bound is too small\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ade689496c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mquoted_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbisection_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoted_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'implied volativity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    }
  ]
}