{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/From%20Colab/Combined3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "363d11e3-2c6b-45eb-b52a-93e34320522e"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0   8681      0 --:--:-- --:--:-- --:--:--  8681\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# Train(Erin Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "0f7f5d2f-96fe-459e-abef-f209eeaa2fac"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          X = cupy.array([])\n",
        "          K_rand = cupy.random.rand(1)[0]\n",
        "          B_rand = cupy.random.rand(1)[0]\n",
        "          r_rand = cupy.random.rand(1)[0]\n",
        "          for i in range(0,self.N_STOCKS):\n",
        "            X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "          X = X.reshape(self.N_STOCKS,6)\n",
        "          X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          # randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i[0])\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f2fb48-f8a2-47b1-981a-e57316c93c95"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([200.0, 0.1, 200.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab9c134-e9cb-4703-c339-5c456276f252"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.5)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c741502-655a-4a0c-884b-65b323e5ebb2"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 1346.9241943359375 average time 0.02587521595000908 iter num 20\n",
            "loss 1156.866455078125 average time 0.01438921719998234 iter num 40\n",
            "loss 1680.4915771484375 average time 0.010558301916663975 iter num 60\n",
            "loss 738.170654296875 average time 0.008617530437521736 iter num 80\n",
            "loss 1043.528076171875 average time 0.007467535940004382 iter num 100\n",
            "loss 300.12469482421875 average time 0.02481187019996014 iter num 20\n",
            "loss 61.856712341308594 average time 0.01389514477494913 iter num 40\n",
            "loss 78.31651306152344 average time 0.010319517383292502 iter num 60\n",
            "loss 49.4041748046875 average time 0.008450134624956718 iter num 80\n",
            "loss 42.2765998840332 average time 0.007332465029994637 iter num 100\n",
            "loss 29.053237915039062 average time 0.024252385199952188 iter num 20\n",
            "loss 33.81426239013672 average time 0.013537692024999614 iter num 40\n",
            "loss 30.166748046875 average time 0.01000364776666629 iter num 60\n",
            "loss 73.7364730834961 average time 0.0082632795499876 iter num 80\n",
            "loss 25.02761459350586 average time 0.007217052059995694 iter num 100\n",
            "loss 49.57526397705078 average time 0.0257433507499627 iter num 20\n",
            "loss 18.063491821289062 average time 0.01427623050000193 iter num 40\n",
            "loss 46.14141845703125 average time 0.010468582949988559 iter num 60\n",
            "loss 19.76070785522461 average time 0.008617048612484269 iter num 80\n",
            "loss 35.191856384277344 average time 0.007462669979986458 iter num 100\n",
            "loss 43.455421447753906 average time 0.024306457750003573 iter num 20\n",
            "loss 27.562145233154297 average time 0.013673547599989889 iter num 40\n",
            "loss 30.057695388793945 average time 0.010121661583336087 iter num 60\n",
            "loss 40.95891571044922 average time 0.008315440825009545 iter num 80\n",
            "loss 45.76764678955078 average time 0.007284832289997212 iter num 100\n",
            "loss 46.50542449951172 average time 0.02595421749997513 iter num 20\n",
            "loss 48.22746276855469 average time 0.014440936750020228 iter num 40\n",
            "loss 40.49089431762695 average time 0.010586936066670206 iter num 60\n",
            "loss 22.48834991455078 average time 0.008659084800001438 iter num 80\n",
            "loss 43.505645751953125 average time 0.007516291710003315 iter num 100\n",
            "loss 17.392301559448242 average time 0.02442272944990691 iter num 20\n",
            "loss 33.05669403076172 average time 0.013688200699914433 iter num 40\n",
            "loss 33.576820373535156 average time 0.010123563083266163 iter num 60\n",
            "loss 34.52019119262695 average time 0.00833998348743421 iter num 80\n",
            "loss 57.86778259277344 average time 0.007246501889958381 iter num 100\n",
            "loss 52.76967239379883 average time 0.024498566700071934 iter num 20\n",
            "loss 18.17660903930664 average time 0.013724325775046963 iter num 40\n",
            "loss 36.26018524169922 average time 0.010123898500023643 iter num 60\n",
            "loss 37.47447204589844 average time 0.008337067812510668 iter num 80\n",
            "loss 25.979562759399414 average time 0.007274663900002452 iter num 100\n",
            "loss 27.18217658996582 average time 0.025969551149978544 iter num 20\n",
            "loss 20.481552124023438 average time 0.014485904375044356 iter num 40\n",
            "loss 31.14248275756836 average time 0.010607543633356423 iter num 60\n",
            "loss 28.313934326171875 average time 0.00868554407501847 iter num 80\n",
            "loss 27.329776763916016 average time 0.007516502610014868 iter num 100\n",
            "loss 26.197872161865234 average time 0.02430057614994894 iter num 20\n",
            "loss 32.225303649902344 average time 0.013618536174953988 iter num 40\n",
            "loss 42.77125930786133 average time 0.010072486783292334 iter num 60\n",
            "loss 29.122085571289062 average time 0.008317404474956902 iter num 80\n",
            "loss 36.20928955078125 average time 0.007223414629975195 iter num 100\n",
            "loss 28.25076675415039 average time 0.024082086650059863 iter num 20\n",
            "loss 22.75698471069336 average time 0.013458730974991794 iter num 40\n",
            "loss 44.786109924316406 average time 0.009918784283316502 iter num 60\n",
            "loss 29.452152252197266 average time 0.008146575749987051 iter num 80\n",
            "loss 26.806926727294922 average time 0.007114403199998378 iter num 100\n",
            "loss 49.83655548095703 average time 0.02600371444996199 iter num 20\n",
            "loss 17.195987701416016 average time 0.014589658800014149 iter num 40\n",
            "loss 19.837862014770508 average time 0.010729474133328646 iter num 60\n",
            "loss 46.310325622558594 average time 0.008816075887523311 iter num 80\n",
            "loss 51.78681945800781 average time 0.00764227307998226 iter num 100\n",
            "loss 37.67367172241211 average time 0.025625357700118912 iter num 20\n",
            "loss 27.33876609802246 average time 0.014231736324995836 iter num 40\n",
            "loss 44.91889953613281 average time 0.010433247833316273 iter num 60\n",
            "loss 35.81739807128906 average time 0.008536233437473584 iter num 80\n",
            "loss 17.891765594482422 average time 0.007410045850028837 iter num 100\n",
            "loss 47.16441345214844 average time 0.024203902950011978 iter num 20\n",
            "loss 46.044639587402344 average time 0.013619899000036639 iter num 40\n",
            "loss 24.619407653808594 average time 0.010148063750042033 iter num 60\n",
            "loss 33.96515655517578 average time 0.00832274066251557 iter num 80\n",
            "loss 29.843830108642578 average time 0.007264713030044731 iter num 100\n",
            "loss 22.713825225830078 average time 0.024251596600061022 iter num 20\n",
            "loss 34.002647399902344 average time 0.013518775625016133 iter num 40\n",
            "loss 35.90116882324219 average time 0.010007622349985468 iter num 60\n",
            "loss 36.60882568359375 average time 0.008270980612496714 iter num 80\n",
            "loss 24.161849975585938 average time 0.007215756859986868 iter num 100\n",
            "loss 27.246904373168945 average time 0.025783345949866997 iter num 20\n",
            "loss 29.317739486694336 average time 0.014297416424960829 iter num 40\n",
            "loss 28.555984497070312 average time 0.010484166466691628 iter num 60\n",
            "loss 32.38030242919922 average time 0.008578858999999283 iter num 80\n",
            "loss 28.146224975585938 average time 0.007429174950002562 iter num 100\n",
            "loss 33.314659118652344 average time 0.024667232100091495 iter num 20\n",
            "loss 23.805145263671875 average time 0.013810575525053537 iter num 40\n",
            "loss 29.919174194335938 average time 0.010214380533322279 iter num 60\n",
            "loss 33.74898910522461 average time 0.008391907449970403 iter num 80\n",
            "loss 28.035137176513672 average time 0.007292446459987331 iter num 100\n",
            "loss 25.186214447021484 average time 0.02444081274998098 iter num 20\n",
            "loss 34.71283721923828 average time 0.013630636749962833 iter num 40\n",
            "loss 35.250465393066406 average time 0.01008469801663523 iter num 60\n",
            "loss 31.13189697265625 average time 0.008293266749933536 iter num 80\n",
            "loss 36.96271514892578 average time 0.007236225619953985 iter num 100\n",
            "loss 29.4946231842041 average time 0.025828972850058564 iter num 20\n",
            "loss 13.88332748413086 average time 0.014367498675005664 iter num 40\n",
            "loss 40.29451370239258 average time 0.010526492216710419 iter num 60\n",
            "loss 42.05513000488281 average time 0.008603437000056146 iter num 80\n",
            "loss 23.206809997558594 average time 0.007445236020057564 iter num 100\n",
            "loss 43.29789733886719 average time 0.02421882615008144 iter num 20\n",
            "loss 40.30778503417969 average time 0.013584953925010268 iter num 40\n",
            "loss 15.680488586425781 average time 0.010051638366667249 iter num 60\n",
            "loss 23.313274383544922 average time 0.008258477962488086 iter num 80\n",
            "loss 29.35407257080078 average time 0.007214822899968567 iter num 100\n",
            "loss 50.07609176635742 average time 0.026194987350027076 iter num 20\n",
            "loss 22.162599563598633 average time 0.014596373699964716 iter num 40\n",
            "loss 38.647216796875 average time 0.010714438516606606 iter num 60\n",
            "loss 19.078685760498047 average time 0.008744082149939913 iter num 80\n",
            "loss 38.791954040527344 average time 0.007573434489950159 iter num 100\n",
            "loss 34.4000129699707 average time 0.026202320650099864 iter num 20\n",
            "loss 25.390079498291016 average time 0.01457327282503229 iter num 40\n",
            "loss 39.868343353271484 average time 0.010741558416687743 iter num 60\n",
            "loss 25.37142562866211 average time 0.008929448287517516 iter num 80\n",
            "loss 27.10555076599121 average time 0.0077273699300258155 iter num 100\n",
            "loss 28.69344711303711 average time 0.024572319499975494 iter num 20\n",
            "loss 24.27952003479004 average time 0.013733520724963455 iter num 40\n",
            "loss 22.57981300354004 average time 0.010125980983336074 iter num 60\n",
            "loss 37.690773010253906 average time 0.008322463450008399 iter num 80\n",
            "loss 54.03150939941406 average time 0.007250593370017668 iter num 100\n",
            "loss 32.64680480957031 average time 0.026193699349823872 iter num 20\n",
            "loss 32.599857330322266 average time 0.014627495274999092 iter num 40\n",
            "loss 30.585155487060547 average time 0.010692153349979586 iter num 60\n",
            "loss 23.052921295166016 average time 0.008739071824948041 iter num 80\n",
            "loss 22.839820861816406 average time 0.007558299919992351 iter num 100\n",
            "loss 31.002849578857422 average time 0.0251120452500345 iter num 20\n",
            "loss 35.50249099731445 average time 0.013988887400023487 iter num 40\n",
            "loss 23.22399139404297 average time 0.010357624716668094 iter num 60\n",
            "loss 21.95421600341797 average time 0.008518568662486814 iter num 80\n",
            "loss 25.561094284057617 average time 0.007448341290009921 iter num 100\n",
            "loss 26.546077728271484 average time 0.024747270299985757 iter num 20\n",
            "loss 27.440731048583984 average time 0.013806315999931939 iter num 40\n",
            "loss 33.29973602294922 average time 0.010143870099970325 iter num 60\n",
            "loss 38.612422943115234 average time 0.008328103762494266 iter num 80\n",
            "loss 36.097930908203125 average time 0.007253177660013535 iter num 100\n",
            "loss 27.5756778717041 average time 0.025671269749909697 iter num 20\n",
            "loss 18.92130470275879 average time 0.01432576304991926 iter num 40\n",
            "loss 19.341552734375 average time 0.010527869583287004 iter num 60\n",
            "loss 40.90390396118164 average time 0.008640196537464817 iter num 80\n",
            "loss 31.283300399780273 average time 0.0075445125899841516 iter num 100\n",
            "loss 32.67015838623047 average time 0.026208078199852027 iter num 20\n",
            "loss 13.788208961486816 average time 0.014533807674911258 iter num 40\n",
            "loss 19.785320281982422 average time 0.010710571683269639 iter num 60\n",
            "loss 12.993467330932617 average time 0.008791112787457677 iter num 80\n",
            "loss 18.059389114379883 average time 0.007610685759973421 iter num 100\n",
            "loss 40.41224670410156 average time 0.024643400500099232 iter num 20\n",
            "loss 11.095132827758789 average time 0.013895222900077896 iter num 40\n",
            "loss 21.266040802001953 average time 0.010275676600046305 iter num 60\n",
            "loss 30.451725006103516 average time 0.008432284412549507 iter num 80\n",
            "loss 18.959501266479492 average time 0.007338457260020732 iter num 100\n",
            "loss 34.54979705810547 average time 0.024490595650058823 iter num 20\n",
            "loss 33.13802719116211 average time 0.013737125775060121 iter num 40\n",
            "loss 26.989009857177734 average time 0.01017997101665363 iter num 60\n",
            "loss 22.298185348510742 average time 0.008409741975003725 iter num 80\n",
            "loss 19.982202529907227 average time 0.007339665110021087 iter num 100\n",
            "loss 21.959205627441406 average time 0.02690185590008696 iter num 20\n",
            "loss 25.98914337158203 average time 0.014900632000058067 iter num 40\n",
            "loss 11.746983528137207 average time 0.010888243266739058 iter num 60\n",
            "loss 21.76525115966797 average time 0.008883186050059066 iter num 80\n",
            "loss 22.570016860961914 average time 0.007687146090056558 iter num 100\n",
            "loss 26.334487915039062 average time 0.025785926150138038 iter num 20\n",
            "loss 24.988609313964844 average time 0.014430698900059723 iter num 40\n",
            "loss 21.660198211669922 average time 0.010616760166643265 iter num 60\n",
            "loss 53.674659729003906 average time 0.008690865049982222 iter num 80\n",
            "loss 26.14444923400879 average time 0.007522372749999704 iter num 100\n",
            "loss 20.213638305664062 average time 0.02408863490004478 iter num 20\n",
            "loss 17.427391052246094 average time 0.013472890699949858 iter num 40\n",
            "loss 16.715534210205078 average time 0.009953922400003042 iter num 60\n",
            "loss 28.654691696166992 average time 0.008204310787471058 iter num 80\n",
            "loss 20.785701751708984 average time 0.007172355579969008 iter num 100\n",
            "loss 19.218515396118164 average time 0.026235004649970507 iter num 20\n",
            "loss 17.71518325805664 average time 0.014545179199990344 iter num 40\n",
            "loss 33.26908874511719 average time 0.010659679083316102 iter num 60\n",
            "loss 30.215682983398438 average time 0.00870446575003143 iter num 80\n",
            "loss 20.766529083251953 average time 0.007532918490032898 iter num 100\n",
            "loss 21.063989639282227 average time 0.024587572600057683 iter num 20\n",
            "loss 34.057029724121094 average time 0.013797227850113813 iter num 40\n",
            "loss 19.442964553833008 average time 0.010176773633429548 iter num 60\n",
            "loss 34.553524017333984 average time 0.008387330025038863 iter num 80\n",
            "loss 28.428783416748047 average time 0.007306599040002765 iter num 100\n",
            "loss 13.861908912658691 average time 0.027063480549941232 iter num 20\n",
            "loss 26.719058990478516 average time 0.014994889300010073 iter num 40\n",
            "loss 16.346323013305664 average time 0.01103228673329492 iter num 60\n",
            "loss 10.509308815002441 average time 0.009039155249956821 iter num 80\n",
            "loss 18.43074607849121 average time 0.007860505409953476 iter num 100\n",
            "loss 22.019458770751953 average time 0.024142967500029044 iter num 20\n",
            "loss 16.82906723022461 average time 0.013570317525068275 iter num 40\n",
            "loss 33.95844268798828 average time 0.010034518166715618 iter num 60\n",
            "loss 25.51889419555664 average time 0.00829565021255121 iter num 80\n",
            "loss 21.709060668945312 average time 0.00720381281002119 iter num 100\n",
            "loss 22.101119995117188 average time 0.02464864779999516 iter num 20\n",
            "loss 28.972667694091797 average time 0.013863217849984722 iter num 40\n",
            "loss 17.755922317504883 average time 0.010190685583317342 iter num 60\n",
            "loss 15.275444030761719 average time 0.008361708462450679 iter num 80\n",
            "loss 20.45387840270996 average time 0.007308070159942872 iter num 100\n",
            "loss 24.62704849243164 average time 0.026029640350134286 iter num 20\n",
            "loss 12.276941299438477 average time 0.014508883550047359 iter num 40\n",
            "loss 26.778417587280273 average time 0.010623294083325163 iter num 60\n",
            "loss 25.5194149017334 average time 0.008694284787509332 iter num 80\n",
            "loss 24.17325782775879 average time 0.00752350402999582 iter num 100\n",
            "loss 20.119388580322266 average time 0.024265203400000247 iter num 20\n",
            "loss 22.176677703857422 average time 0.013604769675021089 iter num 40\n",
            "loss 18.164737701416016 average time 0.010100282566721337 iter num 60\n",
            "loss 20.527172088623047 average time 0.008346231250050096 iter num 80\n",
            "loss 29.317920684814453 average time 0.007244035850035289 iter num 100\n",
            "loss 17.326801300048828 average time 0.02449396825013537 iter num 20\n",
            "loss 18.131193161010742 average time 0.013742261700076596 iter num 40\n",
            "loss 26.376354217529297 average time 0.010114187033392834 iter num 60\n",
            "loss 21.90594482421875 average time 0.008332243462541555 iter num 80\n",
            "loss 12.279422760009766 average time 0.007264450740021858 iter num 100\n",
            "loss 14.104381561279297 average time 0.026486627550184494 iter num 20\n",
            "loss 20.267555236816406 average time 0.014742823125106951 iter num 40\n",
            "loss 11.935258865356445 average time 0.01081194288340157 iter num 60\n",
            "loss 11.7990083694458 average time 0.008881206700039002 iter num 80\n",
            "loss 33.333404541015625 average time 0.0077022745999965996 iter num 100\n",
            "loss 18.57213592529297 average time 0.02565990124990094 iter num 20\n",
            "loss 13.846794128417969 average time 0.014312785199945211 iter num 40\n",
            "loss 13.827171325683594 average time 0.010557492033331072 iter num 60\n",
            "loss 11.633062362670898 average time 0.008667881624978691 iter num 80\n",
            "loss 9.648876190185547 average time 0.007509046149971254 iter num 100\n",
            "loss 14.102027893066406 average time 0.024444007650026835 iter num 20\n",
            "loss 27.053447723388672 average time 0.013713179924980067 iter num 40\n",
            "loss 13.136648178100586 average time 0.010144634066667399 iter num 60\n",
            "loss 21.512725830078125 average time 0.008341467887498765 iter num 80\n",
            "loss 12.858844757080078 average time 0.007243434109996088 iter num 100\n",
            "loss 17.098196029663086 average time 0.024227407700072943 iter num 20\n",
            "loss 19.928518295288086 average time 0.013565807000077257 iter num 40\n",
            "loss 8.646257400512695 average time 0.010016035833374796 iter num 60\n",
            "loss 6.10895299911499 average time 0.008278323137517418 iter num 80\n",
            "loss 15.435705184936523 average time 0.007223145260031743 iter num 100\n",
            "loss 23.647003173828125 average time 0.025513650399989273 iter num 20\n",
            "loss 15.041481971740723 average time 0.014230843175005248 iter num 40\n",
            "loss 12.100305557250977 average time 0.010433004000014989 iter num 60\n",
            "loss 23.845111846923828 average time 0.008565845037514919 iter num 80\n",
            "loss 8.777082443237305 average time 0.007419771349987059 iter num 100\n",
            "loss 16.178577423095703 average time 0.02560547189987119 iter num 20\n",
            "loss 12.841322898864746 average time 0.014327819399954932 iter num 40\n",
            "loss 7.059320449829102 average time 0.010570754149966888 iter num 60\n",
            "loss 9.212759017944336 average time 0.008656912237495363 iter num 80\n",
            "loss 7.654721736907959 average time 0.007528104799985158 iter num 100\n",
            "loss 10.823163032531738 average time 0.023788340950113708 iter num 20\n",
            "loss 11.350469589233398 average time 0.013550679650052189 iter num 40\n",
            "loss 23.89421844482422 average time 0.009989520583333918 iter num 60\n",
            "loss 10.185653686523438 average time 0.008247250800002348 iter num 80\n",
            "loss 10.924652099609375 average time 0.007199400600002264 iter num 100\n",
            "loss 8.305147171020508 average time 0.025713950900035344 iter num 20\n",
            "loss 11.425909042358398 average time 0.014328236025039586 iter num 40\n",
            "loss 8.827569007873535 average time 0.010508691616693492 iter num 60\n",
            "loss 12.808509826660156 average time 0.008599162725022324 iter num 80\n",
            "loss 7.595105171203613 average time 0.007448207150000599 iter num 100\n",
            "loss 12.8153657913208 average time 0.024145678700051576 iter num 20\n",
            "loss 7.945408821105957 average time 0.013638436299993372 iter num 40\n",
            "loss 7.375859260559082 average time 0.01009177756667062 iter num 60\n",
            "loss 6.689700603485107 average time 0.008284035575002235 iter num 80\n",
            "loss 7.816278457641602 average time 0.007246676129980188 iter num 100\n",
            "loss 13.662893295288086 average time 0.025695981099988784 iter num 20\n",
            "loss 8.14459228515625 average time 0.014353554174999773 iter num 40\n",
            "loss 8.199931144714355 average time 0.010512921016713032 iter num 60\n",
            "loss 5.652602195739746 average time 0.008599873825085069 iter num 80\n",
            "loss 8.444234848022461 average time 0.007466882100079602 iter num 100\n",
            "loss 9.504598617553711 average time 0.024724109000135285 iter num 20\n",
            "loss 9.625679016113281 average time 0.013789379850049955 iter num 40\n",
            "loss 6.303773880004883 average time 0.010258921833398441 iter num 60\n",
            "loss 6.962873458862305 average time 0.008467553150057937 iter num 80\n",
            "loss 10.842506408691406 average time 0.007372931330055507 iter num 100\n",
            "loss 5.732738494873047 average time 0.024211586199862724 iter num 20\n",
            "loss 4.773778438568115 average time 0.01351223424994714 iter num 40\n",
            "loss 6.293226718902588 average time 0.009958709599914073 iter num 60\n",
            "loss 7.055440902709961 average time 0.008187461437455568 iter num 80\n",
            "loss 7.9231719970703125 average time 0.007137465809964851 iter num 100\n",
            "loss 11.870668411254883 average time 0.026310502899968925 iter num 20\n",
            "loss 6.701329231262207 average time 0.014633722375015168 iter num 40\n",
            "loss 8.06491756439209 average time 0.010716088049988078 iter num 60\n",
            "loss 5.202908039093018 average time 0.008754383962491374 iter num 80\n",
            "loss 4.966534614562988 average time 0.007582186119998368 iter num 100\n",
            "loss 10.712137222290039 average time 0.024550893500099848 iter num 20\n",
            "loss 5.761386394500732 average time 0.013850262375058264 iter num 40\n",
            "loss 5.034152984619141 average time 0.010211362266697202 iter num 60\n",
            "loss 6.313050746917725 average time 0.008430353750020459 iter num 80\n",
            "loss 4.838501930236816 average time 0.007334551890016883 iter num 100\n",
            "loss 9.703376770019531 average time 0.024237697249964184 iter num 20\n",
            "loss 14.42370891571045 average time 0.013557368999931896 iter num 40\n",
            "loss 5.990309715270996 average time 0.009986224066627377 iter num 60\n",
            "loss 3.242459297180176 average time 0.008203316762433133 iter num 80\n",
            "loss 3.428881883621216 average time 0.007189800709975316 iter num 100\n",
            "loss 5.017245292663574 average time 0.027387160850094006 iter num 20\n",
            "loss 8.6876220703125 average time 0.015225099850090374 iter num 40\n",
            "loss 6.1914143562316895 average time 0.011133698400074839 iter num 60\n",
            "loss 4.492578029632568 average time 0.009131096537601024 iter num 80\n",
            "loss 5.8768205642700195 average time 0.007912312020089303 iter num 100\n",
            "loss 5.888784885406494 average time 0.025609032799957278 iter num 20\n",
            "loss 10.242106437683105 average time 0.014214286699916557 iter num 40\n",
            "loss 5.701796054840088 average time 0.01044909188328044 iter num 60\n",
            "loss 6.442419052124023 average time 0.008547078862443413 iter num 80\n",
            "loss 5.19345760345459 average time 0.007420125319949875 iter num 100\n",
            "loss 5.6937761306762695 average time 0.024339918150053562 iter num 20\n",
            "loss 5.453457832336426 average time 0.013663705150065653 iter num 40\n",
            "loss 5.705745220184326 average time 0.010137201500037918 iter num 60\n",
            "loss 2.9808084964752197 average time 0.008346980625015021 iter num 80\n",
            "loss 5.601724147796631 average time 0.007238422470027217 iter num 100\n",
            "loss 5.932306289672852 average time 0.024740282850098085 iter num 20\n",
            "loss 8.057087898254395 average time 0.013797736049991727 iter num 40\n",
            "loss 8.998746871948242 average time 0.010158257583331458 iter num 60\n",
            "loss 2.9423253536224365 average time 0.00835306123749433 iter num 80\n",
            "loss 3.7147412300109863 average time 0.007288491049994263 iter num 100\n",
            "loss 5.275689125061035 average time 0.025749430000041686 iter num 20\n",
            "loss 4.807204246520996 average time 0.014274409475001448 iter num 40\n",
            "loss 4.131728649139404 average time 0.010476380166619493 iter num 60\n",
            "loss 3.399200439453125 average time 0.008562201549977999 iter num 80\n",
            "loss 5.300699234008789 average time 0.007419974709973758 iter num 100\n",
            "loss 6.182906627655029 average time 0.024644397299925912 iter num 20\n",
            "loss 6.2436017990112305 average time 0.01383068814998296 iter num 40\n",
            "loss 2.5619778633117676 average time 0.010216577666703112 iter num 60\n",
            "loss 2.6440234184265137 average time 0.008393280025018157 iter num 80\n",
            "loss 4.5537896156311035 average time 0.007343389819998265 iter num 100\n",
            "loss 9.129454612731934 average time 0.024242614599961597 iter num 20\n",
            "loss 6.406430244445801 average time 0.013573266224966574 iter num 40\n",
            "loss 4.590378761291504 average time 0.009986325399980463 iter num 60\n",
            "loss 7.254608154296875 average time 0.00824050791246691 iter num 80\n",
            "loss 5.429543495178223 average time 0.007258256879977125 iter num 100\n",
            "loss 5.966869354248047 average time 0.026298240249798256 iter num 20\n",
            "loss 3.953599452972412 average time 0.014603191174910535 iter num 40\n",
            "loss 2.9595956802368164 average time 0.010682719716593661 iter num 60\n",
            "loss 4.032412528991699 average time 0.008715418324936764 iter num 80\n",
            "loss 3.47415828704834 average time 0.007540484009941792 iter num 100\n",
            "loss 9.753277778625488 average time 0.024261348500021995 iter num 20\n",
            "loss 8.631080627441406 average time 0.013660409300064203 iter num 40\n",
            "loss 4.064198017120361 average time 0.010238861033349167 iter num 60\n",
            "loss 5.29509162902832 average time 0.00841171512499841 iter num 80\n",
            "loss 3.8578124046325684 average time 0.0073217970000041535 iter num 100\n",
            "loss 3.3774666786193848 average time 0.025908729300044796 iter num 20\n",
            "loss 2.281693935394287 average time 0.01443316360002882 iter num 40\n",
            "loss 4.985774040222168 average time 0.01058521101667793 iter num 60\n",
            "loss 3.436472177505493 average time 0.008676043374998698 iter num 80\n",
            "loss 4.188394546508789 average time 0.007515617969984305 iter num 100\n",
            "loss 3.063293695449829 average time 0.025441848300124546 iter num 20\n",
            "loss 18.103673934936523 average time 0.014162891475098149 iter num 40\n",
            "loss 6.074551582336426 average time 0.010416820066742124 iter num 60\n",
            "loss 2.9150278568267822 average time 0.008552878312548274 iter num 80\n",
            "loss 6.064565658569336 average time 0.007465546750017893 iter num 100\n",
            "loss 4.687898635864258 average time 0.02398720210007923 iter num 20\n",
            "loss 5.361148834228516 average time 0.013433558350061502 iter num 40\n",
            "loss 5.138821125030518 average time 0.00996780568336059 iter num 60\n",
            "loss 4.323765754699707 average time 0.008185940725059026 iter num 80\n",
            "loss 3.1741957664489746 average time 0.007122534070049369 iter num 100\n",
            "loss 5.986695289611816 average time 0.02599712895016637 iter num 20\n",
            "loss 7.619479656219482 average time 0.014496640375091375 iter num 40\n",
            "loss 3.122421979904175 average time 0.010629733383378455 iter num 60\n",
            "loss 2.9197521209716797 average time 0.008706508400064195 iter num 80\n",
            "loss 3.483898639678955 average time 0.007548488270049347 iter num 100\n",
            "loss 6.920245170593262 average time 0.025907029100017097 iter num 20\n",
            "loss 3.8527631759643555 average time 0.01442672012501589 iter num 40\n",
            "loss 4.5926032066345215 average time 0.010598471783335601 iter num 60\n",
            "loss 3.455026149749756 average time 0.008701433437465766 iter num 80\n",
            "loss 2.680063247680664 average time 0.0075892513699909615 iter num 100\n",
            "loss 5.81497859954834 average time 0.026301685250018636 iter num 20\n",
            "loss 4.212822437286377 average time 0.014643217150000965 iter num 40\n",
            "loss 4.4711103439331055 average time 0.010754358683334431 iter num 60\n",
            "loss 3.1452527046203613 average time 0.008793281362511608 iter num 80\n",
            "loss 4.804754257202148 average time 0.007605374020004092 iter num 100\n",
            "loss 1.4426463842391968 average time 0.026638653800000613 iter num 20\n",
            "loss 13.984779357910156 average time 0.014814518950061029 iter num 40\n",
            "loss 4.073742389678955 average time 0.010865554366728248 iter num 60\n",
            "loss 4.221952438354492 average time 0.008886431075063683 iter num 80\n",
            "loss 3.3072800636291504 average time 0.00773887234004178 iter num 100\n",
            "loss 4.804019927978516 average time 0.025955452300013348 iter num 20\n",
            "loss 3.357358932495117 average time 0.014419449600018197 iter num 40\n",
            "loss 4.847996234893799 average time 0.010561060566700083 iter num 60\n",
            "loss 5.020985126495361 average time 0.008630890887502574 iter num 80\n",
            "loss 4.080702781677246 average time 0.0074905801900058576 iter num 100\n",
            "loss 4.28378963470459 average time 0.024076314299873048 iter num 20\n",
            "loss 5.050782680511475 average time 0.013542063299951224 iter num 40\n",
            "loss 3.323547601699829 average time 0.010053262683247037 iter num 60\n",
            "loss 4.250925540924072 average time 0.008295779149943883 iter num 80\n",
            "loss 3.2271647453308105 average time 0.0072456506499383976 iter num 100\n",
            "loss 7.335385799407959 average time 0.02495290424999439 iter num 20\n",
            "loss 9.672714233398438 average time 0.013887046524951074 iter num 40\n",
            "loss 3.7161364555358887 average time 0.010194783799988727 iter num 60\n",
            "loss 2.7950336933135986 average time 0.008427611674983381 iter num 80\n",
            "loss 2.281036853790283 average time 0.007337879629994859 iter num 100\n",
            "loss 4.364466667175293 average time 0.025890895799966528 iter num 20\n",
            "loss 4.187337875366211 average time 0.014416851475061776 iter num 40\n",
            "loss 2.0350728034973145 average time 0.010599435483421378 iter num 60\n",
            "loss 3.3017306327819824 average time 0.008665912512549312 iter num 80\n",
            "loss 2.746811866760254 average time 0.0075091162800617894 iter num 100\n",
            "loss 3.520855188369751 average time 0.02463866314992629 iter num 20\n",
            "loss 2.9805893898010254 average time 0.013781676049916314 iter num 40\n",
            "loss 3.8260555267333984 average time 0.010198185799951413 iter num 60\n",
            "loss 2.6115400791168213 average time 0.008426319299996977 iter num 80\n",
            "loss 1.6107181310653687 average time 0.007344148749980377 iter num 100\n",
            "loss 6.381960391998291 average time 0.02394651864997286 iter num 20\n",
            "loss 5.489534854888916 average time 0.013426736099950176 iter num 40\n",
            "loss 3.743748664855957 average time 0.0099000245000146 iter num 60\n",
            "loss 1.9937245845794678 average time 0.008159154537520408 iter num 80\n",
            "loss 2.300616979598999 average time 0.007131053140019503 iter num 100\n",
            "loss 7.236248016357422 average time 0.025883269650057626 iter num 20\n",
            "loss 5.966886520385742 average time 0.014458457649993761 iter num 40\n",
            "loss 2.382786989212036 average time 0.010607115266626959 iter num 60\n",
            "loss 3.93233585357666 average time 0.008694201899982091 iter num 80\n",
            "loss 2.5174036026000977 average time 0.007645915009961754 iter num 100\n",
            "loss 2.357725143432617 average time 0.026339492999977666 iter num 20\n",
            "loss 4.245580196380615 average time 0.014691259724986593 iter num 40\n",
            "loss 4.717785358428955 average time 0.010801885916725951 iter num 60\n",
            "loss 4.521005630493164 average time 0.008834533687570456 iter num 80\n",
            "loss 4.208782196044922 average time 0.007641248510071819 iter num 100\n",
            "loss 4.331560134887695 average time 0.024669580600038897 iter num 20\n",
            "loss 2.5677366256713867 average time 0.013872683550016519 iter num 40\n",
            "loss 4.272412300109863 average time 0.010269627083334853 iter num 60\n",
            "loss 2.0521152019500732 average time 0.008436669512491335 iter num 80\n",
            "loss 1.7809042930603027 average time 0.007334150810020219 iter num 100\n",
            "loss 6.138523578643799 average time 0.024248501300098722 iter num 20\n",
            "loss 3.642826557159424 average time 0.013589827525083819 iter num 40\n",
            "loss 2.142706871032715 average time 0.010062078933394029 iter num 60\n",
            "loss 3.5922131538391113 average time 0.008307500262537815 iter num 80\n",
            "loss 4.06156587600708 average time 0.007241519920016799 iter num 100\n",
            "loss 3.490607500076294 average time 0.02615016509994348 iter num 20\n",
            "loss 3.9885778427124023 average time 0.01450131404997137 iter num 40\n",
            "loss 2.9031903743743896 average time 0.010651822483320453 iter num 60\n",
            "loss 1.4405021667480469 average time 0.008745143962516976 iter num 80\n",
            "loss 2.2869176864624023 average time 0.00755996820999826 iter num 100\n",
            "loss 12.24168586730957 average time 0.02429061644993453 iter num 20\n",
            "loss 2.102527141571045 average time 0.013653850674927526 iter num 40\n",
            "loss 2.8930230140686035 average time 0.010108474533277937 iter num 60\n",
            "loss 2.0663869380950928 average time 0.008316835999914928 iter num 80\n",
            "loss 2.7127480506896973 average time 0.007220481899921651 iter num 100\n",
            "loss 3.1325602531433105 average time 0.024068517300020176 iter num 20\n",
            "loss 3.7544498443603516 average time 0.013470351025011951 iter num 40\n",
            "loss 2.232973098754883 average time 0.009923951383325403 iter num 60\n",
            "loss 2.607666492462158 average time 0.008248486749994299 iter num 80\n",
            "loss 4.344801425933838 average time 0.007202811580009438 iter num 100\n",
            "loss 3.5242726802825928 average time 0.026459437249923213 iter num 20\n",
            "loss 3.560605049133301 average time 0.014742737324968402 iter num 40\n",
            "loss 4.968482494354248 average time 0.01077874786663718 iter num 60\n",
            "loss 1.9346376657485962 average time 0.008794594012476864 iter num 80\n",
            "loss 3.5824503898620605 average time 0.007638455829983286 iter num 100\n",
            "loss 6.027133941650391 average time 0.024421522349985025 iter num 20\n",
            "loss 4.282022476196289 average time 0.013705978699999833 iter num 40\n",
            "loss 5.401217937469482 average time 0.010153407450009885 iter num 60\n",
            "loss 2.8652403354644775 average time 0.008348754199994346 iter num 80\n",
            "loss 3.351022720336914 average time 0.007293061829977887 iter num 100\n",
            "loss 5.703183174133301 average time 0.025675210099961987 iter num 20\n",
            "loss 7.17208194732666 average time 0.014340129925062683 iter num 40\n",
            "loss 3.990166425704956 average time 0.0104942595834018 iter num 60\n",
            "loss 3.1961569786071777 average time 0.0085949343375205 iter num 80\n",
            "loss 3.2288906574249268 average time 0.0075043137300053785 iter num 100\n",
            "loss 5.277199745178223 average time 0.023978831949943925 iter num 20\n",
            "loss 7.489007949829102 average time 0.013424030325018066 iter num 40\n",
            "loss 3.3595192432403564 average time 0.00995779700000033 iter num 60\n",
            "loss 3.586313486099243 average time 0.008226854837482733 iter num 80\n",
            "loss 1.5024384260177612 average time 0.007189135510025153 iter num 100\n",
            "loss 2.906653881072998 average time 0.024269924899908802 iter num 20\n",
            "loss 3.5088014602661133 average time 0.013577615499889362 iter num 40\n",
            "loss 3.2525243759155273 average time 0.01000487006660175 iter num 60\n",
            "loss 3.650066614151001 average time 0.00821030484999028 iter num 80\n",
            "loss 3.4970219135284424 average time 0.007150870120003674 iter num 100\n",
            "loss 5.4283552169799805 average time 0.025602957750106726 iter num 20\n",
            "loss 4.566642761230469 average time 0.014403173025016258 iter num 40\n",
            "loss 3.232283592224121 average time 0.01057929695001197 iter num 60\n",
            "loss 3.053351640701294 average time 0.008651213000018743 iter num 80\n",
            "loss 2.8658275604248047 average time 0.007497459420010273 iter num 100\n",
            "loss 3.3351736068725586 average time 0.023847931999944193 iter num 20\n",
            "loss 4.480813026428223 average time 0.013406117524959882 iter num 40\n",
            "loss 2.5592525005340576 average time 0.010007750633288499 iter num 60\n",
            "loss 3.3013319969177246 average time 0.008278918937469371 iter num 80\n",
            "loss 2.599447727203369 average time 0.007223649019988443 iter num 100\n",
            "loss 2.7105631828308105 average time 0.024493300150061258 iter num 20\n",
            "loss 2.2079622745513916 average time 0.01373141400001714 iter num 40\n",
            "loss 2.2776122093200684 average time 0.010094114466649746 iter num 60\n",
            "loss 1.821672797203064 average time 0.00831562408748141 iter num 80\n",
            "loss 3.604902982711792 average time 0.007242126349974569 iter num 100\n",
            "loss 5.848840236663818 average time 0.026401863850014706 iter num 20\n",
            "loss 5.8392157554626465 average time 0.014683646900016356 iter num 40\n",
            "loss 2.243955135345459 average time 0.010755960233321578 iter num 60\n",
            "loss 2.461137056350708 average time 0.008838530524974431 iter num 80\n",
            "loss 1.9462311267852783 average time 0.007745440899971073 iter num 100\n",
            "loss 4.056116104125977 average time 0.025727120200008356 iter num 20\n",
            "loss 5.424056529998779 average time 0.014276787675089508 iter num 40\n",
            "loss 2.771637439727783 average time 0.010457310550070058 iter num 60\n",
            "loss 3.851487874984741 average time 0.00855721303756809 iter num 80\n",
            "loss 3.2007555961608887 average time 0.007471853270017164 iter num 100\n",
            "loss 2.7395262718200684 average time 0.024449698750004245 iter num 20\n",
            "loss 3.1279683113098145 average time 0.013717203849955695 iter num 40\n",
            "loss 2.10221529006958 average time 0.010121062716613475 iter num 60\n",
            "loss 1.6799726486206055 average time 0.008357831099999657 iter num 80\n",
            "loss 2.5950264930725098 average time 0.007253602350001529 iter num 100\n",
            "loss 7.406717300415039 average time 0.02442866784999751 iter num 20\n",
            "loss 3.840636968612671 average time 0.01362874302496948 iter num 40\n",
            "loss 2.8808674812316895 average time 0.010025289883333243 iter num 60\n",
            "loss 2.01315975189209 average time 0.008264155224992464 iter num 80\n",
            "loss 3.047729969024658 average time 0.007210747790004461 iter num 100\n",
            "loss 4.565886497497559 average time 0.02518797584998538 iter num 20\n",
            "loss 2.2365102767944336 average time 0.014065918374967623 iter num 40\n",
            "loss 2.13145112991333 average time 0.010365684549985115 iter num 60\n",
            "loss 2.139667510986328 average time 0.008519382224972106 iter num 80\n",
            "loss 2.2492482662200928 average time 0.007385665029942174 iter num 100\n",
            "loss 2.732285261154175 average time 0.024464627950010254 iter num 20\n",
            "loss 1.667688250541687 average time 0.013753504250007608 iter num 40\n",
            "loss 1.4595767259597778 average time 0.01020908076664758 iter num 60\n",
            "loss 1.553058385848999 average time 0.008405749749988445 iter num 80\n",
            "loss 3.273362874984741 average time 0.00731688093998855 iter num 100\n",
            "loss 4.315273284912109 average time 0.024229782899965358 iter num 20\n",
            "loss 3.609349250793457 average time 0.013558742049895045 iter num 40\n",
            "loss 2.1844756603240967 average time 0.010014827399966937 iter num 60\n",
            "loss 2.236931324005127 average time 0.00824011748745761 iter num 80\n",
            "loss 2.032716989517212 average time 0.007192250669977512 iter num 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 10000\n",
              "\tepoch: 100\n",
              "\tepoch_length: 100\n",
              "\tmax_epochs: 100\n",
              "\toutput: 2.032716989517212\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bee8d9-1366-4e69-c546-d931852a4e35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'checkpoint15.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98000a03-77b4-4816-9006-389e863dec3f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876f5341-afac-4960-af68-7352a27190a6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'checkpoint15.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf8d4eb-793e-414b-dc84-61860d4fb90f"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=18, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84288148-ff5d-4a73-a2ba-1aaec8ab4b1b"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=500, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=20)\n",
        "\n",
        "model_save_name = 'checkpoint16.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 3.4715287685394287 average time 0.02594610639998791 iter num 20\n",
            "loss 3.914529800415039 average time 0.014444297549971452 iter num 40\n",
            "loss 4.360954284667969 average time 0.010604737449921232 iter num 60\n",
            "loss 8.731216430664062 average time 0.00867429568741045 iter num 80\n",
            "loss 5.59759521484375 average time 0.007518608259942994 iter num 100\n",
            "loss 5.784728050231934 average time 0.00680891479996717 iter num 120\n",
            "loss 3.214427947998047 average time 0.00626443088567612 iter num 140\n",
            "loss 4.536759376525879 average time 0.00587454908742302 iter num 160\n",
            "loss 5.27166223526001 average time 0.0055516928610207595 iter num 180\n",
            "loss 2.7184598445892334 average time 0.005294802209891714 iter num 200\n",
            "loss 3.3006515502929688 average time 0.005099204027175636 iter num 220\n",
            "loss 2.350712537765503 average time 0.004930914237388606 iter num 240\n",
            "loss 2.6318230628967285 average time 0.004771634349890519 iter num 260\n",
            "loss 3.1269986629486084 average time 0.004635526424895683 iter num 280\n",
            "loss 2.484622001647949 average time 0.004524781733234704 iter num 300\n",
            "loss 1.2294827699661255 average time 0.004436798203028047 iter num 320\n",
            "loss 1.8235582113265991 average time 0.004352507911661472 iter num 340\n",
            "loss 2.549100399017334 average time 0.004276516688807735 iter num 360\n",
            "loss 2.3221864700317383 average time 0.004209918960451002 iter num 380\n",
            "loss 2.131500244140625 average time 0.004141789152431557 iter num 400\n",
            "loss 3.6816394329071045 average time 0.004081246207078948 iter num 420\n",
            "loss 2.218905210494995 average time 0.0040265471703936455 iter num 440\n",
            "loss 1.334122657775879 average time 0.003975996891234029 iter num 460\n",
            "loss 1.471814513206482 average time 0.003930277841603432 iter num 480\n",
            "loss 2.4375321865081787 average time 0.0038941284999382333 iter num 500\n",
            "loss 2.7466201782226562 average time 0.026059194849858615 iter num 20\n",
            "loss 3.40423321723938 average time 0.014542361399753645 iter num 40\n",
            "loss 4.430373668670654 average time 0.010657517316455293 iter num 60\n",
            "loss 2.245573043823242 average time 0.008727790087323228 iter num 80\n",
            "loss 2.3435704708099365 average time 0.007549067559848481 iter num 100\n",
            "loss 1.6848663091659546 average time 0.006766701616500844 iter num 120\n",
            "loss 4.478843688964844 average time 0.006229041985573401 iter num 140\n",
            "loss 5.057981491088867 average time 0.00582382006239186 iter num 160\n",
            "loss 3.100292205810547 average time 0.0055287043610203705 iter num 180\n",
            "loss 2.31425142288208 average time 0.005261097644934125 iter num 200\n",
            "loss 2.093440294265747 average time 0.0050463198772211565 iter num 220\n",
            "loss 1.7342923879623413 average time 0.0048685173540964875 iter num 240\n",
            "loss 2.38712215423584 average time 0.004731248653773106 iter num 260\n",
            "loss 5.025276184082031 average time 0.004615908139209881 iter num 280\n",
            "loss 2.717207431793213 average time 0.004510221579918531 iter num 300\n",
            "loss 3.4210855960845947 average time 0.004415117465538287 iter num 320\n",
            "loss 1.1226489543914795 average time 0.004331645282266382 iter num 340\n",
            "loss 1.7162222862243652 average time 0.0042479992915963926 iter num 360\n",
            "loss 2.0689947605133057 average time 0.0041774274657224225 iter num 380\n",
            "loss 1.937924861907959 average time 0.004110893332435808 iter num 400\n",
            "loss 2.096289873123169 average time 0.0040618305023209745 iter num 420\n",
            "loss 1.6344412565231323 average time 0.0040148025999315 iter num 440\n",
            "loss 1.75730562210083 average time 0.0039743672303658335 iter num 460\n",
            "loss 1.7038047313690186 average time 0.003935113427015343 iter num 480\n",
            "loss 1.106067180633545 average time 0.003892015617930156 iter num 500\n",
            "loss 1.4247782230377197 average time 0.024417698350043794 iter num 20\n",
            "loss 3.083498239517212 average time 0.013765876975003265 iter num 40\n",
            "loss 2.939951181411743 average time 0.010179595049885392 iter num 60\n",
            "loss 1.9169131517410278 average time 0.008364087124869002 iter num 80\n",
            "loss 3.036315441131592 average time 0.007264562859854777 iter num 100\n",
            "loss 6.077886581420898 average time 0.006540431608163999 iter num 120\n",
            "loss 3.5556211471557617 average time 0.006011652664126034 iter num 140\n",
            "loss 2.8400115966796875 average time 0.005626603218593118 iter num 160\n",
            "loss 5.578988075256348 average time 0.005340409466498386 iter num 180\n",
            "loss 3.1897640228271484 average time 0.0051085769098517635 iter num 200\n",
            "loss 2.303994655609131 average time 0.004911035249865976 iter num 220\n",
            "loss 3.2291512489318848 average time 0.0047416745457212525 iter num 240\n",
            "loss 1.9697129726409912 average time 0.004598970303742625 iter num 260\n",
            "loss 1.8511348962783813 average time 0.004484194960611733 iter num 280\n",
            "loss 0.7202113270759583 average time 0.00437795530991328 iter num 300\n",
            "loss 3.0268709659576416 average time 0.004294765762421093 iter num 320\n",
            "loss 2.1854474544525146 average time 0.004224287135227368 iter num 340\n",
            "loss 1.8768179416656494 average time 0.004157001022167606 iter num 360\n",
            "loss 1.139864444732666 average time 0.004093091620990507 iter num 380\n",
            "loss 2.523367404937744 average time 0.004037036474928754 iter num 400\n",
            "loss 1.2553130388259888 average time 0.003984357411831124 iter num 420\n",
            "loss 1.7234442234039307 average time 0.003936200438573161 iter num 440\n",
            "loss 2.083174228668213 average time 0.003893601017337104 iter num 460\n",
            "loss 1.4941953420639038 average time 0.0038578594436974828 iter num 480\n",
            "loss 2.498049736022949 average time 0.0038251423099573004 iter num 500\n",
            "loss 1.510207176208496 average time 0.026177004299916005 iter num 20\n",
            "loss 2.6991097927093506 average time 0.014514500274935926 iter num 40\n",
            "loss 3.131852865219116 average time 0.010676906666715998 iter num 60\n",
            "loss 2.8923516273498535 average time 0.00873097903756843 iter num 80\n",
            "loss 1.7926710844039917 average time 0.007582129060047009 iter num 100\n",
            "loss 1.9652819633483887 average time 0.006817209016708148 iter num 120\n",
            "loss 2.394946575164795 average time 0.006278451178630868 iter num 140\n",
            "loss 3.163090944290161 average time 0.005864959525058566 iter num 160\n",
            "loss 3.028575897216797 average time 0.005541449283403684 iter num 180\n",
            "loss 2.5806846618652344 average time 0.005295900395040008 iter num 200\n",
            "loss 1.98704195022583 average time 0.005094659568220033 iter num 220\n",
            "loss 1.5768616199493408 average time 0.004917657537536494 iter num 240\n",
            "loss 2.6519041061401367 average time 0.004758040369247632 iter num 260\n",
            "loss 1.7235617637634277 average time 0.004624066735732413 iter num 280\n",
            "loss 1.8547462224960327 average time 0.004504301960023441 iter num 300\n",
            "loss 0.9063411951065063 average time 0.004400674443758135 iter num 320\n",
            "loss 1.126373052597046 average time 0.004321741452954216 iter num 340\n",
            "loss 1.9992775917053223 average time 0.004247415841675522 iter num 360\n",
            "loss 2.5576319694519043 average time 0.004184846792132272 iter num 380\n",
            "loss 1.852475881576538 average time 0.0041200737000281155 iter num 400\n",
            "loss 1.7142970561981201 average time 0.00405931626193619 iter num 420\n",
            "loss 1.9563703536987305 average time 0.004007480838675871 iter num 440\n",
            "loss 1.740227222442627 average time 0.003958662441338042 iter num 460\n",
            "loss 1.6983426809310913 average time 0.0039145540000314815 iter num 480\n",
            "loss 1.3994641304016113 average time 0.003880869488028111 iter num 500\n",
            "loss 1.637488842010498 average time 0.02632309705004445 iter num 20\n",
            "loss 3.0113468170166016 average time 0.014692438600013702 iter num 40\n",
            "loss 3.169405460357666 average time 0.010761872250016798 iter num 60\n",
            "loss 1.4377572536468506 average time 0.008791971612527049 iter num 80\n",
            "loss 2.9297542572021484 average time 0.007601341840017995 iter num 100\n",
            "loss 2.050225019454956 average time 0.006829077941680832 iter num 120\n",
            "loss 1.4666845798492432 average time 0.006281484121466617 iter num 140\n",
            "loss 2.2210779190063477 average time 0.005889240450005673 iter num 160\n",
            "loss 1.0319254398345947 average time 0.005577106550006445 iter num 180\n",
            "loss 2.6455087661743164 average time 0.005312738974980675 iter num 200\n",
            "loss 2.236572504043579 average time 0.005100477022708881 iter num 220\n",
            "loss 1.1177458763122559 average time 0.004924883404146385 iter num 240\n",
            "loss 1.4908125400543213 average time 0.004779849115367632 iter num 260\n",
            "loss 1.4056012630462646 average time 0.004644876146390483 iter num 280\n",
            "loss 1.885453462600708 average time 0.004538470396655611 iter num 300\n",
            "loss 0.6664736270904541 average time 0.004440155343723972 iter num 320\n",
            "loss 1.5871487855911255 average time 0.004364169535272274 iter num 340\n",
            "loss 1.8055732250213623 average time 0.004278883588878873 iter num 360\n",
            "loss 1.622493863105774 average time 0.0042091767473577514 iter num 380\n",
            "loss 1.6234633922576904 average time 0.004141845957474288 iter num 400\n",
            "loss 1.6423994302749634 average time 0.004081379642827544 iter num 420\n",
            "loss 2.6318893432617188 average time 0.004045837154527279 iter num 440\n",
            "loss 1.5107262134552002 average time 0.004005538436942279 iter num 460\n",
            "loss 0.981188952922821 average time 0.003964205647904843 iter num 480\n",
            "loss 1.3248072862625122 average time 0.0039307165999853165 iter num 500\n",
            "loss 1.4110724925994873 average time 0.024974678849957854 iter num 20\n",
            "loss 3.6471445560455322 average time 0.014013423749838693 iter num 40\n",
            "loss 2.6156630516052246 average time 0.010335881649886384 iter num 60\n",
            "loss 2.1635875701904297 average time 0.008484778287424888 iter num 80\n",
            "loss 1.000145435333252 average time 0.007359101819929492 iter num 100\n",
            "loss 1.3086248636245728 average time 0.006610556766569668 iter num 120\n",
            "loss 2.259281635284424 average time 0.006087397614230992 iter num 140\n",
            "loss 1.7897284030914307 average time 0.005681619174958996 iter num 160\n",
            "loss 2.5407919883728027 average time 0.005386870377757684 iter num 180\n",
            "loss 1.3101024627685547 average time 0.0051855672599958776 iter num 200\n",
            "loss 1.3806968927383423 average time 0.005003365177278531 iter num 220\n",
            "loss 1.4897269010543823 average time 0.0048337691375081706 iter num 240\n",
            "loss 1.4700905084609985 average time 0.004682639223098634 iter num 260\n",
            "loss 1.4386014938354492 average time 0.004563181657152069 iter num 280\n",
            "loss 1.281705617904663 average time 0.004455510433350961 iter num 300\n",
            "loss 0.8397881388664246 average time 0.004359669553139156 iter num 320\n",
            "loss 1.1397688388824463 average time 0.00428338896472804 iter num 340\n",
            "loss 0.7765023708343506 average time 0.004218444405589454 iter num 360\n",
            "loss 1.4833898544311523 average time 0.004149604589508128 iter num 380\n",
            "loss 1.0474460124969482 average time 0.004096688997528872 iter num 400\n",
            "loss 1.4809032678604126 average time 0.004045830228612128 iter num 420\n",
            "loss 1.9538662433624268 average time 0.003991269411409865 iter num 440\n",
            "loss 1.0184969902038574 average time 0.0039422514304770515 iter num 460\n",
            "loss 1.1343419551849365 average time 0.0039021474312789907 iter num 480\n",
            "loss 0.5168022513389587 average time 0.003873881700030324 iter num 500\n",
            "loss 2.0866479873657227 average time 0.025733044099888504 iter num 20\n",
            "loss 6.876551151275635 average time 0.01432127702487378 iter num 40\n",
            "loss 3.5623815059661865 average time 0.010531257949908952 iter num 60\n",
            "loss 2.4482555389404297 average time 0.008646532012403441 iter num 80\n",
            "loss 1.9955390691757202 average time 0.007498748499929206 iter num 100\n",
            "loss 2.222269058227539 average time 0.006758565374911996 iter num 120\n",
            "loss 1.4065861701965332 average time 0.00622179389277205 iter num 140\n",
            "loss 2.0033085346221924 average time 0.005864127456129608 iter num 160\n",
            "loss 1.736171841621399 average time 0.0055428592999406745 iter num 180\n",
            "loss 1.2013111114501953 average time 0.0052977898849349 iter num 200\n",
            "loss 1.5777536630630493 average time 0.005095024309023508 iter num 220\n",
            "loss 0.8849211931228638 average time 0.004932552937460363 iter num 240\n",
            "loss 1.4922451972961426 average time 0.004778526511521565 iter num 260\n",
            "loss 1.083071231842041 average time 0.004644042042835801 iter num 280\n",
            "loss 1.7739224433898926 average time 0.004530435963309477 iter num 300\n",
            "loss 2.007882833480835 average time 0.004426645678114483 iter num 320\n",
            "loss 0.793595552444458 average time 0.004337977720580752 iter num 340\n",
            "loss 0.9673589468002319 average time 0.0042712462305593765 iter num 360\n",
            "loss 0.7138685584068298 average time 0.004206281523697965 iter num 380\n",
            "loss 2.0050950050354004 average time 0.004146485657502126 iter num 400\n",
            "loss 0.8300957679748535 average time 0.00409853758810641 iter num 420\n",
            "loss 1.0792697668075562 average time 0.00404051076593119 iter num 440\n",
            "loss 0.7589206695556641 average time 0.003988924232631373 iter num 460\n",
            "loss 0.9966047406196594 average time 0.003948806377108364 iter num 480\n",
            "loss 1.0578489303588867 average time 0.003909384400027193 iter num 500\n",
            "loss 0.8465434908866882 average time 0.02643536019986641 iter num 20\n",
            "loss 1.2313157320022583 average time 0.01474721089994091 iter num 40\n",
            "loss 3.167952537536621 average time 0.010890715583233639 iter num 60\n",
            "loss 1.912638783454895 average time 0.00889477276245998 iter num 80\n",
            "loss 1.7031077146530151 average time 0.007724017079963233 iter num 100\n",
            "loss 1.565735101699829 average time 0.006913149224965309 iter num 120\n",
            "loss 3.208916187286377 average time 0.006354927421411308 iter num 140\n",
            "loss 1.984818696975708 average time 0.005946151537500555 iter num 160\n",
            "loss 1.1126765012741089 average time 0.005630391388886993 iter num 180\n",
            "loss 2.0347161293029785 average time 0.005361531219969038 iter num 200\n",
            "loss 1.0653091669082642 average time 0.005141660136343324 iter num 220\n",
            "loss 1.4342575073242188 average time 0.00495640011663454 iter num 240\n",
            "loss 1.0291588306427002 average time 0.004801864561522961 iter num 260\n",
            "loss 0.6518977880477905 average time 0.00466495262497639 iter num 280\n",
            "loss 1.126416802406311 average time 0.004555787036661058 iter num 300\n",
            "loss 1.7694834470748901 average time 0.0044611840250126985 iter num 320\n",
            "loss 1.1105763912200928 average time 0.004373294385314854 iter num 340\n",
            "loss 1.101098656654358 average time 0.0042903810972347856 iter num 360\n",
            "loss 1.2327172756195068 average time 0.004232286742104694 iter num 380\n",
            "loss 1.071134328842163 average time 0.00417300200749196 iter num 400\n",
            "loss 0.9275603890419006 average time 0.0041195144095148685 iter num 420\n",
            "loss 0.5141032338142395 average time 0.004066325609082097 iter num 440\n",
            "loss 1.1141963005065918 average time 0.004021824308679441 iter num 460\n",
            "loss 0.6411235928535461 average time 0.003981036977071047 iter num 480\n",
            "loss 1.321900725364685 average time 0.00393660038998496 iter num 500\n",
            "loss 1.501786231994629 average time 0.02415877819976231 iter num 20\n",
            "loss 0.7659412622451782 average time 0.013620694374776577 iter num 40\n",
            "loss 0.7906956672668457 average time 0.010104157399867593 iter num 60\n",
            "loss 1.0964751243591309 average time 0.008334970437499579 iter num 80\n",
            "loss 1.9430198669433594 average time 0.007234358860023349 iter num 100\n",
            "loss 4.29520320892334 average time 0.006518868591698871 iter num 120\n",
            "loss 1.0233821868896484 average time 0.006025067150013845 iter num 140\n",
            "loss 1.0467791557312012 average time 0.00562781017498537 iter num 160\n",
            "loss 2.361344337463379 average time 0.005327073649985121 iter num 180\n",
            "loss 0.8447744846343994 average time 0.0051054857699728015 iter num 200\n",
            "loss 1.260244607925415 average time 0.00493037612268381 iter num 220\n",
            "loss 1.8500678539276123 average time 0.004763903445784005 iter num 240\n",
            "loss 0.8525996208190918 average time 0.004620695957657098 iter num 260\n",
            "loss 1.2794721126556396 average time 0.00449783709996804 iter num 280\n",
            "loss 0.4625106751918793 average time 0.004389702303324156 iter num 300\n",
            "loss 1.1202510595321655 average time 0.004295958084361473 iter num 320\n",
            "loss 0.8481823801994324 average time 0.004224255429402119 iter num 340\n",
            "loss 0.533222496509552 average time 0.004156244647214812 iter num 360\n",
            "loss 0.8064521551132202 average time 0.004094296152628598 iter num 380\n",
            "loss 0.6381015181541443 average time 0.00404072279999582 iter num 400\n",
            "loss 0.6073077917098999 average time 0.00398402922141724 iter num 420\n",
            "loss 0.6071736812591553 average time 0.0039322368363338515 iter num 440\n",
            "loss 0.572365939617157 average time 0.0038853042956203433 iter num 460\n",
            "loss 0.8155209422111511 average time 0.003847229087462741 iter num 480\n",
            "loss 1.156414270401001 average time 0.0038151614419621185 iter num 500\n",
            "loss 0.8641664981842041 average time 0.02589723544988374 iter num 20\n",
            "loss 0.944528341293335 average time 0.014363151049747103 iter num 40\n",
            "loss 1.8958433866500854 average time 0.010595464716576923 iter num 60\n",
            "loss 1.1261612176895142 average time 0.008655504524904245 iter num 80\n",
            "loss 1.4522984027862549 average time 0.007536358999968798 iter num 100\n",
            "loss 2.052220344543457 average time 0.006760705841619104 iter num 120\n",
            "loss 1.2564623355865479 average time 0.006233089392831711 iter num 140\n",
            "loss 1.2521166801452637 average time 0.00583411626872703 iter num 160\n",
            "loss 1.2114331722259521 average time 0.005521384633296596 iter num 180\n",
            "loss 0.9385073184967041 average time 0.00526615735494488 iter num 200\n",
            "loss 1.529080867767334 average time 0.005071517377241848 iter num 220\n",
            "loss 1.2719058990478516 average time 0.004901089987470186 iter num 240\n",
            "loss 1.4907559156417847 average time 0.00474727998455889 iter num 260\n",
            "loss 1.026042103767395 average time 0.004611481149933232 iter num 280\n",
            "loss 0.8027302026748657 average time 0.004500185266600359 iter num 300\n",
            "loss 0.8597121238708496 average time 0.004407179249932369 iter num 320\n",
            "loss 0.7046247720718384 average time 0.0043162220440642746 iter num 340\n",
            "loss 1.127841830253601 average time 0.0042445212305109534 iter num 360\n",
            "loss 0.5807946920394897 average time 0.004183009989427726 iter num 380\n",
            "loss 0.8565595149993896 average time 0.004126261552464712 iter num 400\n",
            "loss 0.6616153717041016 average time 0.004067227588066522 iter num 420\n",
            "loss 0.35046151280403137 average time 0.004023564527249784 iter num 440\n",
            "loss 0.3580513000488281 average time 0.003972867317367237 iter num 460\n",
            "loss 1.0679877996444702 average time 0.003928312204148673 iter num 480\n",
            "loss 1.085747241973877 average time 0.0038910711639910003 iter num 500\n",
            "loss 2.9162096977233887 average time 0.026268189549955422 iter num 20\n",
            "loss 1.1439518928527832 average time 0.014693806049854175 iter num 40\n",
            "loss 0.7518730163574219 average time 0.010762819683228978 iter num 60\n",
            "loss 0.8080764412879944 average time 0.008789136249811235 iter num 80\n",
            "loss 1.1025385856628418 average time 0.007598582889859245 iter num 100\n",
            "loss 2.047253131866455 average time 0.006807384816587121 iter num 120\n",
            "loss 1.1826584339141846 average time 0.006273155335644073 iter num 140\n",
            "loss 1.3211193084716797 average time 0.005862130518676167 iter num 160\n",
            "loss 0.6551781892776489 average time 0.00555045596657793 iter num 180\n",
            "loss 0.8057767748832703 average time 0.005295255804921908 iter num 200\n",
            "loss 1.2599289417266846 average time 0.005074664222651005 iter num 220\n",
            "loss 0.7623394727706909 average time 0.0049034505749204985 iter num 240\n",
            "loss 1.1857106685638428 average time 0.004750147292249424 iter num 260\n",
            "loss 0.6928900480270386 average time 0.0046213026070940294 iter num 280\n",
            "loss 0.6857109069824219 average time 0.004509538256625092 iter num 300\n",
            "loss 1.2806029319763184 average time 0.004430831218689946 iter num 320\n",
            "loss 0.7429148554801941 average time 0.004352018320525316 iter num 340\n",
            "loss 0.7534670829772949 average time 0.0042746959415985155 iter num 360\n",
            "loss 0.7249875664710999 average time 0.0042078889999270595 iter num 380\n",
            "loss 0.6487472057342529 average time 0.004147903149942067 iter num 400\n",
            "loss 0.4762474298477173 average time 0.004098541414233685 iter num 420\n",
            "loss 0.9329123497009277 average time 0.00404495021813008 iter num 440\n",
            "loss 0.6422491669654846 average time 0.003993681139087604 iter num 460\n",
            "loss 0.7382339835166931 average time 0.003946947458287772 iter num 480\n",
            "loss 0.8151680827140808 average time 0.003903110465958889 iter num 500\n",
            "loss 0.9986283183097839 average time 0.024139894200197888 iter num 20\n",
            "loss 1.5453922748565674 average time 0.013587433300017437 iter num 40\n",
            "loss 0.9369625449180603 average time 0.010060220800005482 iter num 60\n",
            "loss 0.9761969447135925 average time 0.008304626275048577 iter num 80\n",
            "loss 1.7496697902679443 average time 0.007219799460017384 iter num 100\n",
            "loss 1.7062982320785522 average time 0.0064931550333767515 iter num 120\n",
            "loss 0.849571168422699 average time 0.005984056357192458 iter num 140\n",
            "loss 0.7879222631454468 average time 0.005602135937556341 iter num 160\n",
            "loss 0.7759021520614624 average time 0.0053069560000242036 iter num 180\n",
            "loss 1.7901406288146973 average time 0.005096971510020012 iter num 200\n",
            "loss 0.7469409704208374 average time 0.004913774768209061 iter num 220\n",
            "loss 1.3334741592407227 average time 0.004749482725022365 iter num 240\n",
            "loss 0.9836069941520691 average time 0.004609185900013611 iter num 260\n",
            "loss 0.7701846957206726 average time 0.00448356785714168 iter num 280\n",
            "loss 0.5672542452812195 average time 0.004375379796674679 iter num 300\n",
            "loss 0.5633440017700195 average time 0.004280944987510793 iter num 320\n",
            "loss 0.4914212226867676 average time 0.004217330788239505 iter num 340\n",
            "loss 0.4358683228492737 average time 0.004150910269451439 iter num 360\n",
            "loss 0.7122949361801147 average time 0.0040908902552712935 iter num 380\n",
            "loss 0.6562759876251221 average time 0.004039302690011936 iter num 400\n",
            "loss 0.35020896792411804 average time 0.003983570764293767 iter num 420\n",
            "loss 0.536174476146698 average time 0.003945982102285681 iter num 440\n",
            "loss 0.9213962554931641 average time 0.0039024698869742796 iter num 460\n",
            "loss 0.35016393661499023 average time 0.0038828318625254116 iter num 480\n",
            "loss 0.6607377529144287 average time 0.0038531787180218087 iter num 500\n",
            "loss 1.3417274951934814 average time 0.02766703819988834 iter num 20\n",
            "loss 0.8799334764480591 average time 0.015330604599876096 iter num 40\n",
            "loss 1.9987736940383911 average time 0.011169365349906002 iter num 60\n",
            "loss 1.7993066310882568 average time 0.009115865012381619 iter num 80\n",
            "loss 1.3384575843811035 average time 0.007854135939924163 iter num 100\n",
            "loss 3.1732640266418457 average time 0.007027759174949703 iter num 120\n",
            "loss 2.6613192558288574 average time 0.006460636621406073 iter num 140\n",
            "loss 1.1797165870666504 average time 0.006033307212464934 iter num 160\n",
            "loss 0.4373944401741028 average time 0.005693622905518067 iter num 180\n",
            "loss 1.5493221282958984 average time 0.005413758204967962 iter num 200\n",
            "loss 0.7446119785308838 average time 0.005207261709057837 iter num 220\n",
            "loss 0.7517131567001343 average time 0.005027816045791648 iter num 240\n",
            "loss 1.4471102952957153 average time 0.004876133303801348 iter num 260\n",
            "loss 0.8645559549331665 average time 0.004737860171358729 iter num 280\n",
            "loss 0.8062604665756226 average time 0.004625921416603281 iter num 300\n",
            "loss 0.6029378175735474 average time 0.004515151181203691 iter num 320\n",
            "loss 0.42378807067871094 average time 0.004415412958769594 iter num 340\n",
            "loss 0.4327694773674011 average time 0.004337785622197367 iter num 360\n",
            "loss 0.5557877421379089 average time 0.004268694323658756 iter num 380\n",
            "loss 0.683785080909729 average time 0.004203891154970734 iter num 400\n",
            "loss 0.8574133515357971 average time 0.004147255130928464 iter num 420\n",
            "loss 0.6791297197341919 average time 0.0040949645772467526 iter num 440\n",
            "loss 0.6483086347579956 average time 0.004046810380405868 iter num 460\n",
            "loss 0.473021924495697 average time 0.003997396224959478 iter num 480\n",
            "loss 0.39540332555770874 average time 0.0039583778339729175 iter num 500\n",
            "loss 0.9492560625076294 average time 0.026298917600070127 iter num 20\n",
            "loss 1.5706630945205688 average time 0.014638514875150577 iter num 40\n",
            "loss 1.3588000535964966 average time 0.010727500750090258 iter num 60\n",
            "loss 0.953413724899292 average time 0.008781508237552771 iter num 80\n",
            "loss 0.2739395797252655 average time 0.007604025320033543 iter num 100\n",
            "loss 1.0323486328125 average time 0.006824488675025956 iter num 120\n",
            "loss 1.7914822101593018 average time 0.006265659592840426 iter num 140\n",
            "loss 0.8911446332931519 average time 0.005862675456251054 iter num 160\n",
            "loss 0.5786811113357544 average time 0.00554389298330433 iter num 180\n",
            "loss 0.6132371425628662 average time 0.0052953048899871645 iter num 200\n",
            "loss 0.8299462795257568 average time 0.0050753805227445395 iter num 220\n",
            "loss 0.5057837963104248 average time 0.004894101437495617 iter num 240\n",
            "loss 0.9501668810844421 average time 0.004738288349989699 iter num 260\n",
            "loss 0.465529203414917 average time 0.0046044510571131726 iter num 280\n",
            "loss 0.2764759063720703 average time 0.00450271173999378 iter num 300\n",
            "loss 0.6754229068756104 average time 0.004410360062485097 iter num 320\n",
            "loss 0.6147222518920898 average time 0.004341950167631962 iter num 340\n",
            "loss 0.5443792939186096 average time 0.004268274586092957 iter num 360\n",
            "loss 0.4826453626155853 average time 0.004212260326298029 iter num 380\n",
            "loss 0.6297072172164917 average time 0.004154087099959724 iter num 400\n",
            "loss 0.5932472944259644 average time 0.0041028014261428325 iter num 420\n",
            "loss 0.3658682703971863 average time 0.004050412309032419 iter num 440\n",
            "loss 0.6439604163169861 average time 0.004013346110822686 iter num 460\n",
            "loss 0.6536502838134766 average time 0.003969601089549238 iter num 480\n",
            "loss 0.40017634630203247 average time 0.003926975847960421 iter num 500\n",
            "loss 0.8019556999206543 average time 0.024650992050010246 iter num 20\n",
            "loss 0.6866121888160706 average time 0.013839427324865029 iter num 40\n",
            "loss 1.8368654251098633 average time 0.010230900383309443 iter num 60\n",
            "loss 1.2303712368011475 average time 0.008401385875049527 iter num 80\n",
            "loss 0.8265485763549805 average time 0.007293353270033549 iter num 100\n",
            "loss 0.9396246671676636 average time 0.006583544108358789 iter num 120\n",
            "loss 0.5600934624671936 average time 0.00606297991433686 iter num 140\n",
            "loss 0.6732997894287109 average time 0.005663463425071314 iter num 160\n",
            "loss 0.49034929275512695 average time 0.005396511922289291 iter num 180\n",
            "loss 0.7851978540420532 average time 0.005164051650044712 iter num 200\n",
            "loss 0.6309360861778259 average time 0.004973853663696817 iter num 220\n",
            "loss 0.49720045924186707 average time 0.004816717416701977 iter num 240\n",
            "loss 0.5200423002243042 average time 0.004669408534615426 iter num 260\n",
            "loss 0.8206155300140381 average time 0.0045449265785789195 iter num 280\n",
            "loss 0.7267380952835083 average time 0.004435801486670244 iter num 300\n",
            "loss 0.6480370759963989 average time 0.004344659665628114 iter num 320\n",
            "loss 0.532455325126648 average time 0.004271729626484825 iter num 340\n",
            "loss 0.3151524066925049 average time 0.004204451438903966 iter num 360\n",
            "loss 0.18917155265808105 average time 0.004137717552648131 iter num 380\n",
            "loss 0.46924203634262085 average time 0.004074929727521521 iter num 400\n",
            "loss 0.41118571162223816 average time 0.004020294888119853 iter num 420\n",
            "loss 0.531082272529602 average time 0.003967301122743678 iter num 440\n",
            "loss 0.5049390196800232 average time 0.003932032358704379 iter num 460\n",
            "loss 0.6263132095336914 average time 0.003897413156255425 iter num 480\n",
            "loss 0.45470118522644043 average time 0.0038670800000036253 iter num 500\n",
            "loss 1.5760866403579712 average time 0.02592402280006354 iter num 20\n",
            "loss 0.7316007614135742 average time 0.014475009449915888 iter num 40\n",
            "loss 0.5784181952476501 average time 0.010645990649967038 iter num 60\n",
            "loss 1.0550225973129272 average time 0.008766988962497635 iter num 80\n",
            "loss 0.5310232043266296 average time 0.007587642989983578 iter num 100\n",
            "loss 0.6831169128417969 average time 0.006807434991651462 iter num 120\n",
            "loss 0.40650486946105957 average time 0.006306555635676757 iter num 140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-522e044abd7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoint16.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mrandoms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m           \u001b[0;31m# randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97e2176-17d7-4d18-d2c8-9a338d2ab7f1"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[13.2237]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6deefa9d-9b4e-4768-dfef-b8706261626d"
      },
      "source": [
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -0.1484,   1.2931,   0.1766,   4.0910,   9.2909, -16.4748,  -0.1571,\n",
              "           6.2653,   0.1782,   3.4652,   9.3130, -13.2611,  -0.1647, -19.7690,\n",
              "           0.1745,   3.1327,   8.9078,   9.6869]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwgeVDsA_Mr"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "ee6630d8-01ae-4b91-bff2-f4e643e3a318"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fae3bab5b90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "VGk5Hw64fMdh",
        "outputId": "6a0599a2-b6c8-4c46-a66c-242baef30483"
      },
      "source": [
        "## Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fae3bc40550>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZf7+8fcngdCLobcA0rt0UVRQREDRtYMN2+qqqItrXZVFv7prxdVVcVlFsSBWEBQF0bUhndBCDb13SCghZZ7fHzPwC2wCAXJyptyv68rFzDkncHMymXtOe4455xARkdgV53cAERHxl4pARCTGqQhERGKcikBEJMapCEREYlwxvwOcqMqVK7t69er5HUNEJKLMnj17u3OuSl7zIq4I6tWrx6xZs/yOISISUcxsTX7ztGtIRCTGqQhERGKcikBEJMapCEREYpyKQEQkxqkIRERinIpARCTGRdx1BCIi0S47J0BWjmPZlnTSMrJ4b8pq4uKMIZe2oFbFUoX+76kIRESK2N6D2Xy3cDPLt6SzaFMa8XHGaaUTGJO84Zjfd0aditzTvWGh51ERiIh44EBmDtv3HmT73oO8NGkpZRKKMWnRllP6O69qX7uQ0h1JRSAiUkgCAcegT+fy1dyNJ/y9iWUSOPP0RLo1rkq3JlWoXLYEZmBmHiQ9kopAROQErdu5n937sygWb6zavo+7P5pz3O9pU6ciFUoV54mLm5FYJoG0A1mUKB7vyT7/E6UiEBE5BuccK7btZcnmdAaOSi7Q97SpXYEezapxY5e6VCydkOcylcuWKMyYp0RFICKSS07AsS8zm28XbGL2ml18Omt9vsvGGQzs3pCaFUvRu2UNKpQuXoRJC4+KQERiXmZ2gCfGLjjmm/4htSqW4qWr29CpfiLxcd7vvy8KKgIRiTnZOQEysgP855eVvPrD8nyX61Q/kZevbkOdxNJFmK7oqQhEJGbMW7ebcfM28s5vq/Kcn1gmgXcGdKBt0mlFnMxfKgIRiUrZOQGmr9rJNws2MWr62jyXaVKtHAPPb0jfNjWLOF14URGISNTIyMrhhe+WMn3VDlI2puW5zDUdavOXnk2oVr5kEacLXyoCEYlYBzJzeOe3lbw0adkxl7vv/IY80LNJEaWKPCoCEYkoOQHHmOQNPPjZvDznP39lK3q3qkG5EsWK5KrcaKAiEJGwt/dgNp/PWseQ8YvynP/QRU24qUtdypWMzPP4/aYiEJGwlJGVw+TFW/K9mrdPq+o8dWlLqpQLnyt0I5WKQETCxta0DL5fvIXHxyzMc/6jvZtyZbvaevMvZCoCEfHdj0u2cOt7s/Kc98TFzbiucxKlE/R25RWtWRHxxbSVO+g3fFqe8244M4khfVtQLF530y0KKgIRKTLOOV6atJRvF2xm5fZ9R8x74uJm9G1Tk3Ili+nTfxHT2hYRzwUCjoe/mM/ns///oG5JiaWpfVopnr6sJQ2rlvUxnagIRMQzB7Nz6PPqr6zYduSn/6/v7UqLmuV1nn+YUBGIiCeG/7KCv09YcsS0kbd24rzGVXxKJPlREYhIocnOCfCPb5f8z+ieE/98Lk2ql/MplRyPikBETllmdoBXJi9j2E8rDk/r0awqj/VpRoMq2v8f7lQEInJK3p2yiqeOGvrhl4e6k1Qpum/mEk1UBCJyUrJyArR5ahL7M3MOT1vyf70oWTzex1RyMlQEInLCMrJyaPrkd4efD7+xPT1bVPcxkZwKFYGInJD0jCxueXfm4edTHzufGhVK+ZhITpWKQEQKbN3O/fzhjSns2JfJ7V3r83CvpiQU0zAQkU5FICIFsmbHPi585RcyswMM6tGY+3s08juSFBJPq9zMepnZUjNLNbNH85j/JzNbYGZzzew3M2vuZR4ROTmLN6Vx3os/kZkdYOg1bVQCUcazIjCzeOANoDfQHOifxxv9KOdcK+fcGcALwFCv8ojIyflh8RZ6v/orAA/2bMwV7Wr7nEgKm5dbBJ2AVOfcSudcJjAauCz3As65tFxPywDOwzwicoImpWzmtpHB+wRc1zmJgedrSyAaeXmMoBawLtfz9UDnoxcys3uAB4AE4Py8/iIzuwO4AyApKanQg4rI/1q9fR93fDAbgC/u6kL7uok+JxKv+H643zn3hnOuAfAI8EQ+ywx3znVwznWoUkUDVol4bfaanXR76ScA7jzvdJVAlPNyi2ADUCfX89qhafkZDQzzMI+IFMCG3Qe4cthUAF68qjVXd6hznO+QSOflFsFMoJGZ1TezBKAfMC73AmaWe4fjxcByD/OIyHGs37Wfs5/7EYD+nZJUAjHCsy0C51y2mQ0EJgLxwAjnXIqZPQ3Mcs6NAwaaWQ8gC9gFDPAqj4gc2/7MbG4L3UD+3vMb8peeTXxOJEXF0wvKnHMTgAlHTRuc6/H9Xv77IlIw2TkBBo5KZumWdG49u75KIMboymKRGOec47EvF/Djkq08eUlzbuta3+9IUsRUBCIx7oFP5zEmeQPXd05SCcQoFYFIjHLO0eapSaRlZFO5bAmevqyl35HEJ75fRyAi/hg3byNpGdkA/PjgecTHmc+JxC8qApEY9N6UVdw/ei4AP/zlPMqXLO5zIvGTdg2JxJjUrXsZErrH8K8Pd6dOou4tHOu0RSASQzbsPkCPoT8DMKRvc5WAACoCkZjhnDt81fD9FzTi5rN1hpAEqQhEYsQ7v60CoFmN8gy6sLHPaSScqAhEYsA38zfxzDeLOb9pVb65t6vfcSTMqAhEotzq7fu4Z9QcWtQsz5vXtyNOp4nKUVQEIlFsz/6sw/cVeO6K1pQsHu9vIAlLKgKRKDZkfAoAT1zcjFa1K/icRsKVikAkSn08Yy1jkjdwV7cG3H7O6X7HkTCmIhCJQvPX7+axLxcAMLB7Q5/TSLhTEYhEmR17D3Lp61MAmDToXMqU0AACcmwqApEoEgg4bhoxA4Bh17ejcbVyPieSSKAiEIki/YZPI2VjGle0rUXvVjX8jiMRQkUgEiU+m7WOGat3AvDi1W18TiORREUgEgXW7tjPQ5/PByD5yQt1bwE5ISoCkQh3MDuHvq//BsCoP3bmtDIJPieSSKMiEIlw/5y8nD0HsriiXS3OalDZ7zgSgVQEIhHs4xlrGfbTCvq0qs7LOi4gJ0lFIBKh5q0LXjTWtHo5hl5zBmY6LiAnR0UgEoEysnK484PZALx1Q3sNJienREUgEmECAcdDn89nc1oGj/VuSr3KZfyOJBFORSASYUbNWMv4eRu5rWt97jyvgd9xJAqoCEQiyNQVO3hi7EI61U/k0d5N/Y4jUUJFIBIhtqUf5N6PkwH4++UtKR6vX18pHHoliUSAQMDxwKdzSc/IYuKfz6VhVQ0mJ4VHRSASAZ75ZjG/Lt/Ok5c0p0l1lYAULhWBSJgb8dsqRkxZxQ1nJnF95yS/40gUUhGIhLH563fz7ITFlCgWxyO9muqiMfGEikAkTO3Zn8U9o+ZQrVwJpj12AeVKFvc7kkQp3cNOJAxl5QS4beRMNu/J4JM7u2hEUfGUtghEwtAzXy9i1ppdDO7bgnZJp/kdR6Kcp0VgZr3MbKmZpZrZo3nMf8DMFpnZfDP7wczqeplHJBK8/etKRk5dw+1d63PjmfqVEO95VgRmFg+8AfQGmgP9zaz5UYslAx2cc62Bz4EXvMojEgnGJK/nmW8W06NZVV05LEXGyy2CTkCqc26lcy4TGA1clnsB59x/nXP7Q0+nAbU9zCMS1hZvSuPBz+ZTq2Iphl57BsV05bAUES9fabWAdbmerw9Ny89twLd5zTCzO8xslpnN2rZtWyFGFAkPW9MyGDBiBqWKx/POzR0orzOEpAiFxUcOM7sB6AC8mNd859xw51wH51yHKlWqFG04EY8Fh4+Yx9b0g4y8tSNNq5f3O5LEGC9PH90A1Mn1vHZo2hHMrAfwOHCec+6gh3lEwk5OwHHD29OZunIHj/RqSvu6iX5Hkhjk5RbBTKCRmdU3swSgHzAu9wJm1hb4N3Cpc26rh1lEwtJLk5YydeUO+nWsw5/OO93vOBKjPCsC51w2MBCYCCwGPnXOpZjZ02Z2aWixF4GywGdmNtfMxuXz14lEnc9mrWPYTys4v2lV/nFFKw0fIb7x9Mpi59wEYMJR0wbnetzDy39fJFy99fMKnvt2CW2TKvLvG9urBMRXYXGwWCSWjElez3PfLgHgvVs66QYz4ju9AkWK0Ppd+xkybhEA3w86lwqldJqo+E+DzokUkd37M7n7ozkEAo6fH+pG3Upl/I4kAqgIRIrEwewc+v9nOos3pfFqvzNUAhJWtGtIxGOZ2QFuHzmLxZvSuKd7Ay4741gX2IsUvQJtEZhZI+AfBAePK3lounNOJz6LHENmdoBBn87l1+XbeaRXU+7q1sDvSCL/o6BbBO8Cw4BsoDvwPvChV6FEooFzjke/mM838zfxYM/GKgEJWwUtglLOuR8Ac86tcc4NAS72LpZI5Bv8VQpfJm/gvgsaMfD8Rn7HEclXQQ8WHzSzOGC5mQ0kOGZQWe9iiUS2//t6ER9MW0PP5tUY1EMlIOGtoFsE9wOlgfuA9sANwE1ehRKJZEMnLeWd31ZRs0JJht2gq4Yl/BW0COo55/Y659Y7525xzl0JJHkZTCQSjZ+3kdd+TKVi6eL88JduxMepBCT8FbQIHivgNJGY9d3Czdw/OpnG1cry80PdKZUQ73ckkQI55jECM+sN9AFqmdlruWaVJ3gGkYgAs9fs4u6PZlOvUhlG39FFQ0dIRDneweKNwGzg0tCfh6QDg7wKJRJJtqZlcOWw30mIj+PdWzqSWCbB70giJ+SYReCcmwfMM7MPQ/cXEJFc9mdmc9vIWQC8fE0bDR0hEel4u4YWAC70+H/mO+daexNLJPxlZOXQ8m8TCTh464Z29GpZw+9IIifleLuGLimSFCIRJisnwD0fzSHg4OLWNVQCEtGOt2tozaHHZlYXaOScm2xmpY73vSLRyjnHU+NT+GHJVh66qAl3a+gIiXAFOn3UzP4IfE7wRvMAtYGxXoUSCVfOOZ79ZjEfTlvLneedzj3dG+qCMYl4Bb2O4B7gbCANwDm3HKjqVSiRcDXy99W8/dsqBnSpyyMXNfU7jkihKPBYQ865zEOffMysGKGDyCKx4oXvlvDmTytoX/c0nrykOXG6aliiREG3CH42s78CpczsQuAzYLx3sUTCy/tTV/PmTyuoVbEU797SkWK64bxEkYK+mh8FtgELgDuBCcATXoUSCSeTUjYz+KsUalYoyfh7u1K+pK4aluhSoF1DzrmAmY0FxjrntnmcSSRsvDRxKa//N5U6iaX44q6zdNWwRKVjbhFY0BAz2w4sBZaa2TYzG1w08UT88+6UVbz+31RKFo9j7N1nU7VcyeN/k0gEOt6uoUEEzxbq6JxLdM4lAp2Bs81MYw1J1Pp6/kaeGr8IgOl/7UGlsiV8TiTineMVwY1Af+fcqkMTnHMr0Y1pJIq9N2UVA0cl06Z2BZKfvFAjiUrUO94xguLOue1HT3TObTMz/XZI1Pl05jqGjF9EYpkE3r+1MxVK62Uu0e94RZB5kvNEIs4HU1fz5FcpnNWgEu8M6Kgby0jMOF4RtDGztDymG6AjZxI1vpq7gSe/SqFMQjzDrm+vEpCYcrxB5/TbIFHv4xlreXzMAjrVT+SdAR0op+sEJMZoBFGJWc45Xpm8nNd+WE63JlV48/p2lE7Qr4TEHr3qJSY553hh4lKG/bSCTvUSVQIS0/TKl5iTnpHF42MWMm7eRi5tU5OXr2lDcY0dJDFMRSAxJT0ji1ZDJgEwqEdj7rtA9xMQ8fRjkJn1MrOlZpZqZo/mMf9cM5tjZtlmdpWXWUR27cuk1z9/BeAPZ9Tk/h6NVAIieFgEZhYPvAH0BpoD/c2s+VGLrQVuBkZ5lUMEYO2O/fQbPo0Nuw/w0EVN+Ge/tn5HEgkbXu4a6gSkhoakwMxGA5cBiw4t4JxbHZoX8DCHxLhZq3dy1VtTAXj35o50b6qb64nk5uWuoVrAulzP14emnTAzu8PMZpnZrG3bNAq2FNx3Czdz7fBpALxybRuVgEgeIuJUCefccOdcB+dchypVqvgdRyLEmz+l8qcPZ1O5bAIT7juHy9vW9juSSFjyctfQBqBOrue1Q9NEPBUIOLo+/yMb92QAMP7errqXgMgxeLlFMBNoZGb1zSwB6AeM8/DfEyEtI4v7RiezcU8GCfFxLH2ml0pA5Dg82yJwzmWb2UBgIhAPjHDOpZjZ08As59w4M+sIjAFOA/qa2VPOuRZeZZLotnH3AXoM/Zn9mTnce35DBvVoTFycTg8VOR5PLyhzzk0geKP73NMG53o8k+AuI5FTMmftLu76cDb7M3MYek0brminl5VIQenKYol4o2esZfBXKVSrUIJv7z+HZjXK+x1JJKKoCCRiZWYHeGp8Ch9NX8s5jSrzr/5tqVg6we9YIhFHRSARaWtaBnd9NIfZa3Zx53mn8/BFTYnX8QCRk6IikIgze03weEB6Rjb/6t+Wvm1q+h1JJKKpCCSiDP9lBS9OXEr1CiUZeetZOh4gUghUBBIRMrJyaPPUJA5mB2hWozwf/7GzjgeIFBIVgYS9TXsOcMu7MzmYHRyb8LM/daFsCb10RQqLfpskrM1es5P7Pp7LngNZDL6kObd2re93JJGooyKQsOSc4+1fV/Hcd0uoXDaBUX/sTOvaFf2OJRKVVAQSdvYezObhz+cxYcFmerWozotXt6ZcyeJ+xxKJWioCCSupW9O584PZrNq+j8d6N+WOc0/X7SRFPKYikLDx9fyNPPz5fEonxPPR7WfSpUElvyOJxAQVgfhu175Mur/8E7v3Z9EuqSJvXt+e6hU0dLRIUVERiK/mr9/Npa9PAeDcxlV4+6YOJBSLiBvniUQNFYH4IjsnwEuTlvHWzysAeLhXE+7u1tDnVCKxSUUgRW7dzv08+uV8pqTu4NzGVXj12jM4rYyuEhbxi4pAitQXs9fzl8/mAfD8la24tmOSz4lEREUgRWLfwWyeGLuQMckbqFWxFK/1b0v7uqf5HUtEUBFIEZi5eif9h08jO+C4rnMST1/agmLxOiAsEi5UBOKZzOwAj345n6/mbqREsThevLwll7fVvYRFwo2KQDyxdHM6949OZsnmdM5vWpV/9juD8homQiQsqQikUAUCjg+nr+EfE5ZQLN4Y0rc5A86qp2EiRMKYikAKzeY9GQz6ZC5TV+7gnEaVef7K1tSsWMrvWCJyHCoCOWXOOZ4av4j3fl9NqeLxvHBla67uUFtbASIRQkUgpyR1614Gf7WQ31fsIKFYHOMGnk2jauX8jiUiJ0BFICclKyfA+1PX8OLEJWRkBXji4mbc1KWexgkSiUAqAjlhvy7fxt/GpbBy2z5a1arA8JvaU6OCjgWIRCoVgRTYjr0HeeSL+UxevBWAYde3o1fL6joWIBLhVARyXM45Xp60jNf/mwoEh4t+6erWVC2newaIRAMVgRxT6tZ0Ln/jd9IPZlOlXAneuqG9xggSiTIqAsnTpj0HePjz+fy6fDsAjauVZdzArpQsHu9zMhEpbCoCOUJ2ToC3f1vFc98uAaBPq+o8eUlzHQwWiWIqAjlsxqqdDP5qIUs2p9O4Wlnu6tZAg8SJxAAVgbBi215e+G4JE1O2UKlMAv/q35ZLWtfQ2UAiMUJFEMN27cvktR+X8+6U1QCc3bAS/+rfjkTdNlIkpnhaBGbWC3gViAfeds49d9T8EsD7QHtgB3Ctc261l5kkOELoO7+t4tkJiwE4q0El/tqnGS1rVfA5mYj4wbMiMLN44A3gQmA9MNPMxjnnFuVa7DZgl3OuoZn1A54HrvUqU6xzzvHNgk38/ZvFbNyTQemEeN69uSOdT6/kdzQR8ZGXWwSdgFTn3EoAMxsNXAbkLoLLgCGhx58Dr5uZOeech7liUsrGPdw+chab9mRQs0JJHuvdlD+eczpxcToOIBLrvCyCWsC6XM/XA53zW8Y5l21me4BKwHYPc8WUdTv3M/T7ZYyduwHn4Oaz6vHQRU0oU0KHh0QkKCLeDczsDuAOgKSkJJ/TRIblW9J54NN5LNiwhxLF4rjz3Abc1a0BFUrpdpEiciQvi2ADUCfX89qhaXkts97MigEVCB40PoJzbjgwHKBDhw7abXQM6RlZXPPvaSzelAZAp/qJvNrvDF0QJiL58rIIZgKNzKw+wTf8fsB1Ry0zDhgATAWuAn7U8YGT9+G0NTwxduHh529e344+rWr4mEhEIoFnRRDa5z8QmEjw9NERzrkUM3samOWcGwe8A3xgZqnAToJlISfAOUfyut08OXYhKRuDWwE3n1WPwZc014FgESkQT48ROOcmABOOmjY41+MM4GovM0SzhRv2cP3b09lzIAuAAV3q8uBFTShXUscBRKTgIuJgsRwpLSOLJ8cu5Ku5GwG44cwkHuzZhIqldUWwiJw4FUEEycjK4ZXJy/j3zysBqFy2BCNu7kDr2hV9TiYikUxFEAGcc4xJ3sBfPpuHc5AQH8fHd3Smfd1Ev6OJSBRQEYS5n5dtY/gvK5iSGjyr9uaz6vHXPs1IKBbnczIRiRYqgjA1edEWXpm8jJSNacTHGX/r25ybutQjXmcCiUghUxGEmXU79/O3cSn8uGQrENwCuO+CRhoaWkQ8oyIIE8lrdzFkXArz1u8BgkND//3yVtSrXMbnZCIS7VQEPlu5bS//+XUVH89YC0DvltV54MLGNKpWzudkIhIrVAQ+2ZZ+kFvfm8mCDcEtgPMaV+HhXk1oUVM3hxGRoqUiKGJ79mfxyuRljJq+lsycAD2bV+Ohi5poC0BEfKMiKCK79mUy9PtlfDR9DQEH3ZtUYdCFjXUxmIj4TkXgsa3pGbzxYyojp64BoE3tCjzzh1a0qq1dQCISHlQEHtmw+wDPfrOICQs2Ex9nXN62FgPOqscZdbQFICLhRUVQyFK3pvP014v5Zdk2AGpVLMXIWzvRsGpZn5OJiORNRVBIUjbu4d6Pk1m5bR8A5zSqzCO9mtKylnYBiUh4UxGcAucc01bu5JXJy5ixaicQLICXr25D1fIlfU4nIlIwKoKTNCZ5PU+PX8Su/VkkxMdxbYc6PNCzMdVUACISYVQEJyAzO8Ark5cxesZadu0P3hXskV5NubFLXcqW0KoUkcikd68CSM/I4pOZ6/jn5OXsPZhN42plOatBZZ66rAWVy5bwO56IyClRERzDpj0HeHJsCj8v20pWjqN4vDGwe0MGXdhYw0GLSNRQERzFOcfcdbsZMGIGaRnZAHSun8h1nZPo27omcSoAEYkyKoKQg9k5vDtlNc99u+TwtItb1eC+CxrRpLrGARKR6BXzRbBoYxp/eGMKmTmBw9MualGNZy9vpf3/IhITYrYIxs/byCuTlx2+AAx0P2ARiU0xVQRZOQE+mLqGz2avZ/GmNAA61UvkzAaV+PMFjbT/X0RiUswUwacz1/HwF/MPP+/Tqjp3d2uoISBEJObFTBFULF2cTvUTaV6jPI/2bkrJ4vF+RxIRCQsxUwQ9W1SnZ4vqfscQEQk7OioqIhLjVAQiIjFORSAiEuNUBCIiMU5FICIS41QEIiIxTkUgIhLjVAQiIjHOnHN+ZzghZrYNWON3jnxUBrb7HeIYlO/UhXtG5Ts10ZyvrnOuSl4zIq4IwpmZzXLOdfA7R36U79SFe0blOzWxmk+7hkREYpyKQEQkxqkICtdwvwMch/KdunDPqHynJibz6RiBiEiM0xaBiEiMUxGIiMQ4FcFJMrM6ZvZfM1tkZilmdn9o+hAz22Bmc0NffXzMuNrMFoRyzApNSzSz781seejP03zK1iTXOpprZmlm9mc/15+ZjTCzrWa2MNe0PNeXBb1mZqlmNt/M2vmU70UzWxLKMMbMKoam1zOzA7nW41s+5cv352lmj4XW31Izu8infJ/kyrbazOaGpvux/vJ7T/H+Neic09dJfAE1gHahx+WAZUBzYAjwoN/5QrlWA5WPmvYC8Gjo8aPA82GQMx7YDNT1c/0B5wLtgIXHW19AH+BbwIAzgek+5esJFAs9fj5Xvnq5l/Nx/eX58wz9rswDSgD1gRVAfFHnO2r+y8BgH9dffu8pnr8GtUVwkpxzm5xzc0KP04HFQC1/UxXIZcDI0OORwB98zHLIBcAK55yvV4w7534Bdh41Ob/1dRnwvguaBlQ0sxpFnc85N8k5lx16Og2o7WWGY8ln/eXnMmC0c+6gc24VkAp08iwcx85nZgZcA3zsZYZjOcZ7iuevQRVBITCzekBbYHpo0sDQptoIv3a9hDhgkpnNNrM7QtOqOec2hR5vBqr5E+0I/TjyFzBc1h/kv75qAetyLbce/z8I3ErwE+Ih9c0s2cx+NrNz/ApF3j/PcFt/5wBbnHPLc03zbf0d9Z7i+WtQRXCKzKws8AXwZ+dcGjAMaACcAWwiuLnpl67OuXZAb+AeMzs390wX3L709fxhM0sALgU+C00Kp/V3hHBYX/kxs8eBbOCj0KRNQJJzri3wADDKzMr7EC1sf55H6c+RH0Z8W395vKcc5tVrUEVwCsysOMEf2EfOuS8BnHNbnHM5zrkA8B883tw9FufchtCfW4ExoSxbDm0+hv7c6le+kN7AHOfcFgiv9ReS3/raANTJtVzt0LQiZ2Y3A5cA14feKAjtctkRejyb4D74xkWd7Rg/z3Baf8WAK4BPDk3za/3l9Z5CEbwGVQQnKbRP8R1gsXNuaK7puffRXQ4sPPp7i4KZlTGzcoceEzyouBAYBwwILTYA+MqPfLkc8UksXNZfLvmtr3HATaEzN84E9uTafC8yZtYLeBi41Dm3P9f0KmYWH3p8OtAIWOlDvvx+nuOAfmZWwszqh/LNKOp8IT2AJc659Ycm+LH+8ntPoSheg0V5VDyavoCuBDfR5gNzQ199gA+ABaHp44AaPuU7neBZGfOAFODx0PRKwA/AcmAykOjjOiwD7AAq5Jrm2/ojWEibgCyC+1tvy299ETxT4w2CnxQXAB18ypdKcD/xodfgW6Flrwz93OcCc4C+PuXL9+cJPB5af0uB3n7kC01/D/jTUcv6sf7ye0/x/DWoISZERGKcdg2JiMQ4FYGISIxTEYiIxIadNnIAAAHSSURBVDgVgYhIjFMRiIjEOBWByEkys6fNrIffOUROlU4fFTkJZhbvnMvxO4dIYdAWgchRQmPRLzGzj8xssZl9bmalQ+PVP29mc4Crzew9M7sq9D0dzex3M5tnZjPMrJyZxVvwfgEzQ4Ou3RlatoaZ/RIa536hzwPCiVDM7wAiYaoJwStPp5jZCODu0PQdLjiQ36HhHQ4NnPcJcK1zbmZocLIDBK+s3eOc62hmJYApZjaJ4Lg2E51zz4aGMShdtP81kSOpCETyts45NyX0+EPgvtDjT/JYtgmwyTk3E8CFRow0s55A60NbDUAFgmPWzARGhAYYG+ucm+vR/0GkQFQEInk7+uDZoef7TuDvMOBe59zE/5kRHBL8YuA9MxvqnHv/5GKKnDodIxDJW5KZdQk9vg747RjLLgVqmFlHgNDxgWLAROCu0Cd/zKxxaFTYugRvgvIf4G2Ct08U8Y2KQCRvSwnezGcxcBrBG6zkyTmXCVwL/MvM5gHfAyUJvskvAuZY8Ibp/ya4Fd4NmGdmyaHve9XD/4fIcen0UZGjhG4T+LVzrqXPUUSKhLYIRERinLYIRERinLYIRERinIpARCTGqQhERGKcikBEJMapCEREYtz/A2765NbDQmT+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLStvS2qCSjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d9632e-ecb8-41e0-8fd7-1750fa1ea8b0"
      },
      "source": [
        "compute_delta(110)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5859]], device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "4O1I8COnUxnz",
        "outputId": "14a57b39-b189-41f3-9bb7-eb691068fcef"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7faf18078290>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySrey9KzB0AF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f28407-a810-4f63-af20-d45241fa51d0"
      },
      "source": [
        "compute_delta(110).item()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5291938781738281"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMyME_1WCGZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "850460e2-cbde-40f8-f42c-37de0ae2fdc9"
      },
      "source": [
        "compute_delta(102).item()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4406929016113281"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB8LPwfBMHQ"
      },
      "source": [
        "# Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoaOyCDxQy0"
      },
      "source": [
        "##Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]  + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsXgOwWZ_ru"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S + epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs3 = torch.tensor([[110.0, 0.0, S - epsilon, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lca2xrBh9a"
      },
      "source": [
        "# Vega"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muozc-hzhSGA"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KATxBCAdlFt"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a timev\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "    inputs2 = torch.tensor([[110.0, 0.0, S, 0.35 + epsilon, 0.1, 0.05]*3]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "prices = np.arange(10, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 0.0, 110.0, sigma, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}