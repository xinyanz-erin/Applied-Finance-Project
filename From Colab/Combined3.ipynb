{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "dBOv_RiBsCWa",
        "u2_89jOknwjH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/From%20Colab/Combined3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxOZk3ls2XQ",
        "outputId": "e85340cc-1db6-4e0d-f397-de5c4d86cf7e"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0   6053      0 --:--:-- --:--:-- --:--:--  6053\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "\u001b[K     |████████████████████████████████| 58.9MB 89kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 39.3MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "ac958b42-0bd5-4600-d2b8-246276885438"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 1.        , 0.05103263, 0.0071633 , 0.52167781,\n",
              "        1.        ],\n",
              "       [1.        , 1.        , 0.64857557, 0.32324551, 0.39745689,\n",
              "        1.        ],\n",
              "       [1.        , 1.        , 0.82301291, 0.46666519, 0.8391176 ,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "264cef2d-1ead-43b8-e5fd-d8a131f38fda"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.00000000e+02, 0.00000000e+00, 1.02065252e+01, 2.86532070e-03,\n",
              "        1.04335564e-01, 2.00000003e-01],\n",
              "       [2.00000000e+02, 0.00000000e+00, 1.29715113e+02, 1.29298207e-01,\n",
              "        7.94913799e-02, 2.00000003e-01],\n",
              "       [2.00000000e+02, 0.00000000e+00, 1.64602581e+02, 1.86666078e-01,\n",
              "        1.67823523e-01, 2.00000003e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "# TEST_ERIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxT9Eida-c_",
        "outputId": "ab96e0af-56ca-4ca0-d21c-920189fc77b9"
      },
      "source": [
        "################################# TEST ########################################\n",
        "%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "@cuda.jit\n",
        "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    tmp3 = math.sqrt(T/N_STEPS)\n",
        "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "        batch_id = i // N_PATHS\n",
        "        path_id = i % N_PATHS\n",
        "        tmp1 = mu[batch_id]*T/N_STEPS\n",
        "        tmp2 = math.exp(-r[batch_id]*T)\n",
        "        running_average = 0.0\n",
        "        s_curr = S0[batch_id]\n",
        "        for n in range(N_STEPS):\n",
        "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "            if i==0 and batch_id == 2:\n",
        "                print(s_curr)\n",
        "            if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "                break\n",
        "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "        d_s[i] = tmp2 * payoff\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        for op in range(self.N_BATCH):\n",
        "          #[K,B,S0,sigma,mu,r]\n",
        "          X = cupy.array([])\n",
        "          K_rand = cupy.random.rand(1)[0]\n",
        "          B_rand = cupy.random.rand(1)[0]\n",
        "          r_rand = cupy.random.rand(1)[0]\n",
        "          for i in range(0,self.N_STOCKS):\n",
        "            X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand])))\n",
        "          X = X.reshape(self.N_STOCKS,6)\n",
        "          X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "          num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "          randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "                                                        num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "          b1_r = randoms_gpu[:,0]\n",
        "          b2_r = randoms_gpu[:,1]\n",
        "          randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "          for i in range(interval):\n",
        "            if i % 2 == 0:\n",
        "                ind = int(i/2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            else:\n",
        "                ind = int(i//2)\n",
        "                randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "          randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "          Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# for i in ds:\n",
        "#     print(i[0])\n",
        "################################# TEST ########################################"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cupy_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59bb002d-1729-411a-9d10-c85522567636"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(18, hidden) # remember to change this!\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        # self.register_buffer('norm',\n",
        "        #                      torch.tensor([200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2,\n",
        "        #                                    200.0, 198.0, 200.0, 0.4, 0.2, 0.2])) # don't use numpy here - will give error later\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([200.0, 0.1, 200.0, 0.4, 0.2, 0.2]*3)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8be7f4f-e3f5-4b2f-da32-55d8df03282a"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/c3/f472843797b5ccbb2f0e806a6927f52c7c9522bfcea8e7e881d39258368b/pytorch_ignite-0.4.5-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 20.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 27.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 16.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 17.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 13.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "117f1147-e396-492b-e77c-0ee96bc6d607"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=100, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 1348.211181640625 average time 0.16685850134999783\n",
            "loss 1128.984375 average time 0.08487756330000593\n",
            "loss 940.9929809570312 average time 0.057521459633339114\n",
            "loss 125.07563781738281 average time 0.04385662178750636\n",
            "loss 72.34493255615234 average time 0.035652158290006356\n",
            "loss 76.38220977783203 average time 0.030184201966676483\n",
            "loss 27.641807556152344 average time 0.026275256678578832\n",
            "loss 31.383182525634766 average time 0.02334492896875844\n",
            "loss 23.642253875732422 average time 0.021068233811120586\n",
            "loss 24.765731811523438 average time 0.019250153810008896\n",
            "loss 27.127243041992188 average time 0.017756466418185565\n",
            "loss 27.962196350097656 average time 0.016511562258339534\n",
            "loss 30.81705665588379 average time 0.015457891973077354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Engine run is terminating due to exception: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-328a3a63f484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m           \u001b[0mb1_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandoms_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m           \u001b[0mb2_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandoms_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m           \u001b[0mrandoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m           \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cupy/_creation/basic.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemset_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020310b7-76bb-4b4c-de91-165ec82df560"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "model_save_name = 'checkpoint3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61222638-260a-416c-bb76-76853b441884"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d413cb-21c6-4280-d158-5dea52dd2c76"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'checkpoint3.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ae4cfa-a439-4f2b-8a91-2d6fd2b6911c"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=18, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244040ee-7117-4772-8fb3-333602311ec1"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "dataset = NumbaOptionDataSet(max_len=500, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs=10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 2.307035207748413 average time 0.027151946149888317 iter num 20\n",
            "loss 2.6403369903564453 average time 0.015118591099940204 iter num 40\n",
            "loss 2.7872674465179443 average time 0.011096830266615143 iter num 60\n",
            "loss 7.431154727935791 average time 0.009161438762509988 iter num 80\n",
            "loss 4.649834156036377 average time 0.00797119027001827 iter num 100\n",
            "loss 3.850369930267334 average time 0.0071777567750435384 iter num 120\n",
            "loss 2.1433186531066895 average time 0.006603424707197649 iter num 140\n",
            "loss 3.320591449737549 average time 0.006172017068792002 iter num 160\n",
            "loss 3.616884708404541 average time 0.005841449127819942 iter num 180\n",
            "loss 1.933947205543518 average time 0.005584042885034251 iter num 200\n",
            "loss 2.2033917903900146 average time 0.005363097540930539 iter num 220\n",
            "loss 1.6963679790496826 average time 0.005177977400027582 iter num 240\n",
            "loss 2.077622652053833 average time 0.0050293419307973425 iter num 260\n",
            "loss 1.8909672498703003 average time 0.004888556982171264 iter num 280\n",
            "loss 2.181884765625 average time 0.004779174176698386 iter num 300\n",
            "loss 1.0194847583770752 average time 0.004676933537535888 iter num 320\n",
            "loss 1.1411374807357788 average time 0.004582266335329918 iter num 340\n",
            "loss 1.4018410444259644 average time 0.004494417963916122 iter num 360\n",
            "loss 1.648126244544983 average time 0.004426749202653618 iter num 380\n",
            "loss 1.5575125217437744 average time 0.004361050172522027 iter num 400\n",
            "loss 2.0024008750915527 average time 0.004306118969060706 iter num 420\n",
            "loss 1.8190796375274658 average time 0.004253805420466961 iter num 440\n",
            "loss 1.234279751777649 average time 0.004205039897843839 iter num 460\n",
            "loss 1.167798638343811 average time 0.004156853606266016 iter num 480\n",
            "loss 1.6852173805236816 average time 0.004113771120011734 iter num 500\n",
            "loss 2.1371302604675293 average time 0.025903134749978563 iter num 20\n",
            "loss 1.8112157583236694 average time 0.014493384650040752 iter num 40\n",
            "loss 2.5807080268859863 average time 0.010702547450030882 iter num 60\n",
            "loss 1.7859344482421875 average time 0.008796650712531573 iter num 80\n",
            "loss 2.043531894683838 average time 0.007655292180015749 iter num 100\n",
            "loss 1.55185866355896 average time 0.0069182335749853035 iter num 120\n",
            "loss 2.6841251850128174 average time 0.006365599321426479 iter num 140\n",
            "loss 3.608773708343506 average time 0.005945777106251171 iter num 160\n",
            "loss 2.2651541233062744 average time 0.0056254815555601256 iter num 180\n",
            "loss 1.803060531616211 average time 0.005364038764996621 iter num 200\n",
            "loss 1.4587780237197876 average time 0.005160694536367704 iter num 220\n",
            "loss 1.2389129400253296 average time 0.0050023499666660125 iter num 240\n",
            "loss 1.5858032703399658 average time 0.004856749326927456 iter num 260\n",
            "loss 4.179372310638428 average time 0.004734335278583655 iter num 280\n",
            "loss 1.7480378150939941 average time 0.004625640570017519 iter num 300\n",
            "loss 2.646956205368042 average time 0.004525690340651067 iter num 320\n",
            "loss 0.7564836740493774 average time 0.004444278400021229 iter num 340\n",
            "loss 1.372281551361084 average time 0.004369258733350358 iter num 360\n",
            "loss 1.5176432132720947 average time 0.004302535776351808 iter num 380\n",
            "loss 1.0669209957122803 average time 0.004242312867545479 iter num 400\n",
            "loss 1.2257882356643677 average time 0.004181582976228865 iter num 420\n",
            "loss 1.01102876663208 average time 0.004136351763672379 iter num 440\n",
            "loss 1.5283907651901245 average time 0.004091281058730522 iter num 460\n",
            "loss 1.0873795747756958 average time 0.004049799022948264 iter num 480\n",
            "loss 0.7326977252960205 average time 0.004012047496038576 iter num 500\n",
            "loss 1.4181994199752808 average time 0.026296542699992643 iter num 20\n",
            "loss 2.0698091983795166 average time 0.014732857199987848 iter num 40\n",
            "loss 2.2505080699920654 average time 0.01085562308333768 iter num 60\n",
            "loss 2.5109012126922607 average time 0.008890037762489556 iter num 80\n",
            "loss 1.5656975507736206 average time 0.007730687449966353 iter num 100\n",
            "loss 6.334035873413086 average time 0.006961697108310242 iter num 120\n",
            "loss 3.5770082473754883 average time 0.006397989471426107 iter num 140\n",
            "loss 1.8211939334869385 average time 0.005973521124991521 iter num 160\n",
            "loss 3.478118896484375 average time 0.005650628038867783 iter num 180\n",
            "loss 1.5760912895202637 average time 0.005393455094972524 iter num 200\n",
            "loss 1.5452497005462646 average time 0.005181248386349001 iter num 220\n",
            "loss 2.1359927654266357 average time 0.005016689016652739 iter num 240\n",
            "loss 1.499772548675537 average time 0.004861492376903698 iter num 260\n",
            "loss 1.1195058822631836 average time 0.004733321149975771 iter num 280\n",
            "loss 0.569299578666687 average time 0.004618174339978699 iter num 300\n",
            "loss 1.8770784139633179 average time 0.004527426306233906 iter num 320\n",
            "loss 1.6956167221069336 average time 0.004446070076456261 iter num 340\n",
            "loss 1.46541166305542 average time 0.00436976273054673 iter num 360\n",
            "loss 0.9751781821250916 average time 0.0043000667447311 iter num 380\n",
            "loss 1.7828789949417114 average time 0.00423586945000352 iter num 400\n",
            "loss 1.477952480316162 average time 0.004177376940482905 iter num 420\n",
            "loss 1.219569206237793 average time 0.004123876261374707 iter num 440\n",
            "loss 1.7658509016036987 average time 0.004077706678273662 iter num 460\n",
            "loss 1.1685361862182617 average time 0.004040030039593982 iter num 480\n",
            "loss 1.6257950067520142 average time 0.0040059141120109415 iter num 500\n",
            "loss 1.837328314781189 average time 0.025458247349888552 iter num 20\n",
            "loss 1.3736425638198853 average time 0.014229700474948004 iter num 40\n",
            "loss 2.7017340660095215 average time 0.010709620183312533 iter num 60\n",
            "loss 1.9149214029312134 average time 0.008849021899970921 iter num 80\n",
            "loss 1.6687219142913818 average time 0.0076950555299845295 iter num 100\n",
            "loss 1.8196604251861572 average time 0.006924030491669934 iter num 120\n",
            "loss 1.7861361503601074 average time 0.006357057714285475 iter num 140\n",
            "loss 1.8136262893676758 average time 0.005937846106235156 iter num 160\n",
            "loss 3.5162808895111084 average time 0.005616911922217898 iter num 180\n",
            "loss 2.1797680854797363 average time 0.005361056384999756 iter num 200\n",
            "loss 2.364232063293457 average time 0.0051550232409144055 iter num 220\n",
            "loss 0.8131882548332214 average time 0.004977505204165785 iter num 240\n",
            "loss 1.6080514192581177 average time 0.004827929326914343 iter num 260\n",
            "loss 1.3768031597137451 average time 0.0047193523178423544 iter num 280\n",
            "loss 1.2628517150878906 average time 0.004616225516662477 iter num 300\n",
            "loss 0.6969723105430603 average time 0.0045196404593667696 iter num 320\n",
            "loss 0.929250955581665 average time 0.004449102697050394 iter num 340\n",
            "loss 1.6083292961120605 average time 0.00437202633887763 iter num 360\n",
            "loss 1.599475622177124 average time 0.00430052753946783 iter num 380\n",
            "loss 1.5807934999465942 average time 0.004247180449992811 iter num 400\n",
            "loss 1.0746040344238281 average time 0.004190035378561723 iter num 420\n",
            "loss 1.2844595909118652 average time 0.004145299727263757 iter num 440\n",
            "loss 1.655327558517456 average time 0.0040962007369434224 iter num 460\n",
            "loss 1.4011608362197876 average time 0.004051842052071682 iter num 480\n",
            "loss 1.01948881149292 average time 0.004015953531987179 iter num 500\n",
            "loss 0.862612783908844 average time 0.026110190550025437 iter num 20\n",
            "loss 2.617307186126709 average time 0.014635327625001082 iter num 40\n",
            "loss 2.7532365322113037 average time 0.01082965968339522 iter num 60\n",
            "loss 1.680885672569275 average time 0.00888072446255137 iter num 80\n",
            "loss 2.7565603256225586 average time 0.007722176850074902 iter num 100\n",
            "loss 1.090086817741394 average time 0.006962523650054208 iter num 120\n",
            "loss 1.126244068145752 average time 0.006404453200022025 iter num 140\n",
            "loss 1.6147184371948242 average time 0.005981582956258081 iter num 160\n",
            "loss 0.9085680246353149 average time 0.005679586333336475 iter num 180\n",
            "loss 2.0690524578094482 average time 0.005421408934998908 iter num 200\n",
            "loss 1.6563034057617188 average time 0.005207278077275326 iter num 220\n",
            "loss 0.736845076084137 average time 0.005030200487491736 iter num 240\n",
            "loss 1.4251993894577026 average time 0.004883286634608689 iter num 260\n",
            "loss 1.520214557647705 average time 0.00475301138928249 iter num 280\n",
            "loss 1.551085352897644 average time 0.004640405956673324 iter num 300\n",
            "loss 0.5005588531494141 average time 0.004545656665621322 iter num 320\n",
            "loss 0.9542324542999268 average time 0.004468216744105709 iter num 340\n",
            "loss 1.3483457565307617 average time 0.0043860512610940025 iter num 360\n",
            "loss 1.3018611669540405 average time 0.0043193584605121845 iter num 380\n",
            "loss 1.1284289360046387 average time 0.004261735164993752 iter num 400\n",
            "loss 1.021723747253418 average time 0.004203212442849863 iter num 420\n",
            "loss 2.0014114379882812 average time 0.004150226836354291 iter num 440\n",
            "loss 1.0183383226394653 average time 0.004109831613034302 iter num 460\n",
            "loss 0.9996205568313599 average time 0.004065522635405235 iter num 480\n",
            "loss 0.9621285200119019 average time 0.004027355643987903 iter num 500\n",
            "loss 1.1100159883499146 average time 0.02621120019994123 iter num 20\n",
            "loss 1.9399585723876953 average time 0.014636208475008062 iter num 40\n",
            "loss 1.9737439155578613 average time 0.010774160233328682 iter num 60\n",
            "loss 1.6399784088134766 average time 0.008839378000004672 iter num 80\n",
            "loss 1.9696446657180786 average time 0.007675983999970413 iter num 100\n",
            "loss 1.6798067092895508 average time 0.006908978074996715 iter num 120\n",
            "loss 1.5208085775375366 average time 0.00635617257141478 iter num 140\n",
            "loss 1.6255879402160645 average time 0.005953857481256364 iter num 160\n",
            "loss 2.0093135833740234 average time 0.005625861216670577 iter num 180\n",
            "loss 0.8452458381652832 average time 0.005369028769996476 iter num 200\n",
            "loss 1.25431227684021 average time 0.005161206959093595 iter num 220\n",
            "loss 1.2061052322387695 average time 0.0049863177208332365 iter num 240\n",
            "loss 0.7464189529418945 average time 0.004841341503851464 iter num 260\n",
            "loss 1.3127806186676025 average time 0.0047186409964394575 iter num 280\n",
            "loss 1.3168044090270996 average time 0.004609268443336987 iter num 300\n",
            "loss 1.0024492740631104 average time 0.004512464178131381 iter num 320\n",
            "loss 1.1442140340805054 average time 0.004429417985302815 iter num 340\n",
            "loss 0.6711956262588501 average time 0.004354677566672965 iter num 360\n",
            "loss 1.3767857551574707 average time 0.004284411557893308 iter num 380\n",
            "loss 0.9617512226104736 average time 0.004221409715000846 iter num 400\n",
            "loss 1.125082015991211 average time 0.0041676955333297796 iter num 420\n",
            "loss 1.4909684658050537 average time 0.004121295640908846 iter num 440\n",
            "loss 0.8680483102798462 average time 0.004076677471737086 iter num 460\n",
            "loss 0.8508278131484985 average time 0.0040336865687493175 iter num 480\n",
            "loss 0.3745741844177246 average time 0.003995103289998952 iter num 500\n",
            "loss 1.2882088422775269 average time 0.0257040549499834 iter num 20\n",
            "loss 7.560905456542969 average time 0.014372593750090345 iter num 40\n",
            "loss 2.847060203552246 average time 0.010608238150067943 iter num 60\n",
            "loss 2.261101484298706 average time 0.008722632112528573 iter num 80\n",
            "loss 2.9869508743286133 average time 0.007595256290023826 iter num 100\n",
            "loss 3.211002826690674 average time 0.006837727266702132 iter num 120\n",
            "loss 1.3406672477722168 average time 0.006295286935749443 iter num 140\n",
            "loss 1.742967128753662 average time 0.005896267193782024 iter num 160\n",
            "loss 0.977772057056427 average time 0.005586830038914236 iter num 180\n",
            "loss 0.9188159704208374 average time 0.005333347510022577 iter num 200\n",
            "loss 1.5217236280441284 average time 0.005130198577303831 iter num 220\n",
            "loss 0.6806530356407166 average time 0.0049696038000244394 iter num 240\n",
            "loss 1.1935163736343384 average time 0.0048245782038730586 iter num 260\n",
            "loss 0.9137603044509888 average time 0.004697422942876983 iter num 280\n",
            "loss 1.6048266887664795 average time 0.004588025030019102 iter num 300\n",
            "loss 1.9437307119369507 average time 0.004492541834389385 iter num 320\n",
            "loss 0.6975406408309937 average time 0.004407705205903767 iter num 340\n",
            "loss 0.7507147192955017 average time 0.0043359236194646655 iter num 360\n",
            "loss 0.5513215065002441 average time 0.004270990794753073 iter num 380\n",
            "loss 1.480351209640503 average time 0.004212380762517114 iter num 400\n",
            "loss 0.8489564657211304 average time 0.004158132626201093 iter num 420\n",
            "loss 1.0672938823699951 average time 0.004110849527284121 iter num 440\n",
            "loss 0.4715021252632141 average time 0.004064142728277701 iter num 460\n",
            "loss 0.8489009141921997 average time 0.00402334097293533 iter num 480\n",
            "loss 0.9303933382034302 average time 0.003993031680012791 iter num 500\n",
            "loss 0.6560541391372681 average time 0.026609420000067985 iter num 20\n",
            "loss 0.8694156408309937 average time 0.014843480725039626 iter num 40\n",
            "loss 1.8354133367538452 average time 0.010936779616667992 iter num 60\n",
            "loss 1.7065112590789795 average time 0.008980206362480203 iter num 80\n",
            "loss 3.32865571975708 average time 0.007798149910013308 iter num 100\n",
            "loss 2.280581474304199 average time 0.007028888525004125 iter num 120\n",
            "loss 4.007318496704102 average time 0.006510152835712039 iter num 140\n",
            "loss 1.7441763877868652 average time 0.006074908543757829 iter num 160\n",
            "loss 0.9880874156951904 average time 0.005739257572218372 iter num 180\n",
            "loss 1.6938302516937256 average time 0.00548106716500115 iter num 200\n",
            "loss 1.126276969909668 average time 0.005265271050004892 iter num 220\n",
            "loss 1.2791595458984375 average time 0.005090267887502857 iter num 240\n",
            "loss 1.038999080657959 average time 0.0049279815384544725 iter num 260\n",
            "loss 0.7392832040786743 average time 0.004793178249997254 iter num 280\n",
            "loss 0.893543541431427 average time 0.004679861263330167 iter num 300\n",
            "loss 1.6122738122940063 average time 0.004580568703127596 iter num 320\n",
            "loss 0.8471940159797668 average time 0.004510652576474219 iter num 340\n",
            "loss 0.9992482662200928 average time 0.004432755180558464 iter num 360\n",
            "loss 1.076934576034546 average time 0.004362996457903615 iter num 380\n",
            "loss 0.7622466087341309 average time 0.004302274840010795 iter num 400\n",
            "loss 0.701522707939148 average time 0.004244557559531174 iter num 420\n",
            "loss 0.41818833351135254 average time 0.004194650718190215 iter num 440\n",
            "loss 1.0150552988052368 average time 0.00414638664348944 iter num 460\n",
            "loss 0.5787259340286255 average time 0.004106599429176564 iter num 480\n",
            "loss 0.8173670768737793 average time 0.004065958826000497 iter num 500\n",
            "loss 1.3322398662567139 average time 0.025843748849911207 iter num 20\n",
            "loss 1.1251510381698608 average time 0.014496282599998267 iter num 40\n",
            "loss 0.8339441418647766 average time 0.010733976333343283 iter num 60\n",
            "loss 1.1748608350753784 average time 0.008830919100023493 iter num 80\n",
            "loss 1.7975295782089233 average time 0.007678682650011978 iter num 100\n",
            "loss 2.5011250972747803 average time 0.006912737666690798 iter num 120\n",
            "loss 0.7367781400680542 average time 0.006362434771433202 iter num 140\n",
            "loss 0.826180100440979 average time 0.005999960106242952 iter num 160\n",
            "loss 2.524484157562256 average time 0.005677243499970549 iter num 180\n",
            "loss 0.7881370186805725 average time 0.005435829819966784 iter num 200\n",
            "loss 1.4759199619293213 average time 0.005217206159047485 iter num 220\n",
            "loss 1.6692783832550049 average time 0.0050437053416279316 iter num 240\n",
            "loss 0.8737137913703918 average time 0.004894542773025443 iter num 260\n",
            "loss 1.175542950630188 average time 0.0047758694428141875 iter num 280\n",
            "loss 0.36232882738113403 average time 0.004664423593291455 iter num 300\n",
            "loss 0.8780394196510315 average time 0.004570524371834495 iter num 320\n",
            "loss 0.6584672927856445 average time 0.004482435658784358 iter num 340\n",
            "loss 0.4694709777832031 average time 0.004404849652744108 iter num 360\n",
            "loss 0.8439500331878662 average time 0.0043434076183865205 iter num 380\n",
            "loss 0.5771713852882385 average time 0.0042762482149692 iter num 400\n",
            "loss 0.6241031885147095 average time 0.004216055371402728 iter num 420\n",
            "loss 0.5974918603897095 average time 0.004167335847703477 iter num 440\n",
            "loss 0.48087868094444275 average time 0.004117478119542284 iter num 460\n",
            "loss 0.7667665481567383 average time 0.004074816258304281 iter num 480\n",
            "loss 0.9489327073097229 average time 0.0040326042899723686 iter num 500\n",
            "loss 0.9631999731063843 average time 0.02611226535000242 iter num 20\n",
            "loss 1.9865368604660034 average time 0.014574755274998097 iter num 40\n",
            "loss 1.7157862186431885 average time 0.010743796499991732 iter num 60\n",
            "loss 1.0097936391830444 average time 0.008813107562468758 iter num 80\n",
            "loss 1.2954221963882446 average time 0.0076510139999663804 iter num 100\n",
            "loss 1.5020606517791748 average time 0.006890095508303299 iter num 120\n",
            "loss 0.9271661639213562 average time 0.006375474099981408 iter num 140\n",
            "loss 1.579390525817871 average time 0.005969617762480084 iter num 160\n",
            "loss 0.9482195377349854 average time 0.005645451955544761 iter num 180\n",
            "loss 1.0428812503814697 average time 0.0053855381899802525 iter num 200\n",
            "loss 1.97894287109375 average time 0.005178364099980503 iter num 220\n",
            "loss 1.091660737991333 average time 0.005009907808308374 iter num 240\n",
            "loss 1.1983325481414795 average time 0.004873572715352846 iter num 260\n",
            "loss 0.9184820652008057 average time 0.00477139452854252 iter num 280\n",
            "loss 0.5760191082954407 average time 0.004666892899967934 iter num 300\n",
            "loss 0.5337473154067993 average time 0.0045683267562196764 iter num 320\n",
            "loss 0.5979987382888794 average time 0.00448587083232911 iter num 340\n",
            "loss 0.911307692527771 average time 0.004406411563867449 iter num 360\n",
            "loss 0.5489118099212646 average time 0.004334444339448103 iter num 380\n",
            "loss 0.8426211476325989 average time 0.0042817614649720784 iter num 400\n",
            "loss 0.5707144737243652 average time 0.004234891433307417 iter num 420\n",
            "loss 0.40792521834373474 average time 0.004182071977249341 iter num 440\n",
            "loss 0.4559828042984009 average time 0.004135815086939819 iter num 460\n",
            "loss 0.8701996207237244 average time 0.004092520160403031 iter num 480\n",
            "loss 1.0829070806503296 average time 0.004051685521993932 iter num 500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 5000\n",
              "\tepoch: 10\n",
              "\tepoch_length: 500\n",
              "\tmax_epochs: 10\n",
              "\toutput: 1.0829070806503296\n",
              "\tbatch: <class 'tuple'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'cupy_dataset.NumbaOptionDataSet'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "### Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94f5ceb-3418-42ec-cea6-371b744999b4"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[11.1003]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b6d784-8e98-4cfd-f0c8-b9f4bd870ea5"
      },
      "source": [
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -0.1436, -27.8777,   0.1877,   4.7987,  11.7131, -20.6870,  -0.1704,\n",
              "         -14.1578,   0.1883,   4.3352,  10.5132,   3.1065,  -0.1772,  35.9697,\n",
              "           0.2047,   4.5613,   9.9930,   5.4173]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "81e34ac5-78b5-48b5-cb62-97d51825606b"
      },
      "source": [
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(10, 300, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1015ec2850>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEGCAYAAABLgMOSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxc5X3v8c9Po837KoOxLW/IBLMaywsECCmbCSlOCikOWcwtiUMKWdveS5ZC6tw0W8ttm0sDTuNAkoJZ0lKTOtdhJwG8yMY22EZYlldhbNmSd20z87t/zJEZhGTJkkZnlu/79ZrXnPOc52h+j481Pz3nOec55u6IiIicqrywAxARkcykBCIiIt2iBCIiIt2iBCIiIt2iBCIiIt2SH3YAfWnkyJE+YcKEsMMQEckoa9as2e/uJW3LcyqBTJgwgYqKirDDEBHJKGa2o71yncISEZFuUQIREZFuUQIREZFuUQIREZFuCTWBmNkcM6s0syozu6ud7beb2etmts7M/mhmU4PyCWbWEJSvM7P7+z56EZHcFtpVWGYWAe4DrgZ2A6vNbKm7b0qq9rC73x/UvwG4F5gTbNvq7hf2ZcwiIvKuMHsgM4Eqd69292ZgCTA3uYK7H05aHQBo6mARkTQR5n0gY4BdSeu7gVltK5nZHcDXgULgT5I2TTSz14DDwLfd/Q/tfYiZLQAWAJSWlvZO5CLSJfG40xSN09gS69J7SyxOLO64O3GHeOt73N9ddg/WE8t5ZkTyIJKXR36ekZdn73mP5BkRM/IjRp61s61NnUheHhFLlBVEjOKCCP0KI/QvjFCcHyEvz8L+Z00baX8jobvfB9xnZrcA3wbmA3uAUnc/YGbTgSfN7Jw2PZbW/RcBiwDKy8vVg5FeFY87a3bW898b9vB85T4ammMURPLIj7z7BZXX+sV04kspj8L8PAqD94JIHkX5eQzuV8DQfgUM7V/AkP6FJ5aH9itkcL98Bhblkx/p2kmDeNxpjsUTr2ji1RIsN0UT5S3R924/sdzePrH3/4yG5hiN0TiNzTEaozEammM0tMRobInTFCSE5lg8xUeg7xXl59GvMEK/gsSrNcEkLw8ojDCoOJ9BxQXveR9cXMCQfgUM7pfP4H4FDCzMz+iEFGYCqQHGJa2PDco6sgT4KYC7NwFNwfIaM9sKTAF0m7n0meff3Mf3lm2mat9RCvPzuLyshJEDC2mJOdF4nGjMicWdWPAXczSeWG+OxTnaFD3xZdwSc5paYhxujHK0KXrSz+xXkPhiys8zHHAHJ/HXeEvSF3803rt/KxXm51EUyaMgSHzFBXkUt35hFkQYMaCQfsMSf6EXBWVFBYnEWFwQOel7UX6E4oLEe0F+ItGatSZfMEu8tyZjMxK9jmDZHaJBDyUad2KxxL95NB4nHodoPNGrOfFyf8+xec+24JV8rBpbYjS2vJsgG1piNDbHOH4iYSbejzdHOXCsmcaWGEebohxpbKGx5eQJ1AwGFSWSyeDiILEUF3Sw3qZeGiSgMBPIaqDMzCaSSBzzgFuSK5hZmbtvCVavB7YE5SVAnbvHzGwSUAZU91nkktOisTh/v+xNFr+8jYkjB3Dvn1/ANeeczsCinv86NUfjHGpo4eDxZg42tHDweAv1x5s50hjlaGPiS+lIY5SYO0biC8gw8vJI9GxaezdJPZvW3k5B0rbkHlDbntCJ+if2S3yhpyszKEzTv+JbYnGOBMftcEPwHiwn3ls43BgN3hPlO+uOnyjv7A+KPOM9vZvB/RK9ncGtZa2Jp7iAj00bQ2F+7w57h5ZA3D1qZncCy4EIsNjdN5rZQqDC3ZcCd5rZVUALUE/i9BXA5cBCM2sB4sDt7l7X962QXHPgaBN3PLyWFdV13HrJBL75kbN79ZeyMD+PkkFFlAwq6rWfKeEpiOQxfEAhwwcUdmv/aNBbfW/CeX8COtTQEiSnKLvqjnOkMbH9SOO7CWjutDN6q1knWC49E728vNw1maJ01+Y9h/ncQxXUHm3i+x8/jxunjw07JJGTisedo82JHs7YYf27/XPMbI27l7ctT/tBdJF0ULG9jvmLVzGouIAnbr+Y88cODTskkU7l5Vni1FZxQUp+vhKISCf2Hm7k9l+vpWRQEUsWXMzpQ4rDDkkkLSiBiJxEczTOF3+9huPNUR7+/CwlD5EkSiAiJ/Hd325i7c6D3HfLRUw5bVDY4YikFc3GK9KBxyt28asVO/jC5ZO4/vzRYYcjknaUQETa8UbNIb715BtcMnkEf3PtWWGHI5KWlEBE2jjeHOXOh9cyckAhP/nktC5PHyKSazQGItLGj/5fJdsPHOfhz89ixEDd0CfSEf1pJZLk1a0HePCV7dx6yQQumTwy7HBE0poSiEigORrnm//5OuNH9Od/ztG4h0hndApLJPDwyh1s23+MX9w6g/6F+tUQ6Yx6ICIkBs7/+dktfPDMEVxxVknY4YhkBCUQEeDJ196m/ngLX71qSlpPXS6STpRARIBHK3bxgdMHUT5+WNihiGQMJRDJebVHmli/6yAfPX+0eh8ip0AJRHLeS2/VAnDFWaNCjkQksyiBSM57vnIfJYOKmDp6cNihiGSUUBOImc0xs0ozqzKzu9rZfruZvW5m68zsj2Y2NWnbN4L9Ks3s2r6NXLJFNBbnpbdquWJKCXlp+lxtkXQVWgIxswhwH3AdMBX4ZHKCCDzs7ue5+4XAj4B7g32nAvOAc4A5wL8GP0/klLy26yCHG6N8+AM6fSVyqsLsgcwEqty92t2bgSXA3OQK7n44aXUA0PoA97nAEndvcvdtQFXw80ROyQuV+4jkGZeWadoSkVMV5u22Y4BdSeu7gVltK5nZHcDXgULgT5L2XdFm3zHtfYiZLQAWAJSWlvY4aMkuz79ZS/n4YSl7ZrRINkv7QXR3v8/dJwP/C/h2N/Zf5O7l7l5eUqI7jOVd7xxqZNOewzp9JdJNYSaQGmBc0vrYoKwjS4CPdXNfkfd58a19AJq6RKSbwkwgq4EyM5toZoUkBsWXJlcws7Kk1euBLcHyUmCemRWZ2USgDFjVBzFLFnmhspbRQ4o5S886F+mW0MZA3D1qZncCy4EIsNjdN5rZQqDC3ZcCd5rZVUALUA/MD/bdaGaPAZuAKHCHu8dCaYhkpJZYnD9s2c+fXnCG7j4X6aZQ56x292XAsjZldyctf+Uk+34P+F7qopNstmH3IY42RfnQFF19JdJdaT+ILpIKFdvrACifMDzkSEQylxKI5KSKHfVMHDmAkXrmuUi3KYFIznF31uyoZ7qmbhfpESUQyTnV+49Rd6yZGROUQER6QglEck7r+Mf08Rr/EOkJJRDJORXb6xnWv4DJJQPCDkUkoymBSM6p2FHP9PHDdf+HSA8pgUhO2X+0iW37j1Gu8Q+RHlMCkZyyZkc9AOW6Akukx5RAJKdUbK+jMD+P88YOCTsUkYynBCI5pWJHPeePGUJRvh5gKdJTSiCSMxpbYrxRc4jpGv8Q6RVKIJIz1u86SEvMmaH7P0R6hRKI5IyKYABdU5iI9A4lEMkZFdvrmFwygGEDCsMORSQrKIFITojHExMoluv0lUivUQKRnFBVe5TDjVHdQCjSi0JNIGY2x8wqzazKzO5qZ/vXzWyTmW0ws2fNbHzStpiZrQteS9vuK5JstR4gJdLrQnukrZlFgPuAq4HdwGozW+rum5KqvQaUu/txM/si8CPg5mBbg7tf2KdBS8Zas72eEQMKmTCif9ihiGSNMHsgM4Eqd69292ZgCTA3uYK7P+/ux4PVFcDYPo5RskRF8AApTaAo0nvCTCBjgF1J67uDso7cBvwuab3YzCrMbIWZfayjncxsQVCvora2tmcRS0bad6SRnXXHNf4h0stCO4V1Kszs00A58KGk4vHuXmNmk4DnzOx1d9/adl93XwQsAigvL/c+CVjSyprtwQSKGv8Q6VVh9kBqgHFJ62ODsvcws6uAbwE3uHtTa7m71wTv1cALwLRUBiuZq2JHPUX5eZx7hiZQFOlNYSaQ1UCZmU00s0JgHvCeq6nMbBrwAInksS+pfJiZFQXLI4EPAsmD7yInVGyv44KxQynM11XrIr0ptN8od48CdwLLgc3AY+6+0cwWmtkNQbUfAwOBx9tcrns2UGFm64HngR+0uXpLBICG5hgb3z6sCRRFUiDUMRB3XwYsa1N2d9LyVR3s9wpwXmqjk2ywbtdBonHXA6REUkB9eslqa3YkbiDUBIoivU8JRLJaxY56ykYNZGh/TaAo0tuUQCRrxePO2h31uv9DJEWUQCRrbdmXmEBxumbgFUkJJRDJWqtaJ1DU+IdISiiBSNZaWX2A0wYXMV4TKIqkhBKIZCV3Z+W2OmZNHKEJFEVSRAlEstK2/ceoPdLE7Ekjwg5FJGspgUhWWrktMf4xa5IG0EVSRQlEstKK6gOMHFjEpJEDwg5FJGspgUjWcXdWVtcxe9JwjX+IpJASiGSdnXXHeedwI7M0/iGSUkogknVWVB8AYPZEjX+IpJISiGSdldV1jBhQyJmjBoYdikhWUwKRrHLi/g+Nf4iknBKIZJXd9Q3UHGxg1kSNf4ikWqgJxMzmmFmlmVWZ2V3tbP+6mW0ysw1m9qyZjU/aNt/MtgSv+X0buaSrV1vHPzSALpJyoSUQM4sA9wHXAVOBT5rZ1DbVXgPK3f184AngR8G+w4F7gFnATOAeM9OMecLLVfsZObCIKadp/EMk1cLsgcwEqty92t2bgSXA3OQK7v68ux8PVlcAY4Pla4Gn3b3O3euBp4E5fRS3pKl43Hm5aj+Xnqn5r0T6QpgJZAywK2l9d1DWkduA33VzX8kBlXuPsP9oMx88c2TYoYjkhPywA+gKM/s0UA58qBv7LgAWAJSWlvZyZJJOXq7aD8ClZUogIn0hzB5IDTAuaX1sUPYeZnYV8C3gBndvOpV9Adx9kbuXu3t5SUlJrwQu6ekPW/YzuWQAo4f0CzsUkZwQZgJZDZSZ2UQzKwTmAUuTK5jZNOABEsljX9Km5cA1ZjYsGDy/JiiTHNUUjbFqWx2XlemPBJG+EtopLHePmtmdJL74I8Bid99oZguBCndfCvwYGAg8HgyK7nT3G9y9zsy+SyIJASx097oQmiFp4rWdB2loiWn8Q6QPhToG4u7LgGVtyu5OWr7qJPsuBhanLjrJJC9X7SeSZ3r+h0gf0p3okhX+sGU/F44byuDigrBDEckZSiCS8Q4db2HD7oM6fSXSx5RAJOO9tKWWuMMVZ2kAXaQvKYFIxnu+ch/D+hdwwdihYYciklOUQCSjxePOi5W1fGhKCZE8TV8i0peUQCSjbag5xIFjzXz4A6PCDkUk5yiBSEZ7/s195BlcrhsIRfpcl+4DMbMy4Pskpl0vbi1390kpikukS56v3Me00mEMG1AYdigiOaerPZBfAD8FosCHgV8Cv05VUCJdUXukiQ27D/FhXX0lEoquJpB+7v4sYO6+w92/A1yfurBEOvdCZWJ6tCvO0viHSBi6OpVJk5nlAVuC+atqSMxRJRKaFyprGTWoiHPOGBx2KCI5qas9kK8A/YEvA9OBTwOfTVVQIp1pjsZ56a1arjirRE8fFAlJVxPIBHc/6u673f1/uPuNgJ7OJKF5Zet+jjRFuWbq6WGHIpKzuppAvtHFMpE+8ftNe+lfGNHTB0VCdNIxEDO7DvgIMMbM/iVp02ASV2SJ9Ll43Hl6016uOKuE4oJI2OGI5KzOBtHfBtYANwTvrY4AX0tVUCIn89quemqPNHHtOTp9JRKmkyYQd18PrDezX7u7ehySFpZv3EtBxDR9iUjITjoGYmavm9kGYK2ZbWj76umHm9kcM6s0syozu6ud7Zeb2Vozi5rZTW22xcxsXfBa2nZfyU7uzvKN73Dx5JF6eJRIyDo7hfXRVH2wmUWA+4Crgd3AajNb6u6bkqrtBG4F/rqdH9Hg7hemKj5JT2/tPcqOA8f5/GWaRUckbJ2dwtrRumxm44Eyd3/GzPp1tm8XzASq3L06+PlLgLnAiQTi7tuDbfEefpZkieUb38EMrpl6WtihiOS8Ll3Ga2afB54AHgiKxgJP9vCzxwC7ktZ3B2VdVWxmFWa2wsw+1lElM1sQ1Kuora3tbqySJn73xjtMGzeUUYOLO68sIinV1ftA7gA+CBwGcPctQNgjmOPdvRy4BfgnM5vcXiV3X+Tu5e5eXlKiSfcyWdW+I2zec5g/veCMsEMREbqeQJrcvbl1xczyAe/hZ9cA45LWxwZlXeLuNcF7NfACMK2H8Uiae2r9Hszg+vNGhx2KiND1BPKimX0T6GdmVwOPA0/18LNXA2VmNtHMCoF5QJeupjKzYWZWFCyPJNE72nTyvSSTuTtPbXib2RNH6PSVSJroagK5C6gFXge+ACwDvt2TDw7uK7kTWA5sBh5z941mttDMbgAwsxlmthv4BPCAmW0Mdj8bqDCz9cDzwA/aXL0lWWbzniNU1x7T6SuRNNKlK6ncPW5mTwJPunuvjUS7+zISySi57O6k5dUkTm213e8V4LzeikPS31Mb3iaSZ8w5V3efi6SLzm4kNDP7jpntByqBSjOrNbO7T7afSG9yd55a/zaXnjmS4Xp0rUja6OwU1tdIjC/McPfh7j4cmAV80Mw0F5b0iXW7DrK7vkGnr0TSTGcJ5DPAJ919W2tBcNWTHiglfWbp+rcpjORxzTm6eVAknXSWQArcfX/bwmAcRBMRScq1xOIsXfc2V00dpbmvRNJMZwmkuZvbRHrFC5W1HDjWzI0Xve9aChEJWWdXYV1gZofbKTdAF+NLyv1mzW5GDizk8imaRUAk3XQ2maIe9yahqT/WzLNv7uWzF0+gINLVW5ZEpK/ot1LS1lMb3qYl5jp9JZKmlEAkbT2xZjdnjx7M1DMGhx2KiLRDCUTS0pa9R9iw+xA3XnQqM/yLSF9SApG09MiqXRREjI9NUwIRSVdKIJJ2GltiPLFmF9eeczojBxaFHY6IdEAJRNLOf2/Yw+HGKJ+aNT7sUETkJJRAJO38+8odTCoZwOxJw8MORUROQglE0srmPYdZu/Mgt8wsxczCDkdETkIJRNLKwyt3Upifx03Tde+HSLpTApG0cbQpypOv1XD9eaMZ2l/P/RBJd6EmEDObY2aVZlZlZne1s/1yM1trZlEzu6nNtvlmtiV4ze+7qCVVHq/YxZGmKJ+9WIPnIpkgtARiZhHgPuA6YCrwSTOb2qbaTuBW4OE2+w4H7iHxcKuZwD1mNizVMUvqxOLOg69s56LSoUwr1aEUyQRh9kBmAlXuXu3uzcASYG5yBXff7u4bgHibfa8Fnnb3OnevB54G5vRF0JIaz27ey44Dx7nt0klhhyIiXRRmAhkD7Epa3x2U9eq+ZrbAzCrMrKK2trZbgUrqLX55G2OG9uNaPXVQJGNk/SC6uy9y93J3Ly8p0TMl0tHGtw+xorqO+ZeMJ1/TtotkjDB/W2uAcUnrY4OyVO8raebnf9xG/8IIN88oDTsUETkFYSaQ1UCZmU00s0JgHrC0i/suB64xs2HB4Pk1QZlkmJqDDSxd9zZ/Xj6OIf30zHORTBJaAnH3KHAniS/+zcBj7r7RzBaa2Q0AZjbDzHYDnwAeMLONwb51wHdJJKHVwMKgTDLMz16qBuDzl2vwXCTTdPZM9JRy92XAsjZldyctryZxeqq9fRcDi1MaoKRU7ZEmHlm1kz+7aAxjhvYLOxwROUUasZTQLH55Gy2xOLd/aHLYoYhINyiBSCgOHW/hV6/u4CPnjWZSycCwwxGRblACkVD8/OVtHG2K8pdXnBl2KCLSTUog0ufqjjXz8z9Uc925pzP1jMFhhyMi3aQEIn3u/he30tAS4+tXTwk7FBHpASUQ6VN7Dzfy0Cvb+di0MZSdNijscESkB5RApE/95LktxOLOV69U70Mk0ymBSJ+prj3KklW7uHnGOEpH9A87HBHpISUQ6TP/+783U1wQ4atXqfchkg2UQKRPvFC5j+fe3MeXrzyTkkFFYYcjIr1ACURSriUW57u/3cSEEf259ZKJYYcjIr1ECURS7lev7mBr7TG+ff1UCvP1X04kW+i3WVJq35FG/umZt7isbCRXnj0q7HBEpBcpgUhKLXxqE43ROH93wzmYWdjhiEgvUgKRlHnuzb38dsMevvThMzVhokgWUgKRlDjWFOVvn9xI2aiBfEHTtYtkpVATiJnNMbNKM6sys7va2V5kZo8G21ea2YSgfIKZNZjZuuB1f1/HLid379NvUXOwgR/ceJ4GzkWyVGhPJDSzCHAfcDWwG1htZkvdfVNStduAenc/08zmAT8Ebg62bXX3C/s0aOmSVdvqWPzyNj49u5Tp44eHHY6IpEiYfxrOBKrcvdrdm4ElwNw2deYCDwXLTwBXmkZi09rRpih/9fg6Sof35xvXnR12OCKSQmEmkDHArqT13UFZu3XcPQocAkYE2yaa2Wtm9qKZXZbqYKVrvvvUJmrqG/jHT1zAgKLQOrgi0gcy9Td8D1Dq7gfMbDrwpJmd4+6H21Y0swXAAoDS0tI+DjO3PLNpL49W7OKLV0ymfIJOXYlkuzB7IDXAuKT1sUFZu3XMLB8YAhxw9yZ3PwDg7muArUC7M/S5+yJ3L3f38pKSkl5ugrTac6iBv3liPWePHszXNFmiSE4IM4GsBsrMbKKZFQLzgKVt6iwF5gfLNwHPububWUkwCI+ZTQLKgOo+ilvaaInF+dLDr9EcjfN/b5mmq65EckRop7DcPWpmdwLLgQiw2N03mtlCoMLdlwI/B35lZlVAHYkkA3A5sNDMWoA4cLu71/V9KwTgH35fScWOev553oVM1g2DIjkj1DEQd18GLGtTdnfSciPwiXb2+w3wm5QHKJ16dvNeHnixmk/NKmXuhW2vgRCRbKZzDdJt2/Yf42uPrmPq6MH87Uenhh2OiPQxJRDplkMNLdz20GryI3k88JnpFBdEwg5JRPqYEoicsljc+fIjr7HzwHH+9VMXMW64nm8ukosy9T4QCdH3l23mxbdq+fuPn8fsSSM630FEspJ6IHJKfvnqdv7tj9uYf/F4bpmlGzNFcpkSiHTZ717fwz1LN3LV2aM0aC4iSiDSNSurD/CVR9cxbdxQfvLJi8iP6L+OSK7Tt4B0auPbh/jcLysoHd6fxbfOoF+hrrgSESUQ6cSb7xzm0/+2kkFF+Tz0FzMZ2r8w7JBEJE0ogUiH3tp7hE/9bCVF+REeWTCbMUP7hR2SiKQRJRBpV9W+I9zysxVE8oxHFsxm/IgBYYckImlGCUTe542aQ9z8wAogkTwmjlTyEJH3UwKR91hRfYB5i1ZQXBDh0S/M1uy6ItIh3YkuJzy9aS93PLyW0uH9+dVtMxk9RGMeItIxJRAB4LGKXXzjP17n3DFDePDWGQwboKutROTklEByXDzu/MPvK/nXF7ZyWdlI7v/0dAYU6b+FiHRO3xQ5rKE5xl89vo5lr7/DJ2eWsnDuORToDnMR6aJQvy3MbI6ZVZpZlZnd1c72IjN7NNi+0swmJG37RlBeaWbX9mXc2WDPoQbmLXqV373xDt++/mz+/uPnKnmIyCkJrQdiZhHgPuBqYDew2syWuvumpGq3AfXufqaZzQN+CNxsZlNJPB/9HOAM4Bkzm+Lusb5tRWZ66a1avvroOppaYiz6TDlXTz0t7JBEJAOF+SfnTKDK3avdvRlYAsxtU2cu8FCw/ARwpZlZUL7E3ZvcfRtQFfw8OYlY3Ln395XM/8UqSgYWsfRLlyp5iEi3hTkGMgbYlbS+G5jVUR13j5rZIWBEUL6izb5j2vsQM1sALAAoLc3d51ds33+Mv3p8PWt21HPT9LF8d+65mhRRRHok6wfR3X0RsAigvLzcQw6nz8Xjzq9X7uD7y96kIGL8n5sv4OPTxoYdlohkgTATSA0wLml9bFDWXp3dZpYPDAEOdHHfnLd5z2Hu+a+NrNpex4emlPDDG8/n9CHFYYclIlkizASyGigzs4kkvvznAbe0qbMUmA+8CtwEPOfubmZLgYfN7F4Sg+hlwKo+izzNHWpo4Z+eeYtfvrqDwcX5/OjG8/lE+VgSw0ciIr0jtAQSjGncCSwHIsBid99oZguBCndfCvwc+JWZVQF1JJIMQb3HgE1AFLhDV2DBsaYoD76ynQde3MqRpii3zCzlb649S8/wEJGUMPfcGRYoLy/3ioqKsMPodUebojyycic/fXErdceauersUXzt6imcc8aQsEMTkSxgZmvcvbxtedYPomez3fXHeeiV7SxZtYsjTVEuKxvJ16+ewrTSYWGHJiI5QAkkw7g7a3bU8+Ar2/ndG+8A8JHzRnPbpRO5cNzQkKMTkVyiBJIhDhxt4j/W1rBk9U621h5jUFE+t106kfmXTNCjZkUkFEogaawpGuPFylqeXFfD05v20hJzLiodyg9vPI+Pnn+GZs0VkVDpGyjNNEfjvFy1n6c2vM3TG/dypCnK8AGFzL94AjfPGEfZaYPCDlFEBFACSQvHm6P8Yct+nt28l+Ub93KooYVBxflce+7pfPT80XzwzJGaKVdE0o4SSEhqDjbw3Oa9PLN5H69WH6A5GmdQcT5XfmAUHz3/DC6bMpKifM1VJSLpSwmkjxxpbGFldR1/rNrPK1v389beowBMGNGfz8wez5Vnj2LGhOHqaYhIxlACSZGmaIy1Ow7yytb9/LFqPxt2HyIWd4oL8pgxYTg3TR/LlWefxuSSgWGHKiLSLUogvaShOcZrO+tZsa2OVdsO8NrOgzRF40TyjPPHDuEvr5jMJZNHctH4oTo1JSJZQQmkm443R6nYXs+K6gOsqD7A6zWHaIk5eQbnnDGET88ez+xJI5g9aTiDigvCDldEpNcpgXRRY0uMtTvqebX6AK9uPcD63QdpiTn5ecZ5Y4dw26WTmDVpONPHD2OwEoaI5AAlkC647cHV/GHLfppjiVNS545JJIyLJ4+gfPww3dAnIjlJ33xdMH7EACaPGsjFk0ZQPmGYTkmJiKAE0iV3/+nUsEMQEUk7uulARES6RQlERES6JZQEYmbDzexpM9sSvLf7BCQzmx/U2WJm85PKXzCzSjNbF7xG9V30IiIC4fVA7gKedfcy4Nlg/T3MbDhwDzALmAnc00YA6H0AAAdSSURBVCbRfMrdLwxe+/oiaBEReVdYCWQu8FCw/BDwsXbqXAs87e517l4PPA3M6aP4RESkE2ElkNPcfU+w/A5wWjt1xgC7ktZ3B2WtfhGcvvpbM7OOPsjMFphZhZlV1NbW9jhwERFJSNllvGb2DHB6O5u+lbzi7m5mfoo//lPuXmNmg4DfAJ8BftleRXdfBCwCKC8vP9XPERGRDqQsgbj7VR1tM7O9Zjba3feY2WigvTGMGuCKpPWxwAvBz64J3o+Y2cMkxkjaTSAiIpIa5t73f5Sb2Y+BA+7+AzO7Cxju7v+zTZ3hwBrgoqBoLTAdOAwMdff9ZlYAPAI84+73d+Fza4EdSUUjgf09blB6ybY2ZVt7IPvalG3tgexrU0/bM97dS9oWhpVARgCPAaUkvtD/3N3rzKwcuN3dPxfU+wvgm8Fu33P3X5jZAOAloACIAM8AX3f3WDfiqHD38p63KH1kW5uyrT2QfW3KtvZA9rUpVe0JZSoTdz8AXNlOeQXwuaT1xcDiNnWOkeiJiIhIiHQnuoiIdEuuJ5BFYQeQAtnWpmxrD2Rfm7KtPZB9bUpJe0IZAxERkcyX6z0QERHpJiUQERHplpxNIGY2J5jRtyq4FyXjmNl2M3s9mNKlIijr0kzH6cLMFpvZPjN7I6ms3TZYwr8Ex2yDmV3U8U8OTwdt+o6Z1STNIP2RpG3fCNpUaWbXhhN1x8xsnJk9b2abzGyjmX0lKM/I43SS9mTyMSo2s1Vmtj5o098F5RPNbGUQ+6NmVhiUFwXrVcH2Cd36YHfPuReJ+0e2ApOAQmA9MDXsuLrRju3AyDZlPwLuCpbvAn4YdpydtOFyEjeLvtFZG4CPAL8DDJgNrAw7/lNo03eAv26n7tTg/18RMDH4fxkJuw1tYhwNXBQsDwLeCuLOyON0kvZk8jEyYGCwXACsDP7tHwPmBeX3A18Mlv8SuD9Yngc82p3PzdUeyEygyt2r3b0ZWEJihuBs0JWZjtOGu78E1LUp7qgNc4FfesIKYGgwFU5a6aBNHZkLLHH3JnffBlSR+P+ZNtx9j7uvDZaPAJtJTGyakcfpJO3pSCYcI3f3o8FqQfBy4E+AJ4Lytseo9dg9AVx5sklpO5KrCaSzmX4zhQO/N7M1ZrYgKOvKTMfprqM2ZPpxuzM4pbM46dRiRrUpONUxjcRfuBl/nNq0BzL4GJlZxMzWkZhb8GkSPaWD7h4NqiTHfaJNwfZDwIhT/cxcTSDZ4lJ3vwi4DrjDzC5P3uiJ/mlGX6edDW0I/BSYDFwI7AH+MdxwTp2ZDSQx+/VX3f1w8rZMPE7ttCejj5G7x9z9QhITz84EPpDqz8zVBFIDjEtaHxuUZRR/d1bifcB/kvhPs7f1dMFJZjpOdx21IWOPm7vvDX7B48DPePcUSEa0KZi49DfAv7v7fwTFGXuc2mtPph+jVu5+EHgeuJjE6cPWKauS4z7RpmD7EODAqX5WriaQ1UBZcIVCIYlBpKUhx3RKzGyAJZ6HgiUmmLwGeINEO1qfHz8f+K9wIuyRjtqwFPhscJXPbOBQ0imUtNZmDODjJI4VJNo0L7gqZiJQBqzq6/hOJjg3/nNgs7vfm7QpI49TR+3J8GNUYmZDg+V+wNUkxnaeB24KqrU9Rq3H7ibguaAXeWrCvnogrBeJK0XeInGe8Fthx9ON+CeRuDJkPbCxtQ0kzmM+C2whMVPx8LBj7aQdj5A4XdBC4hztbR21gcSVJvcFx+x1oDzs+E+hTb8KYt4Q/PKOTqr/raBNlcB1YcffTnsuJXF6agOwLnh9JFOP00nak8nH6HzgtSD2N4C7g/JJJJJdFfA4UBSUFwfrVcH2Sd35XE1lIiIi3ZKrp7BERKSHlEBERKRblEBERKRblEBERKRblEBERKRblEBEQmBmC83sqrDjEOkJXcYr0sfMLOLusbDjEOkp9UBEepGZTTCzN83s381ss5k9YWb9LfHslh+a2VrgE2b2oJndFOwzw8xeCZ7lsMrMBgUT4/3YzFYHk/t9Iag72sxeCp5X8YaZXRZqgyWn5XdeRURO0VnAbe7+spktJvHsBYADnpj8EjObE7wXAo8CN7v7ajMbDDSQuHv9kLvPMLMi4GUz+z3wZ8Byd/+emUWA/n3bNJF3KYGI9L5d7v5ysPxr4MvB8qPt1D0L2OPuqwE8mOXWzK4Bzm/tpZCY7K6MxDxui4PJAJ9093UpaoNIp5RARHpf24HF1vVjp/AzDPiSuy9/34bEtP3XAw+a2b3u/svuhSnSMxoDEel9pWZ2cbB8C/DHk9StBEab2QyAYPwjH1gOfDHoaWBmU4IZmMcDe939Z8C/kXh0rkgolEBEel8liQd8bQaGkXhQUbs88Ujlm4GfmNl6Ek+SKyaRHDYBa83sDeABEmcMrgDWm9lrwX7/nMJ2iJyULuMV6UXBI1J/6+7nhhyKSMqpByIiIt2iHoiIiHSLeiAiItItSiAiItItSiAiItItSiAiItItSiAiItIt/x9rgIpBTHw+BwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2edfb76-29f9-4d9e-c1ed-ec09e5586c3b"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*3]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-2.3553e-03, -4.0876e-01,  7.3882e-03,  1.7640e-02,  4.4605e-01,\n",
              "          -5.5726e-01, -2.4692e-03, -6.0736e-01,  2.2762e-04,  4.3673e-04,\n",
              "           4.1671e-02,  2.1990e-01, -2.4984e-03,  9.2389e-01,  2.0347e-04,\n",
              "          -2.1005e-02,  2.0598e-02,  6.3302e-02]], device='cuda:0'),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "8e18c9e2-f6b3-49d1-b078-c26e8c15b795"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    inputs = torch.tensor([[110.0, 0.0, S, 0.35, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    loss_grads = grad(x, inputs, create_graph=True)\n",
        "    drv = grad(loss_grads[0][0][2], inputs)\n",
        "    return drv[0][0][2]\n",
        "\n",
        "prices = np.arange(10, 250, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_gamma(p).item())\n",
        "fig2 = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig2"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1019492e10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXyV5Zn/8c+VnSVhCSHs+46oKIJbrbuobXHrlNpa7ejYVu06nY5OO23H+dnWzkynttPVSqtWq9at2LqLS1EEwiY7hJ2wJAGSEMh+rt8f5ySEmOUEcvIk53zfrxcvnvNs57o5ybm4l+e+zd0RERGJVlLQAYiISPeixCEiIu2ixCEiIu2ixCEiIu2ixCEiIu2SEnQAnWHAgAE+atSooMMQEek2li1bVuzuOc0dS4jEMWrUKPLy8oIOQ0Sk2zCzHS0dU1OViIi0ixKHiIi0ixKHiIi0ixKHiIi0ixKHiIi0ixKHiIi0ixKHiIi0ixKHSAd6e1MRy3ceCjoMkZhKiAcARTrD/rJKbp63BIDtP7o64GhEYkc1DpEOsGLnIT77u8VBhyHSKVTjEDlJm/cf5tpfvtfwekifjACjEYk91ThETtKv3trSsH3asD4BRiLSOZQ4RE5CcXkVz64oAGDLD65iQm4mHnBMIrGmxCFyEh58ZysA/3LFRJKTDDNwZQ6Jc0ocIidhxa4SAL700bEAGBZkOCKdQolD5ARVVNexcmcJ//SR0SQlHUsYrsYqiXNKHCIn6PR7X6W6LsQ5Y7Mb9qmpShKBEofICfj121uoqg0BMGNU/4b9Zqi+IXFPiUOkndydH720AYDvf3wKWRmpjY6aahwS95Q4RNrpyaW7Grbnzhxx3DFT37gkACUOkXZasKEQgIX/ehEZqcnNnKEqh8Q3JQ6Rdlq7p4zLpuQyrF/PDx0z1Dku8U+JQ6Qd3tlUREFJBacP79vscXWOSyJQ4hBph5fX7gPg4kkDmz2uBwAlEShxiLTDzgNHmTI4i8mDs1o8x9VWJXFOiUMkStW1IZbvPMSMUf1aPEdNVZIIlDhEorS6oJSj1XWcMya7xXPUOS6JQIlDJEpLtx8Ejn9SvCkzU1OVxD0lDpEoVNbU8aOXNjCsXw9yMtODDkckUEocIlH405KdAMxspbZRT/UNiXdKHCJRWLenDID7bzi11fPMUOaQuKfEIRKFZTsPcenkgaQmt/4rY5jyhsS9mCYOM5ttZhvNLN/M7m7meLqZPRk5vtjMRjU6dk9k/0Yzu6LR/q+b2VozW2NmfzKzjFiWQeTgkWq2Fh3hjJEtD8OtF16PQ6lD4lvMEoeZJQO/AK4EpgCfNrMpTU67FTjk7uOA/wXuj1w7BZgLTAVmA780s2QzGwp8BZjh7qcAyZHzRGLmvS3FAJwVRf+GnhuXRBDLGsdMIN/dt7p7NfAEMKfJOXOAhyPbTwOXmJlF9j/h7lXuvg3Ij9wPIAXoYWYpQE9gTwzLIMKbG4ro1zOVM0e0XeMAdXFI/Itl4hgK7Gr0endkX7PnuHstUApkt3StuxcA/w3sBPYCpe7+anNvbma3m1memeUVFRV1QHEkEbk7C/OLOHfcgOPWFW+Jlo6VRNCtOsfNrB/h2shoYAjQy8w+29y57v5bd5/h7jNycnI6M0yJI9uKj7C/rIpzx7b8tHhjZoarziFxLpaJowAY3uj1sMi+Zs+JND31AQ60cu2lwDZ3L3L3GuBZ4NyYRC/CsafFZ41uu38DNOWIJIZYJo6lwHgzG21maYQ7sec3OWc+cHNk+wZggYeHpMwH5kZGXY0GxgNLCDdRnW1mPSN9IZcA62NYBklwS7Ydon+vNMbm9I7uAvWOSwJIidWN3b3WzO4CXiE8+mmeu681s3uBPHefDzwEPGpm+cBBIiOkIuc9BawDaoE73b0OWGxmTwPLI/tXAL+NVRlElmw/wFmj+mHtWExcFQ6JdzFLHADu/iLwYpN93220XQl8soVr7wPua2b/94DvdWykIh+2r7SSXQcruOXc0VFfY2hedYl/3apzXKQzvbmxEICzx0TXvwH163Eoc0h8U+IQacHCzcUM7pPBlFZW+2tKXRySCJQ4RJrh7izedoCzx2S3q38jfG2MghLpIpQ4RJqxpaic4vLqqIfh1tPSsZIIlDhEmvH+1sjzG60sE9scQysASvxT4hBpxntbihmUlcGo7J7tuk41DkkEShwiTdSFnHfzD/CR8QPa3b9R/+T4XY8v580NhbEJUCRgShwiTazdU0ppRQ3njx9wwvf46wd7+fwfllJRXdeBkYl0DTF9AFCkO3o3/wAA545tf+I4cKT6uNc/emk9qclJfGv2JNJS9P80iQ9KHCJNvLelmIm5meRkprf72uU7S457/fCiHQBcccqgqBaCEukO9F8gkUaqautYuv0g545r32iqek9+4Wz+c85UFv7rRcft75Ga3BHhiXQJShwijazYWUJlTeiEmqkAsjJSuemcUQzt24OvXDyOSYMyAT0UKPFFiUOkkfe2HCDJYGY7H/xrysz4xuUT+eblEwHNXyXxRYlDpJH38ouZNrQPfXqkdsj96kfzqsYh8USJQyRib2kFy3Ye4rxxJz4Mt6mGxNFhdxQJnhKHSMTbG4twhzmnD+2we1pkvlxNQyLxRIlDJOKdzUUMyspgQm6Uy8RGQzUOiUNKHCJAbV2Id/MPcMGE9k8z0pr6O6nCIfFEiUMEyNtxiNKKGi6aOLBD73ssCSlzSPxQ4hAB3li/n7TkJC6YkNOh91WNQ+KREocI8PamImaO7k+v9I6dhUejqiQeKXFIwttTUsGm/eV8tINrG9B4VFWH31okMEockvDe2VQEwEcnxiBxNDwAqMwh8UOJQxLe25vCw3DHD+zAYbgR6hqXeKTEIQmtpi7EwvxiPjohp0OH4TbQlCMSh5Q4JKEt33GIw5W1XBiDZiqReKXEIQntjQ2FpCYb553EMrGtaegcV2OVxBElDklY7s5La/Zy7tgBZGV0zGy4Ten5P4lHShySsNbuKWPXwQpmnzIoZu+hvCHxSIlDEtZfVhaQmmxcGcvEYXqOQ+KPEockJHfnxdX7+Mj4HPr2TIvZ+xx7clyZQ+KHEockpDUFZRSUxLaZCjRXlcQnJQ5JSC+v3UtyknHp5NyYvo/mqpJ4FNPEYWazzWyjmeWb2d3NHE83sycjxxeb2ahGx+6J7N9oZlc02t/XzJ42sw1mtt7MzollGST+hEdT7WPW6P707xW7ZqowrQAo8SdmicPMkoFfAFcCU4BPm9mUJqfdChxy93HA/wL3R66dAswFpgKzgV9G7gfwAPCyu08CTgPWx6oMEp/yC8vZWnQkpp3i9VTjkHgUyxrHTCDf3be6ezXwBDCnyTlzgIcj208Dl1h4GMoc4Al3r3L3bUA+MNPM+gAXAA8BuHu1u5fEsAwSh97YUAjApVNi20wFx/o4lDkknsQycQwFdjV6vTuyr9lz3L0WKAWyW7l2NFAE/N7MVpjZ78ysV3Nvbma3m1memeUVFRV1RHkkTry2bj9TBmcxuE+PmL9Xw3BcZQ6JI92tczwFOAP4lbtPB44AH+o7AXD337r7DHefkZOjeYgkrPBwJct3HuKKqbFvpgKNqpL4FMvEUQAMb/R6WGRfs+eYWQrQBzjQyrW7gd3uvjiy/2nCiUQkKi+s2os7XDWtkxKHZseVOBTLxLEUGG9mo80sjXBn9/wm58wHbo5s3wAs8PDwk/nA3Mioq9HAeGCJu+8DdpnZxMg1lwDrYlgGiTPPLNvNtKF9GJ+b2Snvd2ySQ5H40bELLDfi7rVmdhfwCpAMzHP3tWZ2L5Dn7vMJd3I/amb5wEHCyYXIeU8RTgq1wJ3uXhe59ZeBxyLJaCvw+ViVQeLL+r1lrNtbxvc/3nRwX+xoBUCJRzFLHADu/iLwYpN93220XQl8soVr7wPua2b/SmBGx0YqieCZZbtJTTY+cXrTMRqxt3ZPGZd3Ur+KSKx1t85xkRNSVVvH8ysLuGjiwE546O/DHnhjM6VHazr9fUViQYlDEsJbG4soLq/mxlkjOvV9c7MyGrZLKqo79b1FYkWJQxLCG+v3k5mewnnjYrPSX0tyMtP51uzwWI7DlbWd+t4isaLEIXGvsqaOl9bs47KpuaQmd/6P/KRB4RFctSF1kEt8UOKQuPfmhkIOV9Zy7fTO7xSHRkNyNbJK4oQSh8S951YUkJOZzrljO7eZqoEmOpQ4o8Qhce3QkWre3FjInNOGkJxkbV8QA5p2ROKNEofEtb+t3ktNnXPtGcE0U8GxiQ5V55B4EfUDgGZ2CuF1NRrGF7r7I7EISqSjPL+igAm5vZkyOCuwGFTjkHgTVY3DzL4H/Dzy5yLgx8AnYhiXyEnbeeAoeTsOcc30oY3+19/5tJiTxJtom6puIDyh4D53/zzhlff6xCwqkQ7wl5XhyZjnBDDFSGPHRlUFGoZIh4k2cVS4ewioNbMsoJDjpz0X6VLcnedWFjBrdH+G9o39gk2t0USHEm+iTRx5ZtYXeBBYBiwHFsUsKpGTtLqglK1FRwJ7dqMxdY1LvImqc9zd74hs/trMXgay3P2D2IUlcnKeXV5AWnISV04bHHQox57jUOaQONGeUVWnAqPqrzGzce7+bIziEjlhFdV1PLt8N5dPzaVPj9Sgw2m0mJMyh8SHqBKHmc0DTgXWAqHIbgeUOKTLeWXtPsoqa/nMrJFBhwIc6+NQ3pB4EW2N42x377xl00ROwjPLdzO0bw9mje4fdCiA+jgk/kTbOb7IzJQ4pMvbW1rBwvxirj9jKEkBTTHSVP0zJOrjkHgRbY3jEcLJYx9QRfg/Ue7up8YsMpET8NyKAtzh+jOHBR1Kg2MPACpzSHyINnE8BNwErOZYH4dIl+LuPLu8gLNG9WNkdq+gw2mgKUck3kSbOIrcfX5MIxE5SSt3lZBfWM6PrpsWdCjH0ZQjEm+iTRwrzOxx4AXCTVUAaDiudCWPLNpBr7Rkrj61Czy7cRwt5CTxJdrE0YNwwri80T4Nx5UuY39ZJS+s2sNN54wkMyP4ZzcaU41D4k20T45/PtaBiJyMRxftoM6dW84dFXQoH9IwtkuZQ+JEtA8Ajga+TKMnxwHcXVOrS+Aqa+p4bPEOLp2c26U6xes1DMdV5pA4EW1T1fOER1a9gEZVSRfz3IoCDh2t4dbzRwcdSrM0qkriTbSJo9LdfxbTSEROgLszb+E2pgzO6jJPijdlmuRQ4ky0ieOByCqAr3L8qKrlMYlKJEp/31zM5sJy/ueTpwW6yl9rjk1yKBIfok0c0wg/AHgxx09yeHEsghKJ1rx3tzGgdzofO62rDcE9Rgs5SbyJNnF8Ehjj7tWxDEakPfILD/PWxiK+cdkE0lOSgw6nTUobEi+ineRwDdA3loGItNcf399JWkoSN84aEXQorVIfh8SbaGscfYENZraU4/s4NBxXArNiVwkzRvZjQO/0oENplWlidYkz0SaO78U0CpF2OlBexYa9ZV1msabWqMYh8Saqpip3f7u5P21dZ2azzWyjmeWb2d3NHE83sycjxxeb2ahGx+6J7N9oZlc0uS7ZzFaY2V+jiV/iz4N/30Z1XYjPnN21m6ngWOJYmF8cbCAiHSSqxGFmZ5vZUjMrN7NqM6szs7I2rkkGfgFcCUwBPt3MYlC3AofcfRzwv8D9kWunAHOBqcBs4JeR+9X7KrA+mtgl/hw8Us0ji7bz8VOHMDand9DhtKm+qeqxxTvZV1oZcDQiJy/azvH/Az4NbCY84eFthJNCa2YC+e6+NTIa6wlgTpNz5gAPR7afBi6x8GD8OcAT7l7l7tuA/Mj9MLNhwNXA76KMXeLMQwu3UlFTx5cvHhd0KFEZ1q9Hw/a24iMBRiLSMaJNHLh7PpDs7nXu/nvCNYHWDAV2NXq9O7Kv2XPcvRYoBbLbuPanwLdoY+oTM7vdzPLMLK+oqKiNUKW7KDlazcPv7eCqaYMZn5sZdDhR6ZWewuP/NAs41mwl0p1FmziOmlkasMrMfmxmX2/HtR3GzD4GFLr7srbOdfffuvsMd5+Rk5PTCdFJZ5i3cBvlVbV85eLxQYfSLilJ4V+XupB6yKX7i/bL/6bIuXcCR4BhwPVtXFMADG/0elhkX7PnmFkK0Ac40Mq15wGfMLPthJu+LjazP0ZZBunmSo/W8Pt3t3PlKYOYOKh71DbqJSeFqxq1ShwSB1pNHGY2x8zudPcd7l4JvAbcAlwLnN7GvZcC481sdKS2MhdouvzsfODmyPYNwAIPz8swH5gbGXU1GhgPLHH3e9x9mLuPitxvgbt/NtrCSvf2x8U7OFxVy5e7WW0DICWSOOpCmlxaur+2nuP4FuEv6HrpwJlAb+D3hDu0m+XutWZ2F/AKkAzMc/e1ZnYvkBdZw/wh4FEzywcO1r9X5LyngHVALXCnu9edSAElfrybX8wpQ7OYMiQr6FDarb7GUVOnGod0f20ljjR3b9xJvdDdDwIHzazNFXPc/UXgxSb7vttou5LwPFjNXXsfcF8r934LeKutGCQ+HKmqJW/HIW6c2fWf22hOSnI4cSzacoArpg4KOBqRk9NWH0e/xi/c/a5GL9XjLJ3mxdV7qa4Nddsv3YzIJIx/eG97sIGIdIC2EsdiM/unpjvN7AvAktiEJHK8UMj51dtbmDI4i7PHdM3FmtoyMrsnAFMGd79mNpGm2mqq+jrwvJndCNQv2nQm4b6Oa2IZmEi9Z1cUsLXoCP934/Quu1hTW8yMc8ZkaziuxIVWE4e7FwLnmtnFhKf/APibuy+IeWQiQEV1Hf/9ykZOG9aHq07puos1RSM5yaiu06gq6f6imh03kiiULKTTPfj3rewrq+TnN04nKal71jbqJSWZahwSFzr96W+RaBWWVfLrt7cwe+ogzhrVPfs2Gks2PTku8UGJQ7qs/3l1EzV1Ie6+clLQoXSIZNU4JE4ocUiXtOvgUZ5atoubzh7FqAFtPjLULSQnGSF3Xl27j6PVtUGHI3LClDikS3oqbxfu8Llzuv4Kf9FKTjI27DvM7Y8u42dv5AcdjsgJU+KQLmdb8RF+viCfj582JG5qGwCrdpU2bNdqdJV0Y0oc0qW4O//y51UA3HnR2ICj6Vh3XDSWiycNJD0lSbPkSremxCFdyr89t4a8HYf4+qUTmDQovp6y/syskcy75SyqakO8unZf0OGInDAlDukydh08yp+W7ATgSxfGV22jqT1ae1y6MSUO6RLcne/+ZQ0905J59+6LSUuJ3x/NSycPZFBWRtBhiJyw+P3tlG7lpTX7eHNjEd+4bAJD+/YIOpyYGtA7nZCrj0O6LyUOCVxtXYj/fmUjkwdnccu5o4IOJ+ZSkvUgoHRvShwSuN+8s5WtxUf4ysXjSEmO/x/JlCSNqpLuLf5/S6VLW7unlJ++vomPnTqYK6d179lvo5WcZA3PcVTW1HH+/QsYdfffKDlaHXBkItFR4pDAlFbUcMdjy+nfK43/nHNK0OF0mpQka6hx/GnJTnYfqgBgS9GRIMMSiVpU06qLdLRQyPn6kyvZU1LBE7efQ79eaUGH1Gnq+zi+/uRKnltREHQ4Iu2mGocE4mcLNrNgQyHf/dgUzhzZr+0L4kiyhWsc9UnjH88bDaCRVtJtKHFIp1uwYT8/fX0z158xjM+eHT+TGEarf6Pa1Xt3X8xlU3IBqK1T4pDuQU1V0qm2Fx/ha0+sZOqQLO679pRuu4b4yfjcOaN4Z3MxN50zkiF9e1BQEu7j0BBd6S6UOKTTHK2u5Yt/XEZSkvHrz55JRmpy0CEFIinJmHfLWcdeR5JnbUgz5kr3oMQhnaIu5HzjyVVs3H+Yhz8/k+H9ewYdUpeREllLXX0c0l2oj0Nizt35jxfW8vLafXzn6ilcMCEn6JC6lORI4vjHP+RRVVsXcDQibVPikJj75VtbeGTRDr5wwRhuPX900OF0OZkZxyr+m/eXBxiJSHSUOCSmnsrbxX+9spFrpw/lX2dPCjqcLmlkdi8m5PYGUI1DugUlDomZNzcUcs+zq/nI+AHcf/2pJCUl3giqaH33Y1MB0Iqy0h0ocUhMrN5dyh2PLWfy4Ex+9dkz43p9jY6QFPnn0ZBc6Q702ywdbl9pJbc9spT+vdKYd8tZ9E7X4L22pEQyhxKHdAdKHNKhCssq+dy8xZRX1vLQLTMYmKmV7qJRP5t8nYbkSjcQ08RhZrPNbKOZ5ZvZ3c0cTzezJyPHF5vZqEbH7ons32hmV0T2DTezN81snZmtNbOvxjJ+aZ8tReX8w28WsftQBb+5aQaTBmUFHVK3Uf8QYEg1DukGYtaGYGbJwC+Ay4DdwFIzm+/u6xqdditwyN3Hmdlc4H7gU2Y2BZgLTAWGAK+b2QSgFvhnd19uZpnAMjN7rck9pZOFQs69f13HH97bTnavNB69dSZnjuwfdFjdSv2zHGqqku4gljWOmUC+u29192rgCWBOk3PmAA9Htp8GLrHw5EVzgCfcvcrdtwH5wEx33+vuywHc/TCwHhgawzJIFB5fspM/vLedYf168Je7zlPSOAH1iWPNntKAIxFpWywTx1BgV6PXu/nwl3zDOe5eC5QC2dFcG2nWmg4sbu7Nzex2M8szs7yioqITLoS07pllu/nuX9YwfURf3vzmhQzrp6lETkR94vjp65tZv7cs4GhEWtctO8fNrDfwDPA1d2/2t8zdf+vuM9x9Rk6OpriIhXkLt/HPf17F2WOyeey2WaQmwHrhsdKnR2rDdmlFTYCRiLQtlr/pBcDwRq+HRfY1e46ZpQB9gAOtXWtmqYSTxmPu/mxMIpcWuTsvrd7LPc+u5t6/rmP21EH8/vNn0TNNQ25PxuA+PXhg7umA+jmk64vlb/tSYLyZjSb8pT8XuLHJOfOBm4FFwA3AAnd3M5sPPG5mPyHcOT4eWBLp/3gIWO/uP4lh7NKM2roQP3xpAw8t3AbAp2eO4D/nTCVFNY0OUd/MV6PHx6WLi1nicPdaM7sLeAVIBua5+1ozuxfIc/f5hJPAo2aWDxwknFyInPcUsI7wSKo73b3OzM4HbgJWm9nKyFv9m7u/GKtySNj+skq+/PgKlmw/SEqS8V+fPJVrTh+akAsxxUpqcmRdjjqn8HAl81fuYcGGQh67bZb+naVLMU+AB45mzJjheXl5QYfRbRWWVXLTQ0vYdegoP7xuGnNO10C2WFi7p5Srf7bwQ/sfunkGl0zODSAiSWRmtszdZzR3TA3T0qr3tx7g60+upLSiht/eNIPzxw8IOqS4VT/tSFNVtWq6kq5FjdPSorc3FXHjg++TlpLEU184R0kjxlKSjzVH/f1bF/Hj608Fjj1VLtJVqMYhzXpj/X7ueGw5o7J78cyXzqVfr7SgQ4p7o7J78ZVLxjP3rOEM6duDo9XhtTm0pKx0NUoc8iH//cpG/u/NfCYPzuKRf5yppNFJkpOMb1w24bjXALUanitdjBKHNAiFnIcWbuP/3sznE6cN4cc3nEpGanLQYSWsY/NXqY9DuhYlDgHCSeMbT63k+ZV7uHRyLj+4bpqSRsBSGhJHwIGINKHOcaGmLsQ3/7yK51fu4auXjOfBz52pxZe6gPqldveUVPDDF9dTrdFV0kXo2yHB1YWcL/1xOa+v38+XLhzL1y4dr4fNuoj6GsdPXtsEwMdOHcK0YX2CDEkEUOJIaIu2HODTD74PwD9fNoEvXzI+4Iiksaa1Pked5NI1qKkqQW0vPsIdjy0jLSWJB+aezl0Xjws6JGmiV3oK100fSnZkVJsmP5SuQokjAZVW1PCZ3y2mts557o5zmaM5p7qsn3zqdH7yqfCsucob0lWoqSrBFB2u4st/Wk5BSQWP3TaLqUPUZt7VRbo69CCgdBlKHAlk0/7D3PjgYg5X1vDD66Zx3jhNIdIdJEdqgyFVOaSLUOJIAO7On5bs4j9eWEtGajLP33kekwdnBR2WRKm+GbFONQ7pIpQ44lxBSQX//vwaFmwo5MyR/fjlZ84gNysj6LCkHeqfIFfekK5CiSNO1daF+Nvqvdz3t/UcrqzlO1dP5tbzR6sTvBuq7+PQqCrpKpQ44kxlTR0vr9nH/76+iR0HjjJuYG9+c9OZTB/RL+jQ5ATVP0GuznHpKpQ44oC7s2p3KU/l7eKvq/ZQVlnLmAG9+M1NZ3LZ5NyGLx7pnurX46isqaOmLkSq1niXgClxdGP7yyp5bkUBTy/bTX5hORmpSVx5ymCuP2MYs8b01xdMnKgfVfXFPy4H4MKJOfzh8zODDEkSnBJHN1NZU8dr6/bz9LLd/H1zESGHGSP78aPrpnHVqYPJykgNOkTpYE27pd7aWERZZY0+awmMEkc3cKC8irc2FrFgQyFvbyqivKqWIX0yuPOicVx3xjBGD+gVdIgSQwN6pwNw3RlDOXtMNt96+gNKjypxSHCUOLqgypo6/r65mPV7y3hrYyErdpXgDgMz0/nYqYP5+GlDOGdMtvouEsSgPhls/H+zSU9JZv6qPQBUaYp1CZASR8DcnYKSClbtKuWD3SWs2l3Cql2lVNSE15s+dVgfvnrJeC6ZlMvUIVlKFgkqPSW8qFZapN+qqrYuyHAkwSlxnKRQyKP+Mq+tC7H9wBE27DvMpn2HWbOnjFW7SjhwpBoIfylMHpLFJ2cM48KJOZw5oj99eqo5Qo5JTwknjj/n7WbqJzTPmARDiaMNh45U88zy3RSXV3PoSDUHjlRz6Oix7dKKGob168F104fyjcsn4u6UHK2hoKSCHQeOsm5vKbsOVrC5sJwtheVUR9YBTTIYN7A3F00ayGnD+3LasD5MGpRFWopGQknLhvfvCcCLq/fy/U9MDTgaSVRKHC1wdy75n7fZWnwEgNRko3+vNPr1TKN/rzQmD8kiMz2FvB2HKKuo4dfvbOWvq/eyt6SyoZmpXk5mOlMGZ3HB+AFMHJTJhNxMxg3srTW9pd3GDexNblY6Z4/JDjoUSWBKHC0wM2aNyeaCCTlMG9qH685oec2K19ft51dvbyE3K52LJg5kSN8eDOmTwcjsXgzqk0G/nqma6kM6TO/0FGrr9BS5BEeJoxU/vG5aVOddOiWXS6fkxjgakbCUpCRqQxpVJcFRg7pIN5OSbKpxSKCUOES6mZTkJGo1U64ESIlDpD0VNHYAAAzHSURBVJtJSTI1VUmglDhEupmUJDVVNbX70FEeWbSddXvKgg4lIahzXKSbSUk2Kmu6bo2j5Gg1xeVVjBuYGdP3qakL8crafTyVt5t3NhU17P/iR8fy67e38NNPnc4104e2eo/yqlpeXrOPF1btYfqIvnzt0gkxjTlexDRxmNls4AEgGfidu/+oyfF04BHgTOAA8Cl33x45dg9wK1AHfMXdX4nmniLxLjyqqutNORIKOY8t2cm/P78GgOkj+rK2oIzRA3oxLrc337hsAmNzerd4fV3IMcILV1XW1BFyp2fah7+iSo5W8/iSnTy6aAd7SysZ2rcHd140ljUFZby9qYhfv70FgK89uZKvPbmSd/7lIkZk98Td2XnwKC+t2cfzKwrYsO/wcff9++YiLpiQw+HKWs4bm01ReRVpyUlk904nFHK2FJUzqE8GmZpcMnaJw8ySgV8AlwG7gaVmNt/d1zU67VbgkLuPM7O5wP3Ap8xsCjAXmAoMAV43s/r/CrR1T5G4Fm6qClFTF+JodR19ehz/RbZqVwn/9txqHvzcDAb3yeC1dfs5e2w2Ty3dxYUTczqsJuDubNh3mNKKGv7jhXWs33t8M9GKnSUAbNx/mI37D7NwczHZvdK4/sxh3HnROCDcxPTU0l2s23uYtzYWUhtyPjJ+AH/fXAzA0188h+dWFPDY4p187pyR3DvnFG5/ZBlLth/kvHHZ/OecU7ho0kCSk4yauhA/X5DPWxsLycxI4d38AwBc8F9vMrx/Dyqq6ygur26Ib3CfDC6dnMs104eSnGRc84t3ue6X732onKcN78uagtKGpXu/fdVkVu4qYUtROYP7ZNC/VzoL84u45vShrNxVwsjsnkwalMU104fSMy38kO/KXSX065nGxEGxrYV1FvMYLUdpZucA33f3KyKv7wFw9x82OueVyDmLzCwF2AfkAHc3Prf+vMhlrd6zOTNmzPC8vLyOK5xIgL7waB6vrN3f8PofZgyjsibEd66eTG3IOfdHC1q9/r27L2ZI3x4n/P4Hyqt44I3NPLJox4eO/eDaaXx65nBeXrOPw1W1fGT8AB5ZtINfvbXlQ+eOG9ib/MLyhtf9e6Vx8Ej1h85rbGJuJhv3H2bO6UN4YO70Vs91d+b+9n0WbzsIwAUTcrh4Yg4XT8plRHbP486tCzk/fX0T/XqmsbW4nNKKWkoranhnUxGZ6SlMHpzFlqLyhnnlMlKTom4uNIP6r9mhfXtwtLqWsTm9OXfcAIb168Hq3aXsK6tkVHZPZo3OZnNhOduLjxByp6i8iosmDiQ1OYncrHQ27S9nw74y1u8t4wfXTmNgZgaFhyvZceAoW4rKKamoYWjfHlw8aSCTB2dFFV/Lcdsyd5/R7LEYJo4bgNnuflvk9U3ALHe/q9E5ayLn7I683gLMIpwk3nf3P0b2PwS8FLms1Xs2uvftwO0AI0aMOHPHjg//kIt0R3c8towXV+874esnD87i/10zldOG9SUlOaldy9E+s2w3//znVQ2ve6QmM2FQJj+fO/1DX8aNvbelmINHqsnMSOXmeUsa9l87fSgzR/fn8im5ZPdOZ01BKUlmTMjtzb//ZS3D+/fguunD2LCvjFt+v7ThutvOH813PjYlqpjrQk7yCc4qXVFdR3pKUsNEpou2HCAnM52xOb0wM3YfOkpOZjqb95czMrsn1bUhFuYXs25vGTW1TmqKkZJkHDxSzZJtBxmT05u3NxVR3ca0+H16pFJaUdPssWiT1revmsy4gb25cGLOCc1ckZCJozHVOCSeTPnuyxytruOCCTnHdQrX+8WNZ3DJ5IGsLihl6pAs3tpYRG3I+cRpQxh199+OO7dnWjJHq+v4ztWTue0jY1p8z9q6EP/xwjoefT/8H7DHbpt1wmvChELOur1ljMzu2a7+guLyKl5du59DR6uZe9ZwsiMLXHU31bUhthUfoaYuREZqEqMH9Ka4vIp384sZmd2LcQN706dHKqGQs2LXIXqnp3KgvIrUlCQm5GbSp0cqb6zfz9Lthxg9oCc5memM6N+T/r3SqakL8fKafXxv/loAcrPSefObFzbbV9SWoBKHmqpEYuDnb2zm4UU7eOMbH+WaX77LD66dxpTBWfzktY3cdM4oxg1suQP6zseX87cP9rb5HpMGZVJeVUthWRXnjx/Agg2FAHzhgjF884qJWs++iys5Wk1dyCmtqGFMKwMSWhNU4kgBNgGXAAXAUuBGd1/b6Jw7gWnu/sVI5/h17v4PZjYVeByYSbhz/A1gPGBt3bM5ShwSb9z9hJof8gsP8+TSXXx65gh+8OIGPn/eKF5fv5/fv7u9zWt/fP2p/MNZw08gWumOAkkckTe+Cvgp4aGz89z9PjO7F8hz9/lmlgE8CkwHDgJz3X1r5NpvA/8I1AJfc/eXWrpnW3EocYi0rrYuxAcFpZw+rC+/eWcrR6pquWBCDkWHq6gNhTh7TDa5WRlBhymdKLDE0VUocYiItE9riUMNlSIi0i5KHCIi0i5KHCIi0i5KHCIi0i5KHCIi0i5KHCIi0i5KHCIi0i5KHCIi0i4J8QCgmRUBO4ABQHHA4QQpkcuvsieuRC7/yZR9pLvnNHcgIRJHPTPLa+lJyESQyOVX2ROz7JDY5Y9V2dVUJSIi7aLEISIi7ZJoieO3QQcQsEQuv8qeuBK5/DEpe0L1cYiIyMlLtBqHiIicJCUOERFpl4RJHGY228w2mlm+md0ddDyxZmbbzWy1ma00s7zIvv5m9pqZbY783S/oODuKmc0zs0IzW9NoX7PltbCfRX4WPjCzM4KL/OS1UPbvm1lB5PNfGVk5s/7YPZGybzSzK4KJumOY2XAze9PM1pnZWjP7amR/onz2LZU/tp+/u8f9H8LLzG4BxgBpwCpgStBxxbjM24EBTfb9GLg7sn03cH/QcXZgeS8AzgDWtFVe4CrgJcJr2J8NLA46/hiU/fvAN5s5d0rk5z8dGB35vUgOugwnUfbBwBmR7UxgU6SMifLZt1T+mH7+iVLjmAnku/tWd68GngDmBBxTEOYAD0e2HwauCTCWDuXu7xBet76xlso7B3jEw94H+prZ4M6JtOO1UPaWzAGecPcqd98G5BP+/eiW3H2vuy+PbB8G1gNDSZzPvqXyt6RDPv9ESRxDgV2NXu+m9X/ceODAq2a2zMxuj+zLdfe9ke19QG4woXWalsqbKD8Pd0WaY+Y1apaM27Kb2ShgOrCYBPzsm5QfYvj5J0riSETnu/sZwJXAnWZ2QeODHq63JsxY7EQrL/ArYCxwOrAX+J9gw4ktM+sNPAN8zd3LGh9LhM++mfLH9PNPlMRRAAxv9HpYZF/ccveCyN+FwHOEq6P766vlkb8Lg4uwU7RU3rj/eXD3/e5e5+4h4EGONUfEXdnNLJXwl+Zj7v5sZHfCfPbNlT/Wn3+iJI6lwHgzG21macBcYH7AMcWMmfUys8z6beByYA3hMt8cOe1m4C/BRNhpWirvfOBzkRE2ZwOljZo14kKTdvtrCX/+EC77XDNLN7PRwHhgSWfH11HMzICHgPXu/pNGhxLis2+p/DH//IMeFdCJow+uIjziYAvw7aDjiXFZxxAeObEKWFtfXiAbeAPYDLwO9A861g4s858IV8lrCLfb3tpSeQmPqPlF5GdhNTAj6PhjUPZHI2X7IPJlMbjR+d+OlH0jcGXQ8Z9k2c8n3Az1AbAy8ueqBPrsWyp/TD9/TTkiIiLtkihNVSIi0kGUOEREpF2UOEREpF2UOEREpF2UOEREpF2UOEQ6kZnda2aXBh2HyMnQcFyRTmJmye5eF3QcIidLNQ6RDmBmo8xsg5k9ZmbrzexpM+sZWRflfjNbDnzSzP5gZjdErjnLzN4zs1VmtsTMMs0s2cz+y8yWRiao+0Lk3MFm9k5kbYU1ZvaRQAssCS0l6ABE4shE4FZ3f9fM5gF3RPYf8PCEk5jZ7MjfacCTwKfcfamZZQEVhJ/6LnX3s8wsHXjXzF4FrgNecff7zCwZ6Nm5RRM5RolDpOPscvd3I9t/BL4S2X6ymXMnAnvdfSmAR2Z0NbPLgVPrayVAH8LzCS0F5kUmtHve3VfGqAwibVLiEOk4TTsM618facc9DPiyu7/yoQPhqfGvBv5gZj9x90dOLEyRk6M+DpGOM8LMzols3wgsbOXcjcBgMzsLINK/kQK8AnwpUrPAzCZEZjseCex39weB3xFeKlYkEEocIh1nI+FFs9YD/QgvptMsDy9h/Cng52a2CngNyCCcFNYBy81sDfAbwi0DFwKrzGxF5LoHYlgOkVZpOK5IB4gs2/lXdz8l4FBEYk41DhERaRfVOEREpF1U4xARkXZR4hARkXZR4hARkXZR4hARkXZR4hARkXb5/7RScQEFQlCMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "c8a76852-cc9c-42a1-bbef-cf720f168ac5"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "def compute_price(sigma):\n",
        "    inputs = torch.tensor([[110.0, 0.0, 110.0, sigma, 0.1, 0.05] + ([110.0, 0.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    return x.item()\n",
        "sigmas = np.arange(0, 0.5, 0.1)\n",
        "prices = []\n",
        "for s in sigmas:\n",
        "    prices.append(compute_price(s))\n",
        "fig3 = pylab.plot(sigmas, prices)\n",
        "pylab.xlabel('Sigma')\n",
        "pylab.ylabel('Price')\n",
        "fig3"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1015f49050>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5hU9dnG8e9D7x3pSxNcUBBwRMUaNRE0CtYgGjFBiYkm+oo1JtHYS+z6alCxJBHFClEREEWUYFmk9y4svdddtjzvH3N4M667sMPu7JnZvT/XNdee+c05Mw8DszfnPHN+x9wdERGR4qoUdgEiIpJaFBwiIhIXBYeIiMRFwSEiInFRcIiISFyqhF1AWWjSpIm3a9cu7DJERFLKtGnTNrl704LjFSI42rVrR0ZGRthliIikFDNbWdi4DlWJiEhcFBwiIhIXBYeIiMRFwSEiInFRcIiISFwUHCIiEhcFh4iIxEXBISJSDm3fm8NdY+ayIyun1J9bwSEiUs7MWLWNc576gn9+tZJvlm0p9eevEGeOi4hUBPn5zotfLuPhjxfSrF4NRl1zAr3SGpb66yg4RETKgc27shn21kwmLdxI3yOb89CF3alfq2pCXkvBISKS4qYu3cwNb05n654c7ul/JJcf3xYzS9jrKThERFJUXr7z1MTFPP3pYto1rs2IK4/lyJb1E/66Cg4RkRS0bnsW178xna+Xb+GCnq24Z8BR1K5eNr/SFRwiIinmswUbGPbWTLJy8nj04qO58JjWZfr6Cg4RkRSxLzefR8Yt4IUvlpPevC7PXtaLjk3rlHkdCTuPw8xGmNkGM5sTM3axmc01s3wzixSxXRsz+8zM5gXrXh/z2F1mlmlmM4Lb2YmqX0QkmXy/eQ8X/30qL3yxnF8e35b3rz0xlNCAxO5xvAI8A7wWMzYHuAD4+wG2ywWGuft3ZlYXmGZmE9x9XvD44+7+t0QULCKSjD6ctZbb3pkFBs9d1ot+3VqEWk/CgsPdJ5tZuwJj84EDfk3M3dcCa4PlnWY2H2gFzCtyIxGRcigrJ497PpjHv77+nh5tGvD0pT1p06hW2GUld48jCJ6ewNcxw9eZ2RVABtE9k61FbDsUGAqQlpaW2EJFRErZkg07ue716SxYt5PfnNqBm352BFUrJ8csUclRRSHMrA7wDnCDu+8Ihp8DOgI9iO6VPFrU9u4+3N0j7h5p2rRpwusVESkN7s5bGas49+kpbNiZzcu/Opbb+3VJmtCAJN3jMLOqREPjX+7+7v5xd18fs84LwAchlCcikhC7snP58/tzeG96Jid0aMwTA3vQrF6NsMv6kaQLDos2QF4C5rv7YwUeaxH0QADOJ9psFxFJeXMyt/P7kdNZuXk3/3NmZ647/XAqV0rctCElkbDgMLORwGlAEzNbDdwJbAGeBpoCH5rZDHc/y8xaAi+6+9nAicAvgdlmNiN4uj+6+0fAw2bWA3BgBfCbRNUvIlIW3J3Xpq7kvg/n07B2VV6/+niO79A47LIOyNw97BoSLhKJeEZGRthliIj8wPY9OdzyzkzGzV3P6emH8beLj6ZR7Wphl/X/zGyau//onLukO1QlIlIRTFu5hT+MnMGGnVn86ZwuDDmpfUJntC1NCg4RkTKUn+88P3kpj45fRMsGNXj7mj4c3aZB2GXFRcEhIlJGNu7M5sZRM/hi8SbO6d6CBy7oRr0aibnYUiIpOEREysCUJZu44c0Z7Nibw/3nd+PS3m1S5tBUQQoOEZEEys3L54lPFvPspCV0bFqHfwzpTXrzemGXVSIKDhGRBFmzbS/XvzGdb1ds5ZJIa+4670hqVUv9X7up/ycQEUlCE+at5+a3Z5KTm8+TA3vQv0ersEsqNQoOEZFSlJ2bx4NjF/DylBUc2bIezwzqRfsmtcMuq1QpOERESsmKTbu5buR3zMncwZV92nH72elUr1I57LJKnYJDRKQUjJ6RyR3vzaFyJWP4L4/hZ0c2D7ukhFFwiIiUwN59edw1Zi5vZqzimLYNeerSnrRqUDPsshJKwSEicogWrd/Jtf/6jiUbd3HtTzpyw5mdk+q6GYmi4BARiZO788a3q/jrv+dSp3pVXvt1b07uVHEuGKfgEBGJw86sHG5/dzYfzFrLyZ2a8OglR3NY3eS72FIiKThERIpp1uptXPf6dDK37eXms47gt6d2pFKSXmwpkRQcIiIH4e6MmLKCB8fOp2md6rw59Hgi7RqFXVZoFBwiIgewdfc+bnprJhMXbOCnXZvxyEXdaVAreS62FAYFh4hIEb5ZvoXr35jO5l37uOvcrgzu0y5lZ7QtTQoOEZEC8vKd//1sCY9/soi0RrV493d9OKpV/bDLShoKDhGRGBt2ZHHDmzP4z9LN9O/RkvvO70ad6vpVGUvvhohI4PNFG7nxzRns3pfLwxd15+JjWuvQVCESeoqjmY0wsw1mNidm7GIzm2tm+WYWOcC2fc1soZktMbPbYsbbm9nXwfibZlaxu1QiUmI5efk8OHYBg0d8Q5M61fn3dSdxSSR1r9CXaIk+N/4VoG+BsTnABcDkojYys8rAs0A/oCtwqZl1DR5+CHjc3Q8HtgJDSrlmEalAVm/dwy/+PpXnP1/Kpb3TGH3diXRqVjfsspJaQg9VuftkM2tXYGw+cLAk7w0scfdlwbpvAP3NbD5wOjAoWO9V4C7gudKsW0Qqho/nrOWWt2fhDs8M6snPu7cMu6SUkKw9jlbAqpj7q4HjgMbANnfPjRkv9LJaZjYUGAqQlpaWuEpFJOVk5eRx/0fzeW3qSrq3rs8zl/YirXGtsMtKGckaHCXm7sOB4QCRSMRDLkdEksSyjbu47vXpzFu7g6tOas8tfdOpVqX8z2hbmpI1ODKBNjH3Wwdjm4EGZlYl2OvYPy4iclDvfreaP70/h+pVKjHiyginpzcLu6SUlKzB8S3QyczaEw2GgcAgd3cz+wy4CHgDGAyMDq9MEUkFu7Nz+cvoubzz3Wp6t2/EkwN70KJ++b7YUiIl+uu4I4GpwBFmttrMhpjZ+Wa2GjgB+NDMxgXrtjSzjwCCvYnrgHHAfGCUu88NnvZW4EYzW0K05/FSIv8MIpLa5q/dwbnPfMm701fzhzM68fpVxyk0Ssjcy//h/0gk4hkZGWGXISJlyN3559ffc88H82hQsypPDOxBn45Nwi4rpZjZNHf/0fl2yXqoSkTkkG3fm8Nt78xi7Jx1nNq5KY9ecjRN6lQPu6xyQ8EhIuXK9O+38vuR01m3PYvb+6Vz9ckdKuTFlhJJwSEi5UJ+vvPCF8t4ZNxCmtevwahrTqBXWsOwyyqXFBwikvI278pm2FszmbRwI/2Oas6DF3anfs2qYZdVbik4RCSlTV26mRvenM7WPTncM+AoLj8uTZMTJpiCQ0RSUl6+8+TExTz96WLaN6nNy1f2pmvLemGXVSEoOEQk5azbnsX1b0zn6+VbuLBXa+7ufyS1dbGlMqN3WkRSymcLNjDsrZlk5eTx2CVHc0Gv1mGXVOEoOEQkJezLzeeRcQt44YvldGlRj2cG9aRj0zphl1UhKThEJOnNW7ODG0fNYMG6nVxxQlv+eHYXalStHHZZFZaCQ0SSVm5ePn+fvIwnPllE/ZrVePGKCGd21Yy2YVNwiEhSWrpxF8NGzWTGqm2c070F9/Y/ioa1q4VdlqDgEJEkk5/vvDp1BQ99vIAaVSvz9KU9OfdoXdI1mSg4RCRprN66h5vfmsXUZZs5Pf0wHrygG4fVqxF2WVKAgkNEQufujMpYxT0fzMfdeejCblwSaaMzwJOUgkNEQrVhRxa3vzubiQs2cHyHRjxy0dG0aVQr7LLkABQcIhKaf89cw59Hz2HvvjzuPLcrg09opynQU4CCQ0TK3Nbd+/jT6Dl8OGstPdo04NFLjtbJfClEwSEiZWri/PXc9u5stu3Zx81nHcFvTulAlcqVwi5L4qDgEJEysTMrh3s+mMeojNWkN6/Lq7/SbLapKmExb2YjzGyDmc2JGWtkZhPMbHHw80eX5zKzn5jZjJhblpkNCB57xcyWxzzWI1H1i0jp+c+STfR94gvenraaa3/SkdHXnajQSGGJ3D98BehbYOw2YKK7dwImBvd/wN0/c/ce7t4DOB3YA4yPWeXm/Y+7+4zElC4ipWHvvjzuGjOXQS9+TfUqlXj7t324+ax0qlfRPFOpLGGHqtx9spm1KzDcHzgtWH4VmATceoCnuQgY6+57Srk8EUmw777fyrBRM1m+aTdX9mnHrX3TqVlNgVEelHVHqpm7rw2W1wEHm61sIDCywNh9ZjbLzB43s+pFbWhmQ80sw8wyNm7cWIKSRSQe2bl5PPzxAi567j/sy83n9auP467zjlRolCOhfZXB3R3woh43sxZAN2BczPDtQDpwLNCIA+ytuPtwd4+4e6Rp06alU7SIHNC8NTvo/8wU/nfSUi4+pg0f33AyfTo2CbssKWVl/a2q9WbWwt3XBsGw4QDrXgK85+45+wdi9layzexl4KYE1ioixZSbl8/zny/lyYmLaVCrGi8NjnBGF01/Xl6VdXCMAQYDDwY/Rx9g3UuJ7mH8v5jQMWAAMKfQLUWkzCzduIsbR81k5qpt/Lx7C+7R9OflXsKCw8xGEm2ENzGz1cCdRANjlJkNAVYS3avAzCLANe5+VXC/HdAG+LzA0/7LzJoCBswArklU/SJyYPn5ziv/iU5/XrOapj+vSCzaaijfIpGIZ2RkhF2GSLmxassebn57Jl8t26Lpz8sxM5vm7pGC4zpzXESKzd1589tV3PPBPMyMhy/szsWR1pr+vIJRcIhIsazfkcVt78zis4UbOaFDYx65uDutG2r684pIwSEiBzVm5hr+/P4csnPzuOvcrlyh6c8rNAWHiBRpy+59/DmY/rxnWgMevfhoOmj68wpPwSEihfpkXnT68+17Nf25/JCCQ0R+YEdWDnf/ex5vT1tNlxb1+MeQ3nRpoZls5b8UHCLy/6Ys2cQtb89i7fa9XPeTw/nDGZ2oVkV7GfJDCg4RYe++PB4cO59Xp66kQ9PavPPbPvRM+9HlckQABYdIhTdt5VZueis6/fmvT2zPzWcdoZls5YAUHCIVVHZuHk98spi/f76UFvVrMvLq4zmhY+Owy5IUoOAQqYDmrtnOsFEzWbBuJwOPbcMd53Shbo2qYZclKULBIVKB5Obl89yk6PTnjWpX4+Urj+Un6YeFXZakGAWHSAWxZMMuho2awczV2znv6Jbc3f9IGtTS9OcSPwWHSDmXn++8/J8VPPzxAmpVq8yzg3pxTvcWYZclKaxYwWFmnYHniF4z/Cgz6w6c5+73JrQ6ESmRVVv2cNNbM/l6+RbO7HIY91/QjcPqavpzKZnintnzAtGr8eUAuPssYGCiihKRknF3Rn7zPX2fmMy8NTt45KLuvHBFRKEhpaK4h6pqufs3Bebcz01APSJSQut3ZHHrO7OYtHAjfTo25pGLj6ZVg5phlyXlSHGDY5OZdQQcwMwuAtYmrCoRiZu7M2bmGv4yei7ZuXn89bwj+eXxbTX9uZS64gbHtcBwIN3MMoHlwOUJq0pE4rJ5VzZ/Hj2Hj2avo1daAx69pAftm9QOuywpp4oVHO6+DDjTzGoDldx9Z2LLEpHimjBvPbe/O4sde3O5tW86Q0/pQGXtZUgCFas5bmb3m1kDd9/t7jvNrKGZHfQbVWY2wsw2mNmcmLFGZjbBzBYHPwudSc3M8sxsRnAbEzPe3sy+NrMlZvammemL6FIh7cjK4aa3ZnL1axkcVrcGY35/Ir89raNCQxKuuN+q6ufu2/bfcfetwNnF2O4VoG+BsduAie7eCZgY3C/MXnfvEdzOixl/CHjc3Q8HtgJDivlnECk3pizZRN/HJ/Pe9Ex+f/rhvH/tiaQ31zUzpGwUNzgqm1n1/XfMrCZQ/QDrA+Duk4EtBYb7A68Gy68CA4pZAxb9WtfpwNuHsr1IqtuzL5e/jJ7DZS9+Tc1qlXnnt30Y9rMjdM0MKVPFbY7/C5hoZi8H93/Ff3/5x6uZu+//RtY6oFkR69UwswyiX/t90N3fBxoD29x9/1eBVwOtCtvYzIYCQwHS0tIOsVSR5DFt5RaGjZrJyi17GHJSdPrzGlU1/bmUveI2xx8ys1nAGcHQPe4+rqQv7u5uZl7Ew23dPdPMOgCfmtlsYHsczz2c6DfBiEQiRb2GSNLLzs3jsQmLeGHyMlo2iE5/fnwHTX8u4Sn2XFXuPhYYWwqvud7MWrj7WjNrAWwo4vUyg5/LzGwS0BN4B2hgZlWCvY7WQGYp1CSSlOZkRqc/X7h+J5f2bsMd53SlTnVNMSfhOuCBUTP7Mvi508x2xNx2mtmOQ3zNMcDgYHkwMLqQ1224v6diZk2AE4F57u7AZ8BFB9peJNXl5uXz1MTFDHh2Clv37OPlXx3LAxd0V2hIUjjgv0J3Pyn4WfdQntzMRgKnAU3MbDVwJ/AgMMrMhgArgUuCdSPANe5+FdAF+LuZ5RMNtwfdfV7wtLcCbwRfB54OvHQotYkkqyUbdjJs1Exmrt5O/x4t+et5mv5ckotF/xN/gBXMKgNz3T29bEoqfZFIxDMyMsIuQ+SA8vOdEVOW8/C4hdSuVpn7zu/G2d00/bmEx8ymuXuk4PhB93vdPc/MFppZmrt/n5jyRCq2VVv2MOytmXyzfAtndmnGAxd0o2ndg37jXSQUxT1g2hCYa2bfALv3DxY4MU9E4hSd/nwV9344j8pm/O3io7mwVysKzEQtklSKGxx/TmgVIhXQ2u17ue2d2Xy+aCMnHt6Yhy/S9OeSGg4YHGZWA7gGOByYDbwUc/KdiByCvHzn1f+s4NHxC8l3uLv/kVx+nKY/l9RxsD2OV4le9e8LoB/QFbg+0UWJlFezV2/nj+/NZnbmdk47oin39D+KNo1qhV2WSFwOFhxd3b0bgJm9BHyT+JJEyp9d2bk8Nn4Rr/xnOY3rVOfZQb04u1tz9TIkJR0sOHL2L7h7rv6Ri8Rv/Nx13DlmLut2ZHH5cW256awjqF+zathliRyygwXH0TFniBtQM7hvRKea0jzOIkVYs20vd42Zy/h560lvXpdnL+tFr7RCLz8jklIOdua4pt4UiVNs8zvPndv6pTPkpPZUraypz6V80MQ3IqUotvl9auem3DtAzW8pfxQcIqVgd3Yuj8Y0v58Z1JNzurVQ81vKJQWHSAmNn7uOu8bMZe2OLC47Lo2bz0pX81vKNQWHyCFau30vd47+b/P76UG9OKatmt9S/ik4ROKUl++8NnUFfxun5rdUTAoOkTjMydzO7e+q+S0Vm4JDpBh2Z+fy2IRFvDxFzW8RBYfIQUyYt547R89R81skoOAQKcLa7dEzv8fNVfNbJJaCQ6QANb9FDkzBIRJjTmb0zO9Zq9X8FilKwv4LZWYjzGyDmc2JGWtkZhPMbHHw80f7/WbWw8ymmtlcM5tlZr+IeewVM1tuZjOCW49E1S8Vy+7sXO75YB7nPfMla7dn8cygnrzyq2MVGiKFSOS+9ytA3wJjtwET3b0TMDG4X9Ae4Ap3PzLY/gkzaxDz+M3u3iO4zUhA3VLBTJi3np8+9jkjpixn0HFpfHLjqfy8e0t9Y0qkCAk7VOXuk82sXYHh/sBpwfKrwCTg1gLbLYpZXmNmG4CmwLYElSoVlJrfIoemrHsczdx9bbC8Dmh2oJXNrDdQDVgaM3yfmf2FYI/F3bMTUqmUWwWb37f2Teeqk9X8Fimu0Jrj7u5m5kU9bmYtgH8Ag909Pxi+nWjgVAOGE91bubuI7YcCQwHS0tJKsXJJZQWb3/f0P4q0xupjiMSjrINjvZm1cPe1QTBsKGwlM6sHfAjc4e5f7R+P2VvJNrOXgZuKeiF3H040XIhEIkUGlFQMsWd+N6pdnacv7cnPu+vMb5FDUdbBMQYYDDwY/BxdcAUzqwa8B7zm7m8XeGx/6BgwAJhTcHuRgvaf+b1me/TM71v66sxvkZJIWHCY2UiijfAmZrYauJNoYIwysyHASuCSYN0IcI27XxWMnQI0NrMrg6e7MvgG1b/MrCnRa57PAK5JVP2S+mKb30c0q8s7g3pyTNtGYZclkvLMvfwfxYlEIp6RkRF2GVJGCja/rz+js5rfIofAzKa5e6TguM4cl3Iltvl9Suem3Kvmt0ipU3BIubA7O5fHJyxihJrfIgmn4JCU98m89fxFzW+RMqPgkJS1bnsWd42Zy8dz16n5LVKGFByScvLynX9MXcHfxi8iNz9fZ36LlDEFh6QUNb9FwqfgkJRQsPn91KU9OVfNb5FQKDgk6cU2vwcdl8atan6LhErBIUlLzW+R5KTgkKQT2/zOycvnlr5HcPXJHdT8FkkSCg5JKmp+iyQ/BYckBTW/RVKHgkNC96Pm91np1K+l5rdIslJwSGhim9+dm9XhnUEnqPktkgIUHFLmCmt+X3VSB6pVUfNbJBUoOKRMzV2znT++O5uZq7dzcqcm3Degm5rfIilGwSFlYnd2Lk98sogRU1bQsFY1Nb9FUpiCQxJu4vz1/GX0XDK37VXzW6QcUHBIwqzbnsVf/z2XsXOize+3rzmBSDs1v0VSnYJDSl1evvPPr1byyLiFan6LlEMKDilVBZvf9w44iraNa4ddloiUooT+F9DMRpjZBjObEzPWyMwmmNni4GfDIrYdHKyz2MwGx4wfY2azzWyJmT1l6q4mhc27srlz9BzOe2YKmdv28uTAHrz2694KDZFyKNHHDl4B+hYYuw2Y6O6dgInB/R8ws0bAncBxQG/gzpiAeQ64GugU3Ao+v5ShrJw8nv1sCac9Mol/fv09l/Zuw8QbT6N/j1b6xpRIOZXQQ1XuPtnM2hUY7g+cFiy/CkwCbi2wzlnABHffAmBmE4C+ZjYJqOfuXwXjrwEDgLGlX70cSH6+8970TB4dv5A127M4s0szbuuXzuGH1Qm7NBFJsDB6HM3cfW2wvA5oVsg6rYBVMfdXB2OtguWC4z9iZkOBoQBpaWklLFliTVmyifs/ms/cNTvo3ro+j/2iB8d3aBx2WSJSRkJtjru7m5kn6LmHA8MBIpFIQl6jolm0ficPfDSfzxZupFWDmjw5sAfndm9JpUo6JCVSkYQRHOvNrIW7rzWzFsCGQtbJ5L+HswBaEz2klRksx45nJqhOCWzYkcXjnyzizW9XUbt6FW7vl87gPu2oUbVy2KWJSAjCCI4xwGDgweDn6ELWGQfcH9MQ/xlwu7tvMbMdZnY88DVwBfB0GdRcIe3OzuWFL5YxfPIycvLyubJPe35/+uE0rF0t7NJEJEQJDQ4zG0l0z6GJma0m+k2pB4FRZjYEWAlcEqwbAa5x96uCgLgH+DZ4qrv3N8qB3xH9tlZNok1xNcZLWV6+81bGKh6dsIiNO7M5u1tzbjkrnXZN9NVaEQFzL/+H/yORiGdkZIRdRtJzdyYt2sgDH81n0fpd9EprwB3ndNE1MkQqKDOb5u6RguM6c1yA6LW+Hxg7nylLNtO2cS2eu6wXfY9qrnMxRORHFBwV3Jpte/nb+IW8Nz2TBjWrcue5XbnsuLaaV0pEiqTgqKB2ZuXw3KSlvPTlchwYekoHfnfa4dSvqenOReTAFBwVTE5ePiO/+Z4nP1nM5t37GNCjJTeddQStG+oqfCJSPAqOCsLdGT9vPQ+NXcCyTbs5vkMjXj67C91bNwi7NBFJMQqOCmD691u5/6P5fLtiKx2b1ubFKyKc0eUwNb5F5JAoOMqxVVv28NDHC/hg1lqa1KnGvQOOYuCxbahSWY1vETl0Co5yaNuefTzz6RJem7qSSpXgD6cfztBTO1Knuv66RaTk9JukHMnOzeMfU1fy9KdL2JGVw8XHtObGnx5B8/o1wi5NRMoRBUc54O58MGstD49bwKotezmlc1Nu75dOlxb1wi5NRMohBUeK+3bFFu77cD4zVm0jvXldXvt1b07p3DTsskSkHFNwpKhlG3fx0McLGDd3Pc3qVefhi7pzYa/WVNa1MUQkwRQcKWbzrmyenLiY17/+nupVKjHsp5256uQO1Kyma2OISNlQcKSIrJw8XvpyOc9NWsrenDwGHtuGG87sTNO61cMuTUQqGAVHksvPd96bnsmj4xeyZnsWZ3Y5jNv6pXP4YXXDLk1EKigFRxKbsmQT9380n7lrdtCtVX0evaQHJ3RsHHZZIlLBKTiS0KL1O3ngo/l8tnAjrRrU5MmBPTi3e0sqqfEtIklAwZFENuzI4vFPFvHmt6uoXb0Kt/dLZ3CfdtSoqsa3iCQPBUcS2J2dywtfLGP45GXsy81ncJ92/OH0TjSsXS3s0kREfkTBEaK8fOetjFU8OmERG3dmc3a35txyVjrtmtQOuzQRkSKFEhxmdj1wNWDAC+7+RIHHbwYuC+5WAboATd19i5mtAHYCeUBuYRdST3buzqSFG3lg7HwWrd9Fr7QGPH95L45p2yjs0kREDqrMg8PMjiIaGr2BfcDHZvaBuy/Zv467PwI8Eqx/LvA/7r4l5ml+4u6byrDsUjMnczsPjJ3PlCWbadu4Fs9d1ou+RzXXtTFEJGWEscfRBfja3fcAmNnnwAXAw0WsfykwsoxqS5g12/byt/ELeW96JvVrVuXOc7ty2XFtqVZF18YQkdQSRnDMAe4zs8bAXuBsIKOwFc2sFtAXuC5m2IHxZubA3919eBHbDgWGAqSlpZVe9XHakZXD85OW8tKXy3Fg6Ckd+N1ph1O/ZtXQahIRKYkyDw53n29mDwHjgd3ADKL9isKcC0wpcJjqJHfPNLPDgAlmtsDdJxfyOsOB4QCRSMRL9Q9RDDl5+Yz85nue+GQxW3bvY0CPltx01hG0blirrEsRESlVoTTH3f0l4CUAM7sfWF3EqgMpcJjK3TODnxvM7D2ivZIfBUdY3J3x89bz0NgFLNu0m+M7NOKPZ3ehe+sGYZcmIlIqwvpW1WHBL/40ov2N4wtZpz5wKnB5zFhtoJK77wyWfwbcXUZlH9T077dy/0fz+XbFVjo2rc2LV0Q4o8thanyLSLkS1nkc7wQ9jhzgWnffZmbXALj788E65wPj3X13zHbNgPeCX8RVgNfd/eMyrLtQ32/ew8PjFvDBrLU0qVONewccxcBj21ClshrfIlL+mHuZH50XMXkAAAdYSURBVP4vc5FIxDMyCu2/l8i2Pft45tMlvDp1BZUrGVef3IHfnNqROtV1XqWIpD4zm1bYuXL6DXcIsnPz+MfUlTz96RJ2ZOVwUa/WDPvZETSvXyPs0kREEk7BEQd354NZa3l43AJWbdnLyZ2acHu/LnRtWS/s0kREyoyCo5i+Wb6F+z6az8xV20hvXpfXft2bUzo3DbssEZEyp+A4iGUbd/Hg2AWMn7eeZvWq8/BF3bmwV2sq69oYIlJBKTgO4KmJi3lq4mKqV6nEsJ92ZsjJ7alVTW+ZiFRs+i14AG0a1eQXx7bhhjM707Ru9bDLERFJCgqOAzi/Z2vO79k67DJERJKKzlATEZG4KDhERCQuCg4REYmLgkNEROKi4BARkbgoOEREJC4KDhERiYuCQ0RE4lIhrsdhZhuBlYe4eRNgUymWU1pUV3xUV3xUV3yStS4oWW1t3f1Hs7lWiOAoCTPLKOxCJmFTXfFRXfFRXfFJ1rogMbXpUJWIiMRFwSEiInFRcBzc8LALKILqio/qio/qik+y1gUJqE09DhERiYv2OEREJC4KDhERiUuFDg4z62tmC81siZndVsjj1c3szeDxr82sXcxjtwfjC83srGSoy8zamdleM5sR3J4v47pOMbPvzCzXzC4q8NhgM1sc3AYnUV15Me/XmDKu60Yzm2dms8xsopm1jXkszPfrQHWF+X5dY2azg9f+0sy6xjwW5uex0LrC/jzGrHehmbmZRWLGSvZ+uXuFvAGVgaVAB6AaMBPoWmCd3wHPB8sDgTeD5a7B+tWB9sHzVE6CutoBc0J8v9oB3YHXgItixhsBy4KfDYPlhmHXFTy2K8T36ydArWD5tzF/j2G/X4XWlQTvV72Y5fOAj4PlsD+PRdUV6ucxWK8uMBn4CoiU1vtVkfc4egNL3H2Zu+8D3gD6F1inP/BqsPw2cIaZWTD+hrtnu/tyYEnwfGHXlUgHrcvdV7j7LCC/wLZnARPcfYu7bwUmAH2ToK5EKk5dn7n7nuDuV8D+6xSH/X4VVVciFaeuHTF3awP7v9kT6ufxAHUlUnF+TwDcAzwEZMWMlfj9qsjB0QpYFXN/dTBW6DrungtsBxoXc9sw6gJob2bTzexzMzu5lGoqbl2J2DbRz13DzDLM7CszG1BKNR1KXUOAsYe4bVnVBSG/X2Z2rZktBR4G/hDPtiHUBSF+Hs2sF9DG3T+Md9uDqRLPypL01gJp7r7ZzI4B3jezIwv8j0h+qK27Z5pZB+BTM5vt7kvLsgAzuxyIAKeW5eseTBF1hfp+ufuzwLNmNgj4E1Cq/Z9DVURdoX0ezawS8BhwZSKevyLvcWQCbWLutw7GCl3HzKoA9YHNxdy2zOsKdj03A7j7NKLHLjuXYV2J2Dahz+3umcHPZcAkoGdZ1mVmZwJ3AOe5e3Y824ZQV+jvV4w3gP17PKG/X4XVFfLnsS5wFDDJzFYAxwNjggZ5yd+vRDRuUuFGdG9rGdHm0P7m0pEF1rmWHzahRwXLR/LD5tIySq8ZV5K6mu6vg2jTLBNoVFZ1xaz7Cj9uji8n2uhtGCwnQ10NgerBchNgMYU0GBP499iT6C+TTgXGQ32/DlBX2O9Xp5jlc4GMYDnsz2NRdSXF5zFYfxL/bY6X+P0q8R8glW/A2cCi4ENyRzB2N9H/ZQHUAN4i2jz6BugQs+0dwXYLgX7JUBdwITAXmAF8B5xbxnUdS/R46W6ie2ZzY7b9dVDvEuBXyVAX0AeYHXyIZgNDyriuT4D1wd/XDGBMkrxfhdaVBO/XkzH/vj8j5hdlyJ/HQusK+/NYYN1JBMFRGu+XphwREZG4VOQeh4iIHAIFh4iIxEXBISIicVFwiIhIXBQcIiISFwWHSCkwszvMbG4wo+wMMzvOzF6MncFVpLzQ13FFSsjMTiA6vcNp7p5tZk2Aau6+JuTSRBJCexwiJdcC2OTB1Bzuvsnd15jZpP3XQDCzIWa2yMy+MbMXzOyZYPwVM3sumDRwmZmdZmYjzGy+mb2y/wWCdTKCvZq/hvGHFNlPwSFScuOBNkEw/K+Z/WCyQjNrCfyZ6HxBJwLpBbZvCJwA/A8wBnic6LQQ3cysR7DOHe4eIXpdkVPNrHvC/jQiB6HgECkhd98FHAMMBTYCb5rZlTGr9AY+9+j1NXKIThcT698ePWY8G1jv7rPdPZ/odBXtgnUuMbPvgOlEQ0W9EwmNplUXKQXunkd0PqBJZjab+Kb73j/7bH7M8v77VcysPXATcKy7bw0OYdUocdEih0h7HCIlZGZHmFmnmKEewMqY+98SPbzUMJgG/8I4X6Ie0Qkat5tZM6BfiQoWKSHtcYiUXB3gaTNrAOQSndF2KNHL+uLRCx/dT3Qm4y3AAqJXbSwWd59pZtOD7VYBU0q3fJH46Ou4ImXAzOq4+65gj+M9YIS7vxd2XSKHQoeqRMrGXWY2A5hD9MJM74dcj8gh0x6HiIjERXscIiISFwWHiIjERcEhIiJxUXCIiEhcFBwiIhKX/wMk1CheNx5W4wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "3b564d6a-8e79-48a5-d27e-80d7550f8e72"
      },
      "source": [
        "def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "    if fun(large) - target < 0:\n",
        "        print('upper bound is too small')\n",
        "        return None\n",
        "    if fun(small) - target > 0:\n",
        "        print('lower bound is too large')\n",
        "        return None\n",
        "    while large - small > EPS:\n",
        "        mid = (large + small) / 2.0\n",
        "        if fun(mid) - target >= 0:\n",
        "            large = mid\n",
        "        else:\n",
        "            small = mid\n",
        "    mid = (large + small) / 2.0\n",
        "    return mid, abs(fun(mid) - target)\n",
        "quoted_price = 16.0\n",
        "sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "upper bound is too small\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ade689496c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mquoted_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbisection_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoted_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'implied volativity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    }
  ]
}