{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid Test_Knock Out Call 1stock Monte Carlo",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Lilian/Grid_Test_European_Call_3stock_OldMethod_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYigDkiy0HU9",
        "outputId": "7a74cbe6-a24e-415f-b103-d0b2ae062960"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 10)\n",
        "K_range = jnp.linspace(0.75, 1.25, 8)\n",
        "B_range = jnp.linspace(0.5, 1.0, 8)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "\n",
        "print(S_range)\n",
        "print(K_range)\n",
        "print(B_range)\n",
        "print(sigma_range)\n",
        "print(r_range)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75       0.8055556  0.86111116 0.9166666  0.97222227 1.0277778\n",
            " 1.0833334  1.138889   1.1944445  1.25      ]\n",
            "[0.75       0.82142854 0.89285713 0.9642857  1.0357143  1.1071429\n",
            " 1.1785713  1.25      ]\n",
            "[0.5        0.5714286  0.6428572  0.71428573 0.78571427 0.85714287\n",
            " 0.92857146 1.        ]\n",
            "[0.15       0.25       0.35000002 0.45      ]\n",
            "[0.01  0.025 0.04 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQxpJqK6OZr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "504a0dec-e664-45bb-9c52-e541593694a1"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "goptionvalueavg = jax.grad(optionvalueavg, argnums=1)\n",
        "\n",
        "#################################################################### Adjust all parameters here (not inside class)\n",
        "numstocks = 3\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "\n",
        "S1_range = jnp.linspace(0.75, 1.25, 6)\n",
        "S2_range = jnp.linspace(0.75, 1.25, 6)\n",
        "S3_range = jnp.linspace(0.75, 1.25, 6)\n",
        "K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 3)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "T = 1.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "####################################################################\n",
        "\n",
        "call = []\n",
        "count = 0\n",
        "\n",
        "for S1 in S1_range:\n",
        "  for S2 in S2_range:\n",
        "    for S3 in S3_range:\n",
        "      for K in K_range:\n",
        "        for r in r_range:\n",
        "          for sigma in sigma_range:\n",
        "            \n",
        "            initial_stocks = jnp.array([S1, S2, S3]) # must be float\n",
        "            r_tmp = jnp.array([r]*numstocks)\n",
        "            drift = r_tmp\n",
        "            cov = jnp.identity(numstocks)*sigma*sigma\n",
        "            \n",
        "            European_Call_price = optionvalueavg(key, initial_stocks, numsteps, drift, r_tmp, cov, K, T)\n",
        "            Deltas = goptionvalueavg(keys, initial_stocks, numsteps, drift, r_tmp, cov, K, T)\n",
        "            call.append([T, K, S1, sigma, r, r,\n",
        "                         T, K, S2, sigma, r, r,\n",
        "                         T, K, S3, sigma, r, r, European_Call_price] + list(Deltas)) #T, K, S, sigma, mu, r, price, delta\n",
        "                         \n",
        "            count += 1\n",
        "            print(count)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1d4f0ca291f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             call.append([T, K, S1, sigma, r, r,\n\u001b[1;32m     70\u001b[0m                          \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                          T, K, S3, sigma, r, r, European_Call_price] + list(Deltas)) #T, K, S, sigma, mu, r, price, delta\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1597\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration over a 0-d array\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# same as numpy error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_OUtP8GUwj5"
      },
      "source": [
        "Thedataset = pd.DataFrame(call)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "skGWSSsG8TGG",
        "outputId": "ef7dd0e2-6c98-4072-cfdf-7b3550dafbe2"
      },
      "source": [
        "# read csv\n",
        "import pandas as pd\n",
        "\n",
        "Thedataset.to_csv('European_Call_3stock_MC_Datset.csv', index=False, header=False)\n",
        "Thedataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3300512bad05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mThedataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'European_Call_3stock_MC_Datset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mThedataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Thedataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2XdDnpLjYTk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "3a12b6f7-036b-46d4-eea0-1736d96d346c"
      },
      "source": [
        "Thedataset = pd.read_csv('European_Call_3stock_MC_Datset.csv', header=None)\n",
        "Thedataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.029766</td>\n",
              "      <td>0.187636</td>\n",
              "      <td>0.187619</td>\n",
              "      <td>0.187619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.056042</td>\n",
              "      <td>0.185698</td>\n",
              "      <td>0.185641</td>\n",
              "      <td>0.185651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.083269</td>\n",
              "      <td>0.188792</td>\n",
              "      <td>0.188706</td>\n",
              "      <td>0.188727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.035984</td>\n",
              "      <td>0.209865</td>\n",
              "      <td>0.209841</td>\n",
              "      <td>0.209858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.061615</td>\n",
              "      <td>0.196833</td>\n",
              "      <td>0.196782</td>\n",
              "      <td>0.196802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9715</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.102692</td>\n",
              "      <td>0.196833</td>\n",
              "      <td>0.196783</td>\n",
              "      <td>0.196802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9716</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.147457</td>\n",
              "      <td>0.196091</td>\n",
              "      <td>0.195995</td>\n",
              "      <td>0.196037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9717</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.071392</td>\n",
              "      <td>0.230893</td>\n",
              "      <td>0.230873</td>\n",
              "      <td>0.230878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9718</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.112457</td>\n",
              "      <td>0.207721</td>\n",
              "      <td>0.207684</td>\n",
              "      <td>0.207704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9719</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.156412</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.203308</td>\n",
              "      <td>0.203333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9720 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0     1     2     3   ...        18        19        20        21\n",
              "0     1.0  0.75  0.75  0.15  ...  0.029766  0.187636  0.187619  0.187619\n",
              "1     1.0  0.75  0.75  0.30  ...  0.056042  0.185698  0.185641  0.185651\n",
              "2     1.0  0.75  0.75  0.45  ...  0.083269  0.188792  0.188706  0.188727\n",
              "3     1.0  0.75  0.75  0.15  ...  0.035984  0.209865  0.209841  0.209858\n",
              "4     1.0  0.75  0.75  0.30  ...  0.061615  0.196833  0.196782  0.196802\n",
              "...   ...   ...   ...   ...  ...       ...       ...       ...       ...\n",
              "9715  1.0  1.25  1.25  0.30  ...  0.102692  0.196833  0.196783  0.196802\n",
              "9716  1.0  1.25  1.25  0.45  ...  0.147457  0.196091  0.195995  0.196037\n",
              "9717  1.0  1.25  1.25  0.15  ...  0.071392  0.230893  0.230873  0.230878\n",
              "9718  1.0  1.25  1.25  0.30  ...  0.112457  0.207721  0.207684  0.207704\n",
              "9719  1.0  1.25  1.25  0.45  ...  0.156412  0.203386  0.203308  0.203333\n",
              "\n",
              "[9720 rows x 22 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4HV-G44th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4426b333-2b0d-43e4-da53-540e93f08434"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch\n",
        "torch.set_printoptions(precision=6)\n",
        "\n",
        "Thedataset_X = Thedataset.iloc[:,:18]\n",
        "Thedataset_Y = Thedataset.iloc[:,18:]\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.X = cupy.array(Thedataset_X)\n",
        "        self.Y = cupy.array(Thedataset_Y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(self.X.toDlpack()), from_dlpack(self.Y.toDlpack()))\n",
        "\n",
        "# print\n",
        "ds = OptionDataSet(max_len = 1)\n",
        "for i in ds:\n",
        "    print(i[0])\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    print(i[1].shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.000000, 0.750000, 0.750000,  ..., 0.150000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.750000,  ..., 0.300000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.750000,  ..., 0.450000, 0.010000, 0.010000],\n",
            "        ...,\n",
            "        [1.000000, 1.250000, 1.250000,  ..., 0.150000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.250000,  ..., 0.300000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.250000,  ..., 0.450000, 0.040000, 0.040000]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([9720, 18])\n",
            "tensor([[0.029766, 0.187636, 0.187619, 0.187619],\n",
            "        [0.056042, 0.185698, 0.185641, 0.185651],\n",
            "        [0.083269, 0.188792, 0.188706, 0.188727],\n",
            "        ...,\n",
            "        [0.071392, 0.230893, 0.230873, 0.230878],\n",
            "        [0.112457, 0.207721, 0.207684, 0.207704],\n",
            "        [0.156412, 0.203386, 0.203308, 0.203333]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "torch.Size([9720, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "7e7a68d0-7184-4952-efb8-25f46175d3f2"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        # self.fc1 = nn.Linear(7*1, 32) # remember to change this!\n",
        "        self.fc1 = nn.Linear(6*3, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 256)\n",
        "        self.fc4 = nn.Linear(256, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 4) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.03, 0.03]*3)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, B, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.01, 0.01]*3).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "cacbb82d-f766-481d-da4c-f3ac23c91c93"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 12.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeLVZiiaDS4y",
        "outputId": "5f9e0846-6c8e-453d-b37f-371603214c4f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CyULkENYKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "411e1b12-5b75-4e1b-a9d7-00df12844194"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    # print(x.shape)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "    y_pred = model(x.float())\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    # def compute_deltas(x):\n",
        "    #   inputs = x.float()\n",
        "    #   inputs.requires_grad = True\n",
        "    #   first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "    #   return first_order_gradient[0][[2, 8, 14]]  # Now index 3 is stock price, not 2\n",
        "\n",
        "    # deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    # # print(deltas)\n",
        "    # y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # # print(y_pred)\n",
        "    # # print(y)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1, 1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_european_3stock_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Lilian/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.016156154709703153 average time 0.008711431699987315 iter num 20\n",
            "loss 0.010894140871625036 average time 0.006230035400011502 iter num 40\n",
            "loss 0.009631863468864719 average time 0.0053439163833445495 iter num 60\n",
            "loss 0.009007450139460672 average time 0.004958109375016307 iter num 80\n",
            "loss 0.008871543757792824 average time 0.004667205780012864 iter num 100\n",
            "loss 0.006095698591731227 average time 0.0036867864499981807 iter num 20\n",
            "loss 0.003552399565679286 average time 0.0036299903500037088 iter num 40\n",
            "loss 0.0023759702676999332 average time 0.0035947989999992086 iter num 60\n",
            "loss 0.0020612987695895688 average time 0.003615307724999184 iter num 80\n",
            "loss 0.002011524451928398 average time 0.0036464991999969245 iter num 100\n",
            "loss 0.0014826070685170889 average time 0.0035664201500139824 iter num 20\n",
            "loss 0.001320847049257483 average time 0.004000073749995181 iter num 40\n",
            "loss 0.0012695038962385378 average time 0.00408779298332244 iter num 60\n",
            "loss 0.001254445293016662 average time 0.004023168224992446 iter num 80\n",
            "loss 0.0012517245701509326 average time 0.003965933299991775 iter num 100\n",
            "loss 0.0012126340764774927 average time 0.004043538649989386 iter num 20\n",
            "loss 0.0011930851731744706 average time 0.004073110100000577 iter num 40\n",
            "loss 0.001183963882154441 average time 0.004186077349993411 iter num 60\n",
            "loss 0.001180290538672611 average time 0.0042457916000017805 iter num 80\n",
            "loss 0.0011795394302825072 average time 0.004300100800001019 iter num 100\n",
            "loss 0.0011659462253353447 average time 0.003719581800044125 iter num 20\n",
            "loss 0.0011548910984342975 average time 0.003806530300022359 iter num 40\n",
            "loss 0.0011479074922401255 average time 0.0037612365166864946 iter num 60\n",
            "loss 0.0011447788757119857 average time 0.0037874858750171826 iter num 80\n",
            "loss 0.001144120682601528 average time 0.0038094454200108883 iter num 100\n",
            "loss 0.0011317614673028913 average time 0.003633237800011102 iter num 20\n",
            "loss 0.0011211991094844816 average time 0.0037731632000088666 iter num 40\n",
            "loss 0.0011144232952147318 average time 0.0037803654666655954 iter num 60\n",
            "loss 0.0011113816436213752 average time 0.0038281070750002754 iter num 80\n",
            "loss 0.00111074249720928 average time 0.003867049479997604 iter num 100\n",
            "loss 0.0010987538358595652 average time 0.004501503900030457 iter num 20\n",
            "loss 0.001088494726268487 average time 0.004114861825013349 iter num 40\n",
            "loss 0.001081910539869569 average time 0.0038926746666675172 iter num 60\n",
            "loss 0.001078955846325694 average time 0.0038208957250077447 iter num 80\n",
            "loss 0.0010783357003954614 average time 0.00382945239999799 iter num 100\n",
            "loss 0.0010667068297051645 average time 0.0038737761500101443 iter num 20\n",
            "loss 0.0010567308539241654 average time 0.003947960800007877 iter num 40\n",
            "loss 0.0010503110811717628 average time 0.0038750496833282948 iter num 60\n",
            "loss 0.0010474254523209648 average time 0.003844515724998132 iter num 80\n",
            "loss 0.0010468196767418614 average time 0.003897559830004411 iter num 100\n",
            "loss 0.0010354423714260307 average time 0.004303831400011404 iter num 20\n",
            "loss 0.0010256171186948505 average time 0.004168294275018525 iter num 40\n",
            "loss 0.0010192503046772454 average time 0.003994388816677959 iter num 60\n",
            "loss 0.0010163751938234368 average time 0.00390855651251627 iter num 80\n",
            "loss 0.0010157706311086515 average time 0.0038818568100145966 iter num 100\n",
            "loss 0.001004362404591207 average time 0.00393846814997687 iter num 20\n",
            "loss 0.0009943828168099199 average time 0.003927320274988233 iter num 40\n",
            "loss 0.0009878334541523644 average time 0.0039839662166590035 iter num 60\n",
            "loss 0.0009848512346155747 average time 0.0039740401249929395 iter num 80\n",
            "loss 0.0009842219622675035 average time 0.003913935499990657 iter num 100\n",
            "loss 0.0009722544253934381 average time 0.0036438478999912148 iter num 20\n",
            "loss 0.0009615840789020964 average time 0.0036822846999996274 iter num 40\n",
            "loss 0.0009544562306034346 average time 0.0037541855499853226 iter num 60\n",
            "loss 0.0009511739113033158 average time 0.003744831924979053 iter num 80\n",
            "loss 0.0009504782284306184 average time 0.003765372469979411 iter num 100\n",
            "loss 0.0009371132306412293 average time 0.003631689099995583 iter num 20\n",
            "loss 0.0009249229357467581 average time 0.0035846903249989737 iter num 40\n",
            "loss 0.0009166161490789066 average time 0.0036197637833273195 iter num 60\n",
            "loss 0.0009127447587968089 average time 0.0037175068499891496 iter num 80\n",
            "loss 0.000911920494943205 average time 0.0038015810699926077 iter num 100\n",
            "loss 0.0008959238173743774 average time 0.004091813450020254 iter num 20\n",
            "loss 0.0008810065501341722 average time 0.003917211149996547 iter num 40\n",
            "loss 0.0008706533307465958 average time 0.003848507599995325 iter num 60\n",
            "loss 0.0008657774261848717 average time 0.0038411652250005092 iter num 80\n",
            "loss 0.0008647353944120102 average time 0.0038207885199972227 iter num 100\n",
            "loss 0.0008443519424168487 average time 0.003536252449998756 iter num 20\n",
            "loss 0.0008250195994252371 average time 0.003601574074986047 iter num 40\n",
            "loss 0.0008114407441270436 average time 0.003864828033329104 iter num 60\n",
            "loss 0.0008050123328381813 average time 0.003928321437493309 iter num 80\n",
            "loss 0.0008036372453921335 average time 0.003901057329994728 iter num 100\n",
            "loss 0.0007767095562010081 average time 0.0036340677499993035 iter num 20\n",
            "loss 0.0007511943494763798 average time 0.0035836210500008294 iter num 40\n",
            "loss 0.0007334275257410618 average time 0.003631780766659176 iter num 60\n",
            "loss 0.0007251030783222622 average time 0.003648554099996204 iter num 80\n",
            "loss 0.0007233340299780645 average time 0.003649574659996233 iter num 100\n",
            "loss 0.0006892743006013762 average time 0.0039048068000056444 iter num 20\n",
            "loss 0.0006584260233378895 average time 0.0038416941749972013 iter num 40\n",
            "loss 0.0006380816395172728 average time 0.00392105981665812 iter num 60\n",
            "loss 0.0006289316361040693 average time 0.0038551288999968845 iter num 80\n",
            "loss 0.0006270253436402534 average time 0.0038010134499950254 iter num 100\n",
            "loss 0.0005918636993762043 average time 0.0036061899499713946 iter num 20\n",
            "loss 0.0005627476828261339 average time 0.003732313274986154 iter num 40\n",
            "loss 0.0005449099093918478 average time 0.003779461849993974 iter num 60\n",
            "loss 0.0005372102960309532 average time 0.003771354737494903 iter num 80\n",
            "loss 0.000535632521749932 average time 0.003973734909998257 iter num 100\n",
            "loss 0.0005073406992350689 average time 0.004121722599961686 iter num 20\n",
            "loss 0.0004849959113479161 average time 0.003960060674995702 iter num 40\n",
            "loss 0.0004716591109069458 average time 0.0039422527666602035 iter num 60\n",
            "loss 0.0004659473843550017 average time 0.003999627837507092 iter num 80\n",
            "loss 0.00046477808814694244 average time 0.004012092100006157 iter num 100\n",
            "loss 0.00044373923910742214 average time 0.0037165556999866566 iter num 20\n",
            "loss 0.0004268096812634209 average time 0.003647007949973613 iter num 40\n",
            "loss 0.0004164075323397962 average time 0.003662178599984145 iter num 60\n",
            "loss 0.0004118486073885573 average time 0.003669572299989454 iter num 80\n",
            "loss 0.0004109044036426071 average time 0.0036446866299888825 iter num 100\n",
            "loss 0.000393500073513524 average time 0.0038437629499640026 iter num 20\n",
            "loss 0.00037875775768097684 average time 0.004018015374964534 iter num 40\n",
            "loss 0.00036929603867366277 average time 0.004099037283295578 iter num 60\n",
            "loss 0.0003650440702087224 average time 0.004253183674973115 iter num 80\n",
            "loss 0.00036415440565541176 average time 0.004236507629980224 iter num 100\n",
            "loss 0.00034744299973903667 average time 0.004423545849988386 iter num 20\n",
            "loss 0.0003327926500582406 average time 0.0040962252249642 iter num 40\n",
            "loss 0.0003231669237563728 average time 0.004022173999972741 iter num 60\n",
            "loss 0.00031879379695615124 average time 0.003939746924984888 iter num 80\n",
            "loss 0.0003178753384340499 average time 0.003952558179985317 iter num 100\n",
            "loss 0.00030052635182642413 average time 0.00404024894999111 iter num 20\n",
            "loss 0.0002852098774294471 average time 0.004016396049996729 iter num 40\n",
            "loss 0.0002751462776251956 average time 0.004070327166664356 iter num 60\n",
            "loss 0.000270588927393276 average time 0.004122942137499308 iter num 80\n",
            "loss 0.0002696340568732427 average time 0.0041655194699933415 iter num 100\n",
            "loss 0.00025171815016053326 average time 0.004473433900011514 iter num 20\n",
            "loss 0.00023618736651780084 average time 0.004143581900001437 iter num 40\n",
            "loss 0.00022620339768052172 average time 0.004007626633324435 iter num 60\n",
            "loss 0.00022175620262127925 average time 0.0040025121749920345 iter num 80\n",
            "loss 0.00022083177629446415 average time 0.003980074669984787 iter num 100\n",
            "loss 0.0002037958784055231 average time 0.003946668249977847 iter num 20\n",
            "loss 0.00018966205604067124 average time 0.0038338064749723344 iter num 40\n",
            "loss 0.00018096439509646204 average time 0.0037595607166546565 iter num 60\n",
            "loss 0.00017719997761874332 average time 0.003760540324987005 iter num 80\n",
            "loss 0.0001764273296513426 average time 0.003735881329989752 iter num 100\n",
            "loss 0.00016254342169224307 average time 0.0038308735499754222 iter num 20\n",
            "loss 0.00015161378953171031 average time 0.003706914774983261 iter num 40\n",
            "loss 0.00014517584187389128 average time 0.0038483903166631234 iter num 60\n",
            "loss 0.00014245728704234773 average time 0.0038242419874904955 iter num 80\n",
            "loss 0.00014190448840251303 average time 0.003803221129990106 iter num 100\n",
            "loss 0.00013212479151400696 average time 0.004098575900013657 iter num 20\n",
            "loss 0.0001246072789393245 average time 0.003987698575002696 iter num 40\n",
            "loss 0.00012020848585725507 average time 0.004012805450001148 iter num 60\n",
            "loss 0.00011834351954477222 average time 0.0039154718625042054 iter num 80\n",
            "loss 0.00011796278551915213 average time 0.0038703105800095726 iter num 100\n",
            "loss 0.00011114469624686068 average time 0.0038380047000259763 iter num 20\n",
            "loss 0.0001057173403927338 average time 0.0038288397250198615 iter num 40\n",
            "loss 0.00010241857093565748 average time 0.003809697366682485 iter num 60\n",
            "loss 0.00010098498965099904 average time 0.0037997681125034434 iter num 80\n",
            "loss 0.00010068918618662185 average time 0.0038054592299999967 iter num 100\n",
            "loss 9.528484078205104e-05 average time 0.004126940399964951 iter num 20\n",
            "loss 9.0812238790267e-05 average time 0.004011172750006153 iter num 40\n",
            "loss 8.80132612988465e-05 average time 0.00386577118333283 iter num 60\n",
            "loss 8.677748970517489e-05 average time 0.00382294071249305 iter num 80\n",
            "loss 8.652090018685737e-05 average time 0.003804217509989485 iter num 100\n",
            "loss 8.178040368776146e-05 average time 0.0036835616500184186 iter num 20\n",
            "loss 7.77750564341108e-05 average time 0.0037470850750366937 iter num 40\n",
            "loss 7.523143572002648e-05 average time 0.0038960513833709836 iter num 60\n",
            "loss 7.409940414564776e-05 average time 0.003918230600035599 iter num 80\n",
            "loss 7.386360983369336e-05 average time 0.00396936760002518 iter num 100\n",
            "loss 6.948189390245002e-05 average time 0.004269711550023203 iter num 20\n",
            "loss 6.574129587057143e-05 average time 0.004023668200005659 iter num 40\n",
            "loss 6.335088798752178e-05 average time 0.003989845433333509 iter num 60\n",
            "loss 6.228549082913174e-05 average time 0.004075746800000957 iter num 80\n",
            "loss 6.206359592210336e-05 average time 0.0040454902700003004 iter num 100\n",
            "loss 5.79479661838734e-05 average time 0.0040684688999817805 iter num 20\n",
            "loss 5.446149310976928e-05 average time 0.00399160174998201 iter num 40\n",
            "loss 5.2256256085782316e-05 average time 0.00386423358332119 iter num 60\n",
            "loss 5.128119650081615e-05 average time 0.003853404537488814 iter num 80\n",
            "loss 5.1078892726782026e-05 average time 0.0038939443199910782 iter num 100\n",
            "loss 4.735837495574744e-05 average time 0.0035036759999911736 iter num 20\n",
            "loss 4.42711547609917e-05 average time 0.0035504753249995247 iter num 40\n",
            "loss 4.235863282933617e-05 average time 0.0035566939833339955 iter num 60\n",
            "loss 4.152465396544533e-05 average time 0.0035607776750026686 iter num 80\n",
            "loss 4.135268297684861e-05 average time 0.0035607300199944803 iter num 100\n",
            "loss 3.8228686416119854e-05 average time 0.003611930049999046 iter num 20\n",
            "loss 3.57026196889554e-05 average time 0.003639612850014373 iter num 40\n",
            "loss 3.417342224339926e-05 average time 0.003629904650017579 iter num 60\n",
            "loss 3.351585235051507e-05 average time 0.0037721021750087404 iter num 80\n",
            "loss 3.338105901016495e-05 average time 0.0038927934300068048 iter num 100\n",
            "loss 3.095945490551476e-05 average time 0.003990825500000028 iter num 20\n",
            "loss 2.9043172386803728e-05 average time 0.003820876525008998 iter num 40\n",
            "loss 2.7902129224743077e-05 average time 0.0037751614833458308 iter num 60\n",
            "loss 2.741585559590432e-05 average time 0.003761551012510722 iter num 80\n",
            "loss 2.7316537549307195e-05 average time 0.0037402377400076146 iter num 100\n",
            "loss 2.5541696251113944e-05 average time 0.0036824125999942226 iter num 20\n",
            "loss 2.4149788770631227e-05 average time 0.0036724256000127296 iter num 40\n",
            "loss 2.3324495834533963e-05 average time 0.0036676396333518825 iter num 60\n",
            "loss 2.2973170727710254e-05 average time 0.0036729253500197958 iter num 80\n",
            "loss 2.2901415393075158e-05 average time 0.003713082720021248 iter num 100\n",
            "loss 2.1618645482849034e-05 average time 0.004026545249996616 iter num 20\n",
            "loss 2.0609391863859943e-05 average time 0.004059727049991579 iter num 40\n",
            "loss 2.000732874582923e-05 average time 0.003967535283326621 iter num 60\n",
            "loss 1.9749620234965137e-05 average time 0.003907623625005385 iter num 80\n",
            "loss 1.969682384046974e-05 average time 0.0038909737299968584 iter num 100\n",
            "loss 1.8747154090663323e-05 average time 0.003759354599992548 iter num 20\n",
            "loss 1.7988846489479183e-05 average time 0.003804555525005071 iter num 40\n",
            "loss 1.753009872679557e-05 average time 0.003758461216659725 iter num 60\n",
            "loss 1.7331940985676905e-05 average time 0.0037960779499826456 iter num 80\n",
            "loss 1.729116862051737e-05 average time 0.0037645975799932784 iter num 100\n",
            "loss 1.6552024690485977e-05 average time 0.003845480649977162 iter num 20\n",
            "loss 1.5951912274309802e-05 average time 0.003884205400004248 iter num 40\n",
            "loss 1.5583611650335726e-05 average time 0.003933708000007149 iter num 60\n",
            "loss 1.542311545732616e-05 average time 0.0038645841875080576 iter num 80\n",
            "loss 1.5389986125665105e-05 average time 0.0038962263100097514 iter num 100\n",
            "loss 1.4784804691596529e-05 average time 0.00396982949997664 iter num 20\n",
            "loss 1.4286470394397467e-05 average time 0.0039043749249970004 iter num 40\n",
            "loss 1.3977145578960032e-05 average time 0.003936372599999534 iter num 60\n",
            "loss 1.3841519074928978e-05 average time 0.003992355912495782 iter num 80\n",
            "loss 1.38134522427932e-05 average time 0.003927914089999831 iter num 100\n",
            "loss 1.329867465383171e-05 average time 0.0039154858000074455 iter num 20\n",
            "loss 1.2871528215610028e-05 average time 0.0039352044000111166 iter num 40\n",
            "loss 1.2604998191470766e-05 average time 0.003894711216670051 iter num 60\n",
            "loss 1.248782200918487e-05 average time 0.0039151900000007345 iter num 80\n",
            "loss 1.2463555145860504e-05 average time 0.003949793569995563 iter num 100\n",
            "loss 1.2017619289598283e-05 average time 0.003480768399981571 iter num 20\n",
            "loss 1.164666559646227e-05 average time 0.003535393224990457 iter num 40\n",
            "loss 1.1414947837625982e-05 average time 0.0035482839666542533 iter num 60\n",
            "loss 1.1313076477048422e-05 average time 0.0035541940499939527 iter num 80\n",
            "loss 1.1291974969220504e-05 average time 0.00359242330998768 iter num 100\n",
            "loss 1.090459237742136e-05 average time 0.00363915334999092 iter num 20\n",
            "loss 1.0582930366456066e-05 average time 0.0036092874249845862 iter num 40\n",
            "loss 1.0382331650577135e-05 average time 0.003606855149992801 iter num 60\n",
            "loss 1.0294217620906101e-05 average time 0.0036555834999973057 iter num 80\n",
            "loss 1.0275968878184576e-05 average time 0.003776113419999092 iter num 100\n",
            "loss 9.941061438622573e-06 average time 0.00409996950003233 iter num 20\n",
            "loss 9.663063220622143e-06 average time 0.004390688475007209 iter num 40\n",
            "loss 9.48974164111625e-06 average time 0.00425513030000199 iter num 60\n",
            "loss 9.41361355027917e-06 average time 0.004216462150014877 iter num 80\n",
            "loss 9.39784784231355e-06 average time 0.0041328862700106585 iter num 100\n",
            "loss 9.108509013903037e-06 average time 0.0036482437499671507 iter num 20\n",
            "loss 8.868280472538908e-06 average time 0.0036388808499907555 iter num 40\n",
            "loss 8.71838509526686e-06 average time 0.003637875949982572 iter num 60\n",
            "loss 8.652509282464656e-06 average time 0.0036290890249830454 iter num 80\n",
            "loss 8.638861868432114e-06 average time 0.0036746255799812388 iter num 100\n",
            "loss 8.388184205070056e-06 average time 0.003758094400018308 iter num 20\n",
            "loss 8.179586155284904e-06 average time 0.0036854552499960393 iter num 40\n",
            "loss 8.049126506699092e-06 average time 0.003960431333333266 iter num 60\n",
            "loss 7.991691945400061e-06 average time 0.003955688587501527 iter num 80\n",
            "loss 7.97978837834824e-06 average time 0.003940539340005671 iter num 100\n",
            "loss 7.81815858156464e-06 average time 0.003940110499991078 iter num 20\n",
            "loss 7.5795463423658435e-06 average time 0.0041153348249963525 iter num 40\n",
            "loss 7.465102291152249e-06 average time 0.004028671166664329 iter num 60\n",
            "loss 7.414874333627419e-06 average time 0.003950652999998283 iter num 80\n",
            "loss 7.404448045904927e-06 average time 0.003935725679998541 iter num 100\n",
            "loss 8.198473186924529e-06 average time 0.004013035250011399 iter num 20\n",
            "loss 7.094790116371928e-06 average time 0.004001324074994273 iter num 40\n",
            "loss 7.00636540479473e-06 average time 0.0038478698833235587 iter num 60\n",
            "loss 6.9685339631179094e-06 average time 0.003866666974991517 iter num 80\n",
            "loss 6.959913528274575e-06 average time 0.0038809395699945528 iter num 100\n",
            "loss 6.814175103039405e-06 average time 0.003688831949966698 iter num 20\n",
            "loss 6.689763867139716e-06 average time 0.003870430524978019 iter num 40\n",
            "loss 6.61111785895621e-06 average time 0.003905862499993873 iter num 60\n",
            "loss 6.5762310725450995e-06 average time 0.003934851162495079 iter num 80\n",
            "loss 6.568974512296001e-06 average time 0.003888670120002189 iter num 100\n",
            "loss 6.434467660230907e-06 average time 0.00425833845000625 iter num 20\n",
            "loss 6.320512999385017e-06 average time 0.003876952200010919 iter num 40\n",
            "loss 6.2481376622772025e-06 average time 0.003866847883337717 iter num 60\n",
            "loss 6.2159789084116305e-06 average time 0.0038388031750116623 iter num 80\n",
            "loss 6.20928344481051e-06 average time 0.0038527797600090706 iter num 100\n",
            "loss 6.0851644139257335e-06 average time 0.003695181099999445 iter num 20\n",
            "loss 5.979871106351147e-06 average time 0.0037454156499904913 iter num 40\n",
            "loss 5.912909952717018e-06 average time 0.0039209546666635715 iter num 60\n",
            "loss 5.883130653863941e-06 average time 0.0039589915999954425 iter num 80\n",
            "loss 5.87693374505738e-06 average time 0.0039129686199930805 iter num 100\n",
            "loss 5.761873465017101e-06 average time 0.0037000384500061045 iter num 20\n",
            "loss 5.6641063979984985e-06 average time 0.003955073175012558 iter num 40\n",
            "loss 5.601846904040515e-06 average time 0.0038849960333436685 iter num 60\n",
            "loss 5.5741396836113975e-06 average time 0.0038316371250147084 iter num 80\n",
            "loss 5.568363698018887e-06 average time 0.0038709895700162634 iter num 100\n",
            "loss 5.461184411884684e-06 average time 0.004226140349987873 iter num 20\n",
            "loss 5.3699951600564866e-06 average time 0.003963969224986386 iter num 40\n",
            "loss 5.311867074659832e-06 average time 0.0038475427666639916 iter num 60\n",
            "loss 5.2859821422981815e-06 average time 0.003847354374994438 iter num 80\n",
            "loss 5.280586089524987e-06 average time 0.00386717777999138 iter num 100\n",
            "loss 5.1818016901803995e-06 average time 0.00379725600000711 iter num 20\n",
            "loss 5.09516880881757e-06 average time 0.0038743430750230344 iter num 40\n",
            "loss 5.040642314803688e-06 average time 0.0037788273666819806 iter num 60\n",
            "loss 5.01638187890279e-06 average time 0.00379519362500389 iter num 80\n",
            "loss 5.011327212723227e-06 average time 0.003866078880000714 iter num 100\n",
            "loss 1.325563810910939e-05 average time 0.0046077471999751655 iter num 20\n",
            "loss 4.86393136436472e-06 average time 0.004045491724991734 iter num 40\n",
            "loss 4.868224454907807e-06 average time 0.003905986499986132 iter num 60\n",
            "loss 4.804673611434161e-06 average time 0.003825594887487682 iter num 80\n",
            "loss 4.800901958627342e-06 average time 0.003773224879987538 iter num 100\n",
            "loss 4.730735122221129e-06 average time 0.0036355980000394084 iter num 20\n",
            "loss 4.670568670746861e-06 average time 0.0037193566000155442 iter num 40\n",
            "loss 4.6319394105463075e-06 average time 0.0036937561666794258 iter num 60\n",
            "loss 4.614652691042867e-06 average time 0.003707036362510507 iter num 80\n",
            "loss 4.611044250098659e-06 average time 0.0037058523900100225 iter num 100\n",
            "loss 4.543735065422945e-06 average time 0.003637751400015077 iter num 20\n",
            "loss 4.4859453054721735e-06 average time 0.0039464737500054525 iter num 40\n",
            "loss 4.448821654371075e-06 average time 0.003927895516665103 iter num 60\n",
            "loss 4.43220941517156e-06 average time 0.003961426599997253 iter num 80\n",
            "loss 4.4287399130228046e-06 average time 0.003970502480001414 iter num 100\n",
            "loss 4.364058577543549e-06 average time 0.00353583464999474 iter num 20\n",
            "loss 4.308536578152255e-06 average time 0.003747607674995379 iter num 40\n",
            "loss 4.272878533085615e-06 average time 0.003754972916654727 iter num 60\n",
            "loss 4.256920680304162e-06 average time 0.003778716212482891 iter num 80\n",
            "loss 4.253589421202422e-06 average time 0.003724224529980802 iter num 100\n",
            "loss 4.191477529426774e-06 average time 0.003709396949989241 iter num 20\n",
            "loss 4.138157932123136e-06 average time 0.0036554023499888897 iter num 40\n",
            "loss 4.103918087776773e-06 average time 0.0038425239166637462 iter num 60\n",
            "loss 4.088595683703962e-06 average time 0.0039581071750063755 iter num 80\n",
            "loss 4.085398897020198e-06 average time 0.0039241909600036705 iter num 100\n",
            "loss 4.0257643536051096e-06 average time 0.003824349099988922 iter num 20\n",
            "loss 3.974582631528219e-06 average time 0.003913078549999227 iter num 40\n",
            "loss 3.941719413967375e-06 average time 0.003873277333328436 iter num 60\n",
            "loss 3.927015413951501e-06 average time 0.0038046245750024354 iter num 80\n",
            "loss 3.923946167585377e-06 average time 0.0038381986000013057 iter num 100\n",
            "loss 3.866722948861944e-06 average time 0.004006552599992119 iter num 20\n",
            "loss 3.8176253160656e-06 average time 0.003958029899996518 iter num 40\n",
            "loss 3.7861012171961615e-06 average time 0.003925612016670735 iter num 60\n",
            "loss 3.772005051930663e-06 average time 0.004050402450005208 iter num 80\n",
            "loss 3.769061921009471e-06 average time 0.00403696681000838 iter num 100\n",
            "loss 3.714898348333264e-06 average time 0.0036932076000425695 iter num 20\n",
            "loss 3.6671284916014198e-06 average time 0.003728806250006755 iter num 40\n",
            "loss 3.6369202970388302e-06 average time 0.003675878033338146 iter num 60\n",
            "loss 3.623408482967392e-06 average time 0.0036849879250013375 iter num 80\n",
            "loss 3.620588538623951e-06 average time 0.0036932387000047128 iter num 100\n",
            "loss 1.0853011497813118e-05 average time 0.0036748547000001962 iter num 20\n",
            "loss 3.950820613280984e-06 average time 0.0037925135749844684 iter num 40\n",
            "loss 3.5238654074593856e-06 average time 0.0037843284166607797 iter num 60\n",
            "loss 3.4981890250708273e-06 average time 0.0037401360749953483 iter num 80\n",
            "loss 3.4944137540154754e-06 average time 0.0037921761599932326 iter num 100\n",
            "loss 3.453420851897524e-06 average time 0.0044001229999707904 iter num 20\n",
            "loss 3.414428167055827e-06 average time 0.004105509449976808 iter num 40\n",
            "loss 3.3904267748968708e-06 average time 0.0042252738166515275 iter num 60\n",
            "loss 3.3797280850552254e-06 average time 0.004252553899988243 iter num 80\n",
            "loss 3.377494228652769e-06 average time 0.0042522836199964335 iter num 100\n",
            "loss 3.335778849415331e-06 average time 0.003899591749996034 iter num 20\n",
            "loss 3.299854325084267e-06 average time 0.0038522467999939636 iter num 40\n",
            "loss 3.2767213725133506e-06 average time 0.0038787517500016595 iter num 60\n",
            "loss 3.2663551064299435e-06 average time 0.0038945076249945034 iter num 80\n",
            "loss 3.264188258646428e-06 average time 0.0039102406299934955 iter num 100\n",
            "loss 3.2237379564254098e-06 average time 0.0038283669499833196 iter num 20\n",
            "loss 3.188905072693598e-06 average time 0.003934499425002969 iter num 40\n",
            "loss 3.166475225553411e-06 average time 0.0038259047666580654 iter num 60\n",
            "loss 3.1564209934779953e-06 average time 0.003871624812495611 iter num 80\n",
            "loss 3.15432084417749e-06 average time 0.003831080159998237 iter num 100\n",
            "loss 3.115179979233097e-06 average time 0.00375312274998123 iter num 20\n",
            "loss 3.0813268700194033e-06 average time 0.0037706152999817276 iter num 40\n",
            "loss 3.05957160444117e-06 average time 0.0037508654833269856 iter num 60\n",
            "loss 3.0498220898766816e-06 average time 0.0037217000999959283 iter num 80\n",
            "loss 3.047786698734112e-06 average time 0.0037507762900031594 iter num 100\n",
            "loss 8.48008452682522e-06 average time 0.00394151285006501 iter num 20\n",
            "loss 3.252411511140004e-06 average time 0.003790976750048003 iter num 40\n",
            "loss 2.977917916599858e-06 average time 0.0038816234000364604 iter num 60\n",
            "loss 2.9539168062344494e-06 average time 0.0039881431000310386 iter num 80\n",
            "loss 2.95206840441703e-06 average time 0.0040584718500304 iter num 100\n",
            "loss 2.9221147448654822e-06 average time 0.004142325600037111 iter num 20\n",
            "loss 2.889848242451563e-06 average time 0.004057076975027485 iter num 40\n",
            "loss 2.8712182383236424e-06 average time 0.0039596538333550296 iter num 60\n",
            "loss 2.8628703337537424e-06 average time 0.0038519437000161362 iter num 80\n",
            "loss 2.861126706824239e-06 average time 0.0038635736800142696 iter num 100\n",
            "loss 2.828547335281024e-06 average time 0.0036249597000050926 iter num 20\n",
            "loss 2.8004159309177813e-06 average time 0.003723185625011638 iter num 40\n",
            "loss 2.7822705430586384e-06 average time 0.003795442116669771 iter num 60\n",
            "loss 2.774131838236505e-06 average time 0.0037805631499850277 iter num 80\n",
            "loss 2.7724303106527524e-06 average time 0.0037122577399850342 iter num 100\n",
            "loss 3.6150452731391255e-06 average time 0.0037210669499813777 iter num 20\n",
            "loss 2.7308891363612366e-06 average time 0.0036428737000051116 iter num 40\n",
            "loss 2.697913463224042e-06 average time 0.0036177097833350064 iter num 60\n",
            "loss 2.6899904343874226e-06 average time 0.0036332779750011925 iter num 80\n",
            "loss 2.688083706448268e-06 average time 0.003611855269998614 iter num 100\n",
            "loss 4.606169479847315e-06 average time 0.004026302699969619 iter num 20\n",
            "loss 3.091004618442683e-06 average time 0.004335830324987455 iter num 40\n",
            "loss 2.6532294878900705e-06 average time 0.004336104783324875 iter num 60\n",
            "loss 2.624337746251744e-06 average time 0.004352385674994253 iter num 80\n",
            "loss 2.621491674333601e-06 average time 0.00424937095999212 iter num 100\n",
            "loss 2.599619436261523e-06 average time 0.003723025750025499 iter num 20\n",
            "loss 2.577441310708928e-06 average time 0.003854373600006511 iter num 40\n",
            "loss 2.5640243792891914e-06 average time 0.004018555283342569 iter num 60\n",
            "loss 2.558050600772105e-06 average time 0.004177238112507098 iter num 80\n",
            "loss 2.5568015032772363e-06 average time 0.004218694850007978 iter num 100\n",
            "loss 2.5334457071719266e-06 average time 0.003823780700020052 iter num 20\n",
            "loss 2.5132069472460248e-06 average time 0.0037002003750160385 iter num 40\n",
            "loss 2.500102059331713e-06 average time 0.00369677671668569 iter num 60\n",
            "loss 2.494209565839339e-06 average time 0.0036886494875091103 iter num 80\n",
            "loss 2.4929767734231e-06 average time 0.0037097233700046674 iter num 100\n",
            "loss 2.469875680799083e-06 average time 0.0035689589000071466 iter num 20\n",
            "loss 2.4498622272082108e-06 average time 0.003667310474997976 iter num 40\n",
            "loss 2.4369032366693214e-06 average time 0.0037499138333297803 iter num 60\n",
            "loss 2.4310763838378307e-06 average time 0.003838407575005931 iter num 80\n",
            "loss 2.4298555368042153e-06 average time 0.0038731329100073707 iter num 100\n",
            "loss 2.4070166136937416e-06 average time 0.004014167649972933 iter num 20\n",
            "loss 2.3872347887983393e-06 average time 0.003911527324981989 iter num 40\n",
            "loss 2.3744201469961676e-06 average time 0.003799361749994053 iter num 60\n",
            "loss 2.368662184955243e-06 average time 0.0038319360124944523 iter num 80\n",
            "loss 2.3674571196734807e-06 average time 0.003788094489996183 iter num 100\n",
            "loss 2.34497107122488e-06 average time 0.0036246630000391633 iter num 20\n",
            "loss 2.3253354840352916e-06 average time 0.003811120525028855 iter num 40\n",
            "loss 2.312671570985233e-06 average time 0.0037832255833488188 iter num 60\n",
            "loss 2.306981622256621e-06 average time 0.0037543758250194515 iter num 80\n",
            "loss 2.3057912142707474e-06 average time 0.003705826290013192 iter num 100\n",
            "loss 9.3274376605745e-06 average time 0.0035251669500212302 iter num 20\n",
            "loss 2.2830749713846543e-06 average time 0.0037251351249949494 iter num 40\n",
            "loss 2.2738000893510494e-06 average time 0.003688954466656469 iter num 60\n",
            "loss 2.2529084941883375e-06 average time 0.003774527912494818 iter num 80\n",
            "loss 2.251586699541675e-06 average time 0.003745665740000277 iter num 100\n",
            "loss 2.232273013826151e-06 average time 0.0036445190999984333 iter num 20\n",
            "loss 2.215275195293405e-06 average time 0.0038365012249869323 iter num 40\n",
            "loss 2.2043610979365663e-06 average time 0.004059937266652014 iter num 60\n",
            "loss 2.1994536881950666e-06 average time 0.004016833199978009 iter num 80\n",
            "loss 2.198425954251869e-06 average time 0.004078381569991052 iter num 100\n",
            "loss 2.1791707881659233e-06 average time 0.004642184199974508 iter num 20\n",
            "loss 2.1624671610221065e-06 average time 0.004131954825004414 iter num 40\n",
            "loss 2.151639466464404e-06 average time 0.003976568183342503 iter num 60\n",
            "loss 2.1467663534928336e-06 average time 0.003895490737511409 iter num 80\n",
            "loss 2.1457475052048315e-06 average time 0.0038449475000084023 iter num 100\n",
            "loss 2.127733085285316e-06 average time 0.0036257161500316214 iter num 20\n",
            "loss 2.1100829572532733e-06 average time 0.003993194750012208 iter num 40\n",
            "loss 2.0993135578274355e-06 average time 0.004158670333352651 iter num 60\n",
            "loss 2.094476559669562e-06 average time 0.00423827806251893 iter num 80\n",
            "loss 2.093464358122373e-06 average time 0.004136412700020174 iter num 100\n",
            "loss 2.234601460300401e-06 average time 0.003909734449973712 iter num 20\n",
            "loss 2.6584097060468503e-06 average time 0.0037124697999900034 iter num 40\n",
            "loss 2.0714064449820626e-06 average time 0.0037556941666631854 iter num 60\n",
            "loss 2.0541026691647113e-06 average time 0.0038513679375085986 iter num 80\n",
            "loss 2.0509498044699923e-06 average time 0.00396096813000895 iter num 100\n",
            "loss 2.035302064570844e-06 average time 0.004298613549997299 iter num 20\n",
            "loss 2.021978433466629e-06 average time 0.004266665174981199 iter num 40\n",
            "loss 2.0129836765634655e-06 average time 0.0040538421999902615 iter num 60\n",
            "loss 2.009018678985775e-06 average time 0.003942148624994956 iter num 80\n",
            "loss 2.0081898006948562e-06 average time 0.003872240219993728 iter num 100\n",
            "loss 1.992650716624983e-06 average time 0.004058092449974992 iter num 20\n",
            "loss 1.979141501736456e-06 average time 0.004085078474986403 iter num 40\n",
            "loss 1.970370305747637e-06 average time 0.0039584970999802255 iter num 60\n",
            "loss 1.966418289135783e-06 average time 0.0038767391624787706 iter num 80\n",
            "loss 1.965591566397599e-06 average time 0.003907475959983912 iter num 100\n",
            "loss 1.950066380075265e-06 average time 0.004587347599965597 iter num 20\n",
            "loss 1.9365725812067046e-06 average time 0.004503468499984819 iter num 40\n",
            "loss 1.9278079153154406e-06 average time 0.004444329833324901 iter num 60\n",
            "loss 1.923858856884563e-06 average time 0.004279539712496216 iter num 80\n",
            "loss 1.9230330578304235e-06 average time 0.004171355319999748 iter num 100\n",
            "loss 1.91623782725439e-06 average time 0.004224261550007213 iter num 20\n",
            "loss 1.894200877064923e-06 average time 0.0040070719499908595 iter num 40\n",
            "loss 1.885313700945198e-06 average time 0.003878123016651595 iter num 60\n",
            "loss 1.8813703042319307e-06 average time 0.003901582624988009 iter num 80\n",
            "loss 1.8805452625741555e-06 average time 0.0038738941499832434 iter num 100\n",
            "loss 3.172750672954542e-06 average time 0.003911241499997687 iter num 20\n",
            "loss 1.945680833845997e-06 average time 0.0038725950999889845 iter num 40\n",
            "loss 1.8490563622436754e-06 average time 0.004032840149996749 iter num 60\n",
            "loss 1.8451172336609496e-06 average time 0.003976010574999122 iter num 80\n",
            "loss 1.8439554723564525e-06 average time 0.003996904159996575 iter num 100\n",
            "loss 2.3651054210509004e-06 average time 0.004456257649997042 iter num 20\n",
            "loss 1.8243271109657433e-06 average time 0.004085935199998403 iter num 40\n",
            "loss 1.8149847787445632e-06 average time 0.003962782166665116 iter num 60\n",
            "loss 1.8104828325546406e-06 average time 0.003931047162498658 iter num 80\n",
            "loss 1.8096983983661166e-06 average time 0.0039038190099927304 iter num 100\n",
            "loss 1.7971239990538436e-06 average time 0.0038210659499782196 iter num 20\n",
            "loss 1.7860696070919422e-06 average time 0.003724275199999738 iter num 40\n",
            "loss 1.7789144098753072e-06 average time 0.003827082833337651 iter num 60\n",
            "loss 1.775685627996819e-06 average time 0.003836344325003438 iter num 80\n",
            "loss 1.7750094902088233e-06 average time 0.003886057760005315 iter num 100\n",
            "loss 3.1144398572754317e-06 average time 0.0039806846500141544 iter num 20\n",
            "loss 1.7669415739868962e-06 average time 0.0038742623499956608 iter num 40\n",
            "loss 1.750246193808495e-06 average time 0.003865228283321661 iter num 60\n",
            "loss 1.7419889301614007e-06 average time 0.0038281318249943296 iter num 80\n",
            "loss 1.7412147239180585e-06 average time 0.0038423426199960886 iter num 100\n",
            "loss 2.6118561031161486e-06 average time 0.003883674650012381 iter num 20\n",
            "loss 2.156355458885152e-06 average time 0.003930885249985749 iter num 40\n",
            "loss 1.7315219725173787e-06 average time 0.003964711349995772 iter num 60\n",
            "loss 1.7188678595410865e-06 average time 0.0038896286874944507 iter num 80\n",
            "loss 1.7172533941807585e-06 average time 0.003901579719995425 iter num 100\n",
            "loss 1.707445598576303e-06 average time 0.0035842905000095016 iter num 20\n",
            "loss 1.6985210572426234e-06 average time 0.0035867491000146855 iter num 40\n",
            "loss 1.6927605011480566e-06 average time 0.003570098066673684 iter num 60\n",
            "loss 1.690201040966821e-06 average time 0.003569860587501239 iter num 80\n",
            "loss 1.689665018311097e-06 average time 0.0035462190800035387 iter num 100\n",
            "loss 1.6796182416798528e-06 average time 0.0036080663999882743 iter num 20\n",
            "loss 1.6708575048198993e-06 average time 0.003670326550007985 iter num 40\n",
            "loss 1.665150106542885e-06 average time 0.003647841183340006 iter num 60\n",
            "loss 1.6625756226322864e-06 average time 0.0036225576500072522 iter num 80\n",
            "loss 1.6620358042057619e-06 average time 0.0035972363800101446 iter num 100\n",
            "loss 1.651893654958073e-06 average time 0.0035639179000327204 iter num 20\n",
            "loss 1.6430460487655814e-06 average time 0.003658018075009295 iter num 40\n",
            "loss 1.6372811539689014e-06 average time 0.0036641289166709614 iter num 60\n",
            "loss 1.6346794530046507e-06 average time 0.0036456370999957246 iter num 80\n",
            "loss 1.6341337229864748e-06 average time 0.003626096079997296 iter num 100\n",
            "loss 1.6238831760236553e-06 average time 0.00361307855001769 iter num 20\n",
            "loss 1.6149394710682026e-06 average time 0.003927399025008072 iter num 40\n",
            "loss 1.6091128541182759e-06 average time 0.0038833271500000894 iter num 60\n",
            "loss 1.606484206162707e-06 average time 0.004041185049990759 iter num 80\n",
            "loss 1.605932582292417e-06 average time 0.003940791999991688 iter num 100\n",
            "loss 1.5955734620817667e-06 average time 0.003612777399996503 iter num 20\n",
            "loss 1.586539101676534e-06 average time 0.003721851500000639 iter num 40\n",
            "loss 1.5806527685658019e-06 average time 0.0036469645333302952 iter num 60\n",
            "loss 1.577995566995364e-06 average time 0.003654723412500971 iter num 80\n",
            "loss 1.5774386510095751e-06 average time 0.003697677690004184 iter num 100\n",
            "loss 1.5669810648385051e-06 average time 0.004008862750004028 iter num 20\n",
            "loss 1.5578563480405316e-06 average time 0.003897487224986662 iter num 40\n",
            "loss 1.5519140191601183e-06 average time 0.0038626150833124483 iter num 60\n",
            "loss 1.5492332124550482e-06 average time 0.0038158627874736338 iter num 80\n",
            "loss 1.5486724382040444e-06 average time 0.003922243969977899 iter num 100\n",
            "loss 1.7096828314481806e-06 average time 0.004230782149954848 iter num 20\n",
            "loss 1.5372790212395745e-06 average time 0.004251993949986854 iter num 40\n",
            "loss 1.523524480406051e-06 average time 0.004060920766657243 iter num 60\n",
            "loss 1.520363374322555e-06 average time 0.0040251210374890435 iter num 80\n",
            "loss 1.5197954375733093e-06 average time 0.00396372715999405 iter num 100\n",
            "loss 1.0213028435340466e-05 average time 0.0038857943999914824 iter num 20\n",
            "loss 2.2902242445724473e-06 average time 0.004098129775002235 iter num 40\n",
            "loss 1.5545281738202696e-06 average time 0.0039417591833360655 iter num 60\n",
            "loss 1.5102590517707974e-06 average time 0.003924891074996139 iter num 80\n",
            "loss 1.5060558572059395e-06 average time 0.0038897952799948144 iter num 100\n",
            "loss 1.5018548979991128e-06 average time 0.003869114749977598 iter num 20\n",
            "loss 1.4923914094505546e-06 average time 0.003972163474992385 iter num 40\n",
            "loss 1.4880852512528471e-06 average time 0.004021222100004706 iter num 60\n",
            "loss 1.4862558681420943e-06 average time 0.003977155150005273 iter num 80\n",
            "loss 1.485875371408937e-06 average time 0.003930268970004818 iter num 100\n",
            "loss 1.478738935091624e-06 average time 0.004400129450027634 iter num 20\n",
            "loss 1.4725268142571693e-06 average time 0.004100950800022929 iter num 40\n",
            "loss 1.4684815546051703e-06 average time 0.004065180583351472 iter num 60\n",
            "loss 1.4666576374344422e-06 average time 0.004063811362507863 iter num 80\n",
            "loss 1.4662758332211084e-06 average time 0.0040274011900055485 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3be0cc-7ec7-4b6f-f8b8-dc8109dcbf5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df2b39b-c8f3-4553-87df-67ffdfc6dc90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b63240-f835-49a9-d002-bd595a2dbf30"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0758c7-9bd1-4a88-eea2-92fd19038c70"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=7, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3b18a4-2063-4632-b756-7d2d283c3877"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-4, amsgrad=True) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.035923793568878065 average time 2.1161620507499945 iter num 20\n",
            "loss 0.0359088227884686 average time 2.117986477250014 iter num 40\n",
            "loss 0.035898869725666846 average time 2.119290947166682 iter num 60\n",
            "loss 0.03589445991231862 average time 2.1167064351750016 iter num 80\n",
            "loss 0.035893525670518685 average time 2.1163014866000003 iter num 100\n",
            "loss 0.035876145018231706 average time 2.130255113499993 iter num 20\n",
            "loss 0.03586071681304882 average time 2.1185614623999696 iter num 40\n",
            "loss 0.035850518773534526 average time 2.122669476816653 iter num 60\n",
            "loss 0.03584589810408316 average time 2.1214107102499953 iter num 80\n",
            "loss 0.03584492107817007 average time 2.124587830999994 iter num 100\n",
            "loss 0.035826937819412166 average time 2.1230099534999907 iter num 20\n",
            "loss 0.035811238762083926 average time 2.1204493999750014 iter num 40\n",
            "loss 0.03580098220655523 average time 2.119429244066661 iter num 60\n",
            "loss 0.035796379485267794 average time 2.1215808921000017 iter num 80\n",
            "loss 0.03579542667373802 average time 2.1204596576900006 iter num 100\n",
            "loss 0.03577758185549446 average time 2.1165278543000112 iter num 20\n",
            "loss 0.03576217669123945 average time 2.111668071449998 iter num 40\n",
            "loss 0.035752115621639165 average time 2.1213781003166523 iter num 60\n",
            "loss 0.035747600964277704 average time 2.1208633587374806 iter num 80\n",
            "loss 0.03574666240920758 average time 2.1188522083899786 iter num 100\n",
            "loss 0.035729204391899125 average time 2.1060923174500203 iter num 20\n",
            "loss 0.03571422009974365 average time 2.1051727680500107 iter num 40\n",
            "loss 0.0357045521344185 average time 2.118224651883338 iter num 60\n",
            "loss 0.035700219184156834 average time 2.117615505000009 iter num 80\n",
            "loss 0.035699338460336974 average time 2.119036032519998 iter num 100\n",
            "loss 0.03568263676629199 average time 2.118593621749983 iter num 20\n",
            "loss 0.03566825399692997 average time 2.106464945925006 iter num 40\n",
            "loss 0.03565900997557502 average time 2.1070089071500204 iter num 60\n",
            "loss 0.03565487149787711 average time 2.101505268325025 iter num 80\n",
            "loss 0.03565399643302266 average time 2.1009717139000257 iter num 100\n",
            "loss 0.03563797484149057 average time 2.112051137599974 iter num 20\n",
            "loss 0.03562420192104143 average time 2.111943496674962 iter num 40\n",
            "loss 0.03561533402179883 average time 2.1136256896499996 iter num 60\n",
            "loss 0.03561140521294283 average time 2.1165285636375075 iter num 80\n",
            "loss 0.035610588241606386 average time 2.106577718380022 iter num 100\n",
            "loss 0.035595392009629995 average time 2.0981741081500105 iter num 20\n",
            "loss 0.035582316349927846 average time 2.1042400940999983 iter num 40\n",
            "loss 0.03557388945367844 average time 2.105065682733349 iter num 60\n",
            "loss 0.03557009696372333 average time 2.1003893676375016 iter num 80\n",
            "loss 0.03556931715939621 average time 2.1024357412100105 iter num 100\n",
            "loss 0.03555475714683309 average time 2.090482409050014 iter num 20\n",
            "loss 0.035542219799572526 average time 2.082796974149994 iter num 40\n",
            "loss 0.035534152465715924 average time 2.088302393949986 iter num 60\n",
            "loss 0.03553054525288692 average time 2.084488032675 iter num 80\n",
            "loss 0.035529779392147985 average time 2.0894968644999743 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQs-OZHGEwac"
      },
      "source": [
        "# 12:00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a3e33d-244a-4d36-9e44-7680a81692a1"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1,1, 0.25, 0.02, 0.02] * 3]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "# x.backward()\n",
        "# first_order_gradient = inputs.grad\n",
        "# first_order_gradient[0][[3]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.10632345, 0.5543747)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.068384, 0.194783, 0.194960, 0.194743]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqpasxVi0hx3",
        "outputId": "d5e5583a-eec6-485f-d0cd-8c4506cdb927"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 3\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.02]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.]*numstocks) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)) # here numsteps different from T\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)) # here numsteps different from T"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06788256\n",
            "[0.19420046 0.19421804 0.19411935]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovJwXo3-YEu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "d924af09-8c9f-4ef3-be25-782dbf5a4b7a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.02, 0.02] * 3]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "correct_call_prices = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    correct_call_prices.append(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, correct_call_prices, label = \"correct_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(correct_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a1372fdbe1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minitial_stocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumstocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# must be float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel_call_prices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcorrect_call_prices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ogL13Fxys46",
        "outputId": "2de4938e-077e-4fcc-add1-2409bbec4b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "compute_price(1).item()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9e6a1e48e199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "77bf560d-5ade-49c7-938f-066562b0857a"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.0, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgD1NjP7DOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "4014d547-8d24-4a76-c5bd-3919eb48b2c9"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.775, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 0.775, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7MfujMQ7oij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "db38961a-0b40-4ffa-ff90-3d8fb8671537"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.225, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.225, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}