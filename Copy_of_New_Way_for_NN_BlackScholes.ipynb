{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of New Way for NN_BlackScholes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/Copy_of_New_Way_for_NN_BlackScholes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2gIC6kxF0ks"
      },
      "source": [
        "# Use Black-Scholes to Generate Data\n",
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5_EBlCgHsFt",
        "outputId": "347a3e7b-17c1-4f41-9c72-4f9614f15212"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0  15339      0 --:--:-- --:--:-- --:--:-- 15339\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg1VnsiZF3GM",
        "outputId": "f2f22394-3a3d-477b-ae64-ead0161a1a61"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "    return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "          # intervals\n",
        "          iv_s0 = (1.25-0.75)/80\n",
        "          iv_sigma = (0.45-0.15)/40\n",
        "          iv_r = (0.6-0.25)/40\n",
        "          iv_K = (1.25-0.75)/80\n",
        "\n",
        "          # initial_stocks = np.array(np.random.randint(75,125,self.N_STOCKS)/100)\n",
        "          # iv_s0 = 0.005\n",
        "          rnd_s0 = np.arange(0.75, 1.25 + iv_s0, iv_s0)\n",
        "          initial_stocks = np.array([rnd_s0[i] for i in np.random.randint(0, len(rnd_s0), self.N_STOCKS)])\n",
        "\n",
        "          corr = np.diag(np.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "\n",
        "          # sigma = np.array(np.random.randint(15,45,self.N_STOCKS)/100)\n",
        "\n",
        "          # iv_sigma = 0.005\n",
        "          rnd_sigma = np.arange(0.15, 0.45 + iv_sigma, iv_sigma)\n",
        "          sigma = np.array([rnd_sigma[i] for i in np.random.randint(0, len(rnd_sigma), self.N_STOCKS)])\n",
        "\n",
        "          cov = (np.diag(sigma)).dot(corr).dot(np.diag(sigma))\n",
        "\n",
        "          # r = np.repeat(np.array(np.random.randint(25,60)/100), self.N_STOCKS)\n",
        "          # iv_r = 0.005\n",
        "          rnd_r = np.arange(0.25, 0.6 + iv_r, iv_r)\n",
        "          r = np.repeat(rnd_r[np.random.randint(0,len(rnd_r), 1)], self.N_STOCKS)\n",
        "\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          # K = np.random.randint(75,125)/100\n",
        "          # iv_K = 0.005\n",
        "          rnd_K = np.arange(0.75, 1.25 + iv_K, iv_K)\n",
        "          K = rnd_K[np.random.randint(0, len(rnd_K), 1)]\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "\n",
        "          European_Call_price = bs_call(initial_stocks,K,T,r,sigma)\n",
        "          Deltas = bs_delta(initial_stocks,K,T,r,sigma)\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price[0]\n",
        "          Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (np.repeat(np.array(T), self.N_STOCKS), np.repeat(np.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNmazdSvGEho",
        "outputId": "3b0dcd4e-0093-430d-d568-8e36344411aa"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.35, 0.35]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.25, 0.25]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJNy_JNHGFGh"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNp9_sibGQSJ",
        "outputId": "56c6f2b9-a723-47b0-f6e6-bf53fc68b8de"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lZmDAkPiGWyz",
        "outputId": "e267a0ca-2b26-457b-e3b5-98ecb0b231ea"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.3893764913082123 average time 0.07712499650002655 iter num 20\n",
            "loss 0.36389705538749695 average time 0.054137091075011766 iter num 40\n",
            "loss 0.3042798638343811 average time 0.04680394411667521 iter num 60\n",
            "loss 0.34460604190826416 average time 0.04295286473750366 iter num 80\n",
            "loss 0.2795102000236511 average time 0.040791658370008005 iter num 100\n",
            "loss 0.28387340903282166 average time 0.0327023625000038 iter num 20\n",
            "loss 0.25583401322364807 average time 0.03210939797500032 iter num 40\n",
            "loss 0.2284049093723297 average time 0.03181570838332088 iter num 60\n",
            "loss 0.2218768447637558 average time 0.03154334498748597 iter num 80\n",
            "loss 0.2166953831911087 average time 0.031610717079986445 iter num 100\n",
            "loss 0.18853919208049774 average time 0.03323158695001212 iter num 20\n",
            "loss 0.15265637636184692 average time 0.03246786822501235 iter num 40\n",
            "loss 0.1213235855102539 average time 0.032245764900009515 iter num 60\n",
            "loss 0.09973210841417313 average time 0.03232590555000172 iter num 80\n",
            "loss 0.10111295431852341 average time 0.03218742599000734 iter num 100\n",
            "loss 0.06639497727155685 average time 0.03272865589999583 iter num 20\n",
            "loss 0.034621939063072205 average time 0.03211908122501086 iter num 40\n",
            "loss 0.030064988881349564 average time 0.03189313221666149 iter num 60\n",
            "loss 0.0207804124802351 average time 0.03153968931250404 iter num 80\n",
            "loss 0.0174434594810009 average time 0.031548622380003055 iter num 100\n",
            "loss 0.012726525776088238 average time 0.033177173199999285 iter num 20\n",
            "loss 0.009597815573215485 average time 0.03227454589998615 iter num 40\n",
            "loss 0.010500484146177769 average time 0.031821698699999006 iter num 60\n",
            "loss 0.0075462376698851585 average time 0.03157766740000625 iter num 80\n",
            "loss 0.016269618645310402 average time 0.03161253356000543 iter num 100\n",
            "loss 0.016156263649463654 average time 0.033427366050023014 iter num 20\n",
            "loss 0.014011634513735771 average time 0.03329502485001399 iter num 40\n",
            "loss 0.02578698843717575 average time 0.03258111565000945 iter num 60\n",
            "loss 0.015586145222187042 average time 0.03240957486251546 iter num 80\n",
            "loss 0.020184839144349098 average time 0.03274450270001353 iter num 100\n",
            "loss 0.013326840475201607 average time 0.032754466699998375 iter num 20\n",
            "loss 0.0169052854180336 average time 0.032012134199993623 iter num 40\n",
            "loss 0.009174032136797905 average time 0.03161883046667905 iter num 60\n",
            "loss 0.014284450560808182 average time 0.03154246090001038 iter num 80\n",
            "loss 0.009528674185276031 average time 0.03143247159000339 iter num 100\n",
            "loss 0.011165725998580456 average time 0.03327828734996956 iter num 20\n",
            "loss 0.017060181125998497 average time 0.032199757824957945 iter num 40\n",
            "loss 0.009992443025112152 average time 0.0317536339333022 iter num 60\n",
            "loss 0.008889192715287209 average time 0.031624617712469674 iter num 80\n",
            "loss 0.011910692788660526 average time 0.03150928827997177 iter num 100\n",
            "loss 0.01588522084057331 average time 0.03380652935001081 iter num 20\n",
            "loss 0.01436572428792715 average time 0.033519613575009544 iter num 40\n",
            "loss 0.008189995773136616 average time 0.03351844470001121 iter num 60\n",
            "loss 0.00895362626761198 average time 0.033543866587504564 iter num 80\n",
            "loss 0.011288524605333805 average time 0.03341619841000238 iter num 100\n",
            "loss 0.011322703212499619 average time 0.03640270699999064 iter num 20\n",
            "loss 0.008168993517756462 average time 0.034916296124981726 iter num 40\n",
            "loss 0.006287903059273958 average time 0.03430558973332154 iter num 60\n",
            "loss 0.012817418202757835 average time 0.03408182984998973 iter num 80\n",
            "loss 0.016767440363764763 average time 0.03414687489998641 iter num 100\n",
            "loss 0.007368194870650768 average time 0.03503183490001902 iter num 20\n",
            "loss 0.013666315004229546 average time 0.03404157495000959 iter num 40\n",
            "loss 0.009097610600292683 average time 0.032890184650000265 iter num 60\n",
            "loss 0.015279070474207401 average time 0.03269968900000606 iter num 80\n",
            "loss 0.009969345293939114 average time 0.032399029270006846 iter num 100\n",
            "loss 0.007057115901261568 average time 0.032785718350010026 iter num 20\n",
            "loss 0.01105420384556055 average time 0.032025909625008356 iter num 40\n",
            "loss 0.012889844365417957 average time 0.031803838016662665 iter num 60\n",
            "loss 0.008085725829005241 average time 0.031745065337500475 iter num 80\n",
            "loss 0.009691153652966022 average time 0.0316283996500033 iter num 100\n",
            "loss 0.010591033846139908 average time 0.03226797894999436 iter num 20\n",
            "loss 0.011040695011615753 average time 0.03187144597498559 iter num 40\n",
            "loss 0.018188808113336563 average time 0.031326930849987396 iter num 60\n",
            "loss 0.005754637997597456 average time 0.03126918551248821 iter num 80\n",
            "loss 0.013593480922281742 average time 0.03143622759999744 iter num 100\n",
            "loss 0.008371368050575256 average time 0.03241585519997443 iter num 20\n",
            "loss 0.01302049495279789 average time 0.03164699842497498 iter num 40\n",
            "loss 0.015506145544350147 average time 0.03181738349998113 iter num 60\n",
            "loss 0.008428296074271202 average time 0.031717895599985016 iter num 80\n",
            "loss 0.008291089907288551 average time 0.031721506249980394 iter num 100\n",
            "loss 0.009737983345985413 average time 0.03253289515000688 iter num 20\n",
            "loss 0.009164846502244473 average time 0.03182513779999567 iter num 40\n",
            "loss 0.00651559978723526 average time 0.032174628049998925 iter num 60\n",
            "loss 0.01385990809649229 average time 0.03241110044999971 iter num 80\n",
            "loss 0.012403219938278198 average time 0.03231687107000198 iter num 100\n",
            "loss 0.012819787487387657 average time 0.033047544250007374 iter num 20\n",
            "loss 0.009623231366276741 average time 0.03210704827501445 iter num 40\n",
            "loss 0.007040910888463259 average time 0.031645439366680725 iter num 60\n",
            "loss 0.007129313424229622 average time 0.03162705688751259 iter num 80\n",
            "loss 0.01144643034785986 average time 0.031591843660007723 iter num 100\n",
            "loss 0.005691295024007559 average time 0.03275985230002334 iter num 20\n",
            "loss 0.007732010446488857 average time 0.032529008674993067 iter num 40\n",
            "loss 0.011729065328836441 average time 0.032023682266666735 iter num 60\n",
            "loss 0.00784230139106512 average time 0.032150267662504464 iter num 80\n",
            "loss 0.005369797348976135 average time 0.03205288064000797 iter num 100\n",
            "loss 0.009626828134059906 average time 0.03411756260004495 iter num 20\n",
            "loss 0.007011127192527056 average time 0.032568652375022114 iter num 40\n",
            "loss 0.004848141688853502 average time 0.03242333816668103 iter num 60\n",
            "loss 0.010286510922014713 average time 0.03193206226250993 iter num 80\n",
            "loss 0.007385771721601486 average time 0.0318386525200026 iter num 100\n",
            "loss 0.010193653404712677 average time 0.032040616999995566 iter num 20\n",
            "loss 0.010697668418288231 average time 0.0317310781499998 iter num 40\n",
            "loss 0.01143253780901432 average time 0.03224473286665746 iter num 60\n",
            "loss 0.007765045389533043 average time 0.03216136843748814 iter num 80\n",
            "loss 0.007180666550993919 average time 0.03220891532998394 iter num 100\n",
            "loss 0.0044396244920790195 average time 0.03232486615002017 iter num 20\n",
            "loss 0.009767502546310425 average time 0.031474120625000525 iter num 40\n",
            "loss 0.010350094176828861 average time 0.031548349566666425 iter num 60\n",
            "loss 0.00748402951285243 average time 0.03159583496250491 iter num 80\n",
            "loss 0.004799393005669117 average time 0.032326726590004 iter num 100\n",
            "loss 0.007982269860804081 average time 0.034531911400017636 iter num 20\n",
            "loss 0.003916488494724035 average time 0.03320639094999365 iter num 40\n",
            "loss 0.009787890128791332 average time 0.032627438783337466 iter num 60\n",
            "loss 0.005131714511662722 average time 0.03236227386249766 iter num 80\n",
            "loss 0.005849427543580532 average time 0.03216201341999749 iter num 100\n",
            "loss 0.01167390402406454 average time 0.03417360774997178 iter num 20\n",
            "loss 0.006489668041467667 average time 0.033713649599985726 iter num 40\n",
            "loss 0.0046510519459843636 average time 0.034047449466659906 iter num 60\n",
            "loss 0.00981331430375576 average time 0.03331016571249279 iter num 80\n",
            "loss 0.006004108116030693 average time 0.032884749479990205 iter num 100\n",
            "loss 0.007619986776262522 average time 0.032824303849974965 iter num 20\n",
            "loss 0.009871258400380611 average time 0.03241356512498328 iter num 40\n",
            "loss 0.006688111461699009 average time 0.03221863246664801 iter num 60\n",
            "loss 0.015950696542859077 average time 0.03212062268748639 iter num 80\n",
            "loss 0.01102504599839449 average time 0.0321213947799879 iter num 100\n",
            "loss 0.0088810408487916 average time 0.03349783390002585 iter num 20\n",
            "loss 0.0043015629053115845 average time 0.0328902888250127 iter num 40\n",
            "loss 0.007382519543170929 average time 0.03255917415000719 iter num 60\n",
            "loss 0.00834614597260952 average time 0.032664115800011474 iter num 80\n",
            "loss 0.00840121228247881 average time 0.032241836350006 iter num 100\n",
            "loss 0.009802639484405518 average time 0.032176017249969394 iter num 20\n",
            "loss 0.005043686367571354 average time 0.03264492672499273 iter num 40\n",
            "loss 0.004910777788609266 average time 0.032379803533319776 iter num 60\n",
            "loss 0.0073326025158166885 average time 0.0324575210499944 iter num 80\n",
            "loss 0.005751664284616709 average time 0.032504547699998054 iter num 100\n",
            "loss 0.010979157872498035 average time 0.03299788779999062 iter num 20\n",
            "loss 0.007871733047068119 average time 0.03377945145001036 iter num 40\n",
            "loss 0.012815704569220543 average time 0.033387822516685144 iter num 60\n",
            "loss 0.0066244821064174175 average time 0.032816100087501354 iter num 80\n",
            "loss 0.003887587459757924 average time 0.032662236760002086 iter num 100\n",
            "loss 0.005618707276880741 average time 0.03562530704996334 iter num 20\n",
            "loss 0.005046965554356575 average time 0.0336139080749831 iter num 40\n",
            "loss 0.003131078789010644 average time 0.03264783644999246 iter num 60\n",
            "loss 0.005873823072761297 average time 0.03249122551249854 iter num 80\n",
            "loss 0.006925303488969803 average time 0.032117988369996055 iter num 100\n",
            "loss 0.008929099887609482 average time 0.03207204084999375 iter num 20\n",
            "loss 0.004786145407706499 average time 0.03175741127499236 iter num 40\n",
            "loss 0.006248831748962402 average time 0.031984903366674186 iter num 60\n",
            "loss 0.00713967764750123 average time 0.031883967300001356 iter num 80\n",
            "loss 0.0049463664181530476 average time 0.03207072927999206 iter num 100\n",
            "loss 0.004535160027444363 average time 0.03388239409998732 iter num 20\n",
            "loss 0.003676182357594371 average time 0.032834609249999855 iter num 40\n",
            "loss 0.004124914295971394 average time 0.03279596694999706 iter num 60\n",
            "loss 0.004038497805595398 average time 0.032854197837494326 iter num 80\n",
            "loss 0.0021522855386137962 average time 0.03290473080000538 iter num 100\n",
            "loss 0.00277191330678761 average time 0.03338092699999606 iter num 20\n",
            "loss 0.006918040569871664 average time 0.032507523825006504 iter num 40\n",
            "loss 0.007037419825792313 average time 0.0330104143500004 iter num 60\n",
            "loss 0.004298856947571039 average time 0.032945532487499915 iter num 80\n",
            "loss 0.004368819296360016 average time 0.03263994216000356 iter num 100\n",
            "loss 0.006662191357463598 average time 0.03225205505000304 iter num 20\n",
            "loss 0.0024505923502147198 average time 0.03203300069999955 iter num 40\n",
            "loss 0.002367582404986024 average time 0.03196269780001255 iter num 60\n",
            "loss 0.0033293780870735645 average time 0.03228217778751343 iter num 80\n",
            "loss 0.004241804592311382 average time 0.032666114040009686 iter num 100\n",
            "loss 0.0025420691817998886 average time 0.03372753809999267 iter num 20\n",
            "loss 0.006309974007308483 average time 0.03308567959999209 iter num 40\n",
            "loss 0.005914622917771339 average time 0.03275084453332132 iter num 60\n",
            "loss 0.004896319936960936 average time 0.03259420482499422 iter num 80\n",
            "loss 0.0040126400999724865 average time 0.03248089478998736 iter num 100\n",
            "loss 0.00336670083925128 average time 0.03292467599999327 iter num 20\n",
            "loss 0.004297136329114437 average time 0.0328659209749901 iter num 40\n",
            "loss 0.0024148246739059687 average time 0.032633962966641165 iter num 60\n",
            "loss 0.00342118414118886 average time 0.032434553762467996 iter num 80\n",
            "loss 0.00210386049002409 average time 0.032353476919972764 iter num 100\n",
            "loss 0.001878365408629179 average time 0.033070259999999504 iter num 20\n",
            "loss 0.002826735842972994 average time 0.03331460274999358 iter num 40\n",
            "loss 0.003708353964611888 average time 0.03268055688332273 iter num 60\n",
            "loss 0.0019383806502446532 average time 0.0328131016374897 iter num 80\n",
            "loss 0.002765730256214738 average time 0.03301483015998883 iter num 100\n",
            "loss 0.0026989434845745564 average time 0.031986624750004466 iter num 20\n",
            "loss 0.0015947737265378237 average time 0.031894266500000865 iter num 40\n",
            "loss 0.003496929071843624 average time 0.03158554536666998 iter num 60\n",
            "loss 0.0031728874891996384 average time 0.031683388775005025 iter num 80\n",
            "loss 0.003091176738962531 average time 0.03194560306000085 iter num 100\n",
            "loss 0.001406524213962257 average time 0.03320176890000539 iter num 20\n",
            "loss 0.0025887470692396164 average time 0.032286614074990894 iter num 40\n",
            "loss 0.009608391672372818 average time 0.03212140534999814 iter num 60\n",
            "loss 0.0020791571587324142 average time 0.03199378158749653 iter num 80\n",
            "loss 0.0038429591804742813 average time 0.03214546042000393 iter num 100\n",
            "loss 0.002498997375369072 average time 0.03311893159998362 iter num 20\n",
            "loss 0.002428195206448436 average time 0.032536397174988 iter num 40\n",
            "loss 0.0014087404124438763 average time 0.03233143213333657 iter num 60\n",
            "loss 0.0024488188792020082 average time 0.03253055056250105 iter num 80\n",
            "loss 0.0012957061408087611 average time 0.032791054779993375 iter num 100\n",
            "loss 0.004615660756826401 average time 0.0336224183000013 iter num 20\n",
            "loss 0.0026575906667858362 average time 0.03309379499999636 iter num 40\n",
            "loss 0.004649966489523649 average time 0.032644084549997385 iter num 60\n",
            "loss 0.0033298921771347523 average time 0.03261108166249187 iter num 80\n",
            "loss 0.0025738112162798643 average time 0.032729989439994826 iter num 100\n",
            "loss 0.0013903641374781728 average time 0.03267071829998258 iter num 20\n",
            "loss 0.0020451305899769068 average time 0.032197892149974906 iter num 40\n",
            "loss 0.0015046222833916545 average time 0.032362028916641826 iter num 60\n",
            "loss 0.002070011803880334 average time 0.03234001687499131 iter num 80\n",
            "loss 0.002782992785796523 average time 0.03221486914000025 iter num 100\n",
            "loss 0.0020759333856403828 average time 0.03396406105001688 iter num 20\n",
            "loss 0.0033677166793495417 average time 0.03351659730001302 iter num 40\n",
            "loss 0.006049435120075941 average time 0.03326386123334638 iter num 60\n",
            "loss 0.002404457423835993 average time 0.032853021587513354 iter num 80\n",
            "loss 0.003627106314525008 average time 0.03255137955001146 iter num 100\n",
            "loss 0.0021073645912110806 average time 0.03407464549999304 iter num 20\n",
            "loss 0.002189293969422579 average time 0.03268207737499438 iter num 40\n",
            "loss 0.0024752956815063953 average time 0.03330522854997753 iter num 60\n",
            "loss 0.002514991909265518 average time 0.033795008274967134 iter num 80\n",
            "loss 0.0004498035996221006 average time 0.03351487656998188 iter num 100\n",
            "loss 0.0026041180826723576 average time 0.03395048460002954 iter num 20\n",
            "loss 0.0032524506095796824 average time 0.03305963055001371 iter num 40\n",
            "loss 0.0052408818155527115 average time 0.032952324966674953 iter num 60\n",
            "loss 0.0018588058883324265 average time 0.03328315613751158 iter num 80\n",
            "loss 0.002872176468372345 average time 0.03335800129000972 iter num 100\n",
            "loss 0.003230093512684107 average time 0.03318709004998936 iter num 20\n",
            "loss 0.007438630796968937 average time 0.03251514042499366 iter num 40\n",
            "loss 0.001267335144802928 average time 0.03306661099999246 iter num 60\n",
            "loss 0.0013264756416901946 average time 0.03253290511249531 iter num 80\n",
            "loss 0.0034373803064227104 average time 0.032544578969998385 iter num 100\n",
            "loss 0.0033029925543814898 average time 0.03195133595003199 iter num 20\n",
            "loss 0.0023734711576253176 average time 0.03172161265003979 iter num 40\n",
            "loss 0.0012140147155150771 average time 0.03202209678335445 iter num 60\n",
            "loss 0.00223207613453269 average time 0.03171587151252027 iter num 80\n",
            "loss 0.0021628690883517265 average time 0.03172500622001735 iter num 100\n",
            "loss 0.002708937507122755 average time 0.033970578900016334 iter num 20\n",
            "loss 0.0026960219256579876 average time 0.033522051475017633 iter num 40\n",
            "loss 0.0013376775896176696 average time 0.033222507566680784 iter num 60\n",
            "loss 0.004509620368480682 average time 0.03300204448751174 iter num 80\n",
            "loss 0.0015874324599280953 average time 0.03312506835000477 iter num 100\n",
            "loss 0.0038568098098039627 average time 0.034777693099977115 iter num 20\n",
            "loss 0.0009504834888502955 average time 0.033440773749998695 iter num 40\n",
            "loss 0.0015621011843904853 average time 0.032586576733331186 iter num 60\n",
            "loss 0.0011929746251553297 average time 0.032314719274992854 iter num 80\n",
            "loss 0.0031168826390057802 average time 0.03215119184999821 iter num 100\n",
            "loss 0.001926247961819172 average time 0.032135606399981496 iter num 20\n",
            "loss 0.002399856224656105 average time 0.0320148588499876 iter num 40\n",
            "loss 0.0005712996353395283 average time 0.03204209091665386 iter num 60\n",
            "loss 0.003608487080782652 average time 0.032304021437497 iter num 80\n",
            "loss 0.0023753722198307514 average time 0.032517529589993044 iter num 100\n",
            "loss 0.002083458239212632 average time 0.03293635139997377 iter num 20\n",
            "loss 0.0009158300817944109 average time 0.032692428774976176 iter num 40\n",
            "loss 0.0016348817152902484 average time 0.032576424366656434 iter num 60\n",
            "loss 0.0037291133776307106 average time 0.03240398466249985 iter num 80\n",
            "loss 0.004075019620358944 average time 0.0322120847400015 iter num 100\n",
            "loss 0.0035748707596212626 average time 0.03470924590001232 iter num 20\n",
            "loss 0.0009777923114597797 average time 0.033412792824992724 iter num 40\n",
            "loss 0.0018012262880802155 average time 0.032985981016687066 iter num 60\n",
            "loss 0.0012961281463503838 average time 0.03259559710001554 iter num 80\n",
            "loss 0.0005011064349673688 average time 0.03243372227000919 iter num 100\n",
            "loss 0.0006970854010432959 average time 0.03445613519998005 iter num 20\n",
            "loss 0.0011076473165303469 average time 0.033386298149991944 iter num 40\n",
            "loss 0.003740837564691901 average time 0.03309776738333691 iter num 60\n",
            "loss 0.001098170760087669 average time 0.03271498879999797 iter num 80\n",
            "loss 0.002693974645808339 average time 0.0326332505599953 iter num 100\n",
            "loss 0.0010055542225018144 average time 0.03431842674999643 iter num 20\n",
            "loss 0.001044378150254488 average time 0.034167166100002075 iter num 40\n",
            "loss 0.0011523582506924868 average time 0.03387135231664994 iter num 60\n",
            "loss 0.0032241211738437414 average time 0.033566402574993506 iter num 80\n",
            "loss 0.0005652019171975553 average time 0.033098907940000116 iter num 100\n",
            "loss 0.004000599030405283 average time 0.0323868647500035 iter num 20\n",
            "loss 0.0015832853969186544 average time 0.03222848120000208 iter num 40\n",
            "loss 0.001927619450725615 average time 0.032314055583333355 iter num 60\n",
            "loss 0.0013104132376611233 average time 0.032210659400001874 iter num 80\n",
            "loss 0.003296137088909745 average time 0.03219266133000701 iter num 100\n",
            "loss 0.0014734541764482856 average time 0.03374615884997638 iter num 20\n",
            "loss 0.0027341616805642843 average time 0.032700807549986166 iter num 40\n",
            "loss 0.0030235496815294027 average time 0.03264735718331622 iter num 60\n",
            "loss 0.0007522890227846801 average time 0.03232737583748531 iter num 80\n",
            "loss 0.0018665542593225837 average time 0.032088462919982706 iter num 100\n",
            "loss 0.0005585188628174365 average time 0.03234698024997442 iter num 20\n",
            "loss 0.0005350844585336745 average time 0.03160360102497748 iter num 40\n",
            "loss 0.002263749251142144 average time 0.0315011602333243 iter num 60\n",
            "loss 0.0006427412154152989 average time 0.03163334991248803 iter num 80\n",
            "loss 0.0004702074802480638 average time 0.03192039232998695 iter num 100\n",
            "loss 0.002995806047692895 average time 0.033495014850029745 iter num 20\n",
            "loss 0.001431079232133925 average time 0.032346569950027514 iter num 40\n",
            "loss 0.005524371284991503 average time 0.03243878603334451 iter num 60\n",
            "loss 0.002402497222647071 average time 0.032650528012507604 iter num 80\n",
            "loss 0.0014606855111196637 average time 0.03250777565000817 iter num 100\n",
            "loss 0.00038097394281066954 average time 0.0324171621500227 iter num 20\n",
            "loss 0.0017333996947854757 average time 0.03185478667502366 iter num 40\n",
            "loss 0.0015861670253798366 average time 0.03208256746668212 iter num 60\n",
            "loss 0.001277279108762741 average time 0.0320770221125116 iter num 80\n",
            "loss 0.0004020603373646736 average time 0.032387361470014184 iter num 100\n",
            "loss 0.003150983713567257 average time 0.03378076710001778 iter num 20\n",
            "loss 0.002451171400025487 average time 0.032932253000024045 iter num 40\n",
            "loss 0.0012676747282966971 average time 0.03321214288334507 iter num 60\n",
            "loss 0.0009315040661022067 average time 0.03276879618750002 iter num 80\n",
            "loss 0.0009014417300932109 average time 0.032661229899997576 iter num 100\n",
            "loss 0.0016173438634723425 average time 0.032989735649982775 iter num 20\n",
            "loss 0.006189550738781691 average time 0.03249802952498726 iter num 40\n",
            "loss 0.0019725041929632425 average time 0.03220333184999617 iter num 60\n",
            "loss 0.0008720859768800437 average time 0.0322054880624961 iter num 80\n",
            "loss 0.0028806389309465885 average time 0.032105483330001336 iter num 100\n",
            "loss 0.0011312214192003012 average time 0.03299399345000893 iter num 20\n",
            "loss 0.0010175302159041166 average time 0.032850355950012046 iter num 40\n",
            "loss 0.00048585611511953175 average time 0.032353085866679974 iter num 60\n",
            "loss 0.0005198897561058402 average time 0.03198207277499989 iter num 80\n",
            "loss 0.00047131426981650293 average time 0.032023054370001774 iter num 100\n",
            "loss 0.001772309304215014 average time 0.03415671495004062 iter num 20\n",
            "loss 0.0021119469311088324 average time 0.032952389975019966 iter num 40\n",
            "loss 0.0006375865777954459 average time 0.0327298541833405 iter num 60\n",
            "loss 0.00044413324212655425 average time 0.03243910743750291 iter num 80\n",
            "loss 0.0011151410872116685 average time 0.0327924541800121 iter num 100\n",
            "loss 0.003076697699725628 average time 0.03299937354998974 iter num 20\n",
            "loss 0.0004965605912730098 average time 0.03299706010000136 iter num 40\n",
            "loss 0.0008965861052274704 average time 0.03265895413333434 iter num 60\n",
            "loss 0.003415768500417471 average time 0.03265951946249288 iter num 80\n",
            "loss 0.0011030137538909912 average time 0.03304531495999527 iter num 100\n",
            "loss 0.0029388880357146263 average time 0.03465679215001956 iter num 20\n",
            "loss 0.0011590193025767803 average time 0.033478941575009455 iter num 40\n",
            "loss 0.004697672091424465 average time 0.03283009901667053 iter num 60\n",
            "loss 0.00030286965193226933 average time 0.03261733982500346 iter num 80\n",
            "loss 0.0008003656403161585 average time 0.032644856580000124 iter num 100\n",
            "loss 0.0008062612032517791 average time 0.03217759270000897 iter num 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f6f8c98e5d5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f6f8c98e5d5b>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f6f8c98e5d5b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f6f8c98e5d5b>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ziJDSnnOuii"
      },
      "source": [
        "9 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0cc_DoENm4G"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tt0PFffNmVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6cce35-5038-4351-94a9-8a8a5ba0e50b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMkQh_5rNxAY"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_grid_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT2g0J3qN284"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eChrShF1N4nu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoOdFI5VN7zN"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_grid_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVPuJKhrOAet"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhKSwGk-O6R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b19f3a-cb24-42ec-9b20-d06cbd877ad2"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 500000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 200)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_grid_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 2.2037447706679814e-05 average time 0.03285779039961199 iter num 20\n",
            "loss 1.0010827281803358e-05 average time 0.03228145447474162 iter num 40\n",
            "loss 3.697285592352273e-06 average time 0.03216572104987184 iter num 60\n",
            "loss 4.466521659196587e-06 average time 0.03192798128739014 iter num 80\n",
            "loss 2.1637948520947248e-05 average time 0.03168255676988338 iter num 100\n",
            "loss 8.99094175110804e-06 average time 0.03262864320022345 iter num 20\n",
            "loss 4.488891136134043e-06 average time 0.03177473587520581 iter num 40\n",
            "loss 1.653195795370266e-05 average time 0.03193636596679426 iter num 60\n",
            "loss 4.3711338548746426e-06 average time 0.03177032508765478 iter num 80\n",
            "loss 4.190393610770116e-06 average time 0.03169099040020228 iter num 100\n",
            "loss 5.389084890339291e-06 average time 0.03296557780013245 iter num 20\n",
            "loss 1.008342132990947e-05 average time 0.0321114579002824 iter num 40\n",
            "loss 6.749827662133612e-06 average time 0.03216327000027377 iter num 60\n",
            "loss 7.1030012804840226e-06 average time 0.03192433475019243 iter num 80\n",
            "loss 3.0738206078240182e-06 average time 0.032067084680165864 iter num 100\n",
            "loss 2.8144697807874763e-06 average time 0.03327557569991768 iter num 20\n",
            "loss 4.9567997848498635e-06 average time 0.032614983000075884 iter num 40\n",
            "loss 1.9446037185844034e-05 average time 0.03231194171670116 iter num 60\n",
            "loss 4.749158051708946e-06 average time 0.032296386787584196 iter num 80\n",
            "loss 6.143468453956302e-06 average time 0.032216289850075554 iter num 100\n",
            "loss 7.887481842772104e-06 average time 0.0331123823993039 iter num 20\n",
            "loss 7.2182347139460035e-06 average time 0.032509066999500645 iter num 40\n",
            "loss 4.455887847143458e-06 average time 0.032370352316199086 iter num 60\n",
            "loss 0.00011725471267709509 average time 0.03203536288724536 iter num 80\n",
            "loss 3.0964465622673742e-06 average time 0.03210732518975419 iter num 100\n",
            "loss 3.863209258270217e-06 average time 0.03250767900008213 iter num 20\n",
            "loss 2.894983481382951e-06 average time 0.03164372152496071 iter num 40\n",
            "loss 3.218252822989598e-05 average time 0.031630704316679234 iter num 60\n",
            "loss 4.595246537064668e-06 average time 0.031602335062507335 iter num 80\n",
            "loss 5.949168553343043e-06 average time 0.03159533115001977 iter num 100\n",
            "loss 2.9371308301051613e-06 average time 0.03191604905023269 iter num 20\n",
            "loss 8.046035873121582e-06 average time 0.031590732450240466 iter num 40\n",
            "loss 7.796244062774349e-06 average time 0.031661982400207005 iter num 60\n",
            "loss 6.846871201560134e-06 average time 0.032030287050065456 iter num 80\n",
            "loss 4.232128958392423e-06 average time 0.03204630283002188 iter num 100\n",
            "loss 7.187488790805219e-06 average time 0.03390470040048967 iter num 20\n",
            "loss 4.374268428364303e-06 average time 0.03298458777526321 iter num 40\n",
            "loss 2.501657036191318e-06 average time 0.033109999566841 iter num 60\n",
            "loss 6.287503765634028e-06 average time 0.033067925012801425 iter num 80\n",
            "loss 3.7849354157515336e-06 average time 0.033130256780168565 iter num 100\n",
            "loss 9.41996586334426e-06 average time 0.033881495300011014 iter num 20\n",
            "loss 3.880508302245289e-06 average time 0.0330149018251177 iter num 40\n",
            "loss 5.4179522521735635e-06 average time 0.032851362583278386 iter num 60\n",
            "loss 4.421501671458827e-06 average time 0.03260233506243822 iter num 80\n",
            "loss 1.7421398297301494e-05 average time 0.03229399675987224 iter num 100\n",
            "loss 7.932215339678805e-06 average time 0.033860750150051896 iter num 20\n",
            "loss 4.939362952427473e-06 average time 0.03293824167485582 iter num 40\n",
            "loss 7.92625178291928e-06 average time 0.03247115704986451 iter num 60\n",
            "loss 6.896466402395163e-06 average time 0.03233138214986866 iter num 80\n",
            "loss 5.17679882250377e-06 average time 0.03217971631987893 iter num 100\n",
            "loss 6.72621899866499e-05 average time 0.033671345350558116 iter num 20\n",
            "loss 1.5066430023580324e-05 average time 0.03310997725029665 iter num 40\n",
            "loss 6.915374797245022e-06 average time 0.03298378738357618 iter num 60\n",
            "loss 0.00011298642493784428 average time 0.03257078022515998 iter num 80\n",
            "loss 4.066618203069083e-06 average time 0.03270261820016458 iter num 100\n",
            "loss 1.033478110912256e-05 average time 0.03252044175023912 iter num 20\n",
            "loss 4.548939159576548e-06 average time 0.03278307967511864 iter num 40\n",
            "loss 4.012398221675539e-06 average time 0.03218532021692226 iter num 60\n",
            "loss 3.671007107186597e-06 average time 0.032018213387618746 iter num 80\n",
            "loss 4.822198661713628e-06 average time 0.0321794027400756 iter num 100\n",
            "loss 6.544607458636165e-06 average time 0.03224110624996683 iter num 20\n",
            "loss 4.157986040809192e-06 average time 0.03216938387522532 iter num 40\n",
            "loss 4.956642897013808e-06 average time 0.031960105916793206 iter num 60\n",
            "loss 2.467066451572464e-06 average time 0.031811289749975914 iter num 80\n",
            "loss 5.845070518262219e-06 average time 0.03228233561007073 iter num 100\n",
            "loss 1.0838254638656508e-05 average time 0.03405516915026965 iter num 20\n",
            "loss 5.141942892805673e-06 average time 0.03329152972528391 iter num 40\n",
            "loss 3.948775429307716e-06 average time 0.03318561903367178 iter num 60\n",
            "loss 8.336843166034669e-06 average time 0.03284310795029342 iter num 80\n",
            "loss 8.639352017780766e-06 average time 0.03332248997012357 iter num 100\n",
            "loss 6.575989118573489e-06 average time 0.034287904699885983 iter num 20\n",
            "loss 6.2938188420957886e-06 average time 0.032851114299865 iter num 40\n",
            "loss 1.3492403013515286e-05 average time 0.03291203301644904 iter num 60\n",
            "loss 1.291589614993427e-05 average time 0.03263347917491046 iter num 80\n",
            "loss 4.599042313202517e-06 average time 0.03243697683989012 iter num 100\n",
            "loss 0.00021203912911005318 average time 0.03287849135012948 iter num 20\n",
            "loss 2.619487077026861e-06 average time 0.03257382140018308 iter num 40\n",
            "loss 4.7018934310472105e-06 average time 0.03303291061686953 iter num 60\n",
            "loss 6.932104497536784e-06 average time 0.0327526314125862 iter num 80\n",
            "loss 3.618791424742085e-06 average time 0.0326277932200901 iter num 100\n",
            "loss 1.5298433936550282e-05 average time 0.03425667409956077 iter num 20\n",
            "loss 1.3292336916492786e-05 average time 0.033566013974996166 iter num 40\n",
            "loss 5.300413704389939e-06 average time 0.03274293198325419 iter num 60\n",
            "loss 1.0428912901261356e-05 average time 0.032628318462411696 iter num 80\n",
            "loss 4.865133178100223e-06 average time 0.03266376834992116 iter num 100\n",
            "loss 5.6421458793920465e-06 average time 0.03190677095008141 iter num 20\n",
            "loss 4.508387519308599e-06 average time 0.032451456500075436 iter num 40\n",
            "loss 8.125490239763167e-06 average time 0.032628667983347744 iter num 60\n",
            "loss 8.64082085172413e-06 average time 0.0326534759498827 iter num 80\n",
            "loss 4.618503680831054e-06 average time 0.03250509278997924 iter num 100\n",
            "loss 8.513198736181948e-06 average time 0.03310097775029135 iter num 20\n",
            "loss 5.257537850411609e-06 average time 0.03198338134989172 iter num 40\n",
            "loss 7.336412636504974e-06 average time 0.03165238731647454 iter num 60\n",
            "loss 1.588066152180545e-05 average time 0.03181936119981401 iter num 80\n",
            "loss 8.154303031915333e-06 average time 0.0320350630599205 iter num 100\n",
            "loss 9.295619747717865e-06 average time 0.033386742299808245 iter num 20\n",
            "loss 6.207330898178043e-06 average time 0.032429411274733866 iter num 40\n",
            "loss 3.962902610510355e-06 average time 0.03215862941657785 iter num 60\n",
            "loss 5.070109637017595e-06 average time 0.03218611939996663 iter num 80\n",
            "loss 8.911283657653257e-06 average time 0.03200594471007207 iter num 100\n",
            "loss 3.0140824947011424e-06 average time 0.03276436075084348 iter num 20\n",
            "loss 3.0403584787563886e-06 average time 0.0330751263003549 iter num 40\n",
            "loss 2.6445050025358796e-06 average time 0.032322345816949864 iter num 60\n",
            "loss 3.2231896511802915e-06 average time 0.03208714865027105 iter num 80\n",
            "loss 2.8539468985400163e-05 average time 0.03209174158026144 iter num 100\n",
            "loss 6.0185798247403e-06 average time 0.03354033855011949 iter num 20\n",
            "loss 1.3657225281349383e-05 average time 0.032424017024823115 iter num 40\n",
            "loss 4.141152203374077e-06 average time 0.03214249414989657 iter num 60\n",
            "loss 4.461828211788088e-06 average time 0.03185055649992137 iter num 80\n",
            "loss 1.971394885913469e-05 average time 0.031951519919966816 iter num 100\n",
            "loss 3.902925891452469e-05 average time 0.03263230065076641 iter num 20\n",
            "loss 8.409438123635482e-06 average time 0.03318358340038685 iter num 40\n",
            "loss 5.224625965638552e-06 average time 0.03319198603367113 iter num 60\n",
            "loss 4.292572157282848e-06 average time 0.0332334945002458 iter num 80\n",
            "loss 6.12448684478295e-06 average time 0.03296813318018394 iter num 100\n",
            "loss 3.985414878115989e-06 average time 0.03315215385046031 iter num 20\n",
            "loss 9.433379091206007e-06 average time 0.03281190997495287 iter num 40\n",
            "loss 4.473885837796843e-06 average time 0.03247077331664817 iter num 60\n",
            "loss 4.323597295297077e-06 average time 0.03216209696247461 iter num 80\n",
            "loss 3.4674637845455436e-06 average time 0.03216679350993218 iter num 100\n",
            "loss 2.150464388250839e-05 average time 0.034767682849997074 iter num 20\n",
            "loss 7.242493666126393e-06 average time 0.03326878062543983 iter num 40\n",
            "loss 4.010943484900054e-06 average time 0.0329047135169579 iter num 60\n",
            "loss 3.737746283150045e-06 average time 0.03270682150018729 iter num 80\n",
            "loss 1.6990175936371088e-05 average time 0.03256411373014998 iter num 100\n",
            "loss 5.518571015272755e-06 average time 0.03387005159966065 iter num 20\n",
            "loss 6.116903023212217e-06 average time 0.03281896824983051 iter num 40\n",
            "loss 1.1361563338141423e-05 average time 0.032875595316606145 iter num 60\n",
            "loss 3.910543455276638e-06 average time 0.03247373349990994 iter num 80\n",
            "loss 9.121340553974733e-06 average time 0.032251719280066024 iter num 100\n",
            "loss 1.1008304682036396e-05 average time 0.03280484060014714 iter num 20\n",
            "loss 5.226951088843634e-06 average time 0.031800088874933866 iter num 40\n",
            "loss 4.249051471560961e-06 average time 0.03185760499988343 iter num 60\n",
            "loss 4.313317731430288e-06 average time 0.03224373173743515 iter num 80\n",
            "loss 2.8003712486679433e-06 average time 0.032118220539960024 iter num 100\n",
            "loss 5.132680598762818e-06 average time 0.03313410690025194 iter num 20\n",
            "loss 9.430838872503955e-06 average time 0.03342808152519865 iter num 40\n",
            "loss 4.6118780119286384e-06 average time 0.033166495350087644 iter num 60\n",
            "loss 2.4064836452453164e-06 average time 0.03329908281261851 iter num 80\n",
            "loss 1.6718406186555512e-05 average time 0.033135158750155824 iter num 100\n",
            "loss 7.590952918690164e-06 average time 0.03494749159963249 iter num 20\n",
            "loss 4.647014975489583e-06 average time 0.033734669200111964 iter num 40\n",
            "loss 7.456816547346534e-06 average time 0.033071669366836434 iter num 60\n",
            "loss 2.7415078420744976e-06 average time 0.03268720683763604 iter num 80\n",
            "loss 6.893702902743826e-06 average time 0.03322787144017639 iter num 100\n",
            "loss 1.9195415006834082e-05 average time 0.03381191975040565 iter num 20\n",
            "loss 4.031227945233695e-06 average time 0.03434459375039296 iter num 40\n",
            "loss 5.66704829907394e-06 average time 0.033547245383730724 iter num 60\n",
            "loss 2.9157793051126646e-06 average time 0.03355722360024629 iter num 80\n",
            "loss 9.066426173376385e-06 average time 0.033448300900163305 iter num 100\n",
            "loss 1.684314338490367e-05 average time 0.033606462349780486 iter num 20\n",
            "loss 8.259651076514274e-05 average time 0.032655356174927874 iter num 40\n",
            "loss 6.728685548296198e-05 average time 0.03303741764999965 iter num 60\n",
            "loss 3.8515240703418385e-06 average time 0.0332129433375485 iter num 80\n",
            "loss 3.5453442706057103e-06 average time 0.032928572429991615 iter num 100\n",
            "loss 3.0987998798082117e-06 average time 0.03452499569993961 iter num 20\n",
            "loss 5.858325948793208e-06 average time 0.032951414525086875 iter num 40\n",
            "loss 8.82925269252155e-06 average time 0.03233489673342168 iter num 60\n",
            "loss 1.3692371794604696e-05 average time 0.032118181325131445 iter num 80\n",
            "loss 7.729823664703872e-06 average time 0.03197830266002711 iter num 100\n",
            "loss 1.116566090786364e-05 average time 0.03311478635023377 iter num 20\n",
            "loss 2.688286940610851e-06 average time 0.03206535787503526 iter num 40\n",
            "loss 2.7869166387972655e-06 average time 0.03192902311657235 iter num 60\n",
            "loss 6.0991969803581014e-06 average time 0.03212185717484317 iter num 80\n",
            "loss 7.759005711704958e-06 average time 0.03197327781988861 iter num 100\n",
            "loss 4.937737230648054e-06 average time 0.03474313965016336 iter num 20\n",
            "loss 1.0261922398058232e-05 average time 0.03365215947524121 iter num 40\n",
            "loss 3.3245426038774895e-06 average time 0.03347917831682328 iter num 60\n",
            "loss 1.1151934813824482e-05 average time 0.03335901691270919 iter num 80\n",
            "loss 4.420874574861955e-06 average time 0.033331491960161656 iter num 100\n",
            "loss 4.122140126128215e-06 average time 0.03585138779963017 iter num 20\n",
            "loss 1.5773279301356524e-05 average time 0.03388652212479428 iter num 40\n",
            "loss 9.281927304982673e-06 average time 0.033724910183203366 iter num 60\n",
            "loss 7.878206815803424e-06 average time 0.03354379633719873 iter num 80\n",
            "loss 3.0430437618633732e-06 average time 0.03313159808974888 iter num 100\n",
            "loss 6.510689217975596e-06 average time 0.03422338460004539 iter num 20\n",
            "loss 5.1361539590288885e-06 average time 0.03388797355000861 iter num 40\n",
            "loss 9.899787073663902e-06 average time 0.03320170494995788 iter num 60\n",
            "loss 5.461717137222877e-06 average time 0.032843824700012195 iter num 80\n",
            "loss 9.253806638298556e-06 average time 0.03257498447997932 iter num 100\n",
            "loss 1.0721309990913142e-05 average time 0.031951450050291895 iter num 20\n",
            "loss 4.110851023142459e-06 average time 0.031631414600360584 iter num 40\n",
            "loss 5.986083124298602e-06 average time 0.03173856913369188 iter num 60\n",
            "loss 3.927789748559007e-06 average time 0.03166332598780173 iter num 80\n",
            "loss 3.2896537049964536e-06 average time 0.03182137590014463 iter num 100\n",
            "loss 2.462678457959555e-05 average time 0.03276265534987033 iter num 20\n",
            "loss 1.4003111573401839e-05 average time 0.03204247715002566 iter num 40\n",
            "loss 1.0606193427520338e-05 average time 0.03185690973332385 iter num 60\n",
            "loss 6.04233628109796e-06 average time 0.03214517258747947 iter num 80\n",
            "loss 2.695877583391848e-06 average time 0.03208066688996041 iter num 100\n",
            "loss 1.2396167221595533e-05 average time 0.032263773800332275 iter num 20\n",
            "loss 2.731608901740401e-06 average time 0.03193620420033767 iter num 40\n",
            "loss 7.69132293498842e-06 average time 0.03180839978364626 iter num 60\n",
            "loss 9.071116437553428e-06 average time 0.032089725000150796 iter num 80\n",
            "loss 2.2031872504157946e-06 average time 0.03197623330004717 iter num 100\n",
            "loss 3.0620999496022705e-06 average time 0.03269755074961722 iter num 20\n",
            "loss 4.935216111334739e-06 average time 0.03205800804989849 iter num 40\n",
            "loss 6.570585355802905e-06 average time 0.03211439130003176 iter num 60\n",
            "loss 2.742959850365878e-06 average time 0.031951937037638344 iter num 80\n",
            "loss 1.898768914543325e-06 average time 0.03196156132020406 iter num 100\n",
            "loss 3.4213126127724536e-06 average time 0.03322952409998834 iter num 20\n",
            "loss 1.1483080015750602e-05 average time 0.03190279702494081 iter num 40\n",
            "loss 1.7429687432013452e-05 average time 0.031926340883486164 iter num 60\n",
            "loss 3.167426257277839e-06 average time 0.03206897357517846 iter num 80\n",
            "loss 6.5706985878932755e-06 average time 0.03199159736017464 iter num 100\n",
            "loss 1.5743396943435073e-05 average time 0.033865164700182505 iter num 20\n",
            "loss 1.2898843124276027e-05 average time 0.03348862932507472 iter num 40\n",
            "loss 5.245839929557405e-06 average time 0.03313036966671158 iter num 60\n",
            "loss 3.956924501835601e-06 average time 0.032782878537500434 iter num 80\n",
            "loss 4.383819486974971e-06 average time 0.032729016190023684 iter num 100\n",
            "loss 4.43068029198912e-06 average time 0.032592676049898725 iter num 20\n",
            "loss 5.758029601565795e-06 average time 0.03196194557494891 iter num 40\n",
            "loss 7.06484388501849e-06 average time 0.03192097558324652 iter num 60\n",
            "loss 5.450644493976142e-06 average time 0.031863230287444824 iter num 80\n",
            "loss 6.822002433182206e-06 average time 0.03187645842983329 iter num 100\n",
            "loss 1.1640757293207571e-05 average time 0.03293919255011133 iter num 20\n",
            "loss 4.629887826013146e-06 average time 0.032291250675098124 iter num 40\n",
            "loss 3.5890614071831806e-06 average time 0.03235210283322279 iter num 60\n",
            "loss 3.551146619429346e-06 average time 0.0320973154874082 iter num 80\n",
            "loss 6.264282092161011e-06 average time 0.032085891729984724 iter num 100\n",
            "loss 5.147192950971657e-06 average time 0.03296971740001027 iter num 20\n",
            "loss 9.697566383692902e-06 average time 0.032650304499929915 iter num 40\n",
            "loss 5.233856882114196e-06 average time 0.03256107691662085 iter num 60\n",
            "loss 9.129371392191388e-06 average time 0.03266041384995333 iter num 80\n",
            "loss 4.3628069761325605e-06 average time 0.03261032384991267 iter num 100\n",
            "loss 9.445926480111666e-06 average time 0.0326434912491095 iter num 20\n",
            "loss 3.968597411585506e-06 average time 0.03206617464957162 iter num 40\n",
            "loss 5.446625436889008e-06 average time 0.031975222199616835 iter num 60\n",
            "loss 1.60170402523363e-05 average time 0.03201644252476399 iter num 80\n",
            "loss 3.1641336590837454e-06 average time 0.03237093553976592 iter num 100\n",
            "loss 6.736861450917786e-06 average time 0.03299202380021597 iter num 20\n",
            "loss 2.9751322472293396e-06 average time 0.03273937990034028 iter num 40\n",
            "loss 1.4636407286161557e-05 average time 0.032055328750053985 iter num 60\n",
            "loss 4.295292455935851e-06 average time 0.031846695450121844 iter num 80\n",
            "loss 8.258596608357038e-06 average time 0.03166397661010706 iter num 100\n",
            "loss 5.046691512688994e-06 average time 0.03186476115042751 iter num 20\n",
            "loss 4.792489107785514e-06 average time 0.03115176505034469 iter num 40\n",
            "loss 7.519509836129146e-06 average time 0.031003758683740064 iter num 60\n",
            "loss 2.355553306188085e-06 average time 0.031021403800241387 iter num 80\n",
            "loss 1.1019546946045011e-05 average time 0.031197567020062707 iter num 100\n",
            "loss 2.1163303244975396e-05 average time 0.033145081800103074 iter num 20\n",
            "loss 7.552576335001504e-06 average time 0.032136765950235714 iter num 40\n",
            "loss 1.8483344319975004e-05 average time 0.03248110483333828 iter num 60\n",
            "loss 9.158659850072581e-06 average time 0.032099653449995456 iter num 80\n",
            "loss 1.2770626199198887e-05 average time 0.032086055900108476 iter num 100\n",
            "loss 5.203432465350488e-06 average time 0.03469639280010597 iter num 20\n",
            "loss 9.033640708366875e-06 average time 0.03307232857514464 iter num 40\n",
            "loss 5.827017503179377e-06 average time 0.032455812650187 iter num 60\n",
            "loss 4.09666745326831e-06 average time 0.03256078986269131 iter num 80\n",
            "loss 3.063730446228874e-06 average time 0.03227248400027747 iter num 100\n",
            "loss 4.845473540626699e-06 average time 0.033527340549881046 iter num 20\n",
            "loss 8.191966117010452e-06 average time 0.03218479912484327 iter num 40\n",
            "loss 6.61290960124461e-06 average time 0.03238521564974993 iter num 60\n",
            "loss 8.569000783609226e-06 average time 0.032892721599728246 iter num 80\n",
            "loss 4.048693153890781e-05 average time 0.03282790028970339 iter num 100\n",
            "loss 6.652737283729948e-06 average time 0.03393241175072035 iter num 20\n",
            "loss 4.9602285798755474e-06 average time 0.03309117017561221 iter num 40\n",
            "loss 5.556817995966412e-06 average time 0.03307897766699171 iter num 60\n",
            "loss 3.4723777844192227e-06 average time 0.033198802262586466 iter num 80\n",
            "loss 2.2575935872737318e-05 average time 0.03309458190014993 iter num 100\n",
            "loss 8.045483809837606e-06 average time 0.036198742749729715 iter num 20\n",
            "loss 1.1990045095444657e-05 average time 0.03559829102496224 iter num 40\n",
            "loss 9.209574272972532e-06 average time 0.0354569496333473 iter num 60\n",
            "loss 3.617295533331344e-06 average time 0.03444184402510473 iter num 80\n",
            "loss 1.9104027160210535e-05 average time 0.03390285843019228 iter num 100\n",
            "loss 7.800688763381913e-06 average time 0.03279010619990004 iter num 20\n",
            "loss 6.62442653265316e-06 average time 0.03246679405001487 iter num 40\n",
            "loss 3.728803221747512e-06 average time 0.03322435088315009 iter num 60\n",
            "loss 6.574823601113167e-06 average time 0.033310970949742114 iter num 80\n",
            "loss 5.449040600069566e-06 average time 0.0329823773897806 iter num 100\n",
            "loss 1.4630620171374176e-05 average time 0.032349383749897244 iter num 20\n",
            "loss 3.997697149316082e-06 average time 0.03225628950003738 iter num 40\n",
            "loss 1.0304945135430899e-05 average time 0.03187318796663021 iter num 60\n",
            "loss 5.577775027632015e-06 average time 0.03186800742491869 iter num 80\n",
            "loss 9.571210284775589e-06 average time 0.03212070613990363 iter num 100\n",
            "loss 3.198719696229091e-06 average time 0.03193644485018012 iter num 20\n",
            "loss 1.3579337064584251e-05 average time 0.033224027175128866 iter num 40\n",
            "loss 6.589935765077826e-06 average time 0.03304107473365245 iter num 60\n",
            "loss 5.9462295212142635e-06 average time 0.032854027250186844 iter num 80\n",
            "loss 4.2575634324748535e-06 average time 0.03246011379018455 iter num 100\n",
            "loss 4.042611635668436e-06 average time 0.032516761799161034 iter num 20\n",
            "loss 5.794558546767803e-06 average time 0.0316856911996183 iter num 40\n",
            "loss 7.656176421733107e-06 average time 0.031643371283037895 iter num 60\n",
            "loss 3.390674464753829e-05 average time 0.03161922099975527 iter num 80\n",
            "loss 0.00016605108976364136 average time 0.03169932451986824 iter num 100\n",
            "loss 1.3711525753024034e-05 average time 0.033160582849450296 iter num 20\n",
            "loss 5.941363724559778e-06 average time 0.03253049592467505 iter num 40\n",
            "loss 1.2244348909007385e-05 average time 0.03273782484969464 iter num 60\n",
            "loss 1.2139972568547819e-05 average time 0.032509623499754524 iter num 80\n",
            "loss 4.231805633025942e-06 average time 0.03235669800989854 iter num 100\n",
            "loss 4.28405746788485e-06 average time 0.03342571385037445 iter num 20\n",
            "loss 3.6642909435613547e-06 average time 0.03256336790018395 iter num 40\n",
            "loss 3.2406460377387702e-06 average time 0.03223926533349489 iter num 60\n",
            "loss 5.408831384556834e-06 average time 0.032299259150204304 iter num 80\n",
            "loss 1.7328626199741848e-05 average time 0.03222740249013441 iter num 100\n",
            "loss 8.721742233319674e-06 average time 0.032939496100516406 iter num 20\n",
            "loss 3.081769364143838e-06 average time 0.03253604350002206 iter num 40\n",
            "loss 8.196087037504185e-06 average time 0.0323033115168073 iter num 60\n",
            "loss 1.2699306353169959e-05 average time 0.03236196863758778 iter num 80\n",
            "loss 2.817373569996562e-06 average time 0.03227075075010362 iter num 100\n",
            "loss 1.2520259588200133e-05 average time 0.03392695434995403 iter num 20\n",
            "loss 4.686898137151729e-06 average time 0.03296618052490885 iter num 40\n",
            "loss 2.326817138964543e-06 average time 0.0333894107665401 iter num 60\n",
            "loss 5.832403530803276e-06 average time 0.033272339100039974 iter num 80\n",
            "loss 3.500095317576779e-06 average time 0.0333144485599405 iter num 100\n",
            "loss 2.3817186956875958e-05 average time 0.03320800894980493 iter num 20\n",
            "loss 4.64314553028089e-06 average time 0.03230400247484795 iter num 40\n",
            "loss 1.263375725102378e-05 average time 0.03208864276645424 iter num 60\n",
            "loss 8.610732948000077e-06 average time 0.032468822462487876 iter num 80\n",
            "loss 0.0001625486183911562 average time 0.03256196305996127 iter num 100\n",
            "loss 8.640445230412297e-06 average time 0.033161909749651386 iter num 20\n",
            "loss 4.001300567324506e-06 average time 0.03224394697490425 iter num 40\n",
            "loss 2.4873124857549556e-05 average time 0.03202875266670162 iter num 60\n",
            "loss 3.963621566072106e-06 average time 0.031989452399920994 iter num 80\n",
            "loss 9.219265848514624e-06 average time 0.031845213469969165 iter num 100\n",
            "loss 1.1916493349417578e-05 average time 0.03219754540059512 iter num 20\n",
            "loss 4.66080791738932e-06 average time 0.031566892000682856 iter num 40\n",
            "loss 4.259928573446814e-06 average time 0.031554374433774984 iter num 60\n",
            "loss 3.6750302569998894e-06 average time 0.031782016612851294 iter num 80\n",
            "loss 4.037590315419948e-06 average time 0.03192838054026652 iter num 100\n",
            "loss 1.0702130566642154e-05 average time 0.033508233549946455 iter num 20\n",
            "loss 9.752656296768691e-06 average time 0.032580757975119924 iter num 40\n",
            "loss 5.7159413699992e-06 average time 0.03330628458358357 iter num 60\n",
            "loss 7.700804417254403e-06 average time 0.03284273598769687 iter num 80\n",
            "loss 7.982554961927235e-06 average time 0.03268802325019351 iter num 100\n",
            "loss 3.2332955015590414e-06 average time 0.03245305884975096 iter num 20\n",
            "loss 3.848369033221388e-06 average time 0.03161218432487658 iter num 40\n",
            "loss 1.5813027857802808e-05 average time 0.03182101466669943 iter num 60\n",
            "loss 9.415740350959823e-06 average time 0.03218885013743602 iter num 80\n",
            "loss 8.073150638665538e-06 average time 0.032158233299887796 iter num 100\n",
            "loss 6.081602805352304e-06 average time 0.032692566150581116 iter num 20\n",
            "loss 1.7776459344531759e-06 average time 0.03220397707545999 iter num 40\n",
            "loss 4.0009185795497615e-06 average time 0.03236778053360467 iter num 60\n",
            "loss 2.7819401111628395e-06 average time 0.031965125162605544 iter num 80\n",
            "loss 1.176454679807648e-05 average time 0.032159431120016964 iter num 100\n",
            "loss 6.148148258944275e-06 average time 0.03558867819974694 iter num 20\n",
            "loss 9.146204320131801e-06 average time 0.03379872602463365 iter num 40\n",
            "loss 1.4143437510938384e-05 average time 0.03321022473319317 iter num 60\n",
            "loss 5.117203272675397e-06 average time 0.03326612328746705 iter num 80\n",
            "loss 4.72088413516758e-06 average time 0.033050199249919386 iter num 100\n",
            "loss 4.700208592112176e-06 average time 0.03440402884989453 iter num 20\n",
            "loss 3.7278409763530362e-06 average time 0.03384706669994557 iter num 40\n",
            "loss 6.469491836469388e-06 average time 0.033689762483360634 iter num 60\n",
            "loss 9.354699614050332e-06 average time 0.03356129732510453 iter num 80\n",
            "loss 5.33880620423588e-06 average time 0.03336101930002769 iter num 100\n",
            "loss 4.996870302420575e-06 average time 0.034123670899862193 iter num 20\n",
            "loss 6.672867584711639e-06 average time 0.03327284092492846 iter num 40\n",
            "loss 1.630162660148926e-05 average time 0.03295708808333681 iter num 60\n",
            "loss 3.759600531338947e-06 average time 0.03280643751236312 iter num 80\n",
            "loss 5.021416200179374e-06 average time 0.03267676330993709 iter num 100\n",
            "loss 7.795128112775274e-06 average time 0.032315871100399816 iter num 20\n",
            "loss 9.015579962579068e-06 average time 0.032248374800201415 iter num 40\n",
            "loss 1.8664126400835812e-05 average time 0.03215806693327371 iter num 60\n",
            "loss 1.0011061931436416e-05 average time 0.0318438446748587 iter num 80\n",
            "loss 3.64350466952601e-06 average time 0.0317010600800495 iter num 100\n",
            "loss 9.84755388344638e-06 average time 0.03224158824996266 iter num 20\n",
            "loss 4.7941280172381084e-06 average time 0.03217870747494089 iter num 40\n",
            "loss 6.215275334398029e-06 average time 0.031980027300051006 iter num 60\n",
            "loss 4.618595085048582e-06 average time 0.032250530912460815 iter num 80\n",
            "loss 3.808349447353976e-06 average time 0.032223689259953975 iter num 100\n",
            "loss 7.398779416689649e-06 average time 0.032322076299715265 iter num 20\n",
            "loss 1.1133554835396353e-05 average time 0.032107094999901165 iter num 40\n",
            "loss 1.0922625733655877e-05 average time 0.03188139978337858 iter num 60\n",
            "loss 4.809721303900005e-06 average time 0.03173929866234175 iter num 80\n",
            "loss 9.386814781464636e-06 average time 0.03193716263984243 iter num 100\n",
            "loss 8.158148375514429e-06 average time 0.03251778854992153 iter num 20\n",
            "loss 5.391776085161837e-06 average time 0.032713344750027316 iter num 40\n",
            "loss 1.1390321560611483e-05 average time 0.03252867368343383 iter num 60\n",
            "loss 1.0735629075497855e-05 average time 0.03209999410009914 iter num 80\n",
            "loss 1.8155904399463907e-05 average time 0.032505118310145915 iter num 100\n",
            "loss 7.90645572124049e-06 average time 0.034921157650023814 iter num 20\n",
            "loss 2.8445090265449835e-06 average time 0.03315851527486302 iter num 40\n",
            "loss 4.950294623995433e-06 average time 0.033319006666412555 iter num 60\n",
            "loss 5.009941560274456e-06 average time 0.03286363557472214 iter num 80\n",
            "loss 8.386195077036973e-06 average time 0.032956647999817505 iter num 100\n",
            "loss 8.280359907075763e-06 average time 0.032239971099625106 iter num 20\n",
            "loss 4.69366977995378e-06 average time 0.03228762937460487 iter num 40\n",
            "loss 9.640332791605033e-06 average time 0.03249048901640587 iter num 60\n",
            "loss 9.02378360478906e-06 average time 0.03281539046220132 iter num 80\n",
            "loss 3.949734946218086e-06 average time 0.03260011162976298 iter num 100\n",
            "loss 6.788677183067193e-06 average time 0.033135652700366336 iter num 20\n",
            "loss 8.561012691643555e-06 average time 0.032171307250246174 iter num 40\n",
            "loss 1.1934965186810587e-05 average time 0.03268211100027353 iter num 60\n",
            "loss 6.340524578263285e-06 average time 0.032443400475176534 iter num 80\n",
            "loss 6.517740075651091e-06 average time 0.03236936549015809 iter num 100\n",
            "loss 5.2095756473136134e-06 average time 0.032939206199807815 iter num 20\n",
            "loss 6.326621587504633e-06 average time 0.03230081429992424 iter num 40\n",
            "loss 4.952338713337667e-06 average time 0.0322129797667003 iter num 60\n",
            "loss 6.913870038260939e-06 average time 0.031990368525066516 iter num 80\n",
            "loss 4.200940566079225e-06 average time 0.031846358919938214 iter num 100\n",
            "loss 3.7700381199101685e-06 average time 0.033370718049627615 iter num 20\n",
            "loss 9.611057612346485e-05 average time 0.03247437499976513 iter num 40\n",
            "loss 0.00010613753693178296 average time 0.03193311344966787 iter num 60\n",
            "loss 9.187268005916849e-06 average time 0.03167041374967994 iter num 80\n",
            "loss 3.0720620998181403e-06 average time 0.03187549401976866 iter num 100\n",
            "loss 1.2486721061577555e-05 average time 0.035250837749845235 iter num 20\n",
            "loss 3.337939006087254e-06 average time 0.034002866724767954 iter num 40\n",
            "loss 0.00010209219908574596 average time 0.03314202598339762 iter num 60\n",
            "loss 4.801358954864554e-06 average time 0.03263436813767839 iter num 80\n",
            "loss 4.341857675171923e-06 average time 0.03254606556020008 iter num 100\n",
            "loss 2.0727798982989043e-05 average time 0.03256074560013076 iter num 20\n",
            "loss 6.643885626544943e-06 average time 0.032921301999977004 iter num 40\n",
            "loss 5.371220140659716e-06 average time 0.032850763750078234 iter num 60\n",
            "loss 4.219879429001594e-06 average time 0.03274834968756295 iter num 80\n",
            "loss 8.436295502178837e-06 average time 0.03261287338991679 iter num 100\n",
            "loss 4.0383740270044655e-05 average time 0.03258706875021744 iter num 20\n",
            "loss 1.2804661309928633e-05 average time 0.0318508781253513 iter num 40\n",
            "loss 5.945765678916359e-06 average time 0.03170117001706482 iter num 60\n",
            "loss 9.438550478080288e-06 average time 0.0317962439627081 iter num 80\n",
            "loss 9.125322321779095e-06 average time 0.03169347215021844 iter num 100\n",
            "loss 3.85497742172447e-06 average time 0.03352370619977592 iter num 20\n",
            "loss 4.3819923121191096e-06 average time 0.03229671367480478 iter num 40\n",
            "loss 3.859893240587553e-06 average time 0.03218478153327548 iter num 60\n",
            "loss 4.545395768218441e-06 average time 0.03211601181242259 iter num 80\n",
            "loss 4.858069587498903e-06 average time 0.03221840816990152 iter num 100\n",
            "loss 7.981015187397134e-06 average time 0.03431138345004001 iter num 20\n",
            "loss 3.983092483395012e-06 average time 0.03267311507497652 iter num 40\n",
            "loss 2.8198951440572273e-06 average time 0.03300830933318745 iter num 60\n",
            "loss 3.538453711371403e-06 average time 0.03264652562484116 iter num 80\n",
            "loss 8.017391337489244e-06 average time 0.03287089183988428 iter num 100\n",
            "loss 4.703805643657688e-06 average time 0.036078890750650315 iter num 20\n",
            "loss 4.908126356895082e-06 average time 0.033841524375111474 iter num 40\n",
            "loss 4.929139777232194e-06 average time 0.032849698966917154 iter num 60\n",
            "loss 2.8583997391251614e-06 average time 0.032405808812836764 iter num 80\n",
            "loss 3.3230513508897275e-05 average time 0.03218345308021526 iter num 100\n",
            "loss 6.243888492463157e-05 average time 0.034019697249823365 iter num 20\n",
            "loss 1.4967691640777048e-05 average time 0.032605254524787596 iter num 40\n",
            "loss 7.448985343216918e-06 average time 0.03238465516648527 iter num 60\n",
            "loss 3.1033725917950505e-06 average time 0.032076406287478675 iter num 80\n",
            "loss 8.251562576333527e-06 average time 0.03218109793997428 iter num 100\n",
            "loss 2.0222632883815095e-06 average time 0.03342790490023617 iter num 20\n",
            "loss 2.63473266386427e-05 average time 0.032965335824974316 iter num 40\n",
            "loss 1.8835755327017978e-05 average time 0.03268716483350242 iter num 60\n",
            "loss 3.360376240379992e-06 average time 0.03250769846254116 iter num 80\n",
            "loss 3.7809345485584345e-06 average time 0.03228638539003441 iter num 100\n",
            "loss 3.0366290957317688e-06 average time 0.035012600749905684 iter num 20\n",
            "loss 7.2583761721034534e-06 average time 0.03404059917520499 iter num 40\n",
            "loss 6.7100636442773975e-06 average time 0.03333720889992643 iter num 60\n",
            "loss 5.8832600188907236e-05 average time 0.03274111631253618 iter num 80\n",
            "loss 3.5936357107857475e-06 average time 0.032458373450026556 iter num 100\n",
            "loss 1.6101086657727137e-05 average time 0.032621169800404456 iter num 20\n",
            "loss 7.005786756053567e-06 average time 0.03234045107519705 iter num 40\n",
            "loss 1.2867383702541701e-05 average time 0.03222114300024259 iter num 60\n",
            "loss 4.104627805645578e-06 average time 0.032140852975135206 iter num 80\n",
            "loss 3.5098255466436967e-06 average time 0.03256726999014063 iter num 100\n",
            "loss 3.3135738704004325e-06 average time 0.032532181750502785 iter num 20\n",
            "loss 3.872424713335931e-06 average time 0.03203330482529054 iter num 40\n",
            "loss 7.794927114446182e-06 average time 0.031619401083541256 iter num 60\n",
            "loss 9.628282896301243e-06 average time 0.03193784631262133 iter num 80\n",
            "loss 6.1913633544463664e-06 average time 0.03180164845016407 iter num 100\n",
            "loss 4.662924311560346e-06 average time 0.03291695435000293 iter num 20\n",
            "loss 7.777260179864243e-06 average time 0.03215166649997627 iter num 40\n",
            "loss 5.359029273677152e-06 average time 0.032054986450020806 iter num 60\n",
            "loss 7.013433787506074e-06 average time 0.03201556320000236 iter num 80\n",
            "loss 3.2858329177543055e-06 average time 0.031873738360081914 iter num 100\n",
            "loss 1.0401427061879076e-05 average time 0.03268894485026976 iter num 20\n",
            "loss 4.32703609476448e-06 average time 0.03287848265008506 iter num 40\n",
            "loss 6.0974261941737495e-06 average time 0.03359438520001277 iter num 60\n",
            "loss 4.229159003443783e-06 average time 0.03330411342490151 iter num 80\n",
            "loss 9.589567525836173e-06 average time 0.03304843376998178 iter num 100\n",
            "loss 7.853080205677543e-06 average time 0.03270586214985087 iter num 20\n",
            "loss 4.6747572923777625e-06 average time 0.03322276762473848 iter num 40\n",
            "loss 2.834382939909119e-06 average time 0.03352409603321575 iter num 60\n",
            "loss 1.8914900010713609e-06 average time 0.033268499637506464 iter num 80\n",
            "loss 5.437729214463616e-06 average time 0.03316165486994578 iter num 100\n",
            "loss 5.121717185829766e-05 average time 0.03265499104963965 iter num 20\n",
            "loss 1.4341455425892491e-05 average time 0.031906093225006774 iter num 40\n",
            "loss 8.753383553994354e-06 average time 0.03238982105012837 iter num 60\n",
            "loss 8.699941645318177e-06 average time 0.03216736730018965 iter num 80\n",
            "loss 3.7528914162976434e-06 average time 0.03223333474015817 iter num 100\n",
            "loss 6.552449576702202e-06 average time 0.03230467934972694 iter num 20\n",
            "loss 3.132076244583004e-06 average time 0.032122330724905626 iter num 40\n",
            "loss 3.887070761265932e-06 average time 0.03260478714995164 iter num 60\n",
            "loss 8.247545338235795e-06 average time 0.03221541995003463 iter num 80\n",
            "loss 5.166575647308491e-05 average time 0.032020031029969684 iter num 100\n",
            "loss 5.211564712226391e-06 average time 0.03352073744990776 iter num 20\n",
            "loss 3.2088116768136388e-06 average time 0.03337709697516402 iter num 40\n",
            "loss 1.0144951374968514e-05 average time 0.03323009476692581 iter num 60\n",
            "loss 4.547391654341482e-06 average time 0.032629209100241495 iter num 80\n",
            "loss 8.569761121179909e-06 average time 0.0323191828101335 iter num 100\n",
            "loss 9.690078513813205e-06 average time 0.032086774049639645 iter num 20\n",
            "loss 4.7041603465913795e-06 average time 0.03166940534974856 iter num 40\n",
            "loss 4.755842383019626e-06 average time 0.031266858249910004 iter num 60\n",
            "loss 5.2783866522077005e-06 average time 0.0312406792373622 iter num 80\n",
            "loss 4.321092092141043e-06 average time 0.0313324426897816 iter num 100\n",
            "loss 6.452848083426943e-06 average time 0.03648536410037195 iter num 20\n",
            "loss 4.34098910773173e-06 average time 0.03398880727490905 iter num 40\n",
            "loss 6.731132998538669e-06 average time 0.03307474244981374 iter num 60\n",
            "loss 2.891110852942802e-06 average time 0.03280137462493258 iter num 80\n",
            "loss 9.426892574992962e-06 average time 0.0326324791998195 iter num 100\n",
            "loss 3.1729862257634522e-06 average time 0.03415384195031947 iter num 20\n",
            "loss 1.293923196499236e-05 average time 0.03288744592500734 iter num 40\n",
            "loss 3.55110751115717e-06 average time 0.03239614141657512 iter num 60\n",
            "loss 2.441810465825256e-05 average time 0.03238934017495012 iter num 80\n",
            "loss 7.842076229280792e-06 average time 0.03232597819998773 iter num 100\n",
            "loss 8.5753481471329e-06 average time 0.03346268390014302 iter num 20\n",
            "loss 4.710970188170904e-06 average time 0.03358819710001626 iter num 40\n",
            "loss 5.108046480017947e-06 average time 0.032842767349999726 iter num 60\n",
            "loss 4.544388502836227e-06 average time 0.03252674596246834 iter num 80\n",
            "loss 2.638371370267123e-06 average time 0.03237115497995546 iter num 100\n",
            "loss 0.00013585416309069842 average time 0.034500824450151416 iter num 20\n",
            "loss 3.702003232319839e-05 average time 0.03363824392508832 iter num 40\n",
            "loss 1.2386696653265972e-05 average time 0.032970853999844014 iter num 60\n",
            "loss 0.00014847064448986202 average time 0.032989411275002566 iter num 80\n",
            "loss 1.4926469702913892e-05 average time 0.03276975755994499 iter num 100\n",
            "loss 7.082409410941182e-06 average time 0.032245687449722026 iter num 20\n",
            "loss 6.83328516970505e-06 average time 0.03198443337496428 iter num 40\n",
            "loss 5.980697551422054e-06 average time 0.03169871656664327 iter num 60\n",
            "loss 2.4070336621662136e-06 average time 0.031794970324835956 iter num 80\n",
            "loss 4.133329184696777e-06 average time 0.03192764093990263 iter num 100\n",
            "loss 4.317706498113694e-06 average time 0.03315568569960305 iter num 20\n",
            "loss 9.169850272883195e-06 average time 0.032455353349723735 iter num 40\n",
            "loss 3.1164292977337027e-06 average time 0.032179645483180745 iter num 60\n",
            "loss 9.534455602988601e-05 average time 0.0320153471247977 iter num 80\n",
            "loss 4.8574956963420846e-06 average time 0.032244674199828294 iter num 100\n",
            "loss 5.464787591336062e-06 average time 0.032840143050452755 iter num 20\n",
            "loss 7.099145022948505e-06 average time 0.03260756687532194 iter num 40\n",
            "loss 1.4483359336736612e-05 average time 0.03250695711691757 iter num 60\n",
            "loss 6.3742977545189206e-06 average time 0.03211763593781143 iter num 80\n",
            "loss 5.376040007831762e-06 average time 0.03227496076036914 iter num 100\n",
            "loss 5.940880328125786e-06 average time 0.032793377600319217 iter num 20\n",
            "loss 6.1488308347179554e-06 average time 0.03188366369986397 iter num 40\n",
            "loss 9.411946848558728e-06 average time 0.03173392808318264 iter num 60\n",
            "loss 4.978600372851361e-06 average time 0.032089021712363316 iter num 80\n",
            "loss 4.941780389344785e-06 average time 0.03198878701979993 iter num 100\n",
            "loss 7.792357791913673e-06 average time 0.03389323910032545 iter num 20\n",
            "loss 5.170653821551241e-06 average time 0.03302056437532883 iter num 40\n",
            "loss 3.1908609798847465e-06 average time 0.032543189800162506 iter num 60\n",
            "loss 3.636584096966544e-06 average time 0.03346399010019922 iter num 80\n",
            "loss 9.314626367995515e-06 average time 0.03316323649018159 iter num 100\n",
            "loss 2.979754981424776e-06 average time 0.03345449054941128 iter num 20\n",
            "loss 1.1687165169860236e-05 average time 0.03248115794958721 iter num 40\n",
            "loss 3.5634220694191754e-05 average time 0.03236358559994793 iter num 60\n",
            "loss 6.28646830591606e-06 average time 0.03251687358738309 iter num 80\n",
            "loss 3.3430656003474724e-06 average time 0.032268334099899224 iter num 100\n",
            "loss 4.690363311965484e-06 average time 0.033216691849884225 iter num 20\n",
            "loss 3.215316610294394e-06 average time 0.03305367380007738 iter num 40\n",
            "loss 4.290372089599259e-06 average time 0.03239565831675766 iter num 60\n",
            "loss 4.556876319838921e-06 average time 0.03277961898752437 iter num 80\n",
            "loss 2.5676183668110752e-06 average time 0.03259237158010365 iter num 100\n",
            "loss 9.034712093125563e-06 average time 0.033366814949295075 iter num 20\n",
            "loss 4.5656402107852045e-06 average time 0.032655998274913146 iter num 40\n",
            "loss 4.061404979438521e-06 average time 0.03265117581662101 iter num 60\n",
            "loss 1.1756803360185586e-05 average time 0.03223771097491408 iter num 80\n",
            "loss 6.579118689842289e-06 average time 0.032388595119846285 iter num 100\n",
            "loss 1.8466602114131092e-06 average time 0.03364961325041804 iter num 20\n",
            "loss 7.2938992161653005e-06 average time 0.032515712925214754 iter num 40\n",
            "loss 4.294169684726512e-06 average time 0.03223069201697702 iter num 60\n",
            "loss 3.525697138684336e-06 average time 0.03194737445028295 iter num 80\n",
            "loss 4.244470801495481e-06 average time 0.03235986898020201 iter num 100\n",
            "loss 3.062503310502507e-05 average time 0.03703654929959157 iter num 20\n",
            "loss 1.2210040040372405e-05 average time 0.03564030384986836 iter num 40\n",
            "loss 3.947991899622139e-06 average time 0.034569283133290205 iter num 60\n",
            "loss 2.26713677875523e-06 average time 0.03420454854986019 iter num 80\n",
            "loss 5.302300451148767e-06 average time 0.034002872689816284 iter num 100\n",
            "loss 7.17493139745784e-06 average time 0.03441269125050894 iter num 20\n",
            "loss 4.3420072870503645e-06 average time 0.03395783840060176 iter num 40\n",
            "loss 7.158567768783541e-06 average time 0.03402376991713633 iter num 60\n",
            "loss 3.4582569696794963e-06 average time 0.033829231088020606 iter num 80\n",
            "loss 8.727096428629011e-06 average time 0.03346547914039547 iter num 100\n",
            "loss 1.093282571673626e-05 average time 0.036004909749499346 iter num 20\n",
            "loss 6.966406999708852e-06 average time 0.03481851387487041 iter num 40\n",
            "loss 2.965369276353158e-06 average time 0.034728174116510975 iter num 60\n",
            "loss 4.971105681761401e-06 average time 0.03390137901233174 iter num 80\n",
            "loss 1.6552163287997246e-05 average time 0.03340963072994782 iter num 100\n",
            "loss 3.298187039035838e-06 average time 0.03250307109919959 iter num 20\n",
            "loss 4.514575266512111e-06 average time 0.0327589149996129 iter num 40\n",
            "loss 5.550456990022212e-06 average time 0.032403112999782026 iter num 60\n",
            "loss 4.652222742151935e-06 average time 0.032759855124822934 iter num 80\n",
            "loss 4.0105273910739925e-06 average time 0.032494374759808126 iter num 100\n",
            "loss 2.1577354345936328e-05 average time 0.03342986365023535 iter num 20\n",
            "loss 1.6576987036387436e-05 average time 0.03276894742530203 iter num 40\n",
            "loss 7.04416015651077e-06 average time 0.03325558690024385 iter num 60\n",
            "loss 7.884298611315899e-06 average time 0.03283993436280071 iter num 80\n",
            "loss 3.485315119178267e-06 average time 0.03319104816007894 iter num 100\n",
            "loss 1.0089880561281461e-05 average time 0.033161310549803605 iter num 20\n",
            "loss 2.1399115212261677e-05 average time 0.03210045404975972 iter num 40\n",
            "loss 9.837587640504353e-06 average time 0.03272328611668248 iter num 60\n",
            "loss 3.41281815963157e-06 average time 0.03290361213753386 iter num 80\n",
            "loss 7.094243756000651e-06 average time 0.03269463005999569 iter num 100\n",
            "loss 2.867032208087039e-06 average time 0.031743566950171956 iter num 20\n",
            "loss 8.951871132012457e-05 average time 0.03266615210004602 iter num 40\n",
            "loss 7.267309229064267e-06 average time 0.032297473533374914 iter num 60\n",
            "loss 3.128993512291345e-06 average time 0.03197189623756458 iter num 80\n",
            "loss 3.722409928741399e-06 average time 0.03188914084992575 iter num 100\n",
            "loss 1.6146685084095225e-05 average time 0.03361436800005322 iter num 20\n",
            "loss 7.10807853465667e-06 average time 0.03322867902534199 iter num 40\n",
            "loss 5.016268460167339e-06 average time 0.03336514873365862 iter num 60\n",
            "loss 6.7315004343981855e-06 average time 0.03376915831272527 iter num 80\n",
            "loss 6.4744272094685584e-06 average time 0.033607100660265135 iter num 100\n",
            "loss 7.0531914388993755e-06 average time 0.03222148720033147 iter num 20\n",
            "loss 3.294202542747371e-06 average time 0.032055513675095425 iter num 40\n",
            "loss 2.5208808438037522e-05 average time 0.03191407748339164 iter num 60\n",
            "loss 3.7332899864850333e-06 average time 0.03180993062496782 iter num 80\n",
            "loss 2.705506858546869e-06 average time 0.03180955664007343 iter num 100\n",
            "loss 1.0034925253421534e-05 average time 0.03289818489956815 iter num 20\n",
            "loss 5.94704351897235e-06 average time 0.03224107142495995 iter num 40\n",
            "loss 6.769208084733691e-06 average time 0.03245798589990348 iter num 60\n",
            "loss 3.1777049116499256e-06 average time 0.03284813897494132 iter num 80\n",
            "loss 1.413009249517927e-05 average time 0.03275762664994545 iter num 100\n",
            "loss 5.255139512883034e-06 average time 0.03348496635026095 iter num 20\n",
            "loss 3.077612063862034e-06 average time 0.03222056855011033 iter num 40\n",
            "loss 4.568901204038411e-06 average time 0.0321955206999822 iter num 60\n",
            "loss 3.52520146407187e-06 average time 0.03194473698754337 iter num 80\n",
            "loss 3.2850916795723606e-06 average time 0.03175555941008497 iter num 100\n",
            "loss 6.39984955341788e-06 average time 0.03300428930015187 iter num 20\n",
            "loss 4.055173576489324e-06 average time 0.03229979299994738 iter num 40\n",
            "loss 6.046754151611822e-06 average time 0.03278752633335292 iter num 60\n",
            "loss 2.66367828771763e-06 average time 0.032566787612631744 iter num 80\n",
            "loss 5.425412837212207e-06 average time 0.03277930517015193 iter num 100\n",
            "loss 6.041594588168664e-06 average time 0.03385038700052974 iter num 20\n",
            "loss 1.1349160558893345e-05 average time 0.03327037042527081 iter num 40\n",
            "loss 4.793147127202246e-06 average time 0.033963956950052915 iter num 60\n",
            "loss 4.736333721666597e-06 average time 0.033634657162610895 iter num 80\n",
            "loss 3.326889327581739e-06 average time 0.03369459075001942 iter num 100\n",
            "loss 7.240823833853938e-06 average time 0.034431344099903075 iter num 20\n",
            "loss 1.000670908979373e-05 average time 0.03367290704973129 iter num 40\n",
            "loss 1.3442769159155432e-05 average time 0.0335992771666497 iter num 60\n",
            "loss 6.221209332579747e-06 average time 0.03423514822493416 iter num 80\n",
            "loss 3.5133921301166993e-06 average time 0.03377789900998323 iter num 100\n",
            "loss 7.234389613586245e-06 average time 0.03367226775008021 iter num 20\n",
            "loss 1.0025356459664181e-05 average time 0.03272225307500776 iter num 40\n",
            "loss 6.469095296779415e-06 average time 0.03242908725011754 iter num 60\n",
            "loss 1.1822628039226402e-05 average time 0.03228194065018215 iter num 80\n",
            "loss 5.734502337872982e-06 average time 0.0324646777901944 iter num 100\n",
            "loss 3.997756721219048e-06 average time 0.03462526209932548 iter num 20\n",
            "loss 0.00012411308125592768 average time 0.03370395444953829 iter num 40\n",
            "loss 4.661560524255037e-06 average time 0.03301661614950717 iter num 60\n",
            "loss 1.1113894288428128e-05 average time 0.03269477018707221 iter num 80\n",
            "loss 7.606971394125139e-06 average time 0.03251070678958058 iter num 100\n",
            "loss 5.070872703072382e-06 average time 0.034013220750421166 iter num 20\n",
            "loss 7.991276106622536e-06 average time 0.03303221059995849 iter num 40\n",
            "loss 2.9659183837793535e-06 average time 0.03241675906671541 iter num 60\n",
            "loss 5.623442120850086e-06 average time 0.03222543902511461 iter num 80\n",
            "loss 4.623223958333256e-06 average time 0.03202371173007123 iter num 100\n",
            "loss 7.88061061030021e-06 average time 0.03241569589954452 iter num 20\n",
            "loss 6.150530680315569e-06 average time 0.03376167047499621 iter num 40\n",
            "loss 5.5165469348139595e-06 average time 0.03307467380000162 iter num 60\n",
            "loss 8.944774890551344e-06 average time 0.03283307007504845 iter num 80\n",
            "loss 6.997812306508422e-06 average time 0.032536665440056824 iter num 100\n",
            "loss 1.2576832887134515e-05 average time 0.03206660780033417 iter num 20\n",
            "loss 3.1195052088150987e-06 average time 0.031406715025150335 iter num 40\n",
            "loss 5.491871434060158e-06 average time 0.03149794213337979 iter num 60\n",
            "loss 1.5619301848346367e-05 average time 0.03173078812496897 iter num 80\n",
            "loss 4.247428933012998e-06 average time 0.03187544619999244 iter num 100\n",
            "loss 3.971599198848708e-06 average time 0.032821334800610204 iter num 20\n",
            "loss 6.439811841119081e-06 average time 0.03155046037545617 iter num 40\n",
            "loss 3.3130625070043607e-06 average time 0.03203719543368303 iter num 60\n",
            "loss 5.533865078177769e-06 average time 0.03191101801285186 iter num 80\n",
            "loss 5.672082806995604e-06 average time 0.032285514330324075 iter num 100\n",
            "loss 1.0015827683673706e-05 average time 0.03269659269953991 iter num 20\n",
            "loss 1.6312678781105205e-05 average time 0.032979125850124545 iter num 40\n",
            "loss 5.386995781009318e-06 average time 0.03261806793343567 iter num 60\n",
            "loss 7.339011972362641e-06 average time 0.032647321750027915 iter num 80\n",
            "loss 3.077521250816062e-05 average time 0.03264864526001474 iter num 100\n",
            "loss 1.5013571101007983e-05 average time 0.03380331435037078 iter num 20\n",
            "loss 5.772234999312786e-06 average time 0.03326533975005077 iter num 40\n",
            "loss 3.2698367249395233e-06 average time 0.032530215199949454 iter num 60\n",
            "loss 3.863094207190443e-06 average time 0.03241825621257703 iter num 80\n",
            "loss 1.0547462807153352e-05 average time 0.0322386834200006 iter num 100\n",
            "loss 9.490058801020496e-06 average time 0.032880479000959896 iter num 20\n",
            "loss 4.442415593075566e-05 average time 0.03215529495046212 iter num 40\n",
            "loss 3.926968929590657e-06 average time 0.03249860133364564 iter num 60\n",
            "loss 2.367549996051821e-06 average time 0.03231086913774561 iter num 80\n",
            "loss 6.494472927442985e-06 average time 0.03214864014022169 iter num 100\n",
            "loss 3.987901891377987e-06 average time 0.034185433099628425 iter num 20\n",
            "loss 1.8255299437441863e-05 average time 0.03315190319981411 iter num 40\n",
            "loss 7.20536218068446e-06 average time 0.03265694698311563 iter num 60\n",
            "loss 4.293810434319312e-06 average time 0.032641431712363556 iter num 80\n",
            "loss 9.832929208641872e-06 average time 0.032539603909972355 iter num 100\n",
            "loss 7.340254342125263e-06 average time 0.03306377994977083 iter num 20\n",
            "loss 5.181075721338857e-06 average time 0.03364464552487334 iter num 40\n",
            "loss 4.872220870311139e-06 average time 0.0334971395833539 iter num 60\n",
            "loss 1.1420332157285884e-05 average time 0.03304599944990514 iter num 80\n",
            "loss 5.249803507467732e-06 average time 0.032731888129892465 iter num 100\n",
            "loss 2.6371051262685796e-06 average time 0.03209087159993942 iter num 20\n",
            "loss 3.4160268569394248e-06 average time 0.032111331174928634 iter num 40\n",
            "loss 2.7084852263215e-06 average time 0.0323899424499056 iter num 60\n",
            "loss 5.248220986686647e-06 average time 0.032165278874845174 iter num 80\n",
            "loss 5.926221547269961e-06 average time 0.03229250864995265 iter num 100\n",
            "loss 4.016214461444179e-06 average time 0.03299213755053643 iter num 20\n",
            "loss 3.432984385653981e-06 average time 0.03322828640029911 iter num 40\n",
            "loss 1.0917910003627185e-05 average time 0.03273363151693047 iter num 60\n",
            "loss 3.598272314775386e-06 average time 0.032688330612700156 iter num 80\n",
            "loss 6.020542969054077e-06 average time 0.032719422370209944 iter num 100\n",
            "loss 6.2494282246916555e-06 average time 0.03400067294969631 iter num 20\n",
            "loss 3.016360551555408e-06 average time 0.03265385895010695 iter num 40\n",
            "loss 0.00017517461674287915 average time 0.03309986410010121 iter num 60\n",
            "loss 6.408045464922907e-06 average time 0.03321594337503484 iter num 80\n",
            "loss 4.2293977458029985e-06 average time 0.033227008899993964 iter num 100\n",
            "loss 7.1243298407352995e-06 average time 0.03257301899975573 iter num 20\n",
            "loss 7.555370757472701e-06 average time 0.032082933600213435 iter num 40\n",
            "loss 3.549687562554027e-06 average time 0.03223065436674612 iter num 60\n",
            "loss 2.649063617354841e-06 average time 0.03244795983760014 iter num 80\n",
            "loss 1.3538633538701106e-05 average time 0.03270332932013844 iter num 100\n",
            "loss 3.553114311216632e-06 average time 0.034891519399570824 iter num 20\n",
            "loss 4.268800694262609e-06 average time 0.03409226130006573 iter num 40\n",
            "loss 5.313838482834399e-05 average time 0.03319854206668727 iter num 60\n",
            "loss 4.645447916118428e-06 average time 0.03287414137507767 iter num 80\n",
            "loss 2.8590834517672192e-06 average time 0.03258396155004448 iter num 100\n",
            "loss 5.698702807421796e-06 average time 0.032858839750042536 iter num 20\n",
            "loss 3.0973023967817426e-06 average time 0.032094037649676464 iter num 40\n",
            "loss 2.2993876882537734e-06 average time 0.03227362629974474 iter num 60\n",
            "loss 4.742652436107164e-06 average time 0.03233750134982074 iter num 80\n",
            "loss 3.7492384308279725e-06 average time 0.032282354809867686 iter num 100\n",
            "loss 8.094151780824177e-06 average time 0.03262483885009715 iter num 20\n",
            "loss 1.090850400942145e-05 average time 0.031921602575130235 iter num 40\n",
            "loss 3.681562020574347e-06 average time 0.03179738630024076 iter num 60\n",
            "loss 2.3994170987862162e-06 average time 0.03170036217529741 iter num 80\n",
            "loss 3.3070598419726593e-06 average time 0.03172359850024804 iter num 100\n",
            "loss 1.9736204194487073e-05 average time 0.033909362849044555 iter num 20\n",
            "loss 4.419371634867275e-06 average time 0.03269129367436108 iter num 40\n",
            "loss 4.229012574796798e-06 average time 0.03239261784953366 iter num 60\n",
            "loss 3.90296736441087e-06 average time 0.03197391863709527 iter num 80\n",
            "loss 3.873190053127473e-06 average time 0.032271677309690855 iter num 100\n",
            "loss 6.6793390942621045e-06 average time 0.03375893185020686 iter num 20\n",
            "loss 4.059691946167732e-06 average time 0.03318533345018295 iter num 40\n",
            "loss 2.896824980780366e-06 average time 0.03271410420014339 iter num 60\n",
            "loss 3.6182921121508116e-06 average time 0.03245078801255659 iter num 80\n",
            "loss 4.637426172848791e-06 average time 0.03210373949001223 iter num 100\n",
            "loss 4.948772129864665e-06 average time 0.034080044500115036 iter num 20\n",
            "loss 1.3313307135831565e-05 average time 0.03323733950001042 iter num 40\n",
            "loss 2.525639729356044e-06 average time 0.03263937196679763 iter num 60\n",
            "loss 4.1287921703769825e-06 average time 0.03243103491267903 iter num 80\n",
            "loss 0.00011168422497576103 average time 0.03236295989012433 iter num 100\n",
            "loss 7.132119662855985e-06 average time 0.03413730710035452 iter num 20\n",
            "loss 5.717475687561091e-06 average time 0.03301111830005539 iter num 40\n",
            "loss 4.477442416828126e-06 average time 0.032436706683438385 iter num 60\n",
            "loss 5.713240625482285e-06 average time 0.03222758613765109 iter num 80\n",
            "loss 2.557752395659918e-06 average time 0.032421298430235765 iter num 100\n",
            "loss 2.8232143449713476e-05 average time 0.03269242759943154 iter num 20\n",
            "loss 4.348803031462012e-06 average time 0.03257024284994259 iter num 40\n",
            "loss 9.17117085919017e-06 average time 0.03211739768315359 iter num 60\n",
            "loss 4.877331775787752e-06 average time 0.03195917158723205 iter num 80\n",
            "loss 1.1370217180228792e-05 average time 0.0322123154997098 iter num 100\n",
            "loss 2.4411683625658043e-06 average time 0.03304348389974621 iter num 20\n",
            "loss 7.612284207425546e-06 average time 0.03219324485007746 iter num 40\n",
            "loss 3.7529966903093737e-06 average time 0.032206495766513396 iter num 60\n",
            "loss 4.602045919455122e-06 average time 0.03185414497488637 iter num 80\n",
            "loss 3.013397872564383e-06 average time 0.0316905593298361 iter num 100\n",
            "loss 8.48860690894071e-06 average time 0.033921603949420384 iter num 20\n",
            "loss 7.103357347659767e-06 average time 0.03304937767461524 iter num 40\n",
            "loss 8.47349929244956e-06 average time 0.03226440343308544 iter num 60\n",
            "loss 5.589507509284886e-06 average time 0.032225804324980345 iter num 80\n",
            "loss 2.742508968367474e-06 average time 0.0320226082798763 iter num 100\n",
            "loss 5.1403035286057275e-06 average time 0.031940837649563035 iter num 20\n",
            "loss 4.452222583495313e-06 average time 0.032131641524938456 iter num 40\n",
            "loss 1.7888665752252564e-05 average time 0.03209797816653008 iter num 60\n",
            "loss 1.0308081073162612e-05 average time 0.032151409874859385 iter num 80\n",
            "loss 4.016916591353947e-06 average time 0.03212671705976391 iter num 100\n",
            "loss 5.811642040498555e-06 average time 0.03400821675004408 iter num 20\n",
            "loss 1.2806478480342776e-05 average time 0.0328285993753525 iter num 40\n",
            "loss 1.9808603610727005e-05 average time 0.0324716978836174 iter num 60\n",
            "loss 4.036903192172758e-06 average time 0.0323743117752656 iter num 80\n",
            "loss 5.733106263505761e-06 average time 0.03212634272018477 iter num 100\n",
            "loss 3.645204742497299e-06 average time 0.032627236450571216 iter num 20\n",
            "loss 6.474685960711213e-06 average time 0.03227762372516736 iter num 40\n",
            "loss 9.223850611306261e-06 average time 0.03196448403359682 iter num 60\n",
            "loss 3.0997907742857933e-06 average time 0.03199562480017448 iter num 80\n",
            "loss 4.4395969780453015e-06 average time 0.03179278491021251 iter num 100\n",
            "loss 3.6052340419701068e-06 average time 0.03305456305006373 iter num 20\n",
            "loss 7.552211172878742e-06 average time 0.03249744474996987 iter num 40\n",
            "loss 2.782278443191899e-06 average time 0.03271231108337815 iter num 60\n",
            "loss 6.149534601718187e-05 average time 0.03227844803759581 iter num 80\n",
            "loss 4.235048436385114e-06 average time 0.03210058862008736 iter num 100\n",
            "loss 6.45455975245568e-06 average time 0.033923805650192665 iter num 20\n",
            "loss 5.0309063226450235e-06 average time 0.03329841740014672 iter num 40\n",
            "loss 7.243468644446693e-06 average time 0.033444633050142634 iter num 60\n",
            "loss 4.78900938105653e-06 average time 0.03341810493739104 iter num 80\n",
            "loss 6.8689428189827595e-06 average time 0.03303844888996536 iter num 100\n",
            "loss 1.5773186532896943e-05 average time 0.03382335545047681 iter num 20\n",
            "loss 3.59832811227534e-05 average time 0.03328323882542463 iter num 40\n",
            "loss 4.322758286434691e-06 average time 0.03306418036669735 iter num 60\n",
            "loss 4.5139636313251685e-06 average time 0.033170669837545574 iter num 80\n",
            "loss 3.333874246891355e-06 average time 0.0334257040500961 iter num 100\n",
            "loss 1.2785035323759075e-05 average time 0.03305898050020915 iter num 20\n",
            "loss 1.8956599888042547e-05 average time 0.03202871410021544 iter num 40\n",
            "loss 2.585619085948565e-06 average time 0.03201211630021135 iter num 60\n",
            "loss 3.6808969525736757e-06 average time 0.03211589596262456 iter num 80\n",
            "loss 3.920922608813271e-06 average time 0.03253962176018831 iter num 100\n",
            "loss 7.470744549209485e-06 average time 0.033197360200028926 iter num 20\n",
            "loss 1.4876289242238272e-05 average time 0.03220266144971902 iter num 40\n",
            "loss 5.277007858239813e-06 average time 0.03204198399980669 iter num 60\n",
            "loss 1.1897293916263152e-05 average time 0.03184286189975864 iter num 80\n",
            "loss 9.677852176537272e-06 average time 0.03173880378981266 iter num 100\n",
            "loss 3.5218570246797753e-06 average time 0.03512050825029291 iter num 20\n",
            "loss 9.67134183156304e-05 average time 0.03333609652499945 iter num 40\n",
            "loss 2.87275497612427e-06 average time 0.03253637271664047 iter num 60\n",
            "loss 8.465264727419708e-06 average time 0.032334080662485574 iter num 80\n",
            "loss 4.989909484720556e-06 average time 0.03227170588997978 iter num 100\n",
            "loss 2.927995865320554e-06 average time 0.03299140779981826 iter num 20\n",
            "loss 5.277193849906325e-06 average time 0.03214600204973976 iter num 40\n",
            "loss 1.3295402823132463e-05 average time 0.03217528301641626 iter num 60\n",
            "loss 3.994477083324455e-06 average time 0.03242808841218903 iter num 80\n",
            "loss 2.634961447256501e-06 average time 0.032110401099707815 iter num 100\n",
            "loss 8.608709322288632e-06 average time 0.03257736130017293 iter num 20\n",
            "loss 8.640969099360518e-06 average time 0.03209251340022092 iter num 40\n",
            "loss 5.709563993150368e-06 average time 0.03211203833313145 iter num 60\n",
            "loss 7.472289325960446e-06 average time 0.0321216626997284 iter num 80\n",
            "loss 1.4058251508686226e-05 average time 0.0320339012398108 iter num 100\n",
            "loss 4.0004697439144365e-06 average time 0.03493655175025197 iter num 20\n",
            "loss 5.040156338509405e-06 average time 0.0337100968001323 iter num 40\n",
            "loss 6.018348358338699e-06 average time 0.033310867233315854 iter num 60\n",
            "loss 2.2528842237079516e-05 average time 0.0333569642375096 iter num 80\n",
            "loss 2.8813965400331654e-06 average time 0.03327176860995678 iter num 100\n",
            "loss 4.999215434509097e-06 average time 0.03320319690010365 iter num 20\n",
            "loss 4.189871106063947e-06 average time 0.03291232775045501 iter num 40\n",
            "loss 3.7661297938029747e-06 average time 0.0325302968336473 iter num 60\n",
            "loss 4.144838385400362e-06 average time 0.03207499241266305 iter num 80\n",
            "loss 5.80511459702393e-06 average time 0.03207017726002959 iter num 100\n",
            "loss 1.575428177602589e-05 average time 0.03544922224973561 iter num 20\n",
            "loss 1.9526327378116548e-05 average time 0.03341793374984263 iter num 40\n",
            "loss 7.418698714900529e-06 average time 0.033633354866712276 iter num 60\n",
            "loss 1.1820593499578536e-05 average time 0.033005816700188005 iter num 80\n",
            "loss 0.00012700790830422193 average time 0.03293986520013277 iter num 100\n",
            "loss 4.8532301661907695e-06 average time 0.0338492420505645 iter num 20\n",
            "loss 7.1045037657313515e-06 average time 0.0333437510501426 iter num 40\n",
            "loss 2.12233262573136e-05 average time 0.03355116923351791 iter num 60\n",
            "loss 5.476689693750814e-06 average time 0.03325894737508861 iter num 80\n",
            "loss 2.2848773824080126e-06 average time 0.03300795746010408 iter num 100\n",
            "loss 4.345999059296446e-06 average time 0.03282943389949651 iter num 20\n",
            "loss 7.573828042950481e-06 average time 0.032329962799485655 iter num 40\n",
            "loss 7.645097866770811e-06 average time 0.031860013849654936 iter num 60\n",
            "loss 1.8117696527042426e-05 average time 0.031761380524767444 iter num 80\n",
            "loss 5.496377070812741e-06 average time 0.0318687193797814 iter num 100\n",
            "loss 3.543686034390703e-06 average time 0.03287986860013916 iter num 20\n",
            "loss 3.0560452159988927e-06 average time 0.03231544084992492 iter num 40\n",
            "loss 8.272115337604191e-06 average time 0.03187665761676423 iter num 60\n",
            "loss 5.8084547163161915e-06 average time 0.0317226415125333 iter num 80\n",
            "loss 1.894518209155649e-05 average time 0.03171616952004115 iter num 100\n",
            "loss 6.100680366216693e-06 average time 0.0315624091994323 iter num 20\n",
            "loss 8.63108471094165e-06 average time 0.031801627599816126 iter num 40\n",
            "loss 1.869873472060135e-06 average time 0.03150678826653651 iter num 60\n",
            "loss 5.05530215377803e-06 average time 0.03149395545001425 iter num 80\n",
            "loss 5.87653812544886e-06 average time 0.03165077063007629 iter num 100\n",
            "loss 4.776386504090624e-06 average time 0.03250802435031801 iter num 20\n",
            "loss 4.087423803866841e-06 average time 0.031980363450293224 iter num 40\n",
            "loss 1.5988447557901964e-05 average time 0.031900039700182486 iter num 60\n",
            "loss 2.1094238036312163e-06 average time 0.03182985011267192 iter num 80\n",
            "loss 4.947270099364687e-06 average time 0.031845176730203095 iter num 100\n",
            "loss 1.1413262654969003e-05 average time 0.03244789484997455 iter num 20\n",
            "loss 7.257821380335372e-06 average time 0.03221974142506952 iter num 40\n",
            "loss 5.385022632253822e-06 average time 0.032171465183212906 iter num 60\n",
            "loss 8.240246643254068e-06 average time 0.03261569247488296 iter num 80\n",
            "loss 1.1094111869169865e-05 average time 0.0323202775799291 iter num 100\n",
            "loss 6.762365956092253e-06 average time 0.03433832204991631 iter num 20\n",
            "loss 3.999545697297435e-06 average time 0.03320303087502907 iter num 40\n",
            "loss 3.362060851941351e-06 average time 0.03315703975003999 iter num 60\n",
            "loss 4.08905452786712e-06 average time 0.03320318229989425 iter num 80\n",
            "loss 7.752169949526433e-06 average time 0.03315316004991473 iter num 100\n",
            "loss 1.076922217180254e-05 average time 0.03466772140018293 iter num 20\n",
            "loss 1.0565961019892711e-05 average time 0.03430957012560611 iter num 40\n",
            "loss 6.732571819156874e-06 average time 0.03428678243368874 iter num 60\n",
            "loss 7.834035386622418e-06 average time 0.033573768275300606 iter num 80\n",
            "loss 5.726725703425473e-06 average time 0.03369710105023842 iter num 100\n",
            "loss 6.806911187595688e-06 average time 0.03456136465010786 iter num 20\n",
            "loss 1.2469580724427942e-05 average time 0.03381229310007257 iter num 40\n",
            "loss 8.007993528735824e-06 average time 0.033953545150264594 iter num 60\n",
            "loss 5.010666427551769e-06 average time 0.033944548550107354 iter num 80\n",
            "loss 8.838202120386995e-06 average time 0.0335250125400853 iter num 100\n",
            "loss 6.94601476425305e-06 average time 0.03207037824977306 iter num 20\n",
            "loss 6.701125130348373e-06 average time 0.03203916737511463 iter num 40\n",
            "loss 7.953323802212253e-06 average time 0.03245195768352763 iter num 60\n",
            "loss 2.47655475504871e-06 average time 0.03226842641265648 iter num 80\n",
            "loss 2.925875151049695e-06 average time 0.0322447196000212 iter num 100\n",
            "loss 4.282860572857317e-06 average time 0.033391690250209646 iter num 20\n",
            "loss 4.75977367386804e-06 average time 0.032379780650171594 iter num 40\n",
            "loss 2.399665754637681e-05 average time 0.032022432450200235 iter num 60\n",
            "loss 6.061348358343821e-06 average time 0.03216447038757906 iter num 80\n",
            "loss 4.1582034100429155e-06 average time 0.032264027899946084 iter num 100\n",
            "loss 3.5117977859044913e-06 average time 0.03263640764998854 iter num 20\n",
            "loss 2.8984068194404244e-06 average time 0.03181942814981085 iter num 40\n",
            "loss 1.840933873609174e-05 average time 0.03184253409999656 iter num 60\n",
            "loss 4.494030690693762e-06 average time 0.032029601350132 iter num 80\n",
            "loss 4.5275337470229715e-06 average time 0.03204219994015148 iter num 100\n",
            "loss 1.4237299183150753e-05 average time 0.03290662850067747 iter num 20\n",
            "loss 3.884118086716626e-06 average time 0.031998606225170076 iter num 40\n",
            "loss 3.7844342841708567e-06 average time 0.03220807240031718 iter num 60\n",
            "loss 6.181045318953693e-05 average time 0.031853344012779415 iter num 80\n",
            "loss 2.1983460101182573e-06 average time 0.031771000320186434 iter num 100\n",
            "loss 2.874971187338815e-06 average time 0.03294660495030257 iter num 20\n",
            "loss 6.585932624147972e-06 average time 0.03211941725012366 iter num 40\n",
            "loss 8.193852409021929e-06 average time 0.032298274416947 iter num 60\n",
            "loss 5.689426416211063e-06 average time 0.032294248400330614 iter num 80\n",
            "loss 3.7289655665517785e-06 average time 0.03205534211028862 iter num 100\n",
            "loss 1.4229680346034002e-05 average time 0.03251415850045305 iter num 20\n",
            "loss 1.863788565970026e-05 average time 0.03168946760015388 iter num 40\n",
            "loss 3.4517292988311965e-06 average time 0.031987669733219565 iter num 60\n",
            "loss 3.010334467035136e-06 average time 0.03182692558743838 iter num 80\n",
            "loss 7.259468929987634e-06 average time 0.03179224840991082 iter num 100\n",
            "loss 8.532409992767498e-06 average time 0.03415267789969221 iter num 20\n",
            "loss 4.218222329654964e-06 average time 0.032833316674623345 iter num 40\n",
            "loss 9.272394891013391e-06 average time 0.0324935989497438 iter num 60\n",
            "loss 4.420799541549059e-06 average time 0.0326140400121858 iter num 80\n",
            "loss 5.2704363042721525e-06 average time 0.03282801995981572 iter num 100\n",
            "loss 1.3451127415464725e-05 average time 0.032400144699749944 iter num 20\n",
            "loss 7.0755413617007434e-06 average time 0.03199011164988406 iter num 40\n",
            "loss 6.467126240750076e-06 average time 0.031867175433277835 iter num 60\n",
            "loss 3.3133578654087614e-06 average time 0.0317985514499469 iter num 80\n",
            "loss 5.304623755364446e-06 average time 0.031847875179955736 iter num 100\n",
            "loss 7.373134394583758e-06 average time 0.03430613539985643 iter num 20\n",
            "loss 3.1602335184288677e-06 average time 0.03403803992468966 iter num 40\n",
            "loss 3.150089241898968e-06 average time 0.0332282036165149 iter num 60\n",
            "loss 4.895064193988219e-06 average time 0.03290220365001915 iter num 80\n",
            "loss 3.6359326713863993e-06 average time 0.03263085445993056 iter num 100\n",
            "loss 0.00010516971815377474 average time 0.03385819779978192 iter num 20\n",
            "loss 2.0617317204596475e-05 average time 0.03295197772486062 iter num 40\n",
            "loss 5.0876897148555145e-06 average time 0.032273047749671 iter num 60\n",
            "loss 5.431804765976267e-06 average time 0.03198912218713303 iter num 80\n",
            "loss 5.660038823407376e-06 average time 0.031934192439512114 iter num 100\n",
            "loss 1.1071158951381221e-05 average time 0.03249200925001787 iter num 20\n",
            "loss 5.09314031660324e-06 average time 0.032172462750259 iter num 40\n",
            "loss 3.35528125106066e-06 average time 0.032597353150110094 iter num 60\n",
            "loss 3.5531136290956056e-06 average time 0.03269933481255975 iter num 80\n",
            "loss 1.9731049178517424e-05 average time 0.03276080068004376 iter num 100\n",
            "loss 6.292155376286246e-06 average time 0.03431165964975662 iter num 20\n",
            "loss 5.543801762541989e-06 average time 0.03307603149969509 iter num 40\n",
            "loss 6.737866442563245e-06 average time 0.03269502209974841 iter num 60\n",
            "loss 6.955108801776078e-06 average time 0.03314638597476005 iter num 80\n",
            "loss 3.7121733384992694e-06 average time 0.03275597220992495 iter num 100\n",
            "loss 7.0379273893195204e-06 average time 0.03362133519985946 iter num 20\n",
            "loss 1.7538233805680647e-05 average time 0.032745513224563184 iter num 40\n",
            "loss 4.682227881858125e-06 average time 0.03277102541615022 iter num 60\n",
            "loss 3.365620159456739e-06 average time 0.032435884537107995 iter num 80\n",
            "loss 5.000033979740692e-06 average time 0.03226740398971742 iter num 100\n",
            "loss 3.984221621067263e-06 average time 0.03270546765015751 iter num 20\n",
            "loss 6.219786428118823e-06 average time 0.031783229049869986 iter num 40\n",
            "loss 4.994567007088335e-06 average time 0.03222070716668289 iter num 60\n",
            "loss 3.401749381737318e-06 average time 0.03262848972512984 iter num 80\n",
            "loss 2.512299943191465e-06 average time 0.03237719288019434 iter num 100\n",
            "loss 4.1281632547907066e-06 average time 0.03673568910035101 iter num 20\n",
            "loss 4.825988526135916e-06 average time 0.03421672292515723 iter num 40\n",
            "loss 3.88830630981829e-06 average time 0.03309615991693136 iter num 60\n",
            "loss 6.954234777367674e-06 average time 0.03276950807517096 iter num 80\n",
            "loss 4.074430307809962e-06 average time 0.032518388540083834 iter num 100\n",
            "loss 2.493092506483663e-05 average time 0.033509692100233225 iter num 20\n",
            "loss 1.3548303286370356e-05 average time 0.03293665312539815 iter num 40\n",
            "loss 3.99614691559691e-06 average time 0.032555409600475586 iter num 60\n",
            "loss 1.016758778860094e-05 average time 0.03250610272534686 iter num 80\n",
            "loss 3.9475917219533585e-06 average time 0.03231947307031078 iter num 100\n",
            "loss 4.488738795771496e-06 average time 0.03230215069979749 iter num 20\n",
            "loss 2.9224875106592663e-06 average time 0.032022479800252766 iter num 40\n",
            "loss 2.8607801141333766e-06 average time 0.032327282466818964 iter num 60\n",
            "loss 8.903537491278257e-06 average time 0.032449292812589195 iter num 80\n",
            "loss 1.0243457836622838e-05 average time 0.03256505942015792 iter num 100\n",
            "loss 7.125733191060135e-06 average time 0.0326874760001374 iter num 20\n",
            "loss 9.209107702190522e-06 average time 0.03252744749997873 iter num 40\n",
            "loss 2.7486908038554247e-06 average time 0.03245206296651304 iter num 60\n",
            "loss 7.65955337556079e-06 average time 0.03218784653754483 iter num 80\n",
            "loss 5.082122515887022e-05 average time 0.03218754217992682 iter num 100\n",
            "loss 4.881399036094081e-06 average time 0.0349935270502101 iter num 20\n",
            "loss 3.7420093121909304e-06 average time 0.033288297100079946 iter num 40\n",
            "loss 5.404943749454105e-06 average time 0.03374823161666427 iter num 60\n",
            "loss 8.419968253292609e-06 average time 0.03365242757504348 iter num 80\n",
            "loss 9.222515473084059e-06 average time 0.033399257080054666 iter num 100\n",
            "loss 8.567570148443338e-06 average time 0.033948593050081396 iter num 20\n",
            "loss 1.1041563084290829e-05 average time 0.03335572769992723 iter num 40\n",
            "loss 3.542382182786241e-05 average time 0.033568678766702456 iter num 60\n",
            "loss 4.738779352919664e-06 average time 0.0337930394372961 iter num 80\n",
            "loss 6.6950360633200034e-06 average time 0.033336809079919476 iter num 100\n",
            "loss 7.704546987952199e-06 average time 0.03373578739992809 iter num 20\n",
            "loss 3.943011051887879e-06 average time 0.03252894617507991 iter num 40\n",
            "loss 3.250076133554103e-06 average time 0.032256911533355984 iter num 60\n",
            "loss 6.984696938161505e-06 average time 0.032125287374901744 iter num 80\n",
            "loss 1.8588242483019712e-06 average time 0.03205548458987323 iter num 100\n",
            "loss 2.462948759784922e-05 average time 0.03194678494965046 iter num 20\n",
            "loss 3.641703733592294e-05 average time 0.03182162872481058 iter num 40\n",
            "loss 6.288158601819305e-06 average time 0.03218648246650749 iter num 60\n",
            "loss 1.0042986104963347e-05 average time 0.032438746849948075 iter num 80\n",
            "loss 8.234676352003589e-05 average time 0.032220666789980895 iter num 100\n",
            "loss 7.363011263805674e-06 average time 0.03212340710033459 iter num 20\n",
            "loss 3.516729066177504e-06 average time 0.03328482492506737 iter num 40\n",
            "loss 6.580178251169855e-06 average time 0.03300578170013371 iter num 60\n",
            "loss 4.329994681029348e-06 average time 0.03294958948763451 iter num 80\n",
            "loss 1.199892631120747e-05 average time 0.032653258860045754 iter num 100\n",
            "loss 1.4978856597736012e-05 average time 0.03335921385041729 iter num 20\n",
            "loss 3.790262553593493e-06 average time 0.032230033350333545 iter num 40\n",
            "loss 1.316827183472924e-05 average time 0.03213092231690098 iter num 60\n",
            "loss 2.8383694825606653e-06 average time 0.032090348050178366 iter num 80\n",
            "loss 0.0001310902152908966 average time 0.031958960680349265 iter num 100\n",
            "loss 3.0370033528015483e-06 average time 0.032369060199562226 iter num 20\n",
            "loss 3.1440551993000554e-06 average time 0.03155714027470822 iter num 40\n",
            "loss 7.382287549262401e-06 average time 0.03158490851656097 iter num 60\n",
            "loss 2.651118165886146e-06 average time 0.03178354579999905 iter num 80\n",
            "loss 3.036592261196347e-06 average time 0.03172821474989178 iter num 100\n",
            "loss 6.67696167511167e-06 average time 0.03375293139979476 iter num 20\n",
            "loss 6.962884526728885e-06 average time 0.033148257349785125 iter num 40\n",
            "loss 3.2013538202591008e-06 average time 0.03267391914978361 iter num 60\n",
            "loss 2.561090013841749e-06 average time 0.032296398924927416 iter num 80\n",
            "loss 6.60041860101046e-06 average time 0.03231885530993168 iter num 100\n",
            "loss 4.56474526799866e-06 average time 0.033739922000313526 iter num 20\n",
            "loss 3.5320347251399653e-06 average time 0.03300720134984658 iter num 40\n",
            "loss 4.462461674847873e-06 average time 0.03265883366657363 iter num 60\n",
            "loss 8.473190973745659e-06 average time 0.032753174987601594 iter num 80\n",
            "loss 3.4387337564112386e-06 average time 0.0329916945300647 iter num 100\n",
            "loss 4.18385025113821e-06 average time 0.032074743100565684 iter num 20\n",
            "loss 3.5466166536934907e-06 average time 0.034035395475257246 iter num 40\n",
            "loss 1.7456491150369402e-06 average time 0.03316728343349193 iter num 60\n",
            "loss 4.669488589570392e-06 average time 0.03334182082498956 iter num 80\n",
            "loss 3.63347317033913e-06 average time 0.033261212789911954 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdK0IMDvOIl1"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzXW2m_IOBYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c5a082-f3dd-4efd-fb36-822bc39a7a52"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.3, 0.3]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.27130044, 0.90763223)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.2709]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9087], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4CQ1pnkOKZZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "db868273-33ed-49bd-bc60-959aab110267"
      },
      "source": [
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "BS_call_prices = []\n",
        "for p in prices:\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    BS_call_prices.append(bs_call(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, BS_call_prices, label = \"BS_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(BS_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq-PF4oaPNlh"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6sybszTOo51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "770e5235-70dc-42d6-de91-3a2b24980f4d"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0I0Kg81bxua",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "d6785b05-b42f-4a5a-e1aa-24f8218971c0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.75, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 0.75, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShGjACICb4pL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "c3832955-4f46-4481-8e39-d8ab6ebe604c"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.25, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1.25, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neXnYLzjOtPs"
      },
      "source": [
        ""
      ]
    }
  ]
}