{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of New Way for NN_BlackScholes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/Copy_of_New_Way_for_NN_BlackScholes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtECD9BnR2NC"
      },
      "source": [
        "# import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UddgD2b1Rz0p"
      },
      "source": [
        "# iv = 0.005\n",
        "# rnd_s0 = np.arange(0.75, 1.25 + iv, iv)\n",
        "# print(len(rnd_s0))\n",
        "# initial_stocks = np.array([rnd_s0[i] for i in np.random.randint(0, len(rnd_s0)+1, 2)])\n",
        "# initial_stocks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2gIC6kxF0ks"
      },
      "source": [
        "# Use Black-Scholes to Generate Data\n",
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5_EBlCgHsFt",
        "outputId": "347a3e7b-17c1-4f41-9c72-4f9614f15212"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0  15339      0 --:--:-- --:--:-- --:--:-- 15339\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg1VnsiZF3GM",
        "outputId": "f2f22394-3a3d-477b-ae64-ead0161a1a61"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "import cupy\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "    return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "          # intervals\n",
        "          iv_s0 = (1.25-0.75)/80\n",
        "          iv_sigma = (0.45-0.15)/40\n",
        "          iv_r = (0.6-0.25)/40\n",
        "          iv_K = (1.25-0.75)/80\n",
        "\n",
        "          # initial_stocks = np.array(np.random.randint(75,125,self.N_STOCKS)/100)\n",
        "          # iv_s0 = 0.005\n",
        "          rnd_s0 = np.arange(0.75, 1.25 + iv_s0, iv_s0)\n",
        "          initial_stocks = np.array([rnd_s0[i] for i in np.random.randint(0, len(rnd_s0), self.N_STOCKS)])\n",
        "\n",
        "          corr = np.diag(np.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "\n",
        "          # sigma = np.array(np.random.randint(15,45,self.N_STOCKS)/100)\n",
        "\n",
        "          # iv_sigma = 0.005\n",
        "          rnd_sigma = np.arange(0.15, 0.45 + iv_sigma, iv_sigma)\n",
        "          sigma = np.array([rnd_sigma[i] for i in np.random.randint(0, len(rnd_sigma), self.N_STOCKS)])\n",
        "\n",
        "          cov = (np.diag(sigma)).dot(corr).dot(np.diag(sigma))\n",
        "\n",
        "          # r = np.repeat(np.array(np.random.randint(25,60)/100), self.N_STOCKS)\n",
        "          # iv_r = 0.005\n",
        "          rnd_r = np.arange(0.25, 0.6 + iv_r, iv_r)\n",
        "          r = np.repeat(rnd_r[np.random.randint(0,len(rnd_r), 1)], self.N_STOCKS)\n",
        "\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          # K = np.random.randint(75,125)/100\n",
        "          # iv_K = 0.005\n",
        "          rnd_K = np.arange(0.75, 1.25 + iv_K, iv_K)\n",
        "          K = rnd_K[np.random.randint(0, len(rnd_K), 1)]\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "\n",
        "          European_Call_price = bs_call(initial_stocks,K,T,r,sigma)\n",
        "          Deltas = bs_delta(initial_stocks,K,T,r,sigma)\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price[0]\n",
        "          Y[op, 1:4] = cupy.array(Deltas, dtype=cupy.float32)\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (np.repeat(np.array(T), self.N_STOCKS), np.repeat(np.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNmazdSvGEho",
        "outputId": "3b0dcd4e-0093-430d-d568-8e36344411aa"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.35, 0.35]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.25, 0.25]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJNy_JNHGFGh"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNp9_sibGQSJ",
        "outputId": "56c6f2b9-a723-47b0-f6e6-bf53fc68b8de"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZmDAkPiGWyz",
        "outputId": "e267a0ca-2b26-457b-e3b5-98ecb0b231ea"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.3893764913082123 average time 0.07712499650002655 iter num 20\n",
            "loss 0.36389705538749695 average time 0.054137091075011766 iter num 40\n",
            "loss 0.3042798638343811 average time 0.04680394411667521 iter num 60\n",
            "loss 0.34460604190826416 average time 0.04295286473750366 iter num 80\n",
            "loss 0.2795102000236511 average time 0.040791658370008005 iter num 100\n",
            "loss 0.28387340903282166 average time 0.0327023625000038 iter num 20\n",
            "loss 0.25583401322364807 average time 0.03210939797500032 iter num 40\n",
            "loss 0.2284049093723297 average time 0.03181570838332088 iter num 60\n",
            "loss 0.2218768447637558 average time 0.03154334498748597 iter num 80\n",
            "loss 0.2166953831911087 average time 0.031610717079986445 iter num 100\n",
            "loss 0.18853919208049774 average time 0.03323158695001212 iter num 20\n",
            "loss 0.15265637636184692 average time 0.03246786822501235 iter num 40\n",
            "loss 0.1213235855102539 average time 0.032245764900009515 iter num 60\n",
            "loss 0.09973210841417313 average time 0.03232590555000172 iter num 80\n",
            "loss 0.10111295431852341 average time 0.03218742599000734 iter num 100\n",
            "loss 0.06639497727155685 average time 0.03272865589999583 iter num 20\n",
            "loss 0.034621939063072205 average time 0.03211908122501086 iter num 40\n",
            "loss 0.030064988881349564 average time 0.03189313221666149 iter num 60\n",
            "loss 0.0207804124802351 average time 0.03153968931250404 iter num 80\n",
            "loss 0.0174434594810009 average time 0.031548622380003055 iter num 100\n",
            "loss 0.012726525776088238 average time 0.033177173199999285 iter num 20\n",
            "loss 0.009597815573215485 average time 0.03227454589998615 iter num 40\n",
            "loss 0.010500484146177769 average time 0.031821698699999006 iter num 60\n",
            "loss 0.0075462376698851585 average time 0.03157766740000625 iter num 80\n",
            "loss 0.016269618645310402 average time 0.03161253356000543 iter num 100\n",
            "loss 0.016156263649463654 average time 0.033427366050023014 iter num 20\n",
            "loss 0.014011634513735771 average time 0.03329502485001399 iter num 40\n",
            "loss 0.02578698843717575 average time 0.03258111565000945 iter num 60\n",
            "loss 0.015586145222187042 average time 0.03240957486251546 iter num 80\n",
            "loss 0.020184839144349098 average time 0.03274450270001353 iter num 100\n",
            "loss 0.013326840475201607 average time 0.032754466699998375 iter num 20\n",
            "loss 0.0169052854180336 average time 0.032012134199993623 iter num 40\n",
            "loss 0.009174032136797905 average time 0.03161883046667905 iter num 60\n",
            "loss 0.014284450560808182 average time 0.03154246090001038 iter num 80\n",
            "loss 0.009528674185276031 average time 0.03143247159000339 iter num 100\n",
            "loss 0.011165725998580456 average time 0.03327828734996956 iter num 20\n",
            "loss 0.017060181125998497 average time 0.032199757824957945 iter num 40\n",
            "loss 0.009992443025112152 average time 0.0317536339333022 iter num 60\n",
            "loss 0.008889192715287209 average time 0.031624617712469674 iter num 80\n",
            "loss 0.011910692788660526 average time 0.03150928827997177 iter num 100\n",
            "loss 0.01588522084057331 average time 0.03380652935001081 iter num 20\n",
            "loss 0.01436572428792715 average time 0.033519613575009544 iter num 40\n",
            "loss 0.008189995773136616 average time 0.03351844470001121 iter num 60\n",
            "loss 0.00895362626761198 average time 0.033543866587504564 iter num 80\n",
            "loss 0.011288524605333805 average time 0.03341619841000238 iter num 100\n",
            "loss 0.011322703212499619 average time 0.03640270699999064 iter num 20\n",
            "loss 0.008168993517756462 average time 0.034916296124981726 iter num 40\n",
            "loss 0.006287903059273958 average time 0.03430558973332154 iter num 60\n",
            "loss 0.012817418202757835 average time 0.03408182984998973 iter num 80\n",
            "loss 0.016767440363764763 average time 0.03414687489998641 iter num 100\n",
            "loss 0.007368194870650768 average time 0.03503183490001902 iter num 20\n",
            "loss 0.013666315004229546 average time 0.03404157495000959 iter num 40\n",
            "loss 0.009097610600292683 average time 0.032890184650000265 iter num 60\n",
            "loss 0.015279070474207401 average time 0.03269968900000606 iter num 80\n",
            "loss 0.009969345293939114 average time 0.032399029270006846 iter num 100\n",
            "loss 0.007057115901261568 average time 0.032785718350010026 iter num 20\n",
            "loss 0.01105420384556055 average time 0.032025909625008356 iter num 40\n",
            "loss 0.012889844365417957 average time 0.031803838016662665 iter num 60\n",
            "loss 0.008085725829005241 average time 0.031745065337500475 iter num 80\n",
            "loss 0.009691153652966022 average time 0.0316283996500033 iter num 100\n",
            "loss 0.010591033846139908 average time 0.03226797894999436 iter num 20\n",
            "loss 0.011040695011615753 average time 0.03187144597498559 iter num 40\n",
            "loss 0.018188808113336563 average time 0.031326930849987396 iter num 60\n",
            "loss 0.005754637997597456 average time 0.03126918551248821 iter num 80\n",
            "loss 0.013593480922281742 average time 0.03143622759999744 iter num 100\n",
            "loss 0.008371368050575256 average time 0.03241585519997443 iter num 20\n",
            "loss 0.01302049495279789 average time 0.03164699842497498 iter num 40\n",
            "loss 0.015506145544350147 average time 0.03181738349998113 iter num 60\n",
            "loss 0.008428296074271202 average time 0.031717895599985016 iter num 80\n",
            "loss 0.008291089907288551 average time 0.031721506249980394 iter num 100\n",
            "loss 0.009737983345985413 average time 0.03253289515000688 iter num 20\n",
            "loss 0.009164846502244473 average time 0.03182513779999567 iter num 40\n",
            "loss 0.00651559978723526 average time 0.032174628049998925 iter num 60\n",
            "loss 0.01385990809649229 average time 0.03241110044999971 iter num 80\n",
            "loss 0.012403219938278198 average time 0.03231687107000198 iter num 100\n",
            "loss 0.012819787487387657 average time 0.033047544250007374 iter num 20\n",
            "loss 0.009623231366276741 average time 0.03210704827501445 iter num 40\n",
            "loss 0.007040910888463259 average time 0.031645439366680725 iter num 60\n",
            "loss 0.007129313424229622 average time 0.03162705688751259 iter num 80\n",
            "loss 0.01144643034785986 average time 0.031591843660007723 iter num 100\n",
            "loss 0.005691295024007559 average time 0.03275985230002334 iter num 20\n",
            "loss 0.007732010446488857 average time 0.032529008674993067 iter num 40\n",
            "loss 0.011729065328836441 average time 0.032023682266666735 iter num 60\n",
            "loss 0.00784230139106512 average time 0.032150267662504464 iter num 80\n",
            "loss 0.005369797348976135 average time 0.03205288064000797 iter num 100\n",
            "loss 0.009626828134059906 average time 0.03411756260004495 iter num 20\n",
            "loss 0.007011127192527056 average time 0.032568652375022114 iter num 40\n",
            "loss 0.004848141688853502 average time 0.03242333816668103 iter num 60\n",
            "loss 0.010286510922014713 average time 0.03193206226250993 iter num 80\n",
            "loss 0.007385771721601486 average time 0.0318386525200026 iter num 100\n",
            "loss 0.010193653404712677 average time 0.032040616999995566 iter num 20\n",
            "loss 0.010697668418288231 average time 0.0317310781499998 iter num 40\n",
            "loss 0.01143253780901432 average time 0.03224473286665746 iter num 60\n",
            "loss 0.007765045389533043 average time 0.03216136843748814 iter num 80\n",
            "loss 0.007180666550993919 average time 0.03220891532998394 iter num 100\n",
            "loss 0.0044396244920790195 average time 0.03232486615002017 iter num 20\n",
            "loss 0.009767502546310425 average time 0.031474120625000525 iter num 40\n",
            "loss 0.010350094176828861 average time 0.031548349566666425 iter num 60\n",
            "loss 0.00748402951285243 average time 0.03159583496250491 iter num 80\n",
            "loss 0.004799393005669117 average time 0.032326726590004 iter num 100\n",
            "loss 0.007982269860804081 average time 0.034531911400017636 iter num 20\n",
            "loss 0.003916488494724035 average time 0.03320639094999365 iter num 40\n",
            "loss 0.009787890128791332 average time 0.032627438783337466 iter num 60\n",
            "loss 0.005131714511662722 average time 0.03236227386249766 iter num 80\n",
            "loss 0.005849427543580532 average time 0.03216201341999749 iter num 100\n",
            "loss 0.01167390402406454 average time 0.03417360774997178 iter num 20\n",
            "loss 0.006489668041467667 average time 0.033713649599985726 iter num 40\n",
            "loss 0.0046510519459843636 average time 0.034047449466659906 iter num 60\n",
            "loss 0.00981331430375576 average time 0.03331016571249279 iter num 80\n",
            "loss 0.006004108116030693 average time 0.032884749479990205 iter num 100\n",
            "loss 0.007619986776262522 average time 0.032824303849974965 iter num 20\n",
            "loss 0.009871258400380611 average time 0.03241356512498328 iter num 40\n",
            "loss 0.006688111461699009 average time 0.03221863246664801 iter num 60\n",
            "loss 0.015950696542859077 average time 0.03212062268748639 iter num 80\n",
            "loss 0.01102504599839449 average time 0.0321213947799879 iter num 100\n",
            "loss 0.0088810408487916 average time 0.03349783390002585 iter num 20\n",
            "loss 0.0043015629053115845 average time 0.0328902888250127 iter num 40\n",
            "loss 0.007382519543170929 average time 0.03255917415000719 iter num 60\n",
            "loss 0.00834614597260952 average time 0.032664115800011474 iter num 80\n",
            "loss 0.00840121228247881 average time 0.032241836350006 iter num 100\n",
            "loss 0.009802639484405518 average time 0.032176017249969394 iter num 20\n",
            "loss 0.005043686367571354 average time 0.03264492672499273 iter num 40\n",
            "loss 0.004910777788609266 average time 0.032379803533319776 iter num 60\n",
            "loss 0.0073326025158166885 average time 0.0324575210499944 iter num 80\n",
            "loss 0.005751664284616709 average time 0.032504547699998054 iter num 100\n",
            "loss 0.010979157872498035 average time 0.03299788779999062 iter num 20\n",
            "loss 0.007871733047068119 average time 0.03377945145001036 iter num 40\n",
            "loss 0.012815704569220543 average time 0.033387822516685144 iter num 60\n",
            "loss 0.0066244821064174175 average time 0.032816100087501354 iter num 80\n",
            "loss 0.003887587459757924 average time 0.032662236760002086 iter num 100\n",
            "loss 0.005618707276880741 average time 0.03562530704996334 iter num 20\n",
            "loss 0.005046965554356575 average time 0.0336139080749831 iter num 40\n",
            "loss 0.003131078789010644 average time 0.03264783644999246 iter num 60\n",
            "loss 0.005873823072761297 average time 0.03249122551249854 iter num 80\n",
            "loss 0.006925303488969803 average time 0.032117988369996055 iter num 100\n",
            "loss 0.008929099887609482 average time 0.03207204084999375 iter num 20\n",
            "loss 0.004786145407706499 average time 0.03175741127499236 iter num 40\n",
            "loss 0.006248831748962402 average time 0.031984903366674186 iter num 60\n",
            "loss 0.00713967764750123 average time 0.031883967300001356 iter num 80\n",
            "loss 0.0049463664181530476 average time 0.03207072927999206 iter num 100\n",
            "loss 0.004535160027444363 average time 0.03388239409998732 iter num 20\n",
            "loss 0.003676182357594371 average time 0.032834609249999855 iter num 40\n",
            "loss 0.004124914295971394 average time 0.03279596694999706 iter num 60\n",
            "loss 0.004038497805595398 average time 0.032854197837494326 iter num 80\n",
            "loss 0.0021522855386137962 average time 0.03290473080000538 iter num 100\n",
            "loss 0.00277191330678761 average time 0.03338092699999606 iter num 20\n",
            "loss 0.006918040569871664 average time 0.032507523825006504 iter num 40\n",
            "loss 0.007037419825792313 average time 0.0330104143500004 iter num 60\n",
            "loss 0.004298856947571039 average time 0.032945532487499915 iter num 80\n",
            "loss 0.004368819296360016 average time 0.03263994216000356 iter num 100\n",
            "loss 0.006662191357463598 average time 0.03225205505000304 iter num 20\n",
            "loss 0.0024505923502147198 average time 0.03203300069999955 iter num 40\n",
            "loss 0.002367582404986024 average time 0.03196269780001255 iter num 60\n",
            "loss 0.0033293780870735645 average time 0.03228217778751343 iter num 80\n",
            "loss 0.004241804592311382 average time 0.032666114040009686 iter num 100\n",
            "loss 0.0025420691817998886 average time 0.03372753809999267 iter num 20\n",
            "loss 0.006309974007308483 average time 0.03308567959999209 iter num 40\n",
            "loss 0.005914622917771339 average time 0.03275084453332132 iter num 60\n",
            "loss 0.004896319936960936 average time 0.03259420482499422 iter num 80\n",
            "loss 0.0040126400999724865 average time 0.03248089478998736 iter num 100\n",
            "loss 0.00336670083925128 average time 0.03292467599999327 iter num 20\n",
            "loss 0.004297136329114437 average time 0.0328659209749901 iter num 40\n",
            "loss 0.0024148246739059687 average time 0.032633962966641165 iter num 60\n",
            "loss 0.00342118414118886 average time 0.032434553762467996 iter num 80\n",
            "loss 0.00210386049002409 average time 0.032353476919972764 iter num 100\n",
            "loss 0.001878365408629179 average time 0.033070259999999504 iter num 20\n",
            "loss 0.002826735842972994 average time 0.03331460274999358 iter num 40\n",
            "loss 0.003708353964611888 average time 0.03268055688332273 iter num 60\n",
            "loss 0.0019383806502446532 average time 0.0328131016374897 iter num 80\n",
            "loss 0.002765730256214738 average time 0.03301483015998883 iter num 100\n",
            "loss 0.0026989434845745564 average time 0.031986624750004466 iter num 20\n",
            "loss 0.0015947737265378237 average time 0.031894266500000865 iter num 40\n",
            "loss 0.003496929071843624 average time 0.03158554536666998 iter num 60\n",
            "loss 0.0031728874891996384 average time 0.031683388775005025 iter num 80\n",
            "loss 0.003091176738962531 average time 0.03194560306000085 iter num 100\n",
            "loss 0.001406524213962257 average time 0.03320176890000539 iter num 20\n",
            "loss 0.0025887470692396164 average time 0.032286614074990894 iter num 40\n",
            "loss 0.009608391672372818 average time 0.03212140534999814 iter num 60\n",
            "loss 0.0020791571587324142 average time 0.03199378158749653 iter num 80\n",
            "loss 0.0038429591804742813 average time 0.03214546042000393 iter num 100\n",
            "loss 0.002498997375369072 average time 0.03311893159998362 iter num 20\n",
            "loss 0.002428195206448436 average time 0.032536397174988 iter num 40\n",
            "loss 0.0014087404124438763 average time 0.03233143213333657 iter num 60\n",
            "loss 0.0024488188792020082 average time 0.03253055056250105 iter num 80\n",
            "loss 0.0012957061408087611 average time 0.032791054779993375 iter num 100\n",
            "loss 0.004615660756826401 average time 0.0336224183000013 iter num 20\n",
            "loss 0.0026575906667858362 average time 0.03309379499999636 iter num 40\n",
            "loss 0.004649966489523649 average time 0.032644084549997385 iter num 60\n",
            "loss 0.0033298921771347523 average time 0.03261108166249187 iter num 80\n",
            "loss 0.0025738112162798643 average time 0.032729989439994826 iter num 100\n",
            "loss 0.0013903641374781728 average time 0.03267071829998258 iter num 20\n",
            "loss 0.0020451305899769068 average time 0.032197892149974906 iter num 40\n",
            "loss 0.0015046222833916545 average time 0.032362028916641826 iter num 60\n",
            "loss 0.002070011803880334 average time 0.03234001687499131 iter num 80\n",
            "loss 0.002782992785796523 average time 0.03221486914000025 iter num 100\n",
            "loss 0.0020759333856403828 average time 0.03396406105001688 iter num 20\n",
            "loss 0.0033677166793495417 average time 0.03351659730001302 iter num 40\n",
            "loss 0.006049435120075941 average time 0.03326386123334638 iter num 60\n",
            "loss 0.002404457423835993 average time 0.032853021587513354 iter num 80\n",
            "loss 0.003627106314525008 average time 0.03255137955001146 iter num 100\n",
            "loss 0.0021073645912110806 average time 0.03407464549999304 iter num 20\n",
            "loss 0.002189293969422579 average time 0.03268207737499438 iter num 40\n",
            "loss 0.0024752956815063953 average time 0.03330522854997753 iter num 60\n",
            "loss 0.002514991909265518 average time 0.033795008274967134 iter num 80\n",
            "loss 0.0004498035996221006 average time 0.03351487656998188 iter num 100\n",
            "loss 0.0026041180826723576 average time 0.03395048460002954 iter num 20\n",
            "loss 0.0032524506095796824 average time 0.03305963055001371 iter num 40\n",
            "loss 0.0052408818155527115 average time 0.032952324966674953 iter num 60\n",
            "loss 0.0018588058883324265 average time 0.03328315613751158 iter num 80\n",
            "loss 0.002872176468372345 average time 0.03335800129000972 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ziJDSnnOuii"
      },
      "source": [
        "9 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0cc_DoENm4G"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tt0PFffNmVO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMkQh_5rNxAY"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_grid_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT2g0J3qN284"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eChrShF1N4nu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoOdFI5VN7zN"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_grid_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVPuJKhrOAet"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhKSwGk-O6R_"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 500000, batch = 16, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 10\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_grid_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idQlBe72TamF"
      },
      "source": [
        "4 + 9min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdK0IMDvOIl1"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzXW2m_IOBYP"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.3, 0.3]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.27130044, 0.90763223)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq-PF4oaPNlh"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4CQ1pnkOKZZ"
      },
      "source": [
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "BS_call_prices = []\n",
        "for p in prices:\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    BS_call_prices.append(bs_call(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, BS_call_prices, label = \"BS_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(BS_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6sybszTOo51"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0I0Kg81bxua"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.75, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 0.75, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShGjACICb4pL"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.25, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1.25, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neXnYLzjOtPs"
      },
      "source": [
        ""
      ]
    }
  ]
}