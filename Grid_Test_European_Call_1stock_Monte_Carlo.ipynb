{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid Test_European Call 1stock Monte Carlo",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/Grid_Test_European_Call_1stock_Monte_Carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYigDkiy0HU9",
        "outputId": "7659ebd5-ea5c-48e4-c6fb-9f91650037c5"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 10)\n",
        "K_range = jnp.linspace(0.75, 1.25, 10)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 5)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "\n",
        "print(S_range)\n",
        "print(K_range)\n",
        "print(sigma_range)\n",
        "print(r_range)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75       0.8055556  0.86111116 0.9166666  0.97222227 1.0277778\n",
            " 1.0833334  1.138889   1.1944445  1.25      ]\n",
            "[0.75       0.8055556  0.86111116 0.9166666  0.97222227 1.0277778\n",
            " 1.0833334  1.138889   1.1944445  1.25      ]\n",
            "[0.15       0.225      0.3        0.37499997 0.45      ]\n",
            "[0.01  0.025 0.04 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UIQxpJqK6OZr",
        "outputId": "4edd2973-1616-450e-bc8f-a17e55975db6"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "goptionvalueavg = jax.grad(optionvalueavg, argnums=1)\n",
        "\n",
        "#################################################################### Adjust all parameters here (not inside class)\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 10)\n",
        "K_range = jnp.linspace(0.75, 1.25, 10)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 5)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "T = 1.0\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "####################################################################\n",
        "\n",
        "call = []\n",
        "\n",
        "for S in S_range:\n",
        "  for K in K_range:\n",
        "    for r in r_range:\n",
        "      for sigma in sigma_range:    \n",
        "\n",
        "        initial_stocks = jnp.array([S]*numstocks) # must be float\n",
        "        r_tmp = jnp.array([r]*numstocks)\n",
        "        drift = r_tmp\n",
        "        cov = jnp.identity(numstocks)*sigma*sigma\n",
        "\n",
        "        European_Call_price = optionvalueavg(key, initial_stocks, numsteps, drift, r_tmp, cov, K, T)\n",
        "        Deltas = goptionvalueavg(keys, initial_stocks, numsteps, drift, r_tmp, cov, K, T)\n",
        "        call.append([T, K, S, sigma, r, r, European_Call_price] + list(Deltas)) #T, K, S, sigma, mu, r, price, delta\n",
        "\n",
        "Thedataset = pd.DataFrame(call)\n",
        "Thedataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.048458967</td>\n",
              "      <td>0.556185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.07066231</td>\n",
              "      <td>0.562336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.09278706</td>\n",
              "      <td>0.572583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.114792734</td>\n",
              "      <td>0.584748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.13664411</td>\n",
              "      <td>0.597456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.10037121</td>\n",
              "      <td>0.633561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.13602732</td>\n",
              "      <td>0.613980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.17195362</td>\n",
              "      <td>0.611451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.20784327</td>\n",
              "      <td>0.615662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.24356046</td>\n",
              "      <td>0.623065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1500 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1     2      3     4     5            6         7\n",
              "0     1.0  0.75  0.75  0.150  0.01  0.01  0.048458967  0.556185\n",
              "1     1.0  0.75  0.75  0.225  0.01  0.01   0.07066231  0.562336\n",
              "2     1.0  0.75  0.75  0.300  0.01  0.01   0.09278706  0.572583\n",
              "3     1.0  0.75  0.75  0.375  0.01  0.01  0.114792734  0.584748\n",
              "4     1.0  0.75  0.75  0.450  0.01  0.01   0.13664411  0.597456\n",
              "...   ...   ...   ...    ...   ...   ...          ...       ...\n",
              "1495  1.0  1.25  1.25  0.150  0.04  0.04   0.10037121  0.633561\n",
              "1496  1.0  1.25  1.25  0.225  0.04  0.04   0.13602732  0.613980\n",
              "1497  1.0  1.25  1.25  0.300  0.04  0.04   0.17195362  0.611451\n",
              "1498  1.0  1.25  1.25  0.375  0.04  0.04   0.20784327  0.615662\n",
              "1499  1.0  1.25  1.25  0.450  0.04  0.04   0.24356046  0.623065\n",
              "\n",
              "[1500 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQKnflf6peX"
      },
      "source": [
        "# save to csv\n",
        "Thedataset.to_csv('European_Call_1stock_MC_Datset.csv', index=False, header=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "skGWSSsG8TGG",
        "outputId": "018ae4a2-353e-4373-bdbb-1fc1ec60c974"
      },
      "source": [
        "# read csv\n",
        "import pandas as pd\n",
        "\n",
        "Thedataset = pd.read_csv('European_Call_1stock_MC_Datset.csv', header=None)\n",
        "Thedataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.048459</td>\n",
              "      <td>0.556185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.070662</td>\n",
              "      <td>0.562336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.092787</td>\n",
              "      <td>0.572583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.114793</td>\n",
              "      <td>0.584748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.136644</td>\n",
              "      <td>0.597456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.100371</td>\n",
              "      <td>0.633561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.136027</td>\n",
              "      <td>0.613980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.171954</td>\n",
              "      <td>0.611451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.207843</td>\n",
              "      <td>0.615662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.243560</td>\n",
              "      <td>0.623065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1500 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1     2      3     4     5         6         7\n",
              "0     1.0  0.75  0.75  0.150  0.01  0.01  0.048459  0.556185\n",
              "1     1.0  0.75  0.75  0.225  0.01  0.01  0.070662  0.562336\n",
              "2     1.0  0.75  0.75  0.300  0.01  0.01  0.092787  0.572583\n",
              "3     1.0  0.75  0.75  0.375  0.01  0.01  0.114793  0.584748\n",
              "4     1.0  0.75  0.75  0.450  0.01  0.01  0.136644  0.597456\n",
              "...   ...   ...   ...    ...   ...   ...       ...       ...\n",
              "1495  1.0  1.25  1.25  0.150  0.04  0.04  0.100371  0.633561\n",
              "1496  1.0  1.25  1.25  0.225  0.04  0.04  0.136027  0.613980\n",
              "1497  1.0  1.25  1.25  0.300  0.04  0.04  0.171954  0.611451\n",
              "1498  1.0  1.25  1.25  0.375  0.04  0.04  0.207843  0.615662\n",
              "1499  1.0  1.25  1.25  0.450  0.04  0.04  0.243560  0.623065\n",
              "\n",
              "[1500 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4HV-G44th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a327a656-e62b-4e3b-856f-654aefae28e7"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch\n",
        "torch.set_printoptions(precision=6)\n",
        "\n",
        "Thedataset_X = Thedataset.iloc[:,:6]\n",
        "Thedataset_Y = Thedataset.iloc[:,6:]\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.X = cupy.array(Thedataset_X)\n",
        "        self.Y = cupy.array(Thedataset_Y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(self.X.toDlpack()), from_dlpack(self.Y.toDlpack()))\n",
        "\n",
        "# print\n",
        "ds = OptionDataSet(max_len = 1)\n",
        "for i in ds:\n",
        "    print(i[0])\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    print(i[1].shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.000000, 0.750000, 0.750000, 0.150000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.750000, 0.225000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.750000, 0.300000, 0.010000, 0.010000],\n",
            "        ...,\n",
            "        [1.000000, 1.250000, 1.250000, 0.300000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.250000, 0.375000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.250000, 0.450000, 0.040000, 0.040000]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([1500, 6])\n",
            "tensor([[0.048459, 0.556185],\n",
            "        [0.070662, 0.562336],\n",
            "        [0.092787, 0.572583],\n",
            "        ...,\n",
            "        [0.171954, 0.611451],\n",
            "        [0.207843, 0.615662],\n",
            "        [0.243560, 0.623065]], device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([1500, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "3551e165-5787-487b-ea94-95e6f5b83dd3"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.03, 0.03]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.01, 0.01]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "ee26ec17-6857-48ce-f149-70e7ff703167"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 36.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 44.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 46.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeLVZiiaDS4y",
        "outputId": "b4d69a52-e9ea-4077-e31c-37c38f1a6560"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CyULkENYKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78e4f30-eaba-4a5e-e50d-aab96b5e25f7"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    # print(x.shape)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "    y_pred = model(x.float())\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.22479218252591118 average time 1.3788690109500066 iter num 20\n",
            "loss 0.20514983342853016 average time 1.3789117804500166 iter num 40\n",
            "loss 0.1956636757770327 average time 1.376099280566685 iter num 60\n",
            "loss 0.19155502363259708 average time 1.3739592146500144 iter num 80\n",
            "loss 0.19067695121594255 average time 1.3750027831000102 iter num 100\n",
            "loss 0.1738297573680897 average time 1.3734450604000017 iter num 20\n",
            "loss 0.159275652410698 average time 1.3718154431000074 iter num 40\n",
            "loss 0.14922448499186183 average time 1.3735697645333327 iter num 60\n",
            "loss 0.1443952883937085 average time 1.3748210943124917 iter num 80\n",
            "loss 0.14335149262040922 average time 1.3759926464199952 iter num 100\n",
            "loss 0.12269071761826862 average time 1.3966210240500119 iter num 20\n",
            "loss 0.10299902940746218 average time 1.3876202032249865 iter num 40\n",
            "loss 0.08955682458407643 average time 1.3830835429666613 iter num 60\n",
            "loss 0.08353373766685383 average time 1.3830959378500012 iter num 80\n",
            "loss 0.08229322794343581 average time 1.3825848003900023 iter num 100\n",
            "loss 0.0611813379843262 average time 1.3878689804999795 iter num 20\n",
            "loss 0.04910508642323795 average time 1.3812190222249854 iter num 40\n",
            "loss 0.045366591552791935 average time 1.3793127063666701 iter num 60\n",
            "loss 0.04452955184780555 average time 1.3759697850875114 iter num 80\n",
            "loss 0.044404371911590725 average time 1.3746752213800073 iter num 100\n",
            "loss 0.04331844857704571 average time 1.3766906930499772 iter num 20\n",
            "loss 0.04313843326041649 average time 1.392064413024957 iter num 40\n",
            "loss 0.04293203878609987 average time 1.3909138221166586 iter num 60\n",
            "loss 0.04282069391495762 average time 1.390627934324982 iter num 80\n",
            "loss 0.04279677129394619 average time 1.3876103895199685 iter num 100\n",
            "loss 0.042347843221157984 average time 1.3861223573500638 iter num 20\n",
            "loss 0.04196518311192211 average time 1.383204191950051 iter num 40\n",
            "loss 0.04171589645829697 average time 1.384890689733364 iter num 60\n",
            "loss 0.04160208196622365 average time 1.383978060212519 iter num 80\n",
            "loss 0.04157794086419883 average time 1.381009949340023 iter num 100\n",
            "loss 0.0411169876476957 average time 1.3602014256500297 iter num 20\n",
            "loss 0.0407090782360755 average time 1.3623964222249925 iter num 40\n",
            "loss 0.040439815502324596 average time 1.370940541733315 iter num 60\n",
            "loss 0.0403169069626065 average time 1.3712380520249952 iter num 80\n",
            "loss 0.040290893177939685 average time 1.3709077048699918 iter num 100\n",
            "loss 0.039795813144929824 average time 1.3752547665998918 iter num 20\n",
            "loss 0.03935891623441704 average time 1.3740694684499544 iter num 40\n",
            "loss 0.03907120855783091 average time 1.3760538501833102 iter num 60\n",
            "loss 0.03894014682233866 average time 1.3781620465624826 iter num 80\n",
            "loss 0.03891245934556106 average time 1.3776507363499877 iter num 100\n",
            "loss 0.038387045105995 average time 1.3719698851000204 iter num 20\n",
            "loss 0.037924425026094034 average time 1.3688016341250204 iter num 40\n",
            "loss 0.03762008189028217 average time 1.3711734918166865 iter num 60\n",
            "loss 0.03748156175393012 average time 1.3762105385750147 iter num 80\n",
            "loss 0.037452344702062594 average time 1.3789176864100137 iter num 100\n",
            "loss 0.03689859430202119 average time 1.3883376584499956 iter num 20\n",
            "loss 0.03641154586957438 average time 1.3853171111250504 iter num 40\n",
            "loss 0.03609132823303633 average time 1.3844678380333637 iter num 60\n",
            "loss 0.03594570914733236 average time 1.3844993959500242 iter num 80\n",
            "loss 0.03591500801107429 average time 1.3824074687000256 iter num 100\n",
            "loss 0.035333536201970064 average time 1.3788448465500096 iter num 20\n",
            "loss 0.03482180782332547 average time 1.3797649818999844 iter num 40\n",
            "loss 0.03448546923530757 average time 1.382653109000004 iter num 60\n",
            "loss 0.03433249293886242 average time 1.380120111625007 iter num 80\n",
            "loss 0.034300267253571146 average time 1.3790836686999954 iter num 100\n",
            "loss 0.03368945956408621 average time 1.4047636274500293 iter num 20\n",
            "loss 0.033150660948051934 average time 1.394841317125031 iter num 40\n",
            "loss 0.03279580460296839 average time 1.3908768613333526 iter num 60\n",
            "loss 0.03263424869930087 average time 1.3887494900375372 iter num 80\n",
            "loss 0.03260020148822398 average time 1.3887255738800195 iter num 100\n",
            "loss 0.03195495029979817 average time 1.3762926067000307 iter num 20\n",
            "loss 0.031384923939901624 average time 1.378657863100011 iter num 40\n",
            "loss 0.031008548722390932 average time 1.3786601184999958 iter num 60\n",
            "loss 0.030836964275027835 average time 1.3791574140374905 iter num 80\n",
            "loss 0.03080080390146052 average time 1.3798176253200019 iter num 100\n",
            "loss 0.03011464416888088 average time 1.3855510368499382 iter num 20\n",
            "loss 0.0295062421424819 average time 1.3929485931249475 iter num 40\n",
            "loss 0.02910357516984739 average time 1.3891916580499735 iter num 60\n",
            "loss 0.02891972185374421 average time 1.3882242116499868 iter num 80\n",
            "loss 0.028880951467099143 average time 1.3859009936399616 iter num 100\n",
            "loss 0.028145002066722616 average time 1.3729812780999737 iter num 20\n",
            "loss 0.027490427987069042 average time 1.3726814447999574 iter num 40\n",
            "loss 0.027055984812303903 average time 1.3732296978999785 iter num 60\n",
            "loss 0.026857400308002884 average time 1.37275961201247 iter num 80\n",
            "loss 0.026815511642408644 average time 1.3722895778299744 iter num 100\n",
            "loss 0.026019282088495868 average time 1.3674162030499928 iter num 20\n",
            "loss 0.02530924708635217 average time 1.3781181378000043 iter num 40\n",
            "loss 0.024837372951090744 average time 1.3884870766166766 iter num 60\n",
            "loss 0.024621420100136935 average time 1.385977701737494 iter num 80\n",
            "loss 0.024575878312849526 average time 1.3837481084699903 iter num 100\n",
            "loss 0.02371000101366656 average time 1.3730753239999784 iter num 20\n",
            "loss 0.022937875429796542 average time 1.37279570582499 iter num 40\n",
            "loss 0.022425291355274813 average time 1.3708163159166815 iter num 60\n",
            "loss 0.022191033944846034 average time 1.3709544223625016 iter num 80\n",
            "loss 0.022141658937145325 average time 1.3700638428399952 iter num 100\n",
            "loss 0.021204834237633103 average time 1.3640631360500037 iter num 20\n",
            "loss 0.020372909145319862 average time 1.3660897453750067 iter num 40\n",
            "loss 0.019823103285066875 average time 1.3666484519666786 iter num 60\n",
            "loss 0.019572520952487266 average time 1.3710252073000162 iter num 80\n",
            "loss 0.019519813523360616 average time 1.3711897668200073 iter num 100\n",
            "loss 0.018523192317359466 average time 1.3650726110500273 iter num 20\n",
            "loss 0.017643793177518664 average time 1.3603208628999597 iter num 40\n",
            "loss 0.017065317778615665 average time 1.360235454883294 iter num 60\n",
            "loss 0.01680281461762504 average time 1.3610232244749738 iter num 80\n",
            "loss 0.01674770421652826 average time 1.3601613515699773 iter num 100\n",
            "loss 0.015709808274058493 average time 1.365957892399956 iter num 20\n",
            "loss 0.014800423808618582 average time 1.3638899122249313 iter num 40\n",
            "loss 0.014208695586566473 average time 1.3646515602832854 iter num 60\n",
            "loss 0.013942166074559665 average time 1.3677336584999693 iter num 80\n",
            "loss 0.013886364576961035 average time 1.3672775664699839 iter num 100\n",
            "loss 0.012842878019452338 average time 1.3909211219500321 iter num 20\n",
            "loss 0.011946939217019852 average time 1.3841071122750237 iter num 40\n",
            "loss 0.011375503561122666 average time 1.3788315086000391 iter num 60\n",
            "loss 0.011121818637916652 average time 1.3783749071250555 iter num 80\n",
            "loss 0.01106910331971162 average time 1.376059162330048 iter num 100\n",
            "loss 0.01009963999790391 average time 1.3667323423000197 iter num 20\n",
            "loss 0.009296706076457604 average time 1.365330761825021 iter num 40\n",
            "loss 0.008802461441208274 average time 1.3659068740833693 iter num 60\n",
            "loss 0.00858816663204028 average time 1.3669956235000427 iter num 80\n",
            "loss 0.008544127193749208 average time 1.3689250416600316 iter num 100\n",
            "loss 0.007752484451382081 average time 1.3770467641000779 iter num 20\n",
            "loss 0.007129130944160417 average time 1.3820521612751464 iter num 40\n",
            "loss 0.006763811382077717 average time 1.3789013751001373 iter num 60\n",
            "loss 0.006610486232397585 average time 1.3767466145001095 iter num 80\n",
            "loss 0.0065794051211817434 average time 1.3753747983800713 iter num 100\n",
            "loss 0.006035832778143635 average time 1.3777773665500717 iter num 20\n",
            "loss 0.005632415931254248 average time 1.3837706861000925 iter num 40\n",
            "loss 0.005407265075870601 average time 1.3879124601167405 iter num 60\n",
            "loss 0.0053153471915920085 average time 1.3837571771750958 iter num 80\n",
            "loss 0.005296922675539606 average time 1.3819011435400717 iter num 100\n",
            "loss 0.004980858382216741 average time 1.3703257733499867 iter num 20\n",
            "loss 0.004754072911180643 average time 1.3704292566000276 iter num 40\n",
            "loss 0.00462942402760691 average time 1.3781877080000868 iter num 60\n",
            "loss 0.004578555157271149 average time 1.3791762818750612 iter num 80\n",
            "loss 0.004568328908800924 average time 1.3775651207200463 iter num 100\n",
            "loss 0.004390464111802583 average time 1.3719588764499804 iter num 20\n",
            "loss 0.00425691697290245 average time 1.3698701968249907 iter num 40\n",
            "loss 0.004179022956765324 average time 1.370901599783368 iter num 60\n",
            "loss 0.004145745390989931 average time 1.3699020591750013 iter num 80\n",
            "loss 0.004138909335657037 average time 1.3700380503600171 iter num 100\n",
            "loss 0.004015117475039236 average time 1.372528961649914 iter num 20\n",
            "loss 0.003913744240151284 average time 1.3693406653499323 iter num 40\n",
            "loss 0.0038503860103063286 average time 1.366510742733317 iter num 60\n",
            "loss 0.0038223448289252705 average time 1.3668395388000136 iter num 80\n",
            "loss 0.003816512950848236 average time 1.37115423637003 iter num 100\n",
            "loss 0.0037085464219781144 average time 1.3714299842500168 iter num 20\n",
            "loss 0.003616445281271709 average time 1.3753551062750375 iter num 40\n",
            "loss 0.0035574683602897796 average time 1.3756362910333337 iter num 60\n",
            "loss 0.0035311251320624183 average time 1.3769893403625475 iter num 80\n",
            "loss 0.0035256372946024053 average time 1.3773788252700343 iter num 100\n",
            "loss 0.0034233460278387602 average time 1.379824986550102 iter num 20\n",
            "loss 0.003335851350553338 average time 1.3793964148250097 iter num 40\n",
            "loss 0.0032800514847530935 average time 1.3823346170500068 iter num 60\n",
            "loss 0.003255229221896309 average time 1.3806081126249636 iter num 80\n",
            "loss 0.0032500553055566624 average time 1.3785507397699621 iter num 100\n",
            "loss 0.0031538385400123733 average time 1.39517246804985 iter num 20\n",
            "loss 0.0030719920603049754 average time 1.3855053370999486 iter num 40\n",
            "loss 0.003020032160727963 average time 1.381596084549983 iter num 60\n",
            "loss 0.002996922694514263 average time 1.3798671062374637 iter num 80\n",
            "loss 0.0029921131511855214 average time 1.3802681457899688 iter num 100\n",
            "loss 0.0029025199090376547 average time 1.384902988599879 iter num 20\n",
            "loss 0.002826075588927247 average time 1.382673999749977 iter num 40\n",
            "loss 0.0027772607585378143 average time 1.3798878600333258 iter num 60\n",
            "loss 0.002755527392815637 average time 1.3780403229124771 iter num 80\n",
            "loss 0.00275099840795847 average time 1.3747716970099737 iter num 100\n",
            "loss 0.0026665197949052176 average time 1.3640961911499745 iter num 20\n",
            "loss 0.0025941640284725584 average time 1.3783430308499647 iter num 40\n",
            "loss 0.0025478932597816028 average time 1.3738248162166655 iter num 60\n",
            "loss 0.002527226901425505 average time 1.3684232654999846 iter num 80\n",
            "loss 0.002522907177757527 average time 1.3664290065999831 iter num 100\n",
            "loss 0.002442529784762135 average time 1.3670760670500386 iter num 20\n",
            "loss 0.0023739675776572695 average time 1.3613273705999973 iter num 40\n",
            "loss 0.002330314111840518 average time 1.3566930704666447 iter num 60\n",
            "loss 0.0023108323368151223 average time 1.357710188350029 iter num 80\n",
            "loss 0.002306765181241304 average time 1.3581868644100268 iter num 100\n",
            "loss 0.0022311505571191085 average time 1.349999566400129 iter num 20\n",
            "loss 0.002166602171506841 average time 1.3545403384751353 iter num 40\n",
            "loss 0.0021254081886135467 average time 1.3545770692167631 iter num 60\n",
            "loss 0.0021070954188675143 average time 1.3607500641000798 iter num 80\n",
            "loss 0.002103285893934114 average time 1.3601811365500545 iter num 100\n",
            "loss 0.002032491272064988 average time 1.3595283926499633 iter num 20\n",
            "loss 0.0019722141415020157 average time 1.355477104324973 iter num 40\n",
            "loss 0.0019337802008311913 average time 1.3543710886333125 iter num 60\n",
            "loss 0.0019166444342939833 average time 1.3543413114750138 iter num 80\n",
            "loss 0.0019130713900532578 average time 1.3532077145900074 iter num 100\n",
            "loss 0.001846705388679843 average time 1.3588907056998778 iter num 20\n",
            "loss 0.0017904061812964622 average time 1.3577097694499343 iter num 40\n",
            "loss 0.0017546905148979259 average time 1.3548437278832808 iter num 60\n",
            "loss 0.0017387843389523666 average time 1.3562049088499748 iter num 80\n",
            "loss 0.0017354764471574464 average time 1.359024752559999 iter num 100\n",
            "loss 0.0016738214806443345 average time 1.3479425685999558 iter num 20\n",
            "loss 0.0016215030951321982 average time 1.3515490234249454 iter num 40\n",
            "loss 0.0015881456013264429 average time 1.3511211181666416 iter num 60\n",
            "loss 0.00157329597655675 average time 1.3513270619874675 iter num 80\n",
            "loss 0.0015702067562653073 average time 1.3520501811999566 iter num 100\n",
            "loss 0.001512790216164539 average time 1.3513386557000557 iter num 20\n",
            "loss 0.0014639545037603773 average time 1.348779341725026 iter num 40\n",
            "loss 0.0014329201275325358 average time 1.3487564656333386 iter num 60\n",
            "loss 0.001419129043846158 average time 1.3484533590249725 iter num 80\n",
            "loss 0.0014162592172703439 average time 1.3488535471199612 iter num 100\n",
            "loss 0.0013631617554141788 average time 1.3638436264001483 iter num 20\n",
            "loss 0.001318151505210398 average time 1.3611636067751305 iter num 40\n",
            "loss 0.001289606684695296 average time 1.3569239011667227 iter num 60\n",
            "loss 0.0012769635981370318 average time 1.3553897908125578 iter num 80\n",
            "loss 0.0012743355012643106 average time 1.3547292701300466 iter num 100\n",
            "loss 0.0012255773934036851 average time 1.347294413399959 iter num 20\n",
            "loss 0.0011843471883923866 average time 1.3503056873000332 iter num 40\n",
            "loss 0.001158256575309245 average time 1.3520018996333572 iter num 60\n",
            "loss 0.0011466812808305724 average time 1.3504821427875187 iter num 80\n",
            "loss 0.0011442733055836124 average time 1.3495335910600352 iter num 100\n",
            "loss 0.0010998262588110307 average time 1.350739719450121 iter num 20\n",
            "loss 0.001062271503420078 average time 1.3480875545250683 iter num 40\n",
            "loss 0.0010385655086146731 average time 1.3563766246167082 iter num 60\n",
            "loss 0.0010280931482737445 average time 1.3544137950250275 iter num 80\n",
            "loss 0.0010259232521370857 average time 1.3518195227900105 iter num 100\n",
            "loss 0.0009859430916482434 average time 1.3483271501998844 iter num 20\n",
            "loss 0.0009525056771236518 average time 1.3460731747249155 iter num 40\n",
            "loss 0.0009315129671958613 average time 1.346166070699913 iter num 60\n",
            "loss 0.0009222389363157125 average time 1.3478983598124388 iter num 80\n",
            "loss 0.000920316375380603 average time 1.3476053347099515 iter num 100\n",
            "loss 0.0008848814638427404 average time 1.3519008532000043 iter num 20\n",
            "loss 0.0008551067340667742 average time 1.3524990388999867 iter num 40\n",
            "loss 0.0008364534704768484 average time 1.3501589517166395 iter num 60\n",
            "loss 0.0008282453302509937 average time 1.351652134199935 iter num 80\n",
            "loss 0.0008265420198084117 average time 1.3545503621599437 iter num 100\n",
            "loss 0.0007952517858494314 average time 1.3476098560998708 iter num 20\n",
            "loss 0.0007691836034133962 average time 1.3497616312249192 iter num 40\n",
            "loss 0.000752838709161765 average time 1.3502985798833076 iter num 60\n",
            "loss 0.0007456461815705589 average time 1.352959902074963 iter num 80\n",
            "loss 0.0007441565432928386 average time 1.3532322139299504 iter num 100\n",
            "loss 0.0007166882471031357 average time 1.3444648406500164 iter num 20\n",
            "loss 0.0006938560995757212 average time 1.350004083175031 iter num 40\n",
            "loss 0.0006795946072252376 average time 1.3514367892833283 iter num 60\n",
            "loss 0.0006733292553226092 average time 1.351656364625012 iter num 80\n",
            "loss 0.0006720320376628448 average time 1.3517756430199825 iter num 100\n",
            "loss 0.0006482602687068763 average time 1.3742831972499516 iter num 20\n",
            "loss 0.000628536521219153 average time 1.3610198184249156 iter num 40\n",
            "loss 0.0006161870228662846 average time 1.3576762683332768 iter num 60\n",
            "loss 0.0006107477992453356 average time 1.3551158913999644 iter num 80\n",
            "loss 0.000609618530813585 average time 1.3547304665199318 iter num 100\n",
            "loss 0.0005889414286754366 average time 1.3527313065501403 iter num 20\n",
            "loss 0.0005717924685546717 average time 1.3553731615500966 iter num 40\n",
            "loss 0.0005610943523744541 average time 1.3566158015667193 iter num 60\n",
            "loss 0.0005564106347334259 average time 1.3580129787750366 iter num 80\n",
            "loss 0.0005554414090257014 average time 1.3568817567400402 iter num 100\n",
            "loss 0.0005376521425816589 average time 1.3504370315501546 iter num 20\n",
            "loss 0.000522924405687523 average time 1.3561590435251218 iter num 40\n",
            "loss 0.0005137169004485919 average time 1.3575439407167929 iter num 60\n",
            "loss 0.0005096702077376905 average time 1.3592401776626162 iter num 80\n",
            "loss 0.0005088292991697746 average time 1.3582193946701044 iter num 100\n",
            "loss 0.0004933682073108727 average time 1.353546521700082 iter num 20\n",
            "loss 0.000480466949419137 average time 1.3578698282249662 iter num 40\n",
            "loss 0.00047237104825938544 average time 1.3571952681666517 iter num 60\n",
            "loss 0.0004688075847649899 average time 1.3564438415124642 iter num 80\n",
            "loss 0.00046807040776338015 average time 1.3573659152399704 iter num 100\n",
            "loss 0.00045463574938347194 average time 1.3628628747498623 iter num 20\n",
            "loss 0.0004434889562527274 average time 1.3641941411000515 iter num 40\n",
            "loss 0.0004365248803864354 average time 1.3618766767501862 iter num 60\n",
            "loss 0.0004334587718394979 average time 1.3685927310250918 iter num 80\n",
            "loss 0.0004328221946668797 average time 1.368295884780091 iter num 100\n",
            "loss 0.0004210574008456146 average time 1.36395772604983 iter num 20\n",
            "loss 0.00041134440522438184 average time 1.3631260676499095 iter num 40\n",
            "loss 0.0004052627018071217 average time 1.3660204206499658 iter num 60\n",
            "loss 0.00040259721535888647 average time 1.3644244137124133 iter num 80\n",
            "loss 0.0004020463216424144 average time 1.3645191696398797 iter num 100\n",
            "loss 0.0003919098801667716 average time 1.3714970595001432 iter num 20\n",
            "loss 0.00038344461308838045 average time 1.3686496620250637 iter num 40\n",
            "loss 0.0003781450342735497 average time 1.3674118279166578 iter num 60\n",
            "loss 0.0003758135025521533 average time 1.3671300258374912 iter num 80\n",
            "loss 0.0003753311136764191 average time 1.371088284959951 iter num 100\n",
            "loss 0.00036649555111131776 average time 1.367530947600244 iter num 20\n",
            "loss 0.00035921294293247405 average time 1.3697821478750938 iter num 40\n",
            "loss 0.000354707014253462 average time 1.369634547050191 iter num 60\n",
            "loss 0.0003527233661318835 average time 1.3690776015376058 iter num 80\n",
            "loss 0.00035231351250655476 average time 1.367410767240108 iter num 100\n",
            "loss 0.0003447767796531281 average time 1.3608800513999995 iter num 20\n",
            "loss 0.00033850398521837686 average time 1.3665207003500655 iter num 40\n",
            "loss 0.00033458380230280726 average time 1.3681849992834032 iter num 60\n",
            "loss 0.00033287094904802233 average time 1.3674307152000893 iter num 80\n",
            "loss 0.0003325154060053018 average time 1.3684315259700817 iter num 100\n",
            "loss 0.0003259850786164955 average time 1.3782066407001365 iter num 20\n",
            "loss 0.0003205633437173868 average time 1.3738065962000292 iter num 40\n",
            "loss 0.0003171584780898315 average time 1.3687391470667232 iter num 60\n",
            "loss 0.0003156529784944114 average time 1.3653060035000864 iter num 80\n",
            "loss 0.000315340992166737 average time 1.3628483472001063 iter num 100\n",
            "loss 0.00030961343559447865 average time 1.3473174671000379 iter num 20\n",
            "loss 0.0003048450417229005 average time 1.3538667814001655 iter num 40\n",
            "loss 0.0003018393216997556 average time 1.3531169905000147 iter num 60\n",
            "loss 0.0003005100611537788 average time 1.3529110317625055 iter num 80\n",
            "loss 0.0003002350628685775 average time 1.353341732939989 iter num 100\n",
            "loss 0.0002951946710173013 average time 1.3564555111001027 iter num 20\n",
            "loss 0.0002909571753221182 average time 1.3586976560501172 iter num 40\n",
            "loss 0.0002883175209185191 average time 1.3637355254834802 iter num 60\n",
            "loss 0.00028716036966011036 average time 1.362311701725116 iter num 80\n",
            "loss 0.0002869207107656104 average time 1.3606302152701029 iter num 100\n",
            "loss 0.00028248837680143354 average time 1.360543055800008 iter num 20\n",
            "loss 0.0002787818152913426 average time 1.3620751593498426 iter num 40\n",
            "loss 0.0002764482365194891 average time 1.361479911033257 iter num 60\n",
            "loss 0.0002754185869033299 average time 1.3597517567624209 iter num 80\n",
            "loss 0.0002752050289808462 average time 1.359809226209927 iter num 100\n",
            "loss 0.00027238698103467456 average time 1.3592420027999652 iter num 20\n",
            "loss 0.0002680263275182465 average time 1.356862966775043 iter num 40\n",
            "loss 0.0002660115322959626 average time 1.358128689100037 iter num 60\n",
            "loss 0.00026515556991789206 average time 1.3638327478625343 iter num 80\n",
            "loss 0.00026496668068212806 average time 1.3761953302600158 iter num 100\n",
            "loss 0.00026174526442177616 average time 1.389538642450134 iter num 20\n",
            "loss 0.00025873260935957654 average time 1.3728956457499861 iter num 40\n",
            "loss 0.00025715669125740535 average time 1.368285953983468 iter num 60\n",
            "loss 0.0002564336653267378 average time 1.3659109528125781 iter num 80\n",
            "loss 0.00025628620588595276 average time 1.363011317020064 iter num 100\n",
            "loss 0.000253338440373422 average time 1.3572046005997436 iter num 20\n",
            "loss 0.0002508284397050066 average time 1.3554281740998249 iter num 40\n",
            "loss 0.00024922424424558484 average time 1.3539192004332108 iter num 60\n",
            "loss 0.00024850788311165517 average time 1.3548194765998915 iter num 80\n",
            "loss 0.0002483587992522215 average time 1.355130648739905 iter num 100\n",
            "loss 0.00024557315205495214 average time 1.3807652816501104 iter num 20\n",
            "loss 0.0002431862402164469 average time 1.3665163214750464 iter num 40\n",
            "loss 0.0002416474050572608 average time 1.365547173333289 iter num 60\n",
            "loss 0.0002409585484732991 average time 1.3625728288250003 iter num 80\n",
            "loss 0.00024081510481882134 average time 1.3615498775599917 iter num 100\n",
            "loss 0.00023907533656511264 average time 1.3584114592500556 iter num 20\n",
            "loss 0.0002357760753401343 average time 1.3564498537501095 iter num 40\n",
            "loss 0.00023448450879439922 average time 1.356779913383313 iter num 60\n",
            "loss 0.00023380347769687106 average time 1.3559453298374364 iter num 80\n",
            "loss 0.00023366670036505113 average time 1.3555139461699581 iter num 100\n",
            "loss 0.00023444079890910704 average time 1.3531994390000364 iter num 20\n",
            "loss 0.00023012049537023572 average time 1.369898826549934 iter num 40\n",
            "loss 0.00022855074686589885 average time 1.3679561771332374 iter num 60\n",
            "loss 0.00022808307183690732 average time 1.3658663569499594 iter num 80\n",
            "loss 0.00022799106490930862 average time 1.3625574175899784 iter num 100\n",
            "loss 0.00022600309437099592 average time 1.3490991318501073 iter num 20\n",
            "loss 0.0002243426182020879 average time 1.355655785674935 iter num 40\n",
            "loss 0.00022324924781175657 average time 1.355882633266659 iter num 60\n",
            "loss 0.00022276487638629396 average time 1.356703026687569 iter num 80\n",
            "loss 0.00022266395438255533 average time 1.3580474236100235 iter num 100\n",
            "loss 0.00022076963488322387 average time 1.3562606627505374 iter num 20\n",
            "loss 0.00021913384467347037 average time 1.358989524025219 iter num 40\n",
            "loss 0.00021808129897294277 average time 1.3652493608335134 iter num 60\n",
            "loss 0.00021760830710782652 average time 1.3657386903626958 iter num 80\n",
            "loss 0.0002175097528273198 average time 1.3662339956802134 iter num 100\n",
            "loss 0.00021566878266269393 average time 1.3546775790998253 iter num 20\n",
            "loss 0.00021408344182973426 average time 1.3516554006999741 iter num 40\n",
            "loss 0.00021305796164680833 average time 1.353035997316571 iter num 60\n",
            "loss 0.00021259938653532485 average time 1.3522025627998802 iter num 80\n",
            "loss 0.00021250350388071952 average time 1.3523194901599709 iter num 100\n",
            "loss 0.00021070704438549742 average time 1.3580391515000882 iter num 20\n",
            "loss 0.00020915541053780206 average time 1.3538316229001794 iter num 40\n",
            "loss 0.00020815709990886676 average time 1.3527807125502174 iter num 60\n",
            "loss 0.00020770995751362515 average time 1.3541566302002366 iter num 80\n",
            "loss 0.00020761638213741778 average time 1.359816692980221 iter num 100\n",
            "loss 0.00020588339690858967 average time 1.3582623249500103 iter num 20\n",
            "loss 0.0002043423054349672 average time 1.3599057000999892 iter num 40\n",
            "loss 0.00020336625465760592 average time 1.3584726585666431 iter num 60\n",
            "loss 0.00020292748916490082 average time 1.3589688455624582 iter num 80\n",
            "loss 0.00020283563609749795 average time 1.3580355221299396 iter num 100\n",
            "loss 0.00020512970733669857 average time 1.3587582257502617 iter num 20\n",
            "loss 0.00020023404016276432 average time 1.3564821774001756 iter num 40\n",
            "loss 0.0001990019343973386 average time 1.3567007864167258 iter num 60\n",
            "loss 0.0001985467572854459 average time 1.3547369070874993 iter num 80\n",
            "loss 0.00019847460828380107 average time 1.3546587311100302 iter num 100\n",
            "loss 0.00019685487563358403 average time 1.3693914675000087 iter num 20\n",
            "loss 0.0001956062285453931 average time 1.3691174828749808 iter num 40\n",
            "loss 0.0001947966547299268 average time 1.3677352990500367 iter num 60\n",
            "loss 0.00019441872918855784 average time 1.3670016144875716 iter num 80\n",
            "loss 0.00019433776284612928 average time 1.3645440468200831 iter num 100\n",
            "loss 0.00019277311262338526 average time 1.3476405944500585 iter num 20\n",
            "loss 0.00019161445934450647 average time 1.3553944274499827 iter num 40\n",
            "loss 0.0001907656643391995 average time 1.3544927938000304 iter num 60\n",
            "loss 0.00019040164411233918 average time 1.3551491248000276 iter num 80\n",
            "loss 0.00019032424807780102 average time 1.355395633330063 iter num 100\n",
            "loss 0.0001927827915706717 average time 1.3513345828499042 iter num 20\n",
            "loss 0.00018865673914634395 average time 1.3508567322750422 iter num 40\n",
            "loss 0.00018733399674661434 average time 1.3611069180500939 iter num 60\n",
            "loss 0.00018704529207587138 average time 1.3623820007000176 iter num 80\n",
            "loss 0.00018699963619242833 average time 1.36129951910998 iter num 100\n",
            "loss 0.00018575937381236712 average time 1.357137955750113 iter num 20\n",
            "loss 0.0001847733809619211 average time 1.357268650675087 iter num 40\n",
            "loss 0.0001841212342022502 average time 1.3595474343666563 iter num 60\n",
            "loss 0.0001838341251088327 average time 1.36023010374995 iter num 80\n",
            "loss 0.0001837745478038285 average time 1.3611025230899394 iter num 100\n",
            "loss 0.00018264723105966432 average time 1.3544567296998138 iter num 20\n",
            "loss 0.00018167269797580074 average time 1.358127012849991 iter num 40\n",
            "loss 0.0001810440325850041 average time 1.3564797816166902 iter num 60\n",
            "loss 0.00018076237120465808 average time 1.363833041087537 iter num 80\n",
            "loss 0.0001807037173082796 average time 1.363677520580095 iter num 100\n",
            "loss 0.00017959761097718053 average time 1.3601428947997192 iter num 20\n",
            "loss 0.00017864316259986692 average time 1.357649975799859 iter num 40\n",
            "loss 0.00017802368756569212 average time 1.35840669449996 iter num 60\n",
            "loss 0.00017774428935895574 average time 1.3583358803124157 iter num 80\n",
            "loss 0.00017768598726520136 average time 1.356953718629975 iter num 100\n",
            "loss 0.00017658742587227963 average time 1.3638471333501911 iter num 20\n",
            "loss 0.00017563471226856046 average time 1.360302005525 iter num 40\n",
            "loss 0.00017501868682060084 average time 1.3587497810998987 iter num 60\n",
            "loss 0.00017474179521823478 average time 1.3613671640249323 iter num 80\n",
            "loss 0.00017468403897731385 average time 1.3633665382899927 iter num 100\n",
            "loss 0.00017360587780232366 average time 1.3736315165497217 iter num 20\n",
            "loss 0.0001726609603412719 average time 1.3666984930998296 iter num 40\n",
            "loss 0.0001720561155549923 average time 1.3637091569332975 iter num 60\n",
            "loss 0.0001717829800433914 average time 1.3625748073124213 iter num 80\n",
            "loss 0.00017172571820681212 average time 1.3641680790599275 iter num 100\n",
            "loss 0.00017214820168160878 average time 1.3674442121498942 iter num 20\n",
            "loss 0.00016973840932177096 average time 1.362754816949837 iter num 40\n",
            "loss 0.00016920387626319437 average time 1.3605235361498975 iter num 60\n",
            "loss 0.0001688956879336442 average time 1.359490120562441 iter num 80\n",
            "loss 0.00016884616708562087 average time 1.3596642542399786 iter num 100\n",
            "loss 0.00016816112597504035 average time 1.3578249901000163 iter num 20\n",
            "loss 0.00016733166106482097 average time 1.3694191058249998 iter num 40\n",
            "loss 0.00016683139145085468 average time 1.3646301292167058 iter num 60\n",
            "loss 0.0001665292533678552 average time 1.3654237382375414 iter num 80\n",
            "loss 0.00016646876013047312 average time 1.369872628310004 iter num 100\n",
            "loss 0.0001655688269510908 average time 1.375624757350033 iter num 20\n",
            "loss 0.00016483657055544696 average time 1.3698505938000836 iter num 40\n",
            "loss 0.0001643728770872964 average time 1.3680186918166934 iter num 60\n",
            "loss 0.00016416581189241926 average time 1.3650538006499346 iter num 80\n",
            "loss 0.0001641219363927891 average time 1.3642716419899807 iter num 100\n",
            "loss 0.00016330533406986095 average time 1.3531368921000648 iter num 20\n",
            "loss 0.00016259326126740972 average time 1.354238727699976 iter num 40\n",
            "loss 0.00016213250907531454 average time 1.3669524190166764 iter num 60\n",
            "loss 0.00016192521918492788 average time 1.3670968808999988 iter num 80\n",
            "loss 0.0001618820257853347 average time 1.365398862689999 iter num 100\n",
            "loss 0.00016106630979092023 average time 1.3605022370996267 iter num 20\n",
            "loss 0.00016035320563289148 average time 1.3647881637496995 iter num 40\n",
            "loss 0.0001598904650200025 average time 1.3641665689164861 iter num 60\n",
            "loss 0.00015968240242673735 average time 1.3620024517498677 iter num 80\n",
            "loss 0.00015963903688846716 average time 1.3617247309199412 iter num 100\n",
            "loss 0.00015881993897262503 average time 1.3602614838502631 iter num 20\n",
            "loss 0.00015810745278385563 average time 1.3583937478001644 iter num 40\n",
            "loss 0.00015764059828151695 average time 1.360787672366708 iter num 60\n",
            "loss 0.00015743046019202326 average time 1.3604354490125843 iter num 80\n",
            "loss 0.0001573866199906396 average time 1.3636220535900612 iter num 100\n",
            "loss 0.00015655813928425207 average time 1.3553090186998815 iter num 20\n",
            "loss 0.00015583758585238338 average time 1.3553786572999342 iter num 40\n",
            "loss 0.00015536814752091738 average time 1.3559794072666287 iter num 60\n",
            "loss 0.00015515609209511643 average time 1.357022047925011 iter num 80\n",
            "loss 0.00015511187313448606 average time 1.3593280550000053 iter num 100\n",
            "loss 0.00015427390216753384 average time 1.3771148829498998 iter num 20\n",
            "loss 0.000153543427834888 average time 1.375228873874994 iter num 40\n",
            "loss 0.00015306742734278963 average time 1.3705864992666648 iter num 60\n",
            "loss 0.00015285294619085774 average time 1.3670775234500525 iter num 80\n",
            "loss 0.00015280795856376884 average time 1.3657002841400936 iter num 100\n",
            "loss 0.00015250162959853082 average time 1.3760682745999475 iter num 20\n",
            "loss 0.0001513138976893413 average time 1.367843727224954 iter num 40\n",
            "loss 0.00015076341847824334 average time 1.3658273944832824 iter num 60\n",
            "loss 0.00015056112636878036 average time 1.3661611278124155 iter num 80\n",
            "loss 0.00015051831550475043 average time 1.3644602108199433 iter num 100\n",
            "loss 0.00015631228999883666 average time 1.362631251899802 iter num 20\n",
            "loss 0.00014980743280706914 average time 1.3624757584998406 iter num 40\n",
            "loss 0.0001490170355863129 average time 1.3620555209333967 iter num 60\n",
            "loss 0.0001487720176035251 average time 1.364510895674948 iter num 80\n",
            "loss 0.0001487347700948884 average time 1.363764248319967 iter num 100\n",
            "loss 0.0001479601782658274 average time 1.3557110433498565 iter num 20\n",
            "loss 0.00014738371284456314 average time 1.361282103999929 iter num 40\n",
            "loss 0.0001470318493813399 average time 1.3613302046500393 iter num 60\n",
            "loss 0.00014687150687818447 average time 1.3595328247750331 iter num 80\n",
            "loss 0.00014683832369352152 average time 1.3584197116100405 iter num 100\n",
            "loss 0.0001462151034562187 average time 1.353910218550027 iter num 20\n",
            "loss 0.0001456751351547407 average time 1.3566934407750977 iter num 40\n",
            "loss 0.00014532627398646176 average time 1.3547725956500472 iter num 60\n",
            "loss 0.0001451691532360781 average time 1.3561164794499974 iter num 80\n",
            "loss 0.0001451365264660991 average time 1.3610082102800334 iter num 100\n",
            "loss 0.0001445202165971235 average time 1.3626662876502451 iter num 20\n",
            "loss 0.00014398459740872345 average time 1.3631424812748718 iter num 40\n",
            "loss 0.0001436363269192843 average time 1.360028267933194 iter num 60\n",
            "loss 0.00014347922910436884 average time 1.3645200000249589 iter num 80\n",
            "loss 0.00014344637892865758 average time 1.363305973069946 iter num 100\n",
            "loss 0.00014282854280430683 average time 1.3610952721502145 iter num 20\n",
            "loss 0.00014229093547150098 average time 1.358532620125061 iter num 40\n",
            "loss 0.00014194095292811528 average time 1.3564118465168575 iter num 60\n",
            "loss 0.00014178452407159701 average time 1.355407737987639 iter num 80\n",
            "loss 0.00014175151385979976 average time 1.3560535122201145 iter num 100\n",
            "loss 0.0001411351124836176 average time 1.3557412112997553 iter num 20\n",
            "loss 0.00014059644932941663 average time 1.3580365989998882 iter num 40\n",
            "loss 0.0001402456468667929 average time 1.3559899732832492 iter num 60\n",
            "loss 0.00014008725601342016 average time 1.3557101766875121 iter num 80\n",
            "loss 0.00014005424790339803 average time 1.3586365653899701 iter num 100\n",
            "loss 0.00013942928662428057 average time 1.3498905504498908 iter num 20\n",
            "loss 0.00013888321047125503 average time 1.3514319686999898 iter num 40\n",
            "loss 0.00013852863487765052 average time 1.349268779149952 iter num 60\n",
            "loss 0.00013836868139435008 average time 1.3496163435249628 iter num 80\n",
            "loss 0.00013833521537308355 average time 1.349672053439972 iter num 100\n",
            "loss 0.00013770418506498623 average time 1.3462186244500116 iter num 20\n",
            "loss 0.00013715000466258946 average time 1.3462055265750223 iter num 40\n",
            "loss 0.00013678853023122616 average time 1.3471918923501107 iter num 60\n",
            "loss 0.000136625479360499 average time 1.349615154075036 iter num 80\n",
            "loss 0.0001365913159088966 average time 1.3495354967500317 iter num 100\n",
            "loss 0.0001359496000373637 average time 1.3599826476999624 iter num 20\n",
            "loss 0.00013539219204747194 average time 1.362143909050019 iter num 40\n",
            "loss 0.00013502855381186432 average time 1.3564909951667081 iter num 60\n",
            "loss 0.0001348643410494325 average time 1.3541783806125296 iter num 80\n",
            "loss 0.00013483032986394633 average time 1.353250238669989 iter num 100\n",
            "loss 0.0001343616166096947 average time 1.3398272722499314 iter num 20\n",
            "loss 0.00013363670213075265 average time 1.3456409560499651 iter num 40\n",
            "loss 0.0001332612752939348 average time 1.3462030353165877 iter num 60\n",
            "loss 0.0001330964165274562 average time 1.3474706219749806 iter num 80\n",
            "loss 0.0001330631758682236 average time 1.348727401239994 iter num 100\n",
            "loss 0.0001412220737520008 average time 1.3448660750502313 iter num 20\n",
            "loss 0.00013271008176357635 average time 1.3493319132501256 iter num 40\n",
            "loss 0.00013171037437737892 average time 1.3584941214666892 iter num 60\n",
            "loss 0.00013157109893203377 average time 1.355898249674965 iter num 80\n",
            "loss 0.00013150308584769234 average time 1.3533743350199803 iter num 100\n",
            "loss 0.0001308676358379212 average time 1.3469285304999175 iter num 20\n",
            "loss 0.00013042221912137975 average time 1.3476453292500081 iter num 40\n",
            "loss 0.00013012779670861508 average time 1.34790803178336 iter num 60\n",
            "loss 0.00013000065677007283 average time 1.348657349362543 iter num 80\n",
            "loss 0.00012997347724084696 average time 1.3484200523200889 iter num 100\n",
            "loss 0.00012947018326393917 average time 1.3443789853500676 iter num 20\n",
            "loss 0.00012903318935118005 average time 1.3483602494001388 iter num 40\n",
            "loss 0.00012874791492314773 average time 1.3495454252667514 iter num 60\n",
            "loss 0.0001286193940156059 average time 1.3481281060125638 iter num 80\n",
            "loss 0.00012859244347362985 average time 1.3513224381501094 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_fEzULvKwR-"
      },
      "source": [
        "# 15:08\n",
        "# 18:55"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3be0cc-7ec7-4b6f-f8b8-dc8109dcbf5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a71c75ef-e2f8-4da6-9f24-8839d3bfcd05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2848927e-6eae-4dfa-90d5-e2db657ce19f"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0892888-1c92-4ba0-cc6f-66f073282a44"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_MC_oneDS_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.00013652123916828787 average time 1.3681041822499538 iter num 20\n",
            "loss 0.0001284193753555044 average time 1.3715417652500945 iter num 40\n",
            "loss 0.00012822533391631142 average time 1.3728060930668107 iter num 60\n",
            "loss 0.0001276600512397113 average time 1.379346668612584 iter num 80\n",
            "loss 0.00012755363931269992 average time 1.3769123472000502 iter num 100\n",
            "loss 0.00012708040716049584 average time 1.3697410843001308 iter num 20\n",
            "loss 0.00012675025065587617 average time 1.3681870665500355 iter num 40\n",
            "loss 0.00012658012972776038 average time 1.3663464174000788 iter num 60\n",
            "loss 0.000126492464314464 average time 1.3633718126875465 iter num 80\n",
            "loss 0.0001264736933783537 average time 1.3631075895100184 iter num 100\n",
            "loss 0.00012613121223107064 average time 1.3653008829002828 iter num 20\n",
            "loss 0.00012583458328000233 average time 1.3631229919002634 iter num 40\n",
            "loss 0.0001256403371949046 average time 1.3623458272668358 iter num 60\n",
            "loss 0.00012555022639013533 average time 1.36354067697514 iter num 80\n",
            "loss 0.00012553131898021037 average time 1.364904987210084 iter num 100\n",
            "loss 0.00012515900142016462 average time 1.368468776050031 iter num 20\n",
            "loss 0.00012483055517177662 average time 1.364979562625058 iter num 40\n",
            "loss 0.00012461366197732106 average time 1.3608450569834531 iter num 60\n",
            "loss 0.00012451422349090706 average time 1.359589407362614 iter num 80\n",
            "loss 0.0001244932636591008 average time 1.3602202979301001 iter num 100\n",
            "loss 0.00012408639548976881 average time 1.3559487789999367 iter num 20\n",
            "loss 0.0001237277514590019 average time 1.3568727169249541 iter num 40\n",
            "loss 0.00012349108433376554 average time 1.356758010216739 iter num 60\n",
            "loss 0.00012338265829261067 average time 1.3555355943499763 iter num 80\n",
            "loss 0.00012335977866051464 average time 1.357403258209997 iter num 100\n",
            "loss 0.00012292002140471345 average time 1.3570536875500694 iter num 20\n",
            "loss 0.0001225307016208424 average time 1.3642685185749541 iter num 40\n",
            "loss 0.00012227417067697653 average time 1.3659580975832797 iter num 60\n",
            "loss 0.00012215752873756706 average time 1.3632459377875192 iter num 80\n",
            "loss 0.00012213300495198554 average time 1.3609609706200172 iter num 100\n",
            "loss 0.00012166490155824803 average time 1.3532206387999395 iter num 20\n",
            "loss 0.00012125465964491936 average time 1.3549063899750309 iter num 40\n",
            "loss 0.00012098599904598228 average time 1.3558176459500524 iter num 60\n",
            "loss 0.00012086405202060268 average time 1.35625417990002 iter num 80\n",
            "loss 0.00012083826358448365 average time 1.357443164529941 iter num 100\n",
            "loss 0.00012035174491554345 average time 1.3633954678499323 iter num 20\n",
            "loss 0.00011992620131124284 average time 1.3621727365749394 iter num 40\n",
            "loss 0.0001196487142416412 average time 1.3676344588999807 iter num 60\n",
            "loss 0.00011952248436629139 average time 1.364036481624953 iter num 80\n",
            "loss 0.00011949601437291728 average time 1.3630340616200374 iter num 100\n",
            "loss 0.00011898957104793318 average time 1.3536189374497554 iter num 20\n",
            "loss 0.00011854655280083787 average time 1.3539689433497188 iter num 40\n",
            "loss 0.00011825623868591351 average time 1.3537722443831324 iter num 60\n",
            "loss 0.0001181247977986533 average time 1.354295811462407 iter num 80\n",
            "loss 0.00011809729429627206 average time 1.3550231602899658 iter num 100\n",
            "loss 0.00011757336251832988 average time 1.3650927334001608 iter num 20\n",
            "loss 0.00011711819356857119 average time 1.3629718379502265 iter num 40\n",
            "loss 0.00011682191200227428 average time 1.3609545055501562 iter num 60\n",
            "loss 0.00011668799100701685 average time 1.362029888275083 iter num 80\n",
            "loss 0.00011665989824206633 average time 1.365253495340039 iter num 100\n",
            "loss 0.00011613141921334441 average time 1.3539186022999274 iter num 20\n",
            "loss 0.00011567109113561134 average time 1.355797104974863 iter num 40\n",
            "loss 0.00011537153054307734 average time 1.3569553051331846 iter num 60\n",
            "loss 0.00011523588945461446 average time 1.3559859693998533 iter num 80\n",
            "loss 0.00011520745220038565 average time 1.357687576749995 iter num 100\n",
            "loss 0.00011467055065381906 average time 1.3549512902998686 iter num 20\n",
            "loss 0.00011420427993911068 average time 1.355569145024765 iter num 40\n",
            "loss 0.00011389966156742748 average time 1.3563357636330693 iter num 60\n",
            "loss 0.00011376221932749159 average time 1.355461814612272 iter num 80\n",
            "loss 0.00011373368607481823 average time 1.354675371809899 iter num 100\n",
            "loss 0.00011319518418056684 average time 1.375589426250008 iter num 20\n",
            "loss 0.00011272302903925204 average time 1.3653171645749353 iter num 40\n",
            "loss 0.00011241568986249202 average time 1.3614039873333241 iter num 60\n",
            "loss 0.00011227687246664837 average time 1.3618372047874345 iter num 80\n",
            "loss 0.00011224780436846012 average time 1.3607725164898874 iter num 100\n",
            "loss 0.00011170229480083272 average time 1.3570940665003945 iter num 20\n",
            "loss 0.000111225251448029 average time 1.3567399394252788 iter num 40\n",
            "loss 0.00011091583887410635 average time 1.3561165916168345 iter num 60\n",
            "loss 0.00011077598421012098 average time 1.3541641253126726 iter num 80\n",
            "loss 0.00011074705153220557 average time 1.3558981369502 iter num 100\n",
            "loss 0.00011019868434973301 average time 1.353554080450158 iter num 20\n",
            "loss 0.00010972556540941132 average time 1.3611237162500402 iter num 40\n",
            "loss 0.00010941900627419245 average time 1.3619819534164586 iter num 60\n",
            "loss 0.00010928032134287783 average time 1.3604823730749103 iter num 80\n",
            "loss 0.00010925136053700916 average time 1.3594405026699314 iter num 100\n",
            "loss 0.0001113296962845679 average time 1.35752936530007 iter num 20\n",
            "loss 0.00010832234436623884 average time 1.359306120125075 iter num 40\n",
            "loss 0.00010795696227684997 average time 1.3583550823499535 iter num 60\n",
            "loss 0.00010784422465813927 average time 1.3564133932623008 iter num 80\n",
            "loss 0.00010781368136067206 average time 1.3550744796598155 iter num 100\n",
            "loss 0.00010853675165695634 average time 1.3526228341503157 iter num 20\n",
            "loss 0.00010708768709279584 average time 1.3532153929998458 iter num 40\n",
            "loss 0.00010671074500675898 average time 1.3528307612831971 iter num 60\n",
            "loss 0.00010657727139818704 average time 1.3565415251374815 iter num 80\n",
            "loss 0.00010655249276634355 average time 1.355296559820017 iter num 100\n",
            "loss 0.00010612879060378716 average time 1.3576887118499144 iter num 20\n",
            "loss 0.0001057464726049306 average time 1.3550973694499588 iter num 40\n",
            "loss 0.00010550544162227349 average time 1.357531409750057 iter num 60\n",
            "loss 0.00010539595240040804 average time 1.3573729715500122 iter num 80\n",
            "loss 0.0001053732208853805 average time 1.3564366643500398 iter num 100\n",
            "loss 0.00010494241329818723 average time 1.3533950439503315 iter num 20\n",
            "loss 0.00010456987354515502 average time 1.3569691927748864 iter num 40\n",
            "loss 0.00010432865582289386 average time 1.3563607188166011 iter num 60\n",
            "loss 0.00010422042149565984 average time 1.3570904038124354 iter num 80\n",
            "loss 0.00010419781215927228 average time 1.3599995456899705 iter num 100\n",
            "loss 0.00010382969805360506 average time 1.3600665213496541 iter num 20\n",
            "loss 0.00010340557131875782 average time 1.3584366468498046 iter num 40\n",
            "loss 0.00010316890665354838 average time 1.3573600151497884 iter num 60\n",
            "loss 0.00010306066803310548 average time 1.3548178930247559 iter num 80\n",
            "loss 0.00010303904815533322 average time 1.3539749119696716 iter num 100\n",
            "loss 0.00010657319523202033 average time 1.3613276797497746 iter num 20\n",
            "loss 0.00010304993022035129 average time 1.3595355495247532 iter num 40\n",
            "loss 0.00010219998567791807 average time 1.3583228746165332 iter num 60\n",
            "loss 0.0001020755200779546 average time 1.3584192168248592 iter num 80\n",
            "loss 0.00010204046334706919 average time 1.3567216491897853 iter num 100\n",
            "loss 0.00010165867421460231 average time 1.3575807082996107 iter num 20\n",
            "loss 0.00010136849987576926 average time 1.3656724131999909 iter num 40\n",
            "loss 0.00010119012187596223 average time 1.3602579492666942 iter num 60\n",
            "loss 0.00010111038636567647 average time 1.3589977244126203 iter num 80\n",
            "loss 0.00010109357404891596 average time 1.3581381239100665 iter num 100\n",
            "loss 0.00010077844012612592 average time 1.3417005189498012 iter num 20\n",
            "loss 0.00010050689637485038 average time 1.3489313588998812 iter num 40\n",
            "loss 0.00010032891253272681 average time 1.3503000157665155 iter num 60\n",
            "loss 0.00010024882272294293 average time 1.3519214116623517 iter num 80\n",
            "loss 0.00010023224261551787 average time 1.3533305004199427 iter num 100\n",
            "loss 9.991731559407735e-05 average time 1.3538875202502823 iter num 20\n",
            "loss 9.964453855571205e-05 average time 1.35196571962515 iter num 40\n",
            "loss 9.946805596658086e-05 average time 1.3608889255001486 iter num 60\n",
            "loss 9.93876027593826e-05 average time 1.3567338668502997 iter num 80\n",
            "loss 9.937085605484265e-05 average time 1.3552600723302748 iter num 100\n",
            "loss 9.905293309172076e-05 average time 1.3526332004506911 iter num 20\n",
            "loss 9.877165089413959e-05 average time 1.356743102575456 iter num 40\n",
            "loss 9.858536312298537e-05 average time 1.3568369914167608 iter num 60\n",
            "loss 9.850043414410896e-05 average time 1.357934625987491 iter num 80\n",
            "loss 9.848293750796283e-05 average time 1.3591437520099499 iter num 100\n",
            "loss 9.815087008762382e-05 average time 1.3603047932501795 iter num 20\n",
            "loss 9.78610626040738e-05 average time 1.3575955264502226 iter num 40\n",
            "loss 9.767432004521663e-05 average time 1.3560685090168894 iter num 60\n",
            "loss 9.758977934935953e-05 average time 1.3560154286003125 iter num 80\n",
            "loss 9.757230309834173e-05 average time 1.359191310660317 iter num 100\n",
            "loss 9.726166519294583e-05 average time 1.349703473250338 iter num 20\n",
            "loss 9.695666552115012e-05 average time 1.3565173669750947 iter num 40\n",
            "loss 9.676421631975219e-05 average time 1.3570723002001615 iter num 60\n",
            "loss 9.66812980770736e-05 average time 1.3589828157876582 iter num 80\n",
            "loss 9.666377164088904e-05 average time 1.3589222486501966 iter num 100\n",
            "loss 9.99918892406546e-05 average time 1.3514097573504842 iter num 20\n",
            "loss 9.743516678939338e-05 average time 1.3595305055751852 iter num 40\n",
            "loss 9.629018064034643e-05 average time 1.3586143914999411 iter num 60\n",
            "loss 9.614509286396121e-05 average time 1.3582881354375331 iter num 80\n",
            "loss 9.612788461214891e-05 average time 1.358249909910046 iter num 100\n",
            "loss 9.569114171302797e-05 average time 1.3827125884501583 iter num 20\n",
            "loss 9.5436553374633e-05 average time 1.3672505283502687 iter num 40\n",
            "loss 9.529232848149413e-05 average time 1.3666722054003306 iter num 60\n",
            "loss 9.522739488219677e-05 average time 1.3651394180752505 iter num 80\n",
            "loss 9.521434531463135e-05 average time 1.3642274363102116 iter num 100\n",
            "loss 9.49649965389749e-05 average time 1.3745637900992733 iter num 20\n",
            "loss 9.47523004050865e-05 average time 1.3633306354497108 iter num 40\n",
            "loss 9.461395951345506e-05 average time 1.3628929831498682 iter num 60\n",
            "loss 9.455201328728216e-05 average time 1.3616568497623576 iter num 80\n",
            "loss 9.453937552465735e-05 average time 1.3613556532498479 iter num 100\n",
            "loss 9.429638732805978e-05 average time 1.3630180340502194 iter num 20\n",
            "loss 9.408740576275733e-05 average time 1.3680249133751203 iter num 40\n",
            "loss 9.395213370598678e-05 average time 1.3657581857833065 iter num 60\n",
            "loss 9.389074541973926e-05 average time 1.3656286346750677 iter num 80\n",
            "loss 9.387809519588645e-05 average time 1.3654064416300389 iter num 100\n",
            "loss 9.363744384281563e-05 average time 1.3645221529006448 iter num 20\n",
            "loss 9.342840004154358e-05 average time 1.367759164625204 iter num 40\n",
            "loss 9.329127912106681e-05 average time 1.368652454966832 iter num 60\n",
            "loss 9.322902156128368e-05 average time 1.3681424545252412 iter num 80\n",
            "loss 9.3216197775492e-05 average time 1.3668748155301365 iter num 100\n",
            "loss 9.29716496549171e-05 average time 1.3779907698004537 iter num 20\n",
            "loss 9.275396390095767e-05 average time 1.3715433950748774 iter num 40\n",
            "loss 9.261260585487963e-05 average time 1.369229712283171 iter num 60\n",
            "loss 9.254878419946855e-05 average time 1.3732228493749972 iter num 80\n",
            "loss 9.25356809122281e-05 average time 1.3711340133499834 iter num 100\n",
            "loss 9.228257695591465e-05 average time 1.3687488119006956 iter num 20\n",
            "loss 9.205988236779344e-05 average time 1.3693441796752268 iter num 40\n",
            "loss 9.191504681260603e-05 average time 1.369444805833397 iter num 60\n",
            "loss 9.184940913949988e-05 average time 1.3693876344124873 iter num 80\n",
            "loss 9.183598226327195e-05 average time 1.3680920722300653 iter num 100\n",
            "loss 9.157632752975168e-05 average time 1.3620541373999004 iter num 20\n",
            "loss 9.135159501147252e-05 average time 1.3624487904749913 iter num 40\n",
            "loss 9.120377383730646e-05 average time 1.363414333600061 iter num 60\n",
            "loss 9.113736452227157e-05 average time 1.3648358849998659 iter num 80\n",
            "loss 9.112362285102578e-05 average time 1.3685818564898364 iter num 100\n",
            "loss 9.086247971079814e-05 average time 1.3696989209496677 iter num 20\n",
            "loss 9.063110448358536e-05 average time 1.3695999122249305 iter num 40\n",
            "loss 9.048140435815033e-05 average time 1.3699935353501131 iter num 60\n",
            "loss 9.041360295007355e-05 average time 1.3693382453628602 iter num 80\n",
            "loss 9.039946322153067e-05 average time 1.370204011040296 iter num 100\n",
            "loss 9.233794845359982e-05 average time 1.370288629750212 iter num 20\n",
            "loss 8.992938519742528e-05 average time 1.36520374465008 iter num 40\n",
            "loss 8.981667529187845e-05 average time 1.3696328924336436 iter num 60\n",
            "loss 8.97115574914188e-05 average time 1.3696869963377594 iter num 80\n",
            "loss 8.969372703057722e-05 average time 1.3700837377102653 iter num 100\n",
            "loss 9.150287172816243e-05 average time 1.3859514805499202 iter num 20\n",
            "loss 8.965911201703966e-05 average time 1.3861835140247423 iter num 40\n",
            "loss 8.926018353561781e-05 average time 1.3836506378830866 iter num 60\n",
            "loss 8.92115370459708e-05 average time 1.3815167685623237 iter num 80\n",
            "loss 8.918690190762963e-05 average time 1.3789151313597905 iter num 100\n",
            "loss 8.887158407845467e-05 average time 1.3660114253490974 iter num 20\n",
            "loss 8.863659565207573e-05 average time 1.3675212136494337 iter num 40\n",
            "loss 8.849512802970239e-05 average time 1.3664638267329186 iter num 60\n",
            "loss 8.843395580970556e-05 average time 1.3650781555121738 iter num 80\n",
            "loss 8.842098478562434e-05 average time 1.3645689911696899 iter num 100\n",
            "loss 8.817604659546393e-05 average time 1.360465812249822 iter num 20\n",
            "loss 8.796358737886737e-05 average time 1.359551022100095 iter num 40\n",
            "loss 8.782712092383419e-05 average time 1.3721137705500344 iter num 60\n",
            "loss 8.776578770235436e-05 average time 1.3701755906125073 iter num 80\n",
            "loss 8.775316266164044e-05 average time 1.3693606077299774 iter num 100\n",
            "loss 8.750949495909364e-05 average time 1.3790814257496096 iter num 20\n",
            "loss 8.729440250213782e-05 average time 1.3736217551999288 iter num 40\n",
            "loss 8.715453682219354e-05 average time 1.3723910294163943 iter num 60\n",
            "loss 8.70915117438761e-05 average time 1.370530631287238 iter num 80\n",
            "loss 8.70785786243874e-05 average time 1.367768155389822 iter num 100\n",
            "loss 8.682966273148305e-05 average time 1.3594358586495219 iter num 20\n",
            "loss 8.661331887305751e-05 average time 1.3591659763995267 iter num 40\n",
            "loss 8.647159554031299e-05 average time 1.3584668552830408 iter num 60\n",
            "loss 8.640789531217065e-05 average time 1.36971255977478 iter num 80\n",
            "loss 8.63947399488882e-05 average time 1.3695468087999325 iter num 100\n",
            "loss 8.61436341495229e-05 average time 1.3765817427000002 iter num 20\n",
            "loss 8.592316112221511e-05 average time 1.3796362578999832 iter num 40\n",
            "loss 8.578028272025401e-05 average time 1.3769791472999713 iter num 60\n",
            "loss 8.571529414787492e-05 average time 1.3758367372000067 iter num 80\n",
            "loss 8.570191200879574e-05 average time 1.3748065841000061 iter num 100\n",
            "loss 8.558048676898477e-05 average time 1.3836230504502964 iter num 20\n",
            "loss 8.524168806608434e-05 average time 1.3836298362751223 iter num 40\n",
            "loss 8.506160002706951e-05 average time 1.38097494186671 iter num 60\n",
            "loss 8.499944388279439e-05 average time 1.382153417550171 iter num 80\n",
            "loss 8.498315644289467e-05 average time 1.387275578310073 iter num 100\n",
            "loss 9.27595643931737e-05 average time 1.3798417264002638 iter num 20\n",
            "loss 8.659393158984833e-05 average time 1.3778217658249559 iter num 40\n",
            "loss 8.556330702592547e-05 average time 1.3781523411165835 iter num 60\n",
            "loss 8.524724072589852e-05 average time 1.3734850808500596 iter num 80\n",
            "loss 8.517233691054142e-05 average time 1.371926875810059 iter num 100\n",
            "loss 8.462776316553545e-05 average time 1.3620183195494975 iter num 20\n",
            "loss 8.433504885370801e-05 average time 1.3625345307247698 iter num 40\n",
            "loss 8.41877317607369e-05 average time 1.3607269306333667 iter num 60\n",
            "loss 8.413166776991332e-05 average time 1.3584518219374786 iter num 80\n",
            "loss 8.41197027477973e-05 average time 1.3582146145800653 iter num 100\n",
            "loss 8.389629802864642e-05 average time 1.36174412379969 iter num 20\n",
            "loss 8.370854465285052e-05 average time 1.3728174629999557 iter num 40\n",
            "loss 8.358918940869155e-05 average time 1.3692241452000113 iter num 60\n",
            "loss 8.353667329483646e-05 average time 1.366522188424915 iter num 80\n",
            "loss 8.35260688400073e-05 average time 1.364685185180024 iter num 100\n",
            "loss 8.33205090219819e-05 average time 1.365028341099969 iter num 20\n",
            "loss 8.314558815502917e-05 average time 1.3638120644499394 iter num 40\n",
            "loss 8.303323664909441e-05 average time 1.3623225390666145 iter num 60\n",
            "loss 8.298253778879344e-05 average time 1.36254472644996 iter num 80\n",
            "loss 8.297252670126648e-05 average time 1.3618750178899426 iter num 100\n",
            "loss 8.277384936816588e-05 average time 1.3598791990500103 iter num 20\n",
            "loss 8.260206866200758e-05 average time 1.3612943494747924 iter num 40\n",
            "loss 8.248967953444056e-05 average time 1.3701534030500624 iter num 60\n",
            "loss 8.243920862052757e-05 average time 1.3675403042001109 iter num 80\n",
            "loss 8.242907610612468e-05 average time 1.367638486360047 iter num 100\n",
            "loss 8.223154825793861e-05 average time 1.3679069749996415 iter num 20\n",
            "loss 8.205694250580432e-05 average time 1.3658400497748517 iter num 40\n",
            "loss 8.194309698280858e-05 average time 1.3668637983498533 iter num 60\n",
            "loss 8.189161532549525e-05 average time 1.3656202262248827 iter num 80\n",
            "loss 8.188133099250921e-05 average time 1.3645768942900396 iter num 100\n",
            "loss 8.167982276951233e-05 average time 1.3648968832498212 iter num 20\n",
            "loss 8.150195103809333e-05 average time 1.359746511799949 iter num 40\n",
            "loss 8.138667265891202e-05 average time 1.3619336361334111 iter num 60\n",
            "loss 8.133398185536863e-05 average time 1.3664692629250113 iter num 80\n",
            "loss 8.132347597799719e-05 average time 1.3669958470000347 iter num 100\n",
            "loss 8.111550850788192e-05 average time 1.3693422621001445 iter num 20\n",
            "loss 8.09353826073099e-05 average time 1.365810553825304 iter num 40\n",
            "loss 8.081624191695983e-05 average time 1.3677268579836286 iter num 60\n",
            "loss 8.07625002906346e-05 average time 1.36655077898763 iter num 80\n",
            "loss 8.075163450868262e-05 average time 1.366573413560036 iter num 100\n",
            "loss 8.053793626534544e-05 average time 1.362844853900242 iter num 20\n",
            "loss 8.035178922179302e-05 average time 1.3619979674500429 iter num 40\n",
            "loss 8.022894651003978e-05 average time 1.3644145282500175 iter num 60\n",
            "loss 8.017257133359365e-05 average time 1.364245727224852 iter num 80\n",
            "loss 8.016121750579311e-05 average time 1.3656130788398877 iter num 100\n",
            "loss 7.993944700948856e-05 average time 1.384645732250283 iter num 20\n",
            "loss 7.974331490521727e-05 average time 1.3765881527751844 iter num 40\n",
            "loss 7.961432687219698e-05 average time 1.3730000311334152 iter num 60\n",
            "loss 7.955635695568756e-05 average time 1.3722142324375455 iter num 80\n",
            "loss 7.954443393857775e-05 average time 1.37113484755002 iter num 100\n",
            "loss 7.931083582257993e-05 average time 1.365973213199868 iter num 20\n",
            "loss 7.91133786399028e-05 average time 1.365408438124905 iter num 40\n",
            "loss 7.897898253203807e-05 average time 1.3649100713331184 iter num 60\n",
            "loss 7.891881399946673e-05 average time 1.3662956373373163 iter num 80\n",
            "loss 7.890640205230546e-05 average time 1.3653983790498387 iter num 100\n",
            "loss 8.51904515663428e-05 average time 1.3666363702997841 iter num 20\n",
            "loss 7.912754322754552e-05 average time 1.3792411682245074 iter num 40\n",
            "loss 7.868093097242988e-05 average time 1.3758263240830881 iter num 60\n",
            "loss 7.857159612457642e-05 average time 1.37177238402478 iter num 80\n",
            "loss 7.854375205159423e-05 average time 1.3715449633798198 iter num 100\n",
            "loss 7.827428077454224e-05 average time 1.3596516652998616 iter num 20\n",
            "loss 7.804929990770289e-05 average time 1.364350777324762 iter num 40\n",
            "loss 7.792227579175229e-05 average time 1.3655527147497801 iter num 60\n",
            "loss 7.786612969349294e-05 average time 1.3656211128996802 iter num 80\n",
            "loss 7.785457032946621e-05 average time 1.366231255429884 iter num 100\n",
            "loss 7.763051867448548e-05 average time 1.3649845995496435 iter num 20\n",
            "loss 7.74370358023797e-05 average time 1.3643205474497335 iter num 40\n",
            "loss 7.731066175953171e-05 average time 1.3701109975330836 iter num 60\n",
            "loss 7.725326681492694e-05 average time 1.372304814887184 iter num 80\n",
            "loss 7.724169027983088e-05 average time 1.371529989739829 iter num 100\n",
            "loss 7.701543062004164e-05 average time 1.3654708188001679 iter num 20\n",
            "loss 7.681779625000839e-05 average time 1.3660873804252334 iter num 40\n",
            "loss 7.668907004786716e-05 average time 1.3663544601167208 iter num 60\n",
            "loss 7.663084725405271e-05 average time 1.3658072299625927 iter num 80\n",
            "loss 7.661889887275482e-05 average time 1.3676532423599566 iter num 100\n",
            "loss 7.673691169941166e-05 average time 1.3682002299505256 iter num 20\n",
            "loss 7.625478325288691e-05 average time 1.3711464587251612 iter num 40\n",
            "loss 7.606157592410562e-05 average time 1.371953518466762 iter num 60\n",
            "loss 7.601145694315664e-05 average time 1.371167466437555 iter num 80\n",
            "loss 7.599814816156299e-05 average time 1.3752770206301284 iter num 100\n",
            "loss 8.42088034365251e-05 average time 1.361939886100481 iter num 20\n",
            "loss 7.703982197416918e-05 average time 1.3645799213748433 iter num 40\n",
            "loss 7.66613512828593e-05 average time 1.3658568526331996 iter num 60\n",
            "loss 7.632063249702878e-05 average time 1.3673033649874013 iter num 80\n",
            "loss 7.629115082846352e-05 average time 1.3671181821299252 iter num 100\n",
            "loss 7.582124801650256e-05 average time 1.3795553307498267 iter num 20\n",
            "loss 7.562576377289701e-05 average time 1.3756241009245969 iter num 40\n",
            "loss 7.548982640534657e-05 average time 1.3742607986998336 iter num 60\n",
            "loss 7.543248673557724e-05 average time 1.3715328735499497 iter num 80\n",
            "loss 7.542196430712643e-05 average time 1.3711719748800533 iter num 100\n",
            "loss 7.521695574133318e-05 average time 1.3901181264011029 iter num 20\n",
            "loss 7.504762736141704e-05 average time 1.3785456207503557 iter num 40\n",
            "loss 7.494124095815562e-05 average time 1.3750514843170094 iter num 60\n",
            "loss 7.489379821451151e-05 average time 1.3733369232379118 iter num 80\n",
            "loss 7.488431486773626e-05 average time 1.3716330511104753 iter num 100\n",
            "loss 7.470088411822206e-05 average time 1.3759859124998912 iter num 20\n",
            "loss 7.454396974484219e-05 average time 1.3685232361750423 iter num 40\n",
            "loss 7.444282701759009e-05 average time 1.3924263090166884 iter num 60\n",
            "loss 7.43974562572667e-05 average time 1.3951305726125156 iter num 80\n",
            "loss 7.438832064062051e-05 average time 1.3882574441699398 iter num 100\n",
            "loss 7.421161356885075e-05 average time 1.3552735266999663 iter num 20\n",
            "loss 7.405766885472292e-05 average time 1.366284568449919 iter num 40\n",
            "loss 7.395832823793779e-05 average time 1.3679493790665826 iter num 60\n",
            "loss 7.391382784651444e-05 average time 1.3670064523124439 iter num 80\n",
            "loss 7.390488931923403e-05 average time 1.3658252358398022 iter num 100\n",
            "loss 7.37307075326384e-05 average time 1.3572545859502498 iter num 20\n",
            "loss 7.357775435595287e-05 average time 1.3617221619251723 iter num 40\n",
            "loss 7.347927170066215e-05 average time 1.3632321663000766 iter num 60\n",
            "loss 7.343437587682771e-05 average time 1.3642461355624618 iter num 80\n",
            "loss 7.342534535796153e-05 average time 1.3659633303400187 iter num 100\n",
            "loss 7.324847151806358e-05 average time 1.365235517250221 iter num 20\n",
            "loss 7.309465171290898e-05 average time 1.3635320474251784 iter num 40\n",
            "loss 7.299461050462552e-05 average time 1.3682284031666616 iter num 60\n",
            "loss 7.294918590656683e-05 average time 1.3712436434750543 iter num 80\n",
            "loss 7.294007359539897e-05 average time 1.3698737693400107 iter num 100\n",
            "loss 7.276018620018297e-05 average time 1.3750826901001347 iter num 20\n",
            "loss 7.260478165735138e-05 average time 1.3709642802499729 iter num 40\n",
            "loss 7.25035517257408e-05 average time 1.3696369292167825 iter num 60\n",
            "loss 7.245745298053119e-05 average time 1.3685024311876077 iter num 80\n",
            "loss 7.244826439093981e-05 average time 1.368327024870123 iter num 100\n",
            "loss 7.226713972100484e-05 average time 1.3587438443502833 iter num 20\n",
            "loss 7.210878367015932e-05 average time 1.3629429746000823 iter num 40\n",
            "loss 7.200500480987673e-05 average time 1.3671997675667928 iter num 60\n",
            "loss 7.195827905487123e-05 average time 1.3670414925125898 iter num 80\n",
            "loss 7.194894270024874e-05 average time 1.370563587940087 iter num 100\n",
            "loss 7.176555913920674e-05 average time 1.3561601053997947 iter num 20\n",
            "loss 7.160270368993214e-05 average time 1.360170892699898 iter num 40\n",
            "loss 7.14974316419929e-05 average time 1.3618922406164833 iter num 60\n",
            "loss 7.144959472744512e-05 average time 1.3637818719123516 iter num 80\n",
            "loss 7.144000020073218e-05 average time 1.3638894897797582 iter num 100\n",
            "loss 7.122828290430774e-05 average time 1.3736570130498875 iter num 20\n",
            "loss 7.10817704884925e-05 average time 1.3738760704000925 iter num 40\n",
            "loss 7.098374809347803e-05 average time 1.3706213157500315 iter num 60\n",
            "loss 7.09338688828556e-05 average time 1.3715939655749936 iter num 80\n",
            "loss 7.092281677214175e-05 average time 1.3694148748200314 iter num 100\n",
            "loss 7.921866628442817e-05 average time 1.391336343099829 iter num 20\n",
            "loss 7.19771825367436e-05 average time 1.3802545079498487 iter num 40\n",
            "loss 7.123156553280351e-05 average time 1.374753552583267 iter num 60\n",
            "loss 7.121338344699579e-05 average time 1.3729184980249556 iter num 80\n",
            "loss 7.117101654990195e-05 average time 1.3725462428998798 iter num 100\n",
            "loss 7.081911994574327e-05 average time 1.3560713376500644 iter num 20\n",
            "loss 7.061689151757447e-05 average time 1.3617346053252732 iter num 40\n",
            "loss 7.050756289749188e-05 average time 1.363757773883602 iter num 60\n",
            "loss 7.046077481485254e-05 average time 1.3643905803377039 iter num 80\n",
            "loss 7.045141861251937e-05 average time 1.3658929688701391 iter num 100\n",
            "loss 7.027518279332125e-05 average time 1.36715647120036 iter num 20\n",
            "loss 7.012654649082506e-05 average time 1.3694199161252982 iter num 40\n",
            "loss 7.00329654185426e-05 average time 1.3723674257504173 iter num 60\n",
            "loss 6.999160174708772e-05 average time 1.3702147546754078 iter num 80\n",
            "loss 6.998327100905195e-05 average time 1.3674887809203211 iter num 100\n",
            "loss 6.982334496338586e-05 average time 1.362591821749993 iter num 20\n",
            "loss 6.968432918367532e-05 average time 1.3643274958250913 iter num 40\n",
            "loss 6.959563114583172e-05 average time 1.3658058379500895 iter num 60\n",
            "loss 6.955594537241671e-05 average time 1.3662189993875473 iter num 80\n",
            "loss 6.954803199379282e-05 average time 1.3665058940401287 iter num 100\n",
            "loss 6.939266875921926e-05 average time 1.3743756169995323 iter num 20\n",
            "loss 6.925855211571815e-05 average time 1.3717366818747905 iter num 40\n",
            "loss 6.917117598247927e-05 average time 1.3718382020665862 iter num 60\n",
            "loss 6.913180198390085e-05 average time 1.3794015338623467 iter num 80\n",
            "loss 6.912401910932983e-05 average time 1.3780053793099796 iter num 100\n",
            "loss 6.89677760923879e-05 average time 1.3832052444997316 iter num 20\n",
            "loss 6.883462118892448e-05 average time 1.3829844466748 iter num 40\n",
            "loss 6.874695797473656e-05 average time 1.381212881816585 iter num 60\n",
            "loss 6.870741363330955e-05 average time 1.3820124328751717 iter num 80\n",
            "loss 6.869955025758505e-05 average time 1.3835626073700769 iter num 100\n",
            "loss 6.854374893247297e-05 average time 1.3813385518496943 iter num 20\n",
            "loss 6.840835918023006e-05 average time 1.3806505727497096 iter num 40\n",
            "loss 6.831994980521903e-05 average time 1.3805525269496988 iter num 60\n",
            "loss 6.827987150218766e-05 average time 1.381497372537251 iter num 80\n",
            "loss 6.827175646992065e-05 average time 1.385469238239857 iter num 100\n",
            "loss 6.811547556642173e-05 average time 1.3891641154505123 iter num 20\n",
            "loss 6.797712505766941e-05 average time 1.3856568495001738 iter num 40\n",
            "loss 6.788704605764231e-05 average time 1.3774859912000466 iter num 60\n",
            "loss 6.784610456751906e-05 average time 1.3757778790376052 iter num 80\n",
            "loss 6.783796221732347e-05 average time 1.37674311579005 iter num 100\n",
            "loss 6.767565878906839e-05 average time 1.3723743058002582 iter num 20\n",
            "loss 6.75343647418614e-05 average time 1.369459254350204 iter num 40\n",
            "loss 6.744092685584361e-05 average time 1.3686473459833601 iter num 60\n",
            "loss 6.739855246450796e-05 average time 1.3692881967625454 iter num 80\n",
            "loss 6.739014126704467e-05 average time 1.3690342409799632 iter num 100\n",
            "loss 6.720824011181617e-05 average time 1.388819962249545 iter num 20\n",
            "loss 6.706300259130605e-05 average time 1.3851899355247952 iter num 40\n",
            "loss 6.699184597653505e-05 average time 1.380380121416662 iter num 60\n",
            "loss 6.694124530937485e-05 average time 1.377275595675019 iter num 80\n",
            "loss 6.693290286578576e-05 average time 1.3752260321799985 iter num 100\n",
            "loss 7.286250731854081e-05 average time 1.367873157399481 iter num 20\n",
            "loss 6.914555588500844e-05 average time 1.3694246008747541 iter num 40\n",
            "loss 6.725445033920586e-05 average time 1.3721574461165196 iter num 60\n",
            "loss 6.725722756555801e-05 average time 1.3746219227623897 iter num 80\n",
            "loss 6.721797425807004e-05 average time 1.375854495499989 iter num 100\n",
            "loss 6.690228312577114e-05 average time 1.3636477824504254 iter num 20\n",
            "loss 6.671607014505031e-05 average time 1.3697666533254051 iter num 40\n",
            "loss 6.662824868847799e-05 average time 1.3789351652835344 iter num 60\n",
            "loss 6.658572940430563e-05 average time 1.3770291657001963 iter num 80\n",
            "loss 6.657814889761362e-05 average time 1.3764378008402856 iter num 100\n",
            "loss 6.642238915714578e-05 average time 1.3791545117497663 iter num 20\n",
            "loss 6.629172897982502e-05 average time 1.380809397249959 iter num 40\n",
            "loss 6.620991419879794e-05 average time 1.3784396494666604 iter num 60\n",
            "loss 6.617396314039906e-05 average time 1.3791452566749285 iter num 80\n",
            "loss 6.616676033109632e-05 average time 1.3796339045298374 iter num 100\n",
            "loss 6.602691697178443e-05 average time 1.3798067771000206 iter num 20\n",
            "loss 6.590820736108789e-05 average time 1.3787486407752112 iter num 40\n",
            "loss 6.583272469518253e-05 average time 1.3800477507167064 iter num 60\n",
            "loss 6.579907202995837e-05 average time 1.3847317678626951 iter num 80\n",
            "loss 6.579231792630977e-05 average time 1.3827998711403051 iter num 100\n",
            "loss 6.56601700906596e-05 average time 1.3740932185501151 iter num 20\n",
            "loss 6.554526544412942e-05 average time 1.3776357775001997 iter num 40\n",
            "loss 6.547132781193154e-05 average time 1.377511447966693 iter num 60\n",
            "loss 6.543812472651574e-05 average time 1.378360789012413 iter num 80\n",
            "loss 6.543154793369572e-05 average time 1.377231813560029 iter num 100\n",
            "loss 6.530165715313786e-05 average time 1.3818523008001649 iter num 20\n",
            "loss 6.518822905252792e-05 average time 1.380520388450077 iter num 40\n",
            "loss 6.511506169957719e-05 average time 1.3815806217500721 iter num 60\n",
            "loss 6.508213602448346e-05 average time 1.3797782561001442 iter num 80\n",
            "loss 6.507543960957878e-05 average time 1.3824304894002126 iter num 100\n",
            "loss 6.494573178739649e-05 average time 1.377621781099697 iter num 20\n",
            "loss 6.4832536813322e-05 average time 1.3802260504000514 iter num 40\n",
            "loss 6.475877440546531e-05 average time 1.3828668290667945 iter num 60\n",
            "loss 6.47255686265733e-05 average time 1.3838308426000823 iter num 80\n",
            "loss 6.471888356366327e-05 average time 1.3835078501900717 iter num 100\n",
            "loss 6.458690444695678e-05 average time 1.3825890246998824 iter num 20\n",
            "loss 6.447379788773744e-05 average time 1.3819607146995623 iter num 40\n",
            "loss 6.439890469541772e-05 average time 1.3823373874664564 iter num 60\n",
            "loss 6.436529633096532e-05 average time 1.3814932745748139 iter num 80\n",
            "loss 6.435864148744205e-05 average time 1.3817685150597754 iter num 100\n",
            "loss 6.42253756798126e-05 average time 1.392026890950183 iter num 20\n",
            "loss 6.410931270223298e-05 average time 1.3944375419252537 iter num 40\n",
            "loss 6.403344789942344e-05 average time 1.3905883765668476 iter num 60\n",
            "loss 6.399916047178555e-05 average time 1.399446976337731 iter num 80\n",
            "loss 6.399205103336654e-05 average time 1.4053583297002479 iter num 100\n",
            "loss 6.385655410245562e-05 average time 1.4110398978500598 iter num 20\n",
            "loss 6.373654328489052e-05 average time 1.4048470979750163 iter num 40\n",
            "loss 6.365863308030001e-05 average time 1.3988229040499087 iter num 60\n",
            "loss 6.36232587955901e-05 average time 1.3928284136999083 iter num 80\n",
            "loss 6.36160782833315e-05 average time 1.3896898982798667 iter num 100\n",
            "loss 6.345004793238023e-05 average time 1.3819307275003667 iter num 20\n",
            "loss 6.334754826378825e-05 average time 1.3855914284500614 iter num 40\n",
            "loss 6.327696266979411e-05 average time 1.3924130985165903 iter num 60\n",
            "loss 6.323883610538222e-05 average time 1.3890638556749308 iter num 80\n",
            "loss 6.3230971369347e-05 average time 1.3915093509600773 iter num 100\n",
            "loss 7.184094640048247e-05 average time 1.3738719846000094 iter num 20\n",
            "loss 6.405978699006057e-05 average time 1.3715574846751224 iter num 40\n",
            "loss 6.345080130046192e-05 average time 1.3726742508002039 iter num 60\n",
            "loss 6.341425269107152e-05 average time 1.374232156150174 iter num 80\n",
            "loss 6.339336989579171e-05 average time 1.372895647570258 iter num 100\n",
            "loss 6.318051646705626e-05 average time 1.3736591126498752 iter num 20\n",
            "loss 6.302837246135898e-05 average time 1.3740333451502011 iter num 40\n",
            "loss 6.294364904849912e-05 average time 1.3741140395833402 iter num 60\n",
            "loss 6.29061609155218e-05 average time 1.380332831449914 iter num 80\n",
            "loss 6.28996752583793e-05 average time 1.3793663298699539 iter num 100\n",
            "loss 6.276737877625662e-05 average time 1.3792876529501883 iter num 20\n",
            "loss 6.265502555692938e-05 average time 1.376463902350133 iter num 40\n",
            "loss 6.258456339222595e-05 average time 1.3742323910501606 iter num 60\n",
            "loss 6.255339338550612e-05 average time 1.376236159662676 iter num 80\n",
            "loss 6.254709358029909e-05 average time 1.3785255352800596 iter num 100\n",
            "loss 6.242652615643524e-05 average time 1.3947530967498096 iter num 20\n",
            "loss 6.232215105795194e-05 average time 1.4072884019999947 iter num 40\n",
            "loss 6.225611687333776e-05 average time 1.4104538349999227 iter num 60\n",
            "loss 6.222668090115837e-05 average time 1.4065570332876631 iter num 80\n",
            "loss 6.22206835110309e-05 average time 1.4091577895302543 iter num 100\n",
            "loss 6.210557443285941e-05 average time 1.38290514990058 iter num 20\n",
            "loss 6.200518311822488e-05 average time 1.3813357328502207 iter num 40\n",
            "loss 6.194048837270359e-05 average time 1.3808568639333316 iter num 60\n",
            "loss 6.191148031339192e-05 average time 1.3812142764500095 iter num 80\n",
            "loss 6.190560286398382e-05 average time 1.3812604190700222 iter num 100\n",
            "loss 6.179086319684575e-05 average time 1.3753757697002584 iter num 20\n",
            "loss 6.169210596924982e-05 average time 1.3778414055253052 iter num 40\n",
            "loss 6.162744001251478e-05 average time 1.3775653888668482 iter num 60\n",
            "loss 6.159827224906632e-05 average time 1.3764798540501033 iter num 80\n",
            "loss 6.159255613662803e-05 average time 1.3749029155101744 iter num 100\n",
            "loss 6.14778151079338e-05 average time 1.3978737375004129 iter num 20\n",
            "loss 6.137840137417121e-05 average time 1.386029986649919 iter num 40\n",
            "loss 6.131330846729722e-05 average time 1.3825721597664598 iter num 60\n",
            "loss 6.128414707224648e-05 average time 1.381239065687214 iter num 80\n",
            "loss 6.12782106851473e-05 average time 1.3813198782797917 iter num 100\n",
            "loss 6.116322660017165e-05 average time 1.3754380574995593 iter num 20\n",
            "loss 6.106138892485463e-05 average time 1.3735011236748504 iter num 40\n",
            "loss 6.099523127062883e-05 average time 1.3752630418332046 iter num 60\n",
            "loss 6.096538648824416e-05 average time 1.3798182246123816 iter num 80\n",
            "loss 6.0959353518265954e-05 average time 1.3827980987199044 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQs-OZHGEwac"
      },
      "source": [
        "# 18:58\n",
        "# 22:46"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057d7a59-efd1-460c-be63-9ffe68b5064a"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.02, 0.02]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.10870558, 0.581213937)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.108669]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.584903], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqpasxVi0hx3",
        "outputId": "17165331-9981-4d8e-eb44-6d39b4baa453"
      },
      "source": [
        "import cupy\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import pandas as pd\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "    return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "print(bs_call(1,1,1,0.02,0.25))\n",
        "print(bs_delta(1,1,1,0.02,0.25))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10870558490557591\n",
            "0.5812139374874482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovJwXo3-YEu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e3b51259-909b-4da7-91f8-cce0d26d8915"
      },
      "source": [
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "BS_call_prices = []\n",
        "for p in prices:\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    BS_call_prices.append(bs_call(p, 1, 1, 0.02, 0.25))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, BS_call_prices, label = \"BS_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(BS_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD4CAYAAAAUymoqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hVVdb48e9KISEQAgmEFkISihA6RECKoCjFAmIbkWZBYWzvjM4ojg7O6MxYZnznVQd1BAtiGwsqKgpKEaQHEJCakAIJkAakknr3749c/EUmldybc8v6PE8e7jlnn33XSchd2Wfvs7cYY1BKKaUawsfqAJRSSrkfTR5KKaUaTJOHUkqpBtPkoZRSqsE0eSillGowP6sDaApt27Y1UVFRVoehlFJuZceOHdnGmHbVHfOK5BEVFUV8fLzVYSillFsRkdSajultK6WUUg2myUMppVSDafJQSinVYF7R51GdsrIy0tLSKC4utjoUrxAYGEhERAT+/v5Wh6KUcgCvTR5paWkEBwcTFRWFiFgdjkczxpCTk0NaWhrR0dFWh6OUcgCvvW1VXFxMWFiYJo4mICKEhYVpK08pD+K1yQPQxNGE9HutlGfx2ttWSinXVVRazqp9GeSXlDMsOpQe4S31DxAXo8nDQr6+vvTr14+ysjL8/PyYNWsWv/3tb/Hx8SE+Pp63336bF198kZKSEq6++mqys7N59NFH6dSpE/PmzcPf35/NmzfTvHlzqy9FqUaz2QzbU07x8Y40Vuw9QWFpxc/HQls0Y1h0KMOiQxneLYye4cH4+GgysZImDws1b96cH3/8EYDMzExuvfVW8vLy+POf/0xcXBxxcXEA7Nq1C+DnsvPmzePRRx9lxowZ9XofYwzGGHx8vPoupXJRx8+c5cP4Y3yyM41jp87SopkvV/fvyA2DI+gQEsjW5FNsTTrFlqQcvv7pJABdQpvzyvQh9O0cYnH0XuzcB4snfw0ZMsScb//+/f+1r6m1aNHiF9tHjhwxoaGhxmazmbVr15qrr77aZGRkmG7duplWrVqZAQMGmFdffdW0adPGREVFmVtvvdUYY8xzzz1n4uLiTL9+/cyCBQuMMcYkJyebnj17mpkzZ5rY2FiTkpJSY7levXqZOXPmmNjYWHPllVeaoqIiY4wxCQkJZty4caZ///5m0KBBJjExscb3KygoMFdddZXp37+/6dOnj/nggw/+63pd4XuuXEtCRp7pu+AbEzX/S3Pros1m2c5jprCkrMbyR3MKzYfbj5pL/vad6fX412bFnuNNGK33AeJNDZ+r2vIA/vzFPvYfz3NonbGdWvHEtX0adE5MTAwVFRVkZmb+vC88PJzFixfzj3/8gy+//BKAzZs3c80113DjjTeyatUqEhIS2LZtG8YYJk+ezPr164mMjCQhIYElS5YwfPjwOsu9//77LFq0iJtvvplPPvmEGTNmMH36dObPn8/UqVMpLi7GZrPVWE9WVhadOnXiq6++AiA3N9dx30zlkXKLypizJJ4Afx8+v28MMe1a1nlOl9AguoQGMfaicOYujefX7+7koSt7ct/l3bVPpIlp8nBzq1atYtWqVQwaNAiAgoICEhISiIyMpGvXrgwfPrzOctHR0QwcOBCAIUOGkJKSQn5+Punp6UydOhWofMivtnpGjx7NQw89xCOPPMI111zD6NGjm/T7oNxLeYWN+97fSfqZs7x/1/B6JY6q2gUH8N5dw/nDsr08/+1hEjILeO7G/gT6+zopYnU+TR7Q4BaCsyQlJeHr60t4eDgHDhyo1znGGB599FHmzp37i/0pKSm0aNGiXuUCAgJ+3vb19eXs2bMNfj+AnTt3smLFCh5//HHGjRvHggUL6nUNyvs88/VBNiRk88z1/YiLCr2gOgL9fXn+5gF0b9+Sv688RGpOIa/NiqN9q0AHR6uqoz2oLiIrK4t58+Zx3333Naj5PWHCBN544w0KCgoASE9P/8Vtr4aWOyc4OJiIiAg+++wzAEpKSigqKqqxnuPHjxMUFMSMGTP4/e9/z86dO+t9Dcq7fLIjjcU/JDP7kq7cMjSyUXWJCPeM7c6/ZwwhIbOAKf/ayNGcIgdFqmqjLQ8LnT17loEDB/48VHfmzJk8+OCDDapj/PjxHDhwgEsuuQSAli1b8s477+Dr63tB5apaunQpc+fOZcGCBfj7+/PRRx/VWE9iYiK///3v8fHxwd/fn1deeaVB16G8w66jp3n0071cEhPG49fEOqze8X068PG8EUxbtIW7l8az7J4RBDXTjzdnksoOdc8WFxdnzl8M6sCBA/Tu3duiiLyTfs+9W0ZeMde+9AMB/j4sv3cUbVo0c/h7fH84i9vf3Makfh3517RB2oneSCKywxgTV90xvW2llHK6sgobc5fuoKCknEWz4pySOADG9GzHwxN78dWeE7z6fZJT3kNV0uShlHK6L3Yf58djZ3j6+n706tDKqe8199IYrunfkedWHmTdoZr79VTjeHXy8IZbdq5Cv9feyxjDa+uT6Nm+JZMHdHL6+4kIz93Yn4vaB/PA+7tIyS50+nt6I69NHoGBgeTk5OiHWhMw9vU8zj0rorzLhoRsDp7M567RMU3WBxHUzI9Fs+Lw8RHuXhpPYUl5k7yvN/Ha4QgRERGkpaWRlZVldShe4dxKgsr7LNqQRHhwAJMHOr/VUVWX0CD+NW0ws97Yyu8+2s3L0wdrB7oDeW3y8Pf311XtlHKy/cfz2JCQzcMTLyLAr+mf/h7Voy1/uKo3f/nqAG9sTOHOUfo77ygOuW0lIhNF5JCIJIrI/GqOB4jIf+zHt4pIVJVjj9r3HxKRCQ2o80URKXBE/Eop51i8IYmgZr5MH9rVshjuHBXNuF7h/H3lQe3/cKBGJw8R8QUWApOAWGCaiJz/9M+dwGljTHfgn8Cz9nNjgVuAPsBE4GUR8a2rThGJA9o0NnallPOcyD3L8t3H+dXFXQgJ8rcsDhHhr1P74e/rwyOf7MFm035OR3BEy2MokGiMSTLGlAIfAFPOKzMFWGJ//TEwTipvPk4BPjDGlBhjkoFEe3011mlPLH8HHnZA7EopJ3lrYwo2Y7hjpPW3ijqEBPLHq2PZmnyKd7cdtTocj+CI5NEZOFZlO82+r9oyxphyIBcIq+Xc2uq8D1hujDlRW1AicreIxItIvHaKK9W08ovLeG/rUa7q15EuoUFWhwPATXERjO7RlmdWHCDttM5/1VhuNVRXRDoBNwEv1VXWGPOaMSbOGBPXrl075wenlPrZf7YfI7+knLsvjbE6lJ+JCE9f3w+AR5ft1WH6jeSI5JEOdKmyHWHfV20ZEfEDQoCcWs6taf8goDuQKCIpQJCIJDrgGpSTlJbbSM0ppEDH2XuNsgobb/yQzPCYUPpHtLY6nF+IaBPE/Em92JCQzUfxaVaH49YcMVR3O9BDRKKp/IC/Bbj1vDLLgdnAZuBGYI0xxojIcuA9EflfoBPQA9gGSHV1GmP2AR3OVSoiBfZOeOWC9h/P44EPdpGYWTkoLqiZL+HBAYQHB9KuVQAxbVtw96UxBAda15mqHG/F3hMczy3mL1P7Wh1KtaYP68qXe07w1Ff7ubRnOzqE6MOrF6LRLQ97H8Z9wErgAPChMWafiDwpIpPtxV4HwuythAeB+fZz9wEfAvuBb4B7jTEVNdXZ2FhV0zDG8MYPyVy3cCN5Z8t4ckofHp3Ui2lDI+nbOQSkMrEsXJvIdQs3/pxclPs7NxVJ9/CWjO0ZbnU41fLxEZ69oT9lFTYe+1RvX10or52SXTlHVn4Jv/94N+sOZXFF7/Y8d2N/QmuYQXXTkWzuf28XxWUVPH/zACb27djE0SpH25SYza2Lt/LsDf341cWNW+jJ2RZvSOIvXx3ghVsGMmXg+WN8FOiU7KqJrDuUyaQX1rP5SA5PXdeXRbOG1Jg4AEZ0a8sX94+ie/tg5r2zk2e/OUiFjsF3a29sTCGsRTO3+DC+fWQ0AyJC+NuKAxSVap9cQ2nyUA7x4uoEbntzO21bBvDF/aOYObxrveYR6tS6OR/OHc60oZG8su4Is9/YxqnC0iaIWDna0ZwiVh/M4NZhkQT6N/1UJA3l6yP88ZpYMvJKWLwh2epw3I4mD9Voe9LO8M/vDjN5QCc+u3ckPdsHN+j8AD9fnr6+H8/e0I9tKae49qUfSNZpJNzO25tT8BVh+jDrpiJpqLioUCb26cCr3x8hM7/Y6nDciiYP1Sg2m2HB5/sIaxHAX6f2bdRfnL+6OJKP511CYWk5v35nB8VlFQ6MVDlTYUk5/4k/xqR+Hd1u9NIjk3pRWm7j/75LsDoUt6LJQzXKxzvS+PHYGf5wVS+HDLntH9Ga/715AAdP5vPXrw44IELVFJbtTCO/uJzbRkRZHUqDRbdtwYzhXflg21ESMvKtDsdtaPJQFyy3qIxnvjnIxVFtmDrIcR2kl/dqz5xR0SzdksrXe2udhUa5AJvN8NamFAZEhDA40rUeCqyvB8b1oEWAH09/fdDqUNyGJg91wf7320OcKSrlz5P7OnyRnYcn9mJARAgPf7KHY6d0HiJX9kNiNkeyCrltZJTbLrYU2qIZ913WnTUHM9mYmG11OG5Bk4e6IPuO57J0Syozh3cltlMrh9ffzM+Hl6YNBgP3vb+L0nKbw99DOcZbm1Jo2zKAq/q593M6s0dE0bl1c/624oBO214PmjxUgxljeOLzfbQJasaDV17ktPeJDAvimRv6s/vYGf6x6pDT3kdduOTsQtYczGT6sEhLVgp0pEB/Xx6eeBH7jufx2Y/nT8+nzqfJQzXYp7vSiU89zSMTezl9kZ+r+3dk+rBIXlufxNpDmU59L9Vwb29Owd9XmD7MtZ8mr69r+3eif0QIf195SEf71UGTh2qQ/OIy/rbiIAO7tObGIRFN8p5/vCaWXh2CeejD3ZzM1bH4rqKgpJyP4tO4ul9Hwlu51/Dcmvj4CH+4qjcncot5/Qd9cLA2mjxUg/zfdwnkFJbw5JQ++Pg0TedooL8v/7p1MGdLK/jj5z81yXuqun0cf4yCknJuc4GVAh1peEwYV/Ruz6vfHyH3bJnV4bgsTR6q3rILSliyKYVfxXVp8nUauoe35P5x3fl2fwY/JOhoGKvZbIYlm1MZ2KU1A7u45/Dc2vz2yh7kF5fz9qYUq0NxWZo8VL0t//E45TbDHaOs+UvzjpHRRIYG8eSX+yiv0NFXVvo+IYvk7EJuHxlldShO0adTCFf0Duf1jcm6kFkNNHmoelu2K42+nVs1eO4qRwn09+UPV/XmcEYB7287akkMqtKr644QHhzAJA+eRv/+y3twpqiMpZtTrQ7FJWnyUPWSkJHPT+l5TB3UNJ3kNZnQpz0juoXx/LeHOVOks+9aYfORHLYmn+LXY7vRzM9zP0IGdGnNmJ7tWLQhSadsr4bn/uSVQy3blY6vjzB5QCdL4xARFlwbS97ZMp3IziIvrD5Mu+AApg31jOG5tXlgXHdOFZby3lZt6Z5Pk4eqk81m+GxXOpf2aEu74ACrw6FXh1bcOiySpVtSdSK7Jrb5SA5bkk7x6zHd3GLNjsYa0jWUkd3DePX7JH3u4zyaPFSdtiTlcCK3mKmDrb1lVdWDV15Ei2a+PPnlfl2Dugmda3Xc6iEPBdbH/Zf3ILughA+0n+0XNHmoOi3blU5wgB/jY9tbHcrPQls04zdX9GRDQjarD+iT501hS1Jlq2Oel7Q6zhkeE8bQ6FBe/T6JknJtfZyjyUPV6mxpBV/vPcGkfh1c7gNj5iVd6dauBX/5ar9OnNgEXvgugXbBAR4zFUlDPHB5D07mFfNRfJrVobgMTR6qVqv2n6SwtMLyUVbV8ff14Y/XxJKSU8Rbm3QqCWfampTD5qQcr2t1nDOyexiDI1vzyroj+oeKnSYPVatlO9Pp3Lo5w6JDrQ6lWmMvCueyi9rxrzWJOpWEE72w2ntbHVA5yu/+cT1IP3OWT3dp6wMclDxEZKKIHBKRRBGZX83xABH5j/34VhGJqnLsUfv+QyIyoa46ReRd+/6fROQNEXHutK5eLDO/mA0JWVw3qFOTzWN1IX434SLyistZtD7J6lA80rbkU2w6ksPcS2O8stVxztie7egfEcLCtUd0hgMckDxExBdYCEwCYoFpIhJ7XrE7gdPGmO7AP4Fn7efGArcAfYCJwMsi4ltHne8CvYB+QHNgTmOvQVVv+Y/HsRlc8pZVVX06hXB1/468sTGZ7IISq8PxOC+sPkzblgFMH9bV6lAsJSLcd1l3jp4q4ss9ujyyI1oeQ4FEY0ySMaYU+ACYcl6ZKcAS++uPgXFSuV7lFOADY0yJMSYZSLTXV2OdxpgVxg7YBrj2J5sbW7YznQERIXQPb2l1KHV68MqeFJdV8Mq6I1aH4lG2JZ9iY2IO88bE0LyZ97Y6zrmid3t6tm/Jy+sSvX61QUckj87AsSrbafZ91ZYxxpQDuUBYLefWWaf9dtVM4JvqghKRu0UkXkTis7KyGnhJ6tDJfPafyGPqoPN/lK6pW7uW3DA4gqVbUjmRe9bqcDxChc3w7DcHtdVRhY+PcM/Y7hzOKOC7AxlWh2Mpd+4wfxlYb4zZUN1BY8xrxpg4Y0xcu3btmjg097dsVxp+PsK1Fk9H0hAPjOuBMYaX1iRaHYpHWLg2kR2pp3l0Ui9tdVRxTf+OdAltzsJ1R7z6AVVHJI90oEuV7Qj7vmrLiIgfEALk1HJurXWKyBNAO+BBB8SvzlNhM3y+6zhjL2pHWEvrpyOpry6hQUwbGsmH24+RmlNodThubXvKKf7vu8NcN7AT1w92j9ZnU/Hz9WHemG7sPnaGzUdyrA7HMo5IHtuBHiISLSLNqOwAX35emeXAbPvrG4E19j6L5cAt9tFY0UAPKvsxaqxTROYAE4Bpxhgd8uAE25JPcTKvmOvc5JZVVfdd1h0/X9FJExsht6iM/3l/F11Cg3jqur5Udk+qqm4YHEG74AAWrvPeVm6jk4e9D+M+YCVwAPjQGLNPRJ4Ukcn2Yq8DYSKSSGVrYb793H3Ah8B+Kvsu7jXGVNRUp72uV4H2wGYR+VFEFjT2GtQvrdx3kgA/Hy7vFW51KA0W3iqQ2ZdE8dmP6RzWSRMbzBjD/GV7yMwv4cVbBhEcqCPhqxPo78tdo6PZmJjDrqOnrQ7HEuIN9+zi4uJMfHy81WG4BWMMI55ZQ7/OIbw2K87qcC7I6cJSRj+3llHd2/LqzCFWh+NW3t2aymOf/sSjk3oxd0w3q8NxaQUl5Yx8Zg1Do0NZ5Ka/K3URkR3GmGovzp07zJUT7EnL5URuMRP6dLA6lAvWpkUz5oyO5pt9J9mblmt1OG7jcEY+T36xn9E92nLX6Birw3F5LQP8uG1EFN/uz+DQSe9r5WryUL/wzb6T+PoI43q73y2rqu4cFU2bIH/+seqQ1aG4heKyCu57byfBgX48f/MAl55RwJXcNiKKoGa+vPq99z1fpMlD/cLKfSe5JCaM1kHNrA6lUYID/Zk3phvfH85ia5L3joipD2MMT365n8MZBTx/80DCgwOtDslttGnRjFuHRrJ893GO5hRZHU6T0uShfpaYmU9SViET+rjOuh2NMXtEFO1bBfDcykNePR6/NsYY/rHqEO9tPcrcS2MY01OfiWqoOaNj8BXh3+u9q/WhyUP97JufTgJwZaz79ndUFejvy/+M68mO1NO6YFQ1jDE88/VBFq49wrShXXhkYi+rQ3JLHUICuWFIBB/Fp5GZV2x1OE1Gk4f62cp9GQyKbE2HEM+5bXFTXATRbVvw95WHqPDyuYiqOner6t/rk5h1SVf+el0/7edohHljYii32Xj9B+9ZV0aThwIg7XQRe9Nz3XqUVXX8fX14aHxPDmXk8/mP50984J1sNsOCz/fx5sYU7hgZzZ8n99HE0Uhdw1pwTf9OvLMllTNFpVaH0yQ0eSgAVu2rnOTN05IHwFV9O9KnUyv+99vDXr8KnM1meOyzvSzdksrcMTH88Zre+gS5g/x6bDcKSytYsinV6lCahCYPBVSOsrqofTDRbVtYHYrD+fgID0/sRdrps7y/7ajV4VimvMLGw5/s4f1tx7jvsu7Mn9hLE4cD9e7Yiit6h/PmpmQKS8qtDsfpNHkocgpK2J5yymNGWVXn0h5tGR4TyktrErziF/t8ecVl3LEkno93pPHbK3ryuwkXaeJwgnsu686ZojKv+CNFk4fiuwMZ2AxM6Ot5t6zOEalsfWQXlPLmRu/p1ARIyS5k6sKNbErM5pnr+/E/V/SwOiSPNTiyDcNjQlm0IYmS8gqrw3EqTR6Kb346SUSb5sR2bGV1KE41OLINV8a259/fJ3G60Ds6NTclZjNl4UZOFZbyzpxh3DI00uqQPN69l3UnI6+ET3d69gANTR5eLr+4jI2JOUzo08ErbmP8fsJFFJSW84oXTCfx7tZUZr2xjfDgAD6/dxTDY8KsDskrjOrelv4RIbzy/RHKKzx3gIYmDy+37lAWpRU2JnrwLauqerYPZuqgzizZlEL6Gc9crra8wsaflu/jsU9/YnSPtiy7ZwSRYUFWh+U1RIR7xnYjNaeIFfYHbz2RJg8v982+k7Rt2YzBkW2sDqXJPDT+IkTgz8v31V3YzZzrGH9rUwp3jY5m8eyLdU0OC4yP7UC3di14eW2ix06No8nDixWXVbDuYCZXxnbA14seEuvcujkPjOvBqv0ZrD6QYXU4DnPsVBE3vLzp547xx66O9aqfqyvx8RHuGdudgyfzWXvIM6fG0eThxTYmZlNYWuHRQ3RrMmdUDD3CW/LE8n2cLXX/UTE7Uk8z9eWNZOQV8/YdQ7Vj3AVMHtiJzq2bs3DtEY9sfWjy8GKf7kqndZA/I7q1tTqUJtfMz4enrutL2umzLFzr3utQL999nGmLttAiwI9l94xkRHfv+3m6In9fH+aOiWFH6mm2Jp+yOhyH0+ThpXKLyli1P4MpAzrRzM87/xsMjwnj+kGd+ff6IyRmFlgdToMZY3jhuwQeeH8XAyNa8+k9I+ke3tLqsFQVN8d1oX2rAJ5ecQCbh03M6Z2fGorlu9MpLbdxU1wXq0Ox1B+u7k1zf1/++NlPbnVrwWYzzP9kL//87jDXD+rM0jlDCW3h3gt4eaJAf1/mT+rF7rRcPt6ZZnU4DqXJw0t9vCONXh2C6dPJsx8MrEvblgE8PLEXm5NyWL77uNXh1IvNZvjDp3v5T3zlHFXP3zyAAD9fq8NSNbhuYGcGR7bmuW8OkldcZnU4DqPJwwsdzshnd1ouN8V18YoHA+sybWgkAyJCeOrLA+Sede1fbpvN8PjnP/HB9srE8dD4nvozdHEiwp8n9yWnsJSXVidYHY7DaPLwQh/FH8PPR7huYCerQ3EJvj7CX6f241RhCc+vOmR1ODUyxrBg+U+8t/Uovx7bTROHG+kXEcKv4rrw5sYUt+xfq45DkoeITBSRQyKSKCLzqzkeICL/sR/fKiJRVY49at9/SEQm1FWniETb60i016k3ehugrMLGp7vSubxXOGEtA6wOx2X07RzCrEuiWLollV1HT1sdzn8xxvCn5ft4Z0vlWuMP66y4bud3Ey6ieTNfnvxyv1v1r9Wk0clDRHyBhcAkIBaYJiKx5xW7EzhtjOkO/BN41n5uLHAL0AeYCLwsIr511Pks8E97Xaftdat6Wncoi+yCUq/vKK/Og+N70imkOfe/v8ulVoM7t2Tsks2p3DkqmvmTdB0Od9S2ZQC/uaIn6w9nsfqA+z846IiWx1Ag0RiTZIwpBT4AppxXZgqwxP76Y2CcVP7vnwJ8YIwpMcYkA4n2+qqt037O5fY6sNd5nQOuwWt8FH+Mti2bMfaidlaH4nJaBfqzcPpgMvKKeejD3S4xtNIYw1+/OsCbG1O4fWQUj1+tK/+5s1mXdKV7eEue+mq/20/Z7ojk0Rk4VmU7zb6v2jLGmHIgFwir5dya9ocBZ+x11PReAIjI3SISLyLxWVlZF3BZnienoIQ1BzOZOqgz/r7a3VWdgV1a88drYll9MNPymXfPtTgW/5DM7Eu6suCaWE0cbs7f14cnro0lNaeI139w73VlPPYTxBjzmjEmzhgT166d/pUN8NmPxym3GW4coresajNzeFeuHdCJ51cdYtORbEtiMMbw5y/2/9zi+NPkPpo4PMToHu0YH9uef61J5GRusdXhXDBHJI90oOqnUYR9X7VlRMQPCAFyajm3pv05QGt7HTW9l6qGMYaP4o/RPyKEizoEWx2OSxMRnrm+HzHtWvLA+7vIyGvaX3CbzfDHz3/irU0pzBkVrS0OD/T41bGU2wyPfbqXChe4PXohHJE8tgM97KOgmlHZAb78vDLLgdn21zcCa0zlcIPlwC320VjRQA9gW0112s9Za68De52fO+AaPN6+43kcPJnPTUMirA7FLbQI8OOV6YMpLKngvvd2UtZEi/rYbIbHPvupclTVmBge0z4OjxQZFsQfJvVi9cFMnvxin9NGX324/ZjT1q1pdPKw9z/cB6wEDgAfGmP2iciTIjLZXux1IExEEoEHgfn2c/cBHwL7gW+Ae40xFTXVaa/rEeBBe11h9rpVHT6KP0YzPx8mD6i2i0hVo0f7YJ65oR/bU07z95XOf/7j3JPj7287yj1juzF/oo6q8mS3jYxmzqholmxO5bX1SQ6v/8s9x3n4kz0sckLdAH51F6mbMWYFsOK8fQuqvC4Gbqrh3L8Cf61Pnfb9SVSOxlL1VFJewee7jzM+tj0hQbowUENMGdiZ+JTTvLY+ib6dQ5g8wDkPVlbYDPM/2cNHO9K4//LuPHilPgDoDf5wVW9O5BXz9NcH6RASyJSBjvnjbufR0zz44W7iurZh/qReDqnzfA5JHsq1rT6QyZmiMn224wI9fk1vDpzI438+2MWpghJuGxnt0PpLyiv47X9+ZMXek/zmih785oqeDq1fuS4fH+H5mwaQlV/C7z7aTbvggEYvkXDsVBF3vx1Ph1aBvDYrjkB/58x75rGjrdT/9/62o3RoFcgoXefhggT4+bL0zmFc0bs9f/piP09+sd9hnZyFJeXMWRLPir0nefzq3po4vFCgvy+LZsYRFdaCuW/v4ODJvAuuK6+4jDve2k5puY03brvYqTMta/LwcDyGpbMAABOXSURBVIcz8tmQkM3MS7rqkqSN0LyZL6/OGMLtI6N4Y2Myv35nR6NXIDxTVMqM17eyMTGbv9/YnzmjYxwUrXI3IUH+vHXHUIICfLntje2cyG14J3dZhY17391JcnYhr84c4vS1XTR5eLg3N6YQ4OfDNF2WtNF8fYQnru3Dgmti+fZABrcs2kJWfskF1ZWRV8yv/r2Ffel5vDJjiN5SVHRu3Zw3bxtKQUk5N/97Myv2nqj3KCxjDAs+38eGhGz+dn2/JlkdVJOHBztdWMqnu9KYOqizLhTkQHeMiubfM4Zw6GQeU1/eyKGT+Q06PzWnkBtf3UTa6SLeuv1iJvTp4KRIlbuJ7dSKt26/mAA/X+55dyfXLdzIpsTaH1Q1xrBoQ9LPo/RubqI/RMQTZnesS1xcnImPj7c6jCb38rpEnvvmECt/c6k+GOgEu4+d4c4l8WQXlDA8JpTrB0cwqW8HggP/e0SbzWbYm57LmoOZvLv1KBU2G2/dPpQBXVpbELlydRU2w7Kdafzz28Mczy1mdI+2PDyhF/0iQrDZDAmZBWxLOcX25FNsTznFidxiru7XkZemDcLHgbenRWSHMSau2mOaPDxTWYWN0c+upVt4C96dM9zqcDxWZl4x/9l+jGW70knOLiTQ34cJfTpw/eAIBkSEsDExhzUHM/n+cCbZBaWIQFzXNjx9fT+6h2tCV7UrLqvgnS2pLFybyOmiMgZHtiYpu5AzRZWLloUHB3BxdCjDY8K4aUiEw0dWafLwwuTxxe7j3P/+Ll6fHce43u2tDsfjGWPYdewMy3am8cXuE79YkbBVoB9jLgrn8l7tGNMzXG8hqgbLLy5j0YZk1hzMILZjKy6OCmVodCiRoUFOfR5Ik4cXJo+pL2/kdGEpax4a69BmrKpbSXkFaw9mcuhkASO6hzGoS2v8dBZj5YZqSx76kKAH2nX0NLuOnuFP18Zq4rBAgJ8vE/t2ZGJfqyNRynn0zyEP9ObGFIID/LhRh38qpZxEk4eHOZlbzIq9J7j54i60DNCGpVLKOTR5eJilW1KwGcNtI6KsDkUp5cE0eXiQ4rIK3tt6lCtj29MlNMjqcJRSHkyThwf5bFc6p4vKuN3Bs74qpdT5NHl4CJvN8PoPycR2bMWw6FCrw1FKeThNHh5i1f4MEjILmDsmRhcRUko5nSYPD2CMYeHaRLqGBXF1v45Wh6OU8gKaPDzA+oRs9qbn8usx3fRJZqVUk9BPGg+wcE0iHUMCuX5whNWhKKW8hCYPN7ct+RTbUk5x96UxNPPTH6dSqmnop42b+9faRNq2bMYtF+tKgUqppqPJw43tPnaG9YezuHNUDM2bOXYef6WUqk2jkoeIhIrItyKSYP+3TQ3lZtvLJIjI7Cr7h4jIXhFJFJEXxT7GtKZ6RWS6iOyxn7NJRAY0Jn53t3BtIq0C/ZgxXFsdSqmm1diWx3xgtTGmB7Davv0LIhIKPAEMA4YCT1RJMq8AdwE97F8T66g3GRhjjOkHPAW81sj43dahk/ms2p/BbSOjq132VCmlnKmxyWMKsMT+eglwXTVlJgDfGmNOGWNOA98CE0WkI9DKGLPFVK5I9XaV86ut1xizyV4HwBbAa4cXvbwukaBmvtyuEyAqpSzQ2OTR3hhzwv76JFDdeqedgWNVttPs+zrbX5+/v7713gl8XVNgInK3iMSLSHxWVladF+JOUrIL+WL3cWYM70obXdJUKWWBOhd8EJHvgA7VHHqs6oYxxoiIw9e0ra5eEbmMyuQxqpbzXsN+WysuLs6j1tp99fsj+Pn6MGe0ToColLJGncnDGHNFTcdEJENEOhpjTthvQ2VWUywdGFtlOwJYZ98fcd7+dPvrGusVkf7AYmCSMSanrvg9TfqZs3yyM41pQyMJDw60OhyllJdq7G2r5cC50VOzgc+rKbMSGC8ibewd5eOBlfbbUnkiMtw+ympWlfOrrVdEIoFlwExjzOFGxu6W/rUmEUGYN6ab1aEopbxYY5PHM8CVIpIAXGHfRkTiRGQxgDHmFJUjo7bbv5607wO4h8pWRCJwhP/fh1FtvcACIAx4WUR+FJH4RsbvVo6dKuKj+GP86uIudGrd3OpwlFJeTCoHOnm2uLg4Ex/v/nlm/id7WLYzne8fHkvHEE0eSinnEpEdxpi46o7pE+Zu4tipIj7ekca0oV00cSilLKfJw028tCYBHx/hnsu6Wx2KUkpp8nAHqTmFfLIznVuHRtK+lY6wUkpZT5OHG3hpTSJ+PsI9Y3WElVLKNWjycHHJ2YV8uiud6cO6Eq6tDqWUi9Dk4eJeWp2Av68wb2yM1aEopdTPNHm4sCNZBXz2Yzozh3fVp8mVUi5Fk4cLe2l1AgF+vszVp8mVUi5Gk4eLSswsYPnu48y6pCttWwZYHY5SSv2CJg8X9eLqBAL9fbn7Uu3rUEq5Hk0eLighI58v9hxn1iVRhGmrQynlgjR5uKAXVifQXFsdSikXpsnDxRzOyOervSeYPSKKUF0lUCnlojR5uJgXvksgyN+Xu0drq0Mp5bo0ebiQQycrWx23jYzStcmVUi5Nk4cLeWH1YVoG+HGXtjqUUi5Ok4eLOHAijxV7T3L7yChaB2mrQynl2jR5uIgXvksgOMCPOaO01aGUcn2aPFzAvuO5fLPvJLePiiYkyN/qcJRSqk6aPFzAC98lEBzox52joq0ORSml6kWTh8V+Ss9l1f4M7hwVTUhzbXUopdyDJg+LvbSmstVx+0htdSil3IcmDwsdzshn5b4MbhsRpa0OpZRb0eRhoZfXJhLUzFdbHUopt9Oo5CEioSLyrYgk2P9tU0O52fYyCSIyu8r+ISKyV0QSReRFEZH61CsiF4tIuYjc2Jj4rZSaU8jy3ceZPixS57BSSrmdxrY85gOrjTE9gNX27V8QkVDgCWAYMBR4okoyeAW4C+hh/5pYV70i4gs8C6xqZOyWemXdEfx8ffRpcqWUW2ps8pgCLLG/XgJcV02ZCcC3xphTxpjTwLfARBHpCLQyxmwxxhjg7Srn11bv/cAnQGYjY7fM8TNn+WRnGjfHRRDeStcmV0q5n8Ymj/bGmBP21yeB9tWU6Qwcq7KdZt/X2f76/P011isinYGpVLZYaiUid4tIvIjEZ2Vl1fNymsZr65OwGZh7qa5NrpRyT351FRCR74AO1Rx6rOqGMcaIiHFUYDXU+3/AI8YYm717pLbzXgNeA4iLi3N4XBcqu6CED7Yf5bqBnekSGmR1OEopdUHqTB7GmCtqOiYiGSLS0Rhzwn4bqrpbSenA2CrbEcA6+/6I8/an21/XVG8c8IE9cbQFrhKRcmPMZ3Vdh6t4/YdkSspt3HOZtjqUUu6rsbetlgPnRk/NBj6vpsxKYLyItLF3lI8HVtpvS+WJyHD7KKtZVc6vtl5jTLQxJsoYEwV8DNzjTokjt6iMpZtTuapfR7q1a2l1OEopdcEamzyeAa4UkQTgCvs2IhInIosBjDGngKeA7favJ+37AO4BFgOJwBHg69rqdXdvbUqhoKSce8d2tzoUpZRqFKkc6OTZ4uLiTHx8vKUxFJaUM/LZNQyJbMPrt11saSxKKVUfIrLDGBNX3TF9wryJvLs1lTNFZdx7ubY6lFLuT5NHEygtt/H6D8lcEhPG4MhqH8JXSim3osmjCXy55zgZeSXcfak+Ta6U8gyaPJzMGMOiDcl0D2/JmJ7trA5HKaUcQpOHk20+ksOBE3nMGRWNj0/tDzYqpZS70OThZIs2JNG2ZTOuG9S57sJKKeUmNHk4UWJmPmsPZTFzeBSB/r5Wh6OUUg6jycOJFm9IJsDPhxnDI60ORSmlHEqTh5NkF5SwbFc6NwyJIKxlgNXhKKWUQ2nycJKlm1MpLbdxhy4xq5TyQJo8nKC4rIKlW1IZ1yuc7uE6AaJSyvNo8nCCZTvTOVVYyhxdYlYp5aE0eTiYzWZY/EMSfTu3YnhMqNXhKKWUU2jycLB1hzNJyirkrtEx1LXaoVJKuStNHg62aH0yHUMCuapfR6tDUUopp9Hk4UAHTuSxOSmH2SOi8PfVb61SynPpJ5wDvb05hUB/H265uIvVoSillFNp8nCQM0WlfLornesGdqZ1UDOrw1FKKafS5OEgH8Yfo7jMxuwRUVaHopRSTqfJwwEqbIa3N6cyNDqU3h1bWR2OUko5nSYPB1h9IIO002e5XVsdSikvocnDAZZsTqFTSCBXxra3OhSllGoSmjwaKSEjn42JOUwf3hU/HZ6rlPISjfq0E5FQEflWRBLs/7apodxse5kEEZldZf8QEdkrIoki8qLYH8murV4RGSsiP4rIPhH5vjHxO8KSzSk08/Nh2lBds0Mp5T0a+6fyfGC1MaYHsNq+/QsiEgo8AQwDhgJPVEkGrwB3AT3sXxNrq1dEWgMvA5ONMX2AmxoZf6Pkni1j2c50Jg/oRGgLHZ6rlPIejU0eU4Al9tdLgOuqKTMB+NYYc8oYcxr4FpgoIh2BVsaYLcYYA7xd5fya6r0VWGaMOQpgjMlsZPyN8lH8MYpKK7hNO8qVUl6mscmjvTHmhP31SaC6HuPOwLEq22n2fZ3tr8/fX1u9PYE2IrJORHaIyKyaAhORu0UkXkTis7KyGnRR9WGzGZZuSSWuaxv6dg5xeP1KKeXK/OoqICLfAR2qOfRY1Q1jjBER46jAaqjXDxgCjAOaA5tFZIsx5nA1570GvAYQFxfn8LjWHc4kNaeI342/yNFVK6WUy6szeRhjrqjpmIhkiEhHY8wJ+22o6m4jpQNjq2xHAOvs+yPO259uf11TvWlAjjGmECgUkfXAAOC/koezvbUplfatApjYt7q8qpRSnq2xt62WA+dGT80GPq+mzEpgvIi0sXeUjwdW2m9L5YnIcPsoq1lVzq+p3s+BUSLiJyJBVHbCH2jkNTRYcnYh6w9nMX1YV509VynllRr7yfcMcKWIJABX2LcRkTgRWQxgjDkFPAVst389ad8HcA+wGEgEjgBf11avMeYA8A2wB9gGLDbG/NTIa2iw97am4ucj3DJUZ89VSnknqRzo5Nni4uJMfHy8Q+oqLqtg+NOrGdEtjJenD3FInUop5YpEZIcxJq66Y3rPpYG+/ukEZ4rKmD6sq9WhKKWUZTR5NNB7W48S3bYFl8SEWR2KUkpZRpNHAxw6mc/2lNPcOjQSHx+xOhyllLKMJo8GeG9rKs38fLhhSETdhZVSyoNp8qinotJylu1M56q+HXQeK6WU19PkUU9f7D5Ofkk504drR7lSSmnyqKd3tx6lZ/uWxHWtdtZ5pZTyKpo86mFvWi570nKZPqwr9iVHlFLKq2nyqIf3tqXS3N+XqYM7111YKaW8gCaPOuQVl/H5j8e5dkBHWgX6Wx2OUkq5BE0edfh8VzpFpRX6RLlSSlWhyaMWxhje3XqUvp1b0T9CF3xSSqlzNHnUYufR0xw8ma8d5UopdR5NHnW4tGc7Jg/oZHUYSinlUupcSdCbDekaytt3DLU6DKWUcjna8lBKKdVgmjyUUko1mCYPpZRSDabJQymlVINp8lBKKdVgmjyUUko1mCYPpZRSDabJQymlVIOJMcbqGJxORLKAVKvjuABtgWyrg7CIt167Xrd3cfXr7mqMaVfdAa9IHu5KROKNMXFWx2EFb712vW7v4s7XrbetlFJKNZgmD6WUUg2mycO1vWZ1ABby1mvX6/Yubnvd2uehlFKqwbTloZRSqsE0eSillGowTR4uQEQmisghEUkUkfnVHI8UkbUisktE9ojIVVbE6Wj1uO6uIrLafs3rRCTCijgdTUTeEJFMEfmphuMiIi/avy97RGRwU8foDPW47l4isllESkTkd00dn7PU47qn23/Oe0Vkk4gMaOoYL4QmD4uJiC+wEJgExALTRCT2vGKPAx8aYwYBtwAvN22UjlfP6/4H8LYxpj/wJPB000bpNG8BE2s5PgnoYf+6G3ilCWJqCm9R+3WfAh6g8ufuSd6i9utOBsYYY/oBT+EmneiaPKw3FEg0xiQZY0qBD4Ap55UxQCv76xDgeBPG5yz1ue5YYI399dpqjrslY8x6Kj8oazKFyqRpjDFbgNYi0rFponOeuq7bGJNpjNkOlDVdVM5Xj+veZIw5bd/cArhFC1uTh/U6A8eqbKfZ91X1J2CGiKQBK4D7myY0p6rPde8Grre/ngoEi0hYE8Rmtfp8b5RnuhP42uog6kOTh3uYBrxljIkArgKWiog3/Ox+B4wRkV3AGCAdqLA2JKWcQ0QuozJ5PGJ1LPXhZ3UAinSgS5XtCPu+qu7Efs/UGLNZRAKpnFAts0kidI46r9sYcxx7y0NEWgI3GGPONFmE1qnP/wnlQUSkP7AYmGSMybE6nvrwhr9eXd12oIeIRItIMyo7xJefV+YoMA5ARHoDgUBWk0bpeHVet4i0rdLCehR4o4ljtMpyYJZ91NVwINcYc8LqoJRziEgksAyYaYw5bHU89aUtD4sZY8pF5D5gJeALvGGM2SciTwLxxpjlwEPAIhH5LZWd57cZN58aoJ7XPRZ4WkQMsB6417KAHUhE3qfy2tra+7GeAPwBjDGvUtmvdRWQCBQBt1sTqWPVdd0i0gGIp3JwiE1EfgPEGmPyLArZIerx814AhAEviwhAuTvMtKvTkyillGowvW2llFKqwTR5KKWUajBNHkoppRpMk4dSSqkG0+ShlFKqwTR5KKWUajBNHkoppRrs/wE4SZz3N7d7MwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "593d36eb-8f07-4bab-c9ee-8a38f1e52302"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1, 1, 0.02, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd1yV9f//8cebJaggCm5kOHOPcO+9R5qfNLf1saFm41Np5sisj5WfysqRuc2tpbjSzL0ScIOLUAMnYoCorHPevz8gf+QX9agHrsPhdb/dvN24zvX2nOcF+vTyGu9Laa0RQgiR+zkYHUAIIYR1SKELIYSdkEIXQgg7IYUuhBB2QgpdCCHshJNRH+zt7a39/f2N+nghhMiVQkNDb2iti2a1zrBC9/f3JyQkxKiPF0KIXEkpdfFB6+SQixBC2AkpdCGEsBNS6EIIYScMO4aeldTUVKKjo0lKSjI6isgBrq6u+Pj44OzsbHQUIeyCTRV6dHQ07u7u+Pv7o5QyOo7IRlprYmNjiY6OJiAgwOg4QtgFmzrkkpSUhJeXl5R5HqCUwsvLS/43JoQV2VShA1LmeYj8rIWwLpsrdCGEsFdxd1L47JfTXIy9nS3vb1PH0IUQwh7dTk5j/r7zfL87kltJaZTydGOAVwGrf47soWcjf39/bty48dRjLLVgwQJGjBgBwMSJE5k6dapFv+/ChQtUq1bN4jFHjx5l06ZNTxdWiDwgKdXE3L3nafb5DqZtDefl4uc4Vn01A0pdzZbPkz108diOHj1KSEgInTp1MjqKEDYp1WRmTWg03207jV/iYaZ6HqGp4wGcrsVDvCfEdQTf+lb/XJst9I/WhxF+OcGq71mllAcTulZ96JgLFy7QoUMHGjRowP79+6lbty5DhgxhwoQJXL9+nSVLllC+fHmGDh1KZGQk+fPnZ/bs2dSoUYPY2Fj69u3LpUuXaNiwIZkf7/fjjz/yzTffkJKSQv369ZkxYwaOjo6PzLxo0SKmTp2KUooaNWqwePFi1q9fz+TJk0lJScHLy4slS5ZQvHjxx/pehIaGMnToUADatWt373WTycTo0aPZuXMnycnJDB8+nFdeeeXe+pSUFMaPH8/du3fZu3cvY8aMISAggFGjRpGUlISbmxvz58+nUqVKhIWFMWTIEFJSUjCbzaxZs4YKFSo8Vk4hcpM0k5l1Ry7x27YN1E/8jQ3OwXi6xEFaQXimM1TrBWVbgpNLtny+zRa6kSIiIli1ahXz5s2jbt26LF26lL179xIUFMSnn35KmTJlqF27NmvXrmX79u0MHDiQo0eP8tFHH9GkSRPGjx/Pxo0bmTt3LgCnTp1ixYoV7Nu3D2dnZ15//XWWLFnCwIEDH5ojLCyMyZMns3//fry9vbl58yYATZo04eDBgyilmDNnDp9//jn/+9//HmsbhwwZwnfffUezZs149913770+d+5cChUqRHBwMMnJyTRu3Jh27drduyLFxcWFSZMmERISwnfffQdAQkICe/bswcnJiW3btvHBBx+wZs0aZs2axahRo+jXrx8pKSmYTKbHyihEbmEya3bu28vl3YtolryTXg7XMeXLh0OlDuklXqEdOLtlew6bLfRH7Ulnp4CAAKpXrw5A1apVad26NUopqlevzoULF7h48SJr1qwBoFWrVsTGxpKQkMDu3bv56aefAOjcuTOFCxcG4LfffiM0NJS6desCcPfuXYoVK/bIHNu3b6d37954e3sDUKRIESD9BqwXXniBK1eukJKS8tg35sTFxREXF0ezZs0AGDBgAJs3bwZg69atHD9+nNWrVwMQHx/PuXPnqFix4gPfLz4+nkGDBnHu3DmUUqSmpgLQsGFDPvnkE6Kjo+nZs6fsnQu7Y064xqlt83E6uYrW5ghMOPBXiQbohhNxrNwVXD1yNI/NFrqR8uXLd+9rBweHe8sODg6kpaU99q3qWmsGDRrEf//7X6vkGzlyJG+//TbdunVj586dTJw40SrvC+lZv/32W9q3b/+P1y9cuPDA3zNu3DhatmzJzz//zIULF2jRogUAL774IvXr12fjxo106tSJ77//nlatWlktqxCGSEvGfHozMXvn4311N1Uxc9ahHOHV3+eZNoPxLlTKsGhylcsTaNq0KUuWLAFg586deHt74+HhQbNmzVi6dCkAmzdv5q+//gKgdevWrF69muvXrwNw8+ZNLl584JTG97Rq1YpVq1YRGxt77/dB+h5x6dKlAVi4cOFj5/f09MTT05O9e/cC3NsWgPbt2zNz5sx7e9lnz57l9u1/XjPr7u7OrVu37i1nzrNgwYJ7r0dGRlK2bFneeOMNunfvzvHjxx87qxA2QWu4FIp5wzukfFYeh9WD0FeOsdKlB9tbBVHuw1Cq9PoABwPLHGQP/YlMnDiRoUOHUqNGDfLnz3+vVCdMmEDfvn2pWrUqjRo1wtfXF4AqVaowefJk2rVrh9lsxtnZmenTp+Pn5/fQz6latSpjx46lefPmODo6Urt2bRYsWMDEiRPp3bs3hQsXplWrVpw/f/6xt2H+/PkMHToUpdQ/Toq+/PLLXLhwgTp16qC1pmjRoqxdu/Yfv7dly5ZMmTKFWrVqMWbMGN577z0GDRrE5MmT6dy5871xK1euZPHixTg7O1OiRAk++OCDx84phKFux8Lx5ejDi1Axp0nFhV9MgRx0b0ejds/zrxo+ODrYzh3PKvOVGDkpMDBQ3//EolOnTlG5cmVD8ghjyM9c2ByzCSJ3wOFF6NObUOZUwhwqsTi5KWe82vBym1p0rFYCB4OKXCkVqrUOzGqd7KELIQRAfDQcXgxHfoSEaJKcPVmrOjA3uSkuJaswslV5Pq1iXJFbQgrdBsTGxtK6dev/8/pvv/2Gl5fXU7338OHD2bdv3z9eGzVqFEOGDHmq9xXCLphNELENQubDuS1orblUpAHTHV9gza0aVPMtyge9K9CiUtFcMZmcFLoN8PLy4ujRo9ny3tOnT8+W9xUiV0u4AkcWw+FFEB+FLlCMo75DGB9VhxOXC9OonBcLWpWnYdncNZ23FLoQIm/QGi7ug0Oz4dQG0CZM/s3Z5fcGY8PLcOWMmaYVvFnTpgLP+hUxOu0TkUIXQti35EQ4vgKC58D1cHAthKneq2xw6cCUQ6lcOZ1E/QBPpvWrRL2A3Fnkf7Oo0JVSHYBpgCMwR2s95b71vsBCwDNjzGittUzHJ4QwTuwfcOgHOLoEkhOgRHXudPiaFUn1mHPwGpfiblHH15OpvWvSqFzuOrTyII8sdKWUIzAdaAtEA8FKqSCtdXimYR8CK7XWM5VSVYBNgH825BVCiAfTGiJ3wu+z4OwWcHCCqj2IrtCP2ZHerN50iTspF6jrX5jJz1WjRcXccbLTUpbcKVoPiNBaR2qtU4DlQPf7xmjg70kLCgGXrRcxZzk6OlKrVi1q1qxJnTp12L9/PwB37tyhX79+VK9enWrVqtGkSRMSExOt8pkyj7kQTyn1LoQugBkNYXEPuBSKbvYu+7rtYlD8MJosvcPy4Gg6VivJhpFNWPVqI1pWKmZXZQ6WHXIpDURlWo4G7p/IdyKwVSk1EigAtMnqjZRSw4BhwL27KG2Nm5vbvStOtmzZwpgxY9i1axfTpk2jePHinDhxAoAzZ8489pwuRpN5zIXduXUt/SRnyDy4exNKVCely3esTq7P3IOX+SPmAkXd8/F224r0redLUfd8j37PXMxaJ0X7Agu01v9TSjUEFiulqmmtzZkHaa1nA7Mh/U7Rh77j5tFw9YSV4mUoUR06Tnn0uAwJCQn3Zky8cuXKP27Vr1Sp0kN/r8xjLkQ2un4aDnyXfrLTlAqVOhFb42Xm/lmSpZuiiLtzjuqlC/H1C7XoVL0kLk55Y9oqSwr9ElAm07JPxmuZvQR0ANBaH1BKuQLewHVrhMxJd+/epVatWiQlJXHlyhW2b98OwNChQ2nXrh2rV6+mdevWDBo06IElJ/OYC5ENtIYLe2D/t3BuKzi5Qu0BnA4YwKwTsGHJFcw6knZVSvBS0wAC/Qrb3SGVR7Gk0IOBCkqpANKLvA/w4n1j/gRaAwuUUpUBVyDmqZI9xp60NWU+5HLgwAEGDhzIyZMnqVWrFpGRkWzdupVt27ZRt25dDhw4kOU8JDKPuRBWZDbBqfWw9yu4chTye5PW/AO2unVmdmg8R/depmA+JwY29GdIY3/KFMlvdGLDPLLQtdZpSqkRwBbSL0mcp7UOU0pNAkK01kHAO8APSqm3SD9BOlgbNeuXFTVs2JAbN24QExNDsWLFKFiwID179qRnz544ODiwadOmx5pYSuYxF+IxpCXDseWwbxrc/AOKlCOhzRfMv92QxfuucSPxImW9CzCxaxV6PeuDu2vuOqeVHSw6sKS13qS1rqi1Lqe1/iTjtfEZZY7WOlxr3VhrXVNrXUtrvTU7Q+eU06dPYzKZ8PLyYt++fffmN09JSSE8PPyB09/KPOZCPIWkBNj3DXxdA9a/gc7nTkSLGbzhPZs6m3z4asefVC/twYIhddn2dnMGNw6QMs8gd4re5+9j6JC+17tw4UIcHR35448/eO2119BaYzab6dy5M7169cryPWQecyGewJ2b8Pv38PtMSIrH5N+MnZUnMfVcSU79cgt311gGNPRjUEN//L0LGJ3WJsl86MJQ8jMXJMakX7ESPAdSErldtiPLXJ5n2ml3biWl8UwJdwY29KdH7VLkd5F9UJkPXQhhexIup1+xEjIfnZbEdd9OfJPanSXhBXF2VHSoVoyBDf3y5NUqT0oK/SnIPOZCPIGEy+lXrIQuQJtN/FGyMx/HtWfX2cIUdc/HW2386Fu/DMXcXY1OmuvYXKFrrXPNv8Yyj/nTsYMLocTjSLjy/4tcmzhcpDNjY9pyOrIItcp4Mq2dPx2r5Z2bgLKDTRW6q6srsbGxeHnZx8xn4sG01sTGxuLqKnthdu/W1fQiD5mPNqex3709Y2LacflycbrUKMl/G/lT27ew0Sntgk0Vuo+PD9HR0cTEPN09SSJ3cHV1xcfHx+gYIrvcvgF7v0IHz0GbUtnp2poJcR2J06Xp18yPwY38KVFI/kG3JpsqdGdn58e+e1IIYWPuxsGB79AHZ6BT7vKrUws+SeqKKZ8/Qzr506eeLwXz2VT12A35rgohrCM5EQ59j3nvNByS49lKAz5P7oWbV2Xe6VKWTtVL4uwox8ezkxS6EOLppCWjQ+aRuvMLXJJi2WGqzdfm3vhVbcjnjf2p4yuXHeYUKXQhxJMxm7l7ZDlp2z7G/e5lQk1V+N75bao1bMvsBr6ULORmdMI8RwpdCPF4tCY6ZD2Ov31EyaQIwsx+rPScRLWmzzGrVmlcnR2NTphnSaELISxiMmt+37OVQvsmUzXlOFG6GD/6jKda+yF85FvE6HgCKXQhxCPcSUlj8+6DFDowhTamPfyFB3sqvEfVrqPo71HQ6HgiEyl0IUSWYm4ls2LPcdwPTaOP3gzKkYjKrxPQbQxN3Twe/QYix0mhCyHu0VoTevEv1hyKpMCJhQx3+IlC6jY3K/bGu+tHlPcoZXRE8RBS6EIIom7e4afDl/jpcBSV4nYz1nkZfo5XuVOmGQ6dP8W7RHWjIwoLSKELkUclpZoIOnqZ1YejOXT+Js+oP5nhvoyqLscweVeC9tPJX6GN0THFY5BCFyKPuZtiYsnvF/l+dyQxt5KpUySVjQHrqHJ1LcrREzpNxfHZIeAo9ZDbyE9MiDwiMTmNxQcuMmdPJLG3U2hW1p1V1Q/hFzYTde0O1H8Nmr8LbjLzYW5lUaErpToA0wBHYI7Wesp9678CWmYs5geKaa09rRlUCPFkEpPTmL/3PHP3nSfuTirNKhZlXMUoKhweB4f/gIodoN0n4F3e6KjiKT2y0JVSjsB0oC0QDQQrpYK01uF/j9Fav5Vp/EigdjZkFUI8Bq01m05cZdKGMK4lJNP6mWK8U9eZKkc/hW1bwKsC9FsDcpzcbliyh14PiNBaRwIopZYD3YHwB4zvC0ywTjwhxJO4GHub8evC2HU2hqqlPPj+hWeodX4O/DQdHF2g7cdQ/1VwcjE6qrAiSwq9NBCVaTkaqJ/VQKWUHxAAbH/A+mHAMABfX9/HCiqEeLTkNBPf74pk+o4InB0dmNClMgPdQ3Fc+2+4dQVq9oU2E8G9hNFRRTaw9knRPsBqrbUpq5Va69nAbIDAwEB5oKQQVvR7ZCxjfjpB5I3bdK5Rko8aOeO9axSc3w0la0LvheCb5b6YsBOWFPoloEymZZ+M17LSBxj+tKGEEJYzmTXTfjvHt9vPUaZwfhYPrEbTy/Nh0Xfgkh86TYXAoeAgsyDaO0sKPRiooJQKIL3I+wAv3j9IKfUMUBg4YNWEQogHuhqfxKjlR/j9/E161S7NJ5Uv4LqlM8RHQc0Xoe0kKFjU6Jgihzyy0LXWaUqpEcAW0i9bnKe1DlNKTQJCtNZBGUP7AMu11nIoRYgcsOP0dd5ZdYykVBOzOnvR4c9P4KetUKwqDPkF/BoaHVHkMGVU/wYGBuqQkBBDPluI3CwlzcwXW07zw57zVC3uxqLKh/AK+RqUA7T8IP3qFbnL024ppUK11oFZrZOfuhC5yJ+xdxi5/AjHouIYWy2el+In4XDwNDzTBTp+BoV8jI4oDCSFLkQuse7oJcb+fBJPlcjuZzbjG7EKPHygzzJ4ppPR8YQNkEIXwsbdTk5jYlAYq0KjGFX8OG+kzMXx4l/QcAS0GAP55KlBIp0UuhA2LOxyPCOXHSH5xkW2l1hB2bh9UKo2dP0ZStYwOp6wMVLoQtggrTUL9l/gs03h/Nt1G28WWI7jbaD9p+knPeWacpEFKXQhbMz1hCTeXX2cq+dC2eixgHLJp6Fca+jyFRT2MzqesGFS6ELYkF9OXmHcmsMMNq3hNdd1KMdC0PMHqN4blDI6nrBxUuhC2IDE5DQmrQ/jdOguVuWfg7++CNX+BR2mQAEvo+OJXEIKXQiDhV68yfvLD/GvW4uYkm8zKn8J6LoSKrY3OprIZaTQhTBIcpqJadvOEbp7I/Pz/UAZpytQZxC0+xhcCxkdT+RCUuhCGODkpXg+WPE7z92cwwqXLZg9/KB7EJRtbnQ0kYtJoQuRg1LSzEzfEcGhneuZ7jybMk5Xof6rOLQeDy4FjI4ncjkpdCFyyKkrCXyw4ne63pjDEuctaE8/6LER/JsYHU3YCSl0IbJZmsnMrF1/sHf7eqY5fY+v01WoNyz9UXCyVy6sSApdiGx07totRq8MpsO1H1jqtBldqAz0WA8BzYyOJuyQFLoQ2cBk1szZE8kvv/7CVKcZlHOKTn8MXNuPZTItkW2k0IWwssiYRN5fGUqjywtZ7bw2/RFwPdZA+TZGRxN2TgpdCCsxm9Mn1Fq95Tc+d5xBNec/0NX/her0ObgVNjqeyAOk0IWwgouxt3lv5VGqRi9jrcsKnPIVhK4LUVV7GB1N5CEWFbpSqgMwjfSHRM/RWk/JYsy/gImABo5prV+0Yk4hbJLZrFl88CLzN+9jisNMGjifQFdoj+r6LbgXNzqeyGMeWehKKUdgOtAWiAaClVJBWuvwTGMqAGOAxlrrv5RSxbIrsBC2IurmHd5dfYyiFzawMd8C3JzM0GEaqs4gmRlRGMKSPfR6QITWOhJAKbUc6A6EZxrzb2C61vovAK31dWsHFcJWaK1ZeuhPvtkYwjg1jy4ue9GlAlE9Z4NXOaPjiTzMkkIvDURlWo4G6t83piKAUmof6YdlJmqtf7FKQiFsSNydFN5bfZxbp7ez0W02Xuab0PwDVNN3wFFOSQljWetPoBNQAWgB+AC7lVLVtdZxmQcppYYBwwB8fX2t9NFC5IxD52/yzrJDDLj7I/922QCFyqJ6rgCfZ42OJgRgWaFfAspkWvbJeC2zaOB3rXUqcF4pdZb0gg/OPEhrPRuYDRAYGKifNLQQOclk1ny7/Rzrt+9ijusMKjlGwrOD05/vKbfuCxtiSaEHAxWUUgGkF3kf4P4rWNYCfYH5Silv0g/BRFozqBBGuBJ/l1HLjlA2ag2b8v2Ii4sbdP8RKnc1OpoQ/8cjC11rnaaUGgFsIf34+DytdZhSahIQorUOyljXTikVDpiAd7XWsdkZXIjstvPMdSYs38OH5pm0dQ4G/+bw3PfgUdLoaEJkSWltzJGPwMBAHRISYshnC/EwZrNm2m/nCN6xlm/yzcRL3UK1mQANhoODg9HxRB6nlArVWgdmtU5OywuRyV+3U3hneQh1zs/iR5cgKFwe1XstlKxpdDQhHkkKXYgMx6PjmLRoM2OT/kdtp3Po2gNQHT+TE58i15BCF3me1pplh6IIXv8DC5zm4ObqBN3mo6r1NDqaEI9FCl3kaWkmM5+uDeWZIx/zldMu0krVxbH3XCjsZ3Q0IR6bFLrIs24np/HZwjUMip5AWaermJv+B6cWY+SOT5FryZ9ckSddj7/LqtmTGJv4AyY3T1SfIJQ8Fk7kclLoIs+J+DOaqAUvM9x8gNhSzfDqNy/9qUJC5HJS6CJPOXZwG16bX6WpusnVeh9QosO7cm25sBtS6CJvMJs5tmoyVcK/JtbBi5v/CqJE5SZGpxLCqqTQhd0zJ8Zyfs4Aasbt41D+xjzzykI8POUQi7A/UujCriWfP8DtJQMpkxrLulJv0uml8Tg7ORodS4hsIYUu7JPZzO2dX5Fv9yckmouwo+5CenbpgpJHwwk7JoUu7M+dm9xe8TIFLv7GL+Z6OHb/jl7PVjI6lRDZTgpd2JeoYJKXDcD5TgyfO7xEu6HjqOVb2OhUQuQIKXRhH7SGgzMxbx3HdXNhPiv4OaNf7otP4fxGJxMix0ihi9wvKR7z2uE4nF7PNtOzrPMby5T+zXB3dTY6mRA5Sgpd5G5XjmFeMQgdd5HJqf0w1x/ON12q4OggJz9F3iOFLnInreHwQvSm94g1F2R46nh6dOvFi/V9jU4mhGGk0EXuk3IHNr4Nx5ZxkBqM4Q0+HdKSRuW9jU4mhKGk0EXuciMCVg5EXw/nW1Mvgjz6MX9IAwK85alCQlg0K5FSqoNS6oxSKkIpNTqL9YOVUjFKqaMZv162flSR54WvQ89uwZ2b0QxKeY/f/YaxZngzKXMhMjxyD10p5QhMB9oC0UCwUipIax1+39AVWusR2ZBR5HWmVPh1AhycTmS+ygxIfI1WDeowoWtVnB1lpkQh/mbJIZd6QITWOhJAKbUc6A7cX+hCWF/CFVg1GKIOss6lC+/feoEPutdgYEN/o5MJYXMsKfTSQFSm5WigfhbjeimlmgFngbe01lH3D1BKDQOGAfj6ytUI4hEu7IVVQzAlJzLO4U3WpzRi9uA6NKsoMyUKkRVr/X91PeCvta4B/AoszGqQ1nq21jpQax1YtKj8pRQPoDXs+wYWduOWKkCXu5PY59aCn19vLGUuxENYsod+CSiTadkn47V7tNaxmRbnAJ8/fTSRJyUlwLrX4dR6ThdpxfOXX6RaWR+W9nuWwgVcjE4nhE2zpNCDgQpKqQDSi7wP8GLmAUqpklrrKxmL3YBTVk0p8obrp2BFf/TN86z2epV3LzWlbz1fPupWDRcnOfkpxKM8stC11mlKqRHAFsARmKe1DlNKTQJCtNZBwBtKqW5AGnATGJyNmYU9OrEagkZici7AmAKfsPqyL+O7VGFIY3+Zw1wICymttSEfHBgYqENCQgz5bGFDTKnw63g4OIPEYoE8H/sK0WmF+PbF2rSsVMzodELYHKVUqNY6MKt1cqeoMM6ta7B6CFzcxx9l+9P1bAe8PArw07C6VCzubnQ6IXIdKXRhjD9/h1WD0HfjCCo7kVHhFakXUIRZ/Z+liJz8FOKJSKGLnKU1HPoBtozB5FGGcV5fsTTcgwEN/BjftYrc+SnEU5BCFzkn5Q5seAuOLyfRrw19YgZzJsaR//asRt96cqOZEE9LCl3kjL8uwIr+cPUkEVVH0uNEI1xdXFj27zoE+hcxOp0QdkEKXWS/iG2w+iU0mnVVvuLN0GLU9PFg1oBnKVnIzeh0QtgNKXSRfcxm2PslbJ+MqWhlRju/z6rDzvSsU5pPn6uOq7Oj0QmFsCtS6CJ7JCXA2tfg9AbiynWnV3QfohIVk3tUoV99X7lZSIhsIIUurC/mDCzvBzcjOVz5PfqcqI13gXysfPVZapXxNDqdEHZLCl1Y16n18POraCdXvvf7kilHitK0ghfT+tSW68uFyGZS6MI6zCbY8SnsmUpysVr8O3kUu0/nY2Sr8rzZpiKODnKIRYjsJoUunt7dv2DNvyHiVy769aLH+R5oJ1fmDqpJ68rFjU4nRJ4hhS6ezrUwWN4PHR/Nz6X+w9tnahPoV4Rv+tamlKdckihETpJCF0/u5BpYN4I0Z3fecfuEdZE+vN6iHG+3rYiT3MIvRI6TQhePz5QGv30E+7/hRuFaPHfjFe64FGXh0Fo0l0fECWEYKXTxeO7cTJ/yNnInuwt146UrzxNYtjjT+tSimIer0emEyNOk0IXlrhyHFf0wJ1xlitPrzItpylvtK/Jq83JyFYsQNkAKXVjm+Cp00EgSHQoyIOlD4ovU5KfBtajhIzcKCWErpNDFw5nSYNsEOPAd4U5VGZQwgjZ1qzGuSxUK5JM/PkLYEvkbKR7s9g306iGo87tZotvzlWkwk/vXpkO1kkYnE0JkwaJry5RSHZRSZ5RSEUqp0Q8Z10sppZVSWT7AVOQil49imtWc1PMHeCflVX7xfYcNb7aSMhfChj1yD10p5QhMB9oC0UCwUipIax1+3zh3YBTwe3YEFTno2HJM697gurkgI00T6d61C/0b+MkMiULYOEsOudQDIrTWkQBKqeVAdyD8vnEfA58B71o1ocg5plSSNo7B9fAPBJsrM6vYOL7o04IA7wJGJxNCWMCSQi8NRGVajgbqZx6glKoDlNFab1RKPbDQlVLDgGEAvr7yDEmbkhjDXwtfpHDMIeabOpLUYgJzWlSSOz6FyEWe+qSoUsoB+BIY/KixWuvZwGyAwMBA/bSfLazj1h+/k7asH26pcXxR4C069X+LqqUKGR1LCPGYLCn0S0CZTMs+Ga/9zR2oBuzMOMZaAghSSnXTWodYK6jIHic3zqRC8DjidSE21prHqK5dcIFmOaMAAA8LSURBVHGSvXIhciNLCj0YqKCUCiC9yPsAL/69UmsdD3j/vayU2gn8R8rctsXdus3xucNpFvczR5xq4Np3Ef3LBRgdSwjxFB5Z6FrrNKXUCGAL4AjM01qHKaUmASFa66DsDimsa3vISTw3vEwzThFSqh81Bn+Ni4s8TUiI3M6iY+ha603ApvteG/+AsS2ePpbIDtcSkpi/YjWDosdRWN0mqtW3BDYbaHQsIYSVyJ2ieYDZrFkW/CdnN09nrJ5LklsxHAespUzpmkZHE0JYkRS6nYu4nsj4NaF0ufQ1Hzlt565vMzz6LoT8RYyOJoSwMil0O5WcZuL7XZGs3H6I6c5fUdPpHLrxm7i1Hg8OjkbHE0JkAyl0O7T33A3GrTtJkdjDbMz/LR4OSdBjAarqc0ZHE0JkIyl0O3ItIYmPN4Sz4fhl3vTYxRuu83Ao5AsvLIHiVYyOJ4TIZlLodiDNZGbRgYt8+etZlCmJTb4rqXJ9A1RoDz1ng5s8hEKIvEAKPZcLuXCT8evCCL+SwHNlzUxJ/YJ8109A89HQ/H1wkLs+hcgrpNBzqeu3kpiy+TQ/Hb5EyUKurGibTL2Qd1DmNOi7HCp1NDqiECKHSaHnMqkmMwv3X+DrbedISTMzvEVZRuX/BZcdk8C7Yvrxcu/yRscUQhhACj0X2Rdxg4lBYZy7nkiLSkWZ2N4P/73vwcG1UKU7dJ8O+dyNjimEMIgUei5gNms+++U03++OpEwRN+YMDKR10XjUiq4Qew7afASNR4E8UUiIPE0K3cbdSUnjzeVH2Rp+jf4NfPmwcxVcIzbBD6+BkwsM+BnKtjA6phDCBkih27BrCUm8vDCEsMvxTOhahcENyqB2TIa9X0KpOvCvReBZ5tFvJITIE6TQbVT45QReWhhM/N1UfhgYSGtfJ1jyPETugDoDoeMX4OxqdEwhhA2RQrdB209fY8TSI3i4OrPq1YZU1X/A7IGQeA26fgPPDjI6ohDCBkmh2xCtNQv2X+DjDeFUKeXB3IGBFI9YAZvehYLFYegvUPpZo2MKIWyUFLqNSDOZmbg+jB8P/knbKsWZ1qsS+bf9B478COVaQc85UMDL6JhCCBsmhW4D4u+mMmLpYfacu8Erzcvyfj1XHBZ3gqvHodm70GKMTHkrhHgkKXSD/Rl7h6ELg7lw4zaf9arOC55n4IeXAQ19V0ClDkZHFELkEhbN3KSU6qCUOqOUilBKjc5i/atKqRNKqaNKqb1KKZmr1QIhF27SY8Y+Ym4ls2jIs7xwazEs6Q2FysCwnVLmQojH8sg9dKWUIzAdaAtEA8FKqSCtdXimYUu11rMyxncDvgSkjR7i5yPRvL/6BKU8XVnwQln8d76Sfklirf7QeSo4uxkdUQiRy1hyyKUeEKG1jgRQSi0HugP3Cl1rnZBpfAFAWzOkPTFl3MY/e3ck9QOK8EMrjcfqjnD7BnT7Nv0acyGEeAKWFHppICrTcjRQ//5BSqnhwNuAC9AqqzdSSg0DhgH4+vo+btZcL+5OCiOXHWHPuRsMbODLhOL7cFz2IRQqDS//CiVrGh1RCJGLWe3pB1rr6VrrcsD7wIcPGDNbax2otQ4sWrSotT46Vzh77Rbdp+/jYGQsU7uVZVLqlzhueR/Kt0k/Xi5lLoR4SpbsoV8CMk8Y4pPx2oMsB2Y+TSh7syXsKm+vOEr+fE6sfb4wVff2h5uR0HoCNH5TniokhLAKS5okGKiglApQSrkAfYCgzAOUUhUyLXYGzlkvYu5lNmu++vUsrywOpXxxd7a1jKbqxucgOREGrYemb0uZCyGs5pF76FrrNKXUCGAL4AjM01qHKaUmASFa6yBghFKqDZAK/AXk+clG/rqdwlsrj7LzTAx9a3vzsfMCnLYuhYBm0GsuFCxmdEQhhJ2x6MYirfUmYNN9r43P9PUoK+fK1Y5Hx/Haj4eJuZXMN20L0PXMm6jrp6DZe9BitNz1KYTIFnKnqBVprVl2KIqJQWEUdc/Hr22v4bd/LDjlg/6r00+ACiFENpFCt5K7KSbGrTvJ6tBoWpVzZ4bXClx3/gi+DdMPsRQqbXREIYSdk0K3gsiYRIYvPcLpqwlMbOjMoEvvo46HQ5O3oeVYcJRvsxAi+0nTPKW1Ry4x9ucTODs5sLF5NFVCJ6bftt9/jRxiEULkKCn0J3QnJY2JQWGsDImmia8bs4uuIP/B5eDXGHrNAY9SRkcUQuQxUuhP4Oy1WwxfcpiImEQ+qq8ZGP0+Kuxc+tzlzUfLIRYhhCGkeR6D1poVwVFMXB9GQRcnfm1yjvKHPwU3Txi4Dso2NzqiECIPk0K3UNydFMb+fJKNJ67QvqwL0/LPwzV4E5RvCz1mQsG8NTeNEML2SKFbYH/EDd5eeYwbicl82fAuz/3xHurqVWg3GRoMl9v3hRA2QQr9IZLTTHy59Syz90RSzsuVdQ1DKH74a/AsAy9thdLPGh1RCCHukUJ/gIjrt3hj2VHCryTwem0X3kn8AsfQA1C9N3T+Elw9jI4ohBD/IIV+H7NZs/jgRT7ddIoC+ZwIanmdGocngDbDc7Oh5gtGRxRCiCxJoWcSdfMO760+zoHIWNpXKMjXHktxO7AcSgemX1teJMDoiEII8UBS6KRfjrg8OIrJG9Ifkzq7laLt6TdRUeczri1/HxydDU4phBAPl+cL/Wp8Eu+vOc6uszE0DvBkht8OCh34EtxLwuCN4N/Y6IhCCGGRPFvoWmt+OnyJj9aHkWrSfNnGg+fOT0AdDE4/8dlpavoNQ0IIkUvkyUK/HHeXsT+fYMeZGAJ9PZlVNQzvvRPTb9nvNReqP290RCGEeGx5qtDNZs2y4D/576bTmMyaT9uVoO/VL1A7Nqc/Gq7HTCjkY3RMIYR4Inmm0C/G3mb0mhMciIylUTkvptW8RNFdIyApHtp/CvVfkzs+hRC5mt0XusmsWbD/Al9sOY2zgwNTu/rR69o3qE0roET19Em1ilc1OqYQQjw1iwpdKdUBmAY4AnO01lPuW/828DKQBsQAQ7XWF62c9bGdvprA+2tOcCwqjlbPFOOLOjfw+rU3JF5LvxSx6X/AycXomEIIYRWPLHSllCMwHWgLRAPBSqkgrXV4pmFHgECt9R2l1GvA54Bht1QmpZqYviOCmTv/wMPNme+er0DnKzNRP80D70rQZwmUrmNUPCGEyBaW7KHXAyK01pEASqnlQHfgXqFrrXdkGn8Q6G/NkI8j+MJNRq85zh8xt+lZpzQTq9/EY8vzEPcnNBoJLT8EZ1ej4gkhRLaxpNBLA1GZlqOB+g8Z/xKwOasVSqlhwDAAX19fCyNaJiEplS9+OcPigxcp7enGjwOq0OT8t7BiLhQOgCGbwK+RVT9TCCFsiVVPiiql+gOBQJaP7tFazwZmAwQGBmprfKbWmqBjl5m88RSxicm81CSA/5S/hNvmbhAflT5feasPwSW/NT5OCCFsliWFfgkok2nZJ+O1f1BKtQHGAs211snWifdwkTGJjFt3kn0RsdTwKcSCvpWoevJzWL4IvCrA0C3g+7D/TAghhP2wpNCDgQpKqQDSi7wP8GLmAUqp2sD3QAet9XWrp7xPUqqJGTsimLUrknxODnzcvSoveobjuLY93LoCjUdBizHg7JbdUYQQwmY8stC11mlKqRHAFtIvW5yntQ5TSk0CQrTWQcAXQEFglVIK4E+tdbfsCLwv4gYf/HyCi7F36F6rFONaeOG9Zzxs+RmKVoZ/LQYfeZKQECLvsegYutZ6E7DpvtfGZ/q6jZVzPVDMrWQclWLJS/VofOsXWDAWUu+mX73SeJRcVy6EyLNy3Z2i3WuVolPp27hsGgIX9oBfY+g6DbwrGB1NCCEMlesKXR1dgsuGt8HJNb3Iaw+UOViEEIJcWOgUKQeVOkDHz8G9hNFphBDCZuS+QvdrmP5LCCHEP8ixCiGEsBNS6EIIYSek0IUQwk5IoQshhJ2QQhdCCDshhS6EEHZCCl0IIeyEFLoQQtgJpbVVnjPx+B+sVAxg+IOkn4A3cMPoEAbIq9sNeXfbZbttk5/WumhWKwwr9NxKKRWitQ40OkdOy6vbDXl322W7cx855CKEEHZCCl0IIeyEFPrjm210AIPk1e2GvLvtst25jBxDF0IIOyF76EIIYSek0IUQwk5IoT+AUqqDUuqMUipCKTU6i/W+SqkdSqkjSqnjSqlORuS0Ngu2208p9VvGNu9USvkYkdPalFLzlFLXlVInH7BeKaW+yfi+HFdK1cnpjNnBgu1+Ril1QCmVrJT6T07nyy4WbHe/jJ/zCaXUfqVUzZzO+CSk0LOglHIEpgMdgSpAX6VUlfuGfQis1FrXBvoAM3I2pfVZuN1TgUVa6xrAJOC/OZsy2ywAOjxkfUegQsavYcDMHMiUExbw8O2+CbxB+s/dnizg4dt9Hmiuta4OfEwuOVEqhZ61ekCE1jpSa50CLAe63zdGAx4ZXxcCLudgvuxiyXZXAbZnfL0ji/W5ktZ6N+nl9SDdSf+HTGutDwKeSqmSOZMu+zxqu7XW17XWwUBqzqXKfhZs936t9V8ZiweBXPE/USn0rJUGojItR2e8ltlEoL9SKhrYBIzMmWjZypLtPgb0zPj6OcBdKeWVA9mMZsn3Rtinl4DNRoewhBT6k+sLLNBa+wCdgMVKqbzw/fwP0FwpdQRoDlwCTMZGEiJ7KKVakl7o7xudxRJORgewUZeAMpmWfTJey+wlMo7Baa0PKKVcSZ/U53qOJMwej9xurfVlMvbQlVIFgV5a67gcS2gcS/5MCDuilKoBzAE6aq1jjc5jibywR/kkgoEKSqkApZQL6Sc9g+4b8yfQGkApVRlwBWJyNKX1PXK7lVLemf4nMgaYl8MZjRIEDMy42qUBEK+1vmJ0KJE9lFK+wE/AAK31WaPzWEr20LOgtU5TSo0AtgCOwDytdZhSahIQorUOAt4BflBKvUX6CdLBOpffdmvhdrcA/quU0sBuYLhhga1IKbWM9G3zzjgvMgFwBtBazyL9PEknIAK4AwwxJql1PWq7lVIlgBDSLwAwK6XeBKporRMMimwVFvy8xwNewAylFEBabpiBUW79F0IIOyGHXIQQwk5IoQshhJ2QQhdCCDshhS6EEHZCCl0IIeyEFLoQQtgJKXQhhLAT/w8nTjilFbPqKAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgD1NjP7DOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "49a45034-d3b2-4c8a-c973-0ef66a4bb5ab"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.775, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 0.775, 1, 0.02, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVxU9f7H8deXAQREQRY3EMUFd1PDLU1TS027ebPMpVxLWzTtWpa2Wrft9qtu3bTFrluluWtWbuWaa4KiAm6IqCCyLyo78/39MWTk1UQdODPD5/l48HCW48z7CL79+p1zvkdprRFCCGH/nIwOIIQQwjqk0IUQwkFIoQshhIOQQhdCCAchhS6EEA7C2ag39vPz0w0aNDDq7YUQwi6Fh4enaq39r/acYYXeoEEDwsLCjHp7IYSwS0qp09d6TqZchBDCQUihCyGEg5BCF0IIB2HYHPrVFBYWEh8fT15entFRRAVwc3MjMDAQFxcXo6MI4RBsqtDj4+OpVq0aDRo0QClldBxRjrTWpKWlER8fT3BwsNFxhHAINjXlkpeXh6+vr5R5JaCUwtfXV/43JoQVXbfQlVJzlVLJSqnIazyvlFL/UUrFKKUOKaXa30ogKfPKQ77XQlhXWaZc5gMzga+v8fy9QJOSr07A5yW/CiFEpZdXWMy5zFwSs/JIyMzlXGYuvZrVpE2gt9Xf67qFrrXerpRq8BebDAS+1paF1fcopbyVUnW01olWyiiEEDZPa83Z9FzCTqcTdjqDyIQsEjJySbtU8D/b+npWMabQyyAAOFvqfnzJY/9T6Eqp8cB4gKCgICu8tW37/WxYPz+/W9qmrObPn09YWBgzZ85kxowZeHp68vzzz1/398XFxXHfffcRGXnVWbX/2SYiIoJz587Rv3//W84shL3KLyom+lw2+89kEn46nX1xGaRcyAegWhVnbqvnTatWXtT1cqOut7vly8udWl5VqOJsKpdMFXqUi9Z6NjAbIDQ0VC6VZKciIiIICwuTQheVhtaa+IxcDpzNJOJMJgfOZhCVkE1BsRmAwBrudG3ky+0NfAitX4OQWtUwOVX8Z0TWKPQEoF6p+4Elj92SN36IIvpc9q2+zJ+0qFud1//W8i+3iYuLo1+/fnTu3Jldu3bRoUMHxowZw+uvv05ycjILFy6kcePGjB07ltjYWDw8PJg9ezZt2rQhLS2NYcOGkZCQQJcuXSh9eb9vv/2W//znPxQUFNCpUyc+++wzTKbr/yv99ddf88EHH6CUok2bNnzzzTf88MMPvPXWWxQUFODr68vChQupVavWDf1ZhIeHM3bsWAD69Olz+fHi4mKmTZvG1q1byc/PZ8KECTzxxBOXny8oKOC1114jNzeXHTt2MH36dIKDg5k8eTJ5eXm4u7szb948mjZtSlRUFGPGjKGgoACz2cyKFSto0qTJDeUUwghmsyYm5SJ7T6Xz26l0fjuVRlK2ZfTt5uJEmwBvxnRtQLsgb9oF1aBWdTeDE1tYo9DXABOVUouxfBiaZe/z5zExMSxbtoy5c+fSoUMHFi1axI4dO1izZg3vvPMO9erVo127dqxevZrNmzczcuRIIiIieOONN+jWrRuvvfYaP/30E3PmzAHgyJEjLFmyhJ07d+Li4sLTTz/NwoULGTly5F/miIqK4q233mLXrl34+fmRnp4OQLdu3dizZw9KKf773//y/vvv8+GHH97QPo4ZM4aZM2fSvXt3pk6devnxOXPm4OXlxb59+8jPz6dr16706dPn8hEprq6uvPnmm5endgCys7P59ddfcXZ25pdffuGll15ixYoVfPHFF0yePJlHHnmEgoICiouLbyijEBWl2Kw5kpjN3lPp7I1NY19cOhk5hQDUru5Gp2BfOjSoQbugGjStXQ0Xk00d8X3ZdQtdKfUdcBfgp5SKB14HXAC01l8Aa4H+QAyQA4yxRrDrjaTLU3BwMK1btwagZcuW9O7dG6UUrVu3Ji4ujtOnT7NixQoAevXqRVpaGtnZ2Wzfvp2VK1cCMGDAAGrUqAHApk2bCA8Pp0OHDgDk5uZSs2bN6+bYvHkzgwcPvjy/7uPjA1hOwBoyZAiJiYkUFBTc8Ik5mZmZZGZm0r17dwBGjBjBunXrANi4cSOHDh1i+fLlAGRlZXHixAlCQkKu+XpZWVmMGjWKEydOoJSisNDyF6FLly68/fbbxMfHM2jQIBmdC5tRUGTmcELm5RF4eFwGF/KLAAjy8eDu5rXoGOxDp2Bf6vm4280htmU5ymXYdZ7XwASrJbIBVapUuXzbycnp8n0nJyeKiopu+FR1rTWjRo3i3XfftUq+Z555hilTpnD//fezdetWZsyYYZXXBUvWTz/9lL59+/7p8bi4uGv+nldffZWePXuyatUq4uLiuOuuuwAYPnw4nTp14qeffqJ///58+eWX9OrVy2pZhSirvMJi9p/JKJk+SWf/mQzyCi3z301qenJ/27p0DPahY7APdbzcDU5782zz/w027s4772ThwoUAbN26FT8/P6pXr0737t1ZtGgRAOvWrSMjIwOA3r17s3z5cpKTkwFIT0/n9OlrLml8Wa9evVi2bBlpaWmXfx9YRsQBAQEALFiw4Ibze3t74+3tzY4dOwAu7wtA3759+fzzzy+Pso8fP86lS5f+9PurVavGhQsXLt8vnWf+/PmXH4+NjaVhw4ZMmjSJgQMHcujQoRvOKsTNuJhfxLbjKby//igPfb6L1jM2MPyrvXyy6QRZuYUM7RDEF4+2J/yVu/l5Sg/efqA1A9sG2HWZg42t5WIvZsyYwdixY2nTpg0eHh6XS/X1119n2LBhtGzZkjvuuOPyoZktWrTgrbfeok+fPpjNZlxcXJg1axb169f/y/dp2bIlL7/8Mj169MBkMtGuXTvmz5/PjBkzGDx4MDVq1KBXr16cOnXqhvdh3rx5jB07FqXUnz4Uffzxx4mLi6N9+/ZorfH392f16tV/+r09e/bkvffeo23btkyfPp0XXniBUaNG8dZbbzFgwIDL2y1dupRvvvkGFxcXateuzUsvvXTDOYUoi8ycAvbFZbA3No3f4tKJTMjCrMHZSdE60Iux3YLpFOzD7fV98HIv58Xgigrg4nnITrT8eqHk62ISXEiEC0nQYyq0fMDqb61KH4lRkUJDQ/WVVyw6cuQIzZs3NySPMIZ8z8XNyMopZO+pNHbHprH7ZBpHz1v+x+jq7ES7et50CvahY7Av7YK8qVrFiuPW4kLIPgdZ8ZCdAFlnLfdLf11KAa7oVSdn8Kxl+apWGzo8Bo3vvqkISqlwrXXo1Z6TEboQwublFRazJzaNnTGp7I5NI+pcNlpDFWcnQhvU4Pk+IXRq6EubQK9bO2mnIMdS0plnIfM0ZJ4puX/GUuIXzvM/Ze1eA6oHQLU6UKfNH7er17WUt2dt8PAFp/Kf4ZZCtwFpaWn07t37fx7ftGkTvr6+t/TaEyZMYOfOnX96bPLkyYwZY5WDkYQoN/EZOWw5lsKWo8nsOplKXqEZV5MT7YK8mdy7CV0a+tI2yPvGClxruJQK6SchI87ylX7qj9sXz/95eycX8AoE7yBo1Nty2ysQvALAq56ltF2rWm+nb5EUug3w9fUlIiKiXF571qxZ5fK6Qlib2aw5GJ/J+qjzbD6SzInki4DlMMKhHYK4q6k/nRv64uZShgLPzYDUGEg7AWknIT3WUuLppyC/9AmLyjKirtHAMgVSowHUqG8pcO8gyxSJU/mcpl8epNCFEIYpNmv2xaWzPvI86yPPcz47D2cnRaeGPgzpUI+ezWrS0K/q1Y8D19oyHZJyzPKVehzSYiy/Xkr5YztlspSzT0Oo1wl8Gllu+zQE73rgXOV/X9tOSaELISpUQZGZ3bFprI88z8/R50m9WICrsxM9QvyZ2rIpdzevhZdHqSNRtIasBEiOtnylHIPkI5biLrj4x3YevuDbBEL6gV8T8Aux3PcOAmfXit9RA0ihCyHKXW5BMdtPpLA+8jy/HEniQl4RHq4mejaryb2tatOzaU3L0SgFOZB8EI5EQlIUJEVDUiTkZf7xYp61wL8ZtH0Eajaz3PZvBh4+xu2gjZBCF0KUi6ycQjYfS2JjVBJbj6WQW1iMl7sLfVvWpl/L2nQLNOGWGgmJO+DHQ5B4yDLnrS1ncOLqCTVbQMu/Q61Wlts1m0tx/wUp9CuYTCZat26N1hqTycTMmTO54447yMnJYdy4cRw6dAitNd7e3qxfvx5PT89bfk9Zx1w4ioTMXH6OOs/G6CT2nkqn2Kzxr1aF4W29+XutVFroaEznF8HG/ZbDAn9XPQBqt7GUd+02ULsVeAVVyKF+jkQK/Qru7u6XjzjZsGED06dPZ9u2bXzyySfUqlWLw4cPA3Ds2LEbXtPFaLKOubA2rTXRidn8HJ3Ez9FJRJ3LxkQxvX3TmdX0HB1MMfhkHkYdPgGHS47f9g6Cuu0gdAzUuc1S4FVv/QIvwpYLfd00OH/Yuq9ZuzXc+16ZN8/Ozr68YmJiYuKfTtVv2rTpX/5eWcdcOKrCYjO/nUq/XOIXM1MINR1nrNdZOtc+SZ2L0ThdyoFTQFV/CAiF1g9bSrxuWynvcmS7hW6Q3Nxc2rZtS15eHomJiWzevBmAsWPH0qdPH5YvX07v3r0ZNWrUNUtO1jEXjuZifhHbjqWwMSqRo8eiaVYQRRfn44yrEkOAW5xlozxn8G4DISMgsAPU6wDe9cFOlp51BLZb6Dcwkram0lMuu3fvZuTIkURGRtK2bVtiY2PZuHEjv/zyCx06dGD37t1XXYdE1jEXjiDlQj6bjiSx/2AETmd20oEoppmiqUMauIKuUg1VrxMEjYCgLhDQHlzse7VCe2e7hW4DunTpQmpqKikpKdSsWRNPT08GDRrEoEGDcHJyYu3atTe0sJSsYy5sXVzqJXYcOETG4Y3UyQijm1M0Q1UqmKDQzRdT8J3QoCvU74Kq2cKuzqKsDKTQ/8LRo0cpLi7G19eXnTt30qJFC2rUqEFBQQHR0dGXC/BKvXr14oEHHmDKlCn4+vqSnp6Oj4+PVdcx79at21XXMe/VqxcuLi4cP3788nv97mbWMT9z5gyHDh2SQndQWmsiT53jxG/rIXYLrfP286iT5ZLAuVW8KKx3B7pZT1Rwd1z8m8n0iY2TQr/C73PoYPlhX7BgASaTiZMnT/LUU0+htcZsNjNgwAAefPDBq76GrGMubFl+YRGH9+8mPeIH/M7/SivzMVqrYgpwJcWvPZktxuLdsg/utVrhLocN2hVZD10YSr7nFSMrI52ju36g6NgGGmXtprayfFB/xrUxuUHdCWjfH88md4KLbVy9XlybrIcuRCV07vRxTu9agWfcBprmHaKTKuYiHpzy7kBG074Edx5IkE+g0TGFFUmh3wJZx1zYEm02E3NoN6lhK/FP3Ezj4ljqAmecAjhQdxg+be+jUfvetHapHAtVVUY2V+ha66svlWmDZB3zW2PUdJ8j0WYzpw5tJ2XPEuol/UITnUwjrTju2py9DSYT0PlBgprcRpDRQUWFsKlCd3NzIy0tDV9fX7spdXFztNakpaXh5iZztjfMbCYhchtJe5YQkPgzDXUqgdrEEY/bSWgygcbdBtOsZsD1X0c4HJsq9MDAQOLj40lJSbn+xsLuubm5ERgoc7hlojU5Z/ZzeusC/E//SIA5DT/twmH32zkZ8g+a9xjCbb7+RqcUBrOpQndxcbnhsyeFcGipMSTt+hanyOX4F5ylkTYR7hLK4ebP0aLHw4TWlBIXf7CpQhdCAJfSyI9YwsXfvsU3Kwp/rfiNFmwJeIGmPYfTuXEDmZIUVyWFLoQtKC6EEz+TtXsBVc/8QhVdRIy5Pss9HsOn4xD6dGlPZ3f7Wq5ZVDwpdCGMlBRNQdgCzAeX4FaQQYGuzkrdl/QmD3FX97sYH1RDRuOizKTQhahoBTnoqFVc2j0Hz+RwlDax2Xw7e6r3JeSOgQxq3wAvGY2LmyCFLkRFOR9Jzp45mCKXUqXoIknmOnzGCHJbPMzArm14I9BLRuPilkihC1GeigoojlrNhe2f4522H5N2Ya25Iwf8B9Kqy71MaFPXcrV7IaxAfpKEKA/ZiWT8OhuXiAV4FqaRYa7FXOfRqHbDuL9zKx7wv/WLiwtxpTIVulKqH/AJYAL+q7V+74rn6wNzAX8gHXhUax1v5axC2Lz82N0k//IJdc5txEub2aZvI6LOc7TqPohnmtXCxSTL0Yryc91CV0qZgFnAPUA8sE8ptUZrHV1qsw+Ar7XWC5RSvYB3gRHlEVgIm2MuJmHPMop3fkrQpUi8tAcrXQZQ0H4sfbp1oWd1Wd5AVIyyjNA7AjFa61gApdRiYCBQutBbAFNKbm8B/nxlBSEcUO7FLI6u+5w6R+YRYD7PGV2TFbUmUa/3OAaH1JMPOEWFK0uhBwBnS92PBzpdsc1BYBCWaZkHgGpKKV+tdVrpjZRS44HxAEFBsv6bsE/HY2NJ2vBvbktaQTsuEeXUlMjWz3F7nxE8WE0ukiyMY60PRZ8HZiqlRgPbgQSg+MqNtNazgdlguWKRld5biHJ3Ia+QX/aE47JnJnfnrqcxRRyqdifOd06iZce7aSmjcWEDylLoCUC9UvcDSx67TGt9DssIHaWUJ/Cg1jrTWiGFMILWmvDTGfyyYxdNjn/F/epXlILYgL9R695ptK0nl84TtqUshb4PaKKUCsZS5EOB4aU3UEr5AelaazMwHcsRL0LYpaycQlbsj2f37l8ZmL2QF5z2UmRyJbP5SPz6TCHEW6YLhW26bqFrrYuUUhOBDVgOW5yrtY5SSr0JhGmt1wB3Ae8qpTSWKZcJ5ZhZCKv7fTS+6LczHDkUxtNqGV+a9lJcxYOiDpNx7ToRf09ZqlbYNmXUZcBCQ0N1WFiYIe8tBEBRsZmD8VlsO57C+shEipKPM8V1Nf3VTrSLB6bOT0GXCeDhY3RUIS5TSoVrrUOv9pycKSoqlcSsXLYfT2Hb8RR2nEglO6+I+iqJGV4/0qPKFpSLG6rjZLhjElS9tQt9C1HRpNBFpXA2PYfX10Sx+WgyALWru/FgsyqMKvie+nFLUUXO0OVp6PosyNSKsFNS6MKhFRWbmbvzFP/++QRKwZR7QujXxJMmJ+ehds+CwlxoPxJ6vAjV6xgdV4hbIoUuHFbE2UymrzzMkcRs7m5ekzfuCyEgZgks/hfkpEKLgdDrVfBrYnRUIaxCCl04nAt5hXy48TgLdsdRs1oVvnikHX2dw1ELx0N6LDS4E+5+AwJvNzqqEFYlhS4cysmUi4yZt4+zGTmM7FyfF9oWUnXLeIj7FfyawiPLofHdIGd2CgckhS4cxp7YNJ74JhwXk2LViMa0jZkJ874B9xrQ/wO4fQyY5EdeOC756RYOYeX+eF5ccYhGPq4sbr0f79WjoSgPOj8NPaZaSl0IByeFLuya1pqPfznBJ5tOMC7gNNP0HEy7YiDkXujzFvg1NjqiEBVGCl3YrfyiYqatOMzuA4f4vuZKbkvbAjWCYfgyCOljdDwhKpwUurBLaRfzmfjtXtqcXcR2j9W45Gjo+Qrc8Qy4yBWCROUkhS7sTtS5LGbN/4Z/5n9GY5cEaDIA+r0LNeobHU0IQ0mhC7uyIewYmWum85nTJgqqBcL9Mr0ixO+k0IVdMBeb+XHJ53Q+9j6+TtlcCn2aqn1eAdeqRkcTwmZIoQubdzE5jpj5T3J/zm7i3UMofmQVVeu1NzqWEDZHCl3YLq1J2fo5Vbe9SYg2s6/pFEKHvIQyuRidTAibJIUubFPmGVIXjcc/eTd7aY3zA5/SoW07o1MJYdOk0IVt0ZrisHkUrXsZt2Izn3lOZODjLxNQw8PoZELYPCl0YTsyz1KwagKup7exp7glO1vNYPKDvanibDI6mRB2QQpdGE9riFhE8doXKCos4t3isbS4/1le6CDHlQtxI6TQhbFy0tE/PouK/p5wczM+8HiW10b0p1WAl9HJhLA7UujCOCc3Y171FOaLqfxf4TBiGo9m9pD2eHu4Gp1MCLskhS4qXmEebHoD9nzGGRXA5MI3ua/vvbzYLRgnJ7nwhBA3SwpdVKykaPSKx1DJ0XxT3Id5HmP5cExn2gXJeuVC3CopdFExtIb9C9DrXiTb7M7kgqlUad6PVQ/ehpeHnCgkhDVIoYvyl5cNPz4LkSv4Td3GswVP8eR9XRjZpT5Kru0phNVIoYvyde4AetkYdMYZPiwawtrqQ/jq8VA5ikWIciCFLsqH1rD3S/TGV0hXXjyR/zIBt/Xihwda41lFfuyEKA/yN0tYX14WrH4ajv7Ir+p2Xih8kimDujA4NFCmWIQoR1LowrqSojEveRTS43in8BF+9R3CN4+0p0mtakYnE8LhSaEL6zm8nOLVE8ksduPJ/Jdp3KEPq+9rgburrMUiREVwKstGSql+SqljSqkYpdS0qzwfpJTaopQ6oJQ6pJTqb/2owmYVFZDz/XOw4jHCC4N4oupHTHl8NO8Oai1lLkQFuu4IXSllAmYB9wDxwD6l1BqtdXSpzV4BlmqtP1dKtQDWAg3KIa+wMcVZ50ibN4yamRHMM/fn0p2vsbBniKyQKIQByjLl0hGI0VrHAiilFgMDgdKFroHqJbe9gHPWDClsU0z4L/j++BhVzbl86vsS9w2fSLCfXONTCKOUpdADgLOl7scDna7YZgawUSn1DFAVuPtqL6SUGg+MBwgKCrrRrMJGZOYUsHnRh9x39v84r/w50WsBE7v3kCNYhDCYtT4UHQbM11p/qJTqAnyjlGqltTaX3khrPRuYDRAaGqqt9N6igpjNmuX7TlG07iWGs46T1Tvg/9h39Pb2NzqaEIKyFXoCUK/U/cCSx0p7DOgHoLXerZRyA/yAZGuEFMaLTMjivZW7eSL5n9xpiiSt9eM0+vu/wCQHSglhK8ryt3Ef0EQpFYylyIcCw6/Y5gzQG5ivlGoOuAEp1gwqjJFxqYAPNh7jt327mOv6EXVd0tD3zcS3/QijowkhrnDdQtdaFymlJgIbABMwV2sdpZR6EwjTWq8BngO+Ukr9A8sHpKO11jKlYseKzZrF+87wfxuO0T4/jB/dPsXF3ROnoWuhXkej4wkhrqJM/1/WWq/Fcihi6cdeK3U7Guhq3WjCKOGnM3h9TSSRCdm8WnMnY/XnqJqtYNhi8AowOp4Q4hpkAlRcln6pgHfWHmF5eDx1qjmzqeV6Gp38GkLuhQf/C1U8jY4ohPgLUugCgOhz2Yz7OozkC3lMurMOkzL/hfOJ9dDpKej7NjjJiUJC2DopdMH6yPP8Y0kEXu4ufD+yES22PA5JkXDv/0Gn8UbHE0KUkRR6Jaa15tPNMXz083Ha1vNmTj83fL9/AHIzLfPlIX2NjiiEuAFS6JVUbkExzy87yE+HExnULoD32mfiunQouFaFseuhThujIwohbpAUeiV0LjOXcV+HEZ2YzfR7mzHe9yBq8RNQIxhGrASvQKMjCiFughR6JXMmLYdhX+0hO7eQuaM60DNrFSx/Eep1gmHfgYeP0RGFEDdJCr0S+b3MLxUU8d24TrQ6+jHs+Dc0u89yWKKLu9ERhRC3QAq9kihd5t+OaU+rfdPg4Hdw+xgY8KEcliiEA5BCrwTOpv9R5gtHtaHltich5mfo+TJ0nwqy7K0QDkEK3cGdTc9h6Ow9XMwv4ruRzWmxaTSc2QP3fQyhY4yOJ4SwIil0B1a6zBc/0pjmG4ZDcjQ8NAdaPWh0PCGElUmhO6jUi/kM+8pS5kuGBtFs/RDIPANDv4OQPkbHE0KUAyl0B5RfVMwT34STciGf1UNr02ztYMjNgEdXQgNZFFMIRyWF7mC01kxfeZjw0xl8PaAqzdc9DLoYRv8AddsZHU8IUY6k0B3MF9tiWbk/gfc6F9F95yhw8YCRP4J/U6OjCSHKmZPRAYT1bIg6z/sbjvJMSAZDjkwEt+owdp2UuRCVhBS6g4hMyOLZxRE8XCuBKedfRHn4wui1UKOB0dGEEBVEplwcQPKFPMZ9HUZPt+O8e+ldVPW6MOoHqF7X6GhCiAokI3Q7l1dYzPivw2mWs5+Z5rdx8qoHo3+SMheiEpIRuh3TWvPSysN4JWzjv24f4+TbGEZ+D57+RkcTQhhACt2OzdlxirSDa5lT5SNMNZvBiO+hqq/RsYQQBpFCt1O/nkhh+7qlljKv1dwyMpe1zIWo1GQO3Q7FpV5iwcJv+Mr1Q5z8Q1BS5kIIpNDtzoW8Qj6ZO59PeQ/lE4xp1BopcyEEIIVuV8xmzawFC3nr0hvo6oG4jv0RqvoZHUsIYSOk0O3IklUrmHBuGkVVa+Mxbi141jQ6khDChkih24md239mwKGJ5FfxofoT66BabaMjCSFsjBS6HTgZtY8Wm8aQa6pOtSfWobwCjI4khLBBUug2LjP+GF7LBlOknHEevYYqvvWNjiSEsFFS6DasMOMsBfP+hkkXkTZoKb5BzYyOJISwYVLotupSKplfDMC9KJv93efQrE1HoxMJIWxcmQpdKdVPKXVMKRWjlJp2lef/rZSKKPk6rpTKtH7USiQ3k4wv78MzL5EVzT6id+++RicSQtiB6576r5QyAbOAe4B4YJ9Sao3WOvr3bbTW/yi1/TOAXOvsZhVc4uK8QVTNOs7H/m8y5eGhRicSQtiJsozQOwIxWutYrXUBsBgY+BfbDwO+s0a4SqeogPyFj+CefIB/uj3H+LHjcTbJrJgQomzK0hYBwNlS9+NLHvsfSqn6QDCw+RrPj1dKhSmlwlJSUm40q2Mzmyle9RRVTm/hdfN4RoydhLeHq9GphBB2xNrDv6HAcq118dWe1FrP1lqHaq1D/f1lze7LtEZvmI4pajn/KhpKj6FTCKlVzehUQgg7U5ZCTwDqlbofWPLY1QxFpltu3I6PUHu/YE7RvXj2ep57WtQyOpEQwg6VpdD3AU2UUsFKKVcspb3myo2UUs2AGsBu60Z0cPu/hk1vsrq4K4dbTuXpno2NTiSEsFPXPcpFa12klJoIbABMwFytdZRS6k0gTGv9e7kPBRZrrXX5xXUwR39C/zCZnfo2vqk5lYUPtUUpZXQqIYSdKtMVi7TWa4G1Vzz22hX3Z1gvViVwehd6+ViO0IiXXaayZGQX3FxMRq7t+MsAAA36SURBVKcSQtgxuQSdEZKPor8byjntx5jCqXw59k5qe7kZnUoIYefkIOeKlp0ICx/iYpEzD1+ayksPdaNtPW+jUwkhHIAUekXKvwCLBlN4MZWhl6bwtx6dGdhWlsIVQliHFHpFKS6EpaPQSdE8kT8JvyYdmdq3qdGphBAORObQK4LW8OOzcHITb5me4qRXF9YMbYfJSY5oEUJYjxR6Rdj2Phz4lqUew/ju4l2sGheKl4eL0amEEA5GplzK24GFsPUdwmv044X0+/hg8G00rS2n9QshrE8KvTzFboMfJpHo25mhicN5+q7G9G9dx+hUQggHJYVeXlJPwNIR5FYPpv/5cXRtWofn+siHoEKI8iOFXh4upcHCwRQ7uTDk4hSqe/vyyRD5EFQIUb6k0K2tKB+WPILOPsc0l+mcLPBh9gj5EFQIUf6k0K1Ja1gzCc7s5tva01iWVIePhrSVD0GFEBVCCt2atn8AhxYT3nACr55syrN3N6Fvy9pGpxJCVBJS6NYSuQK2vEVy8N95+GhX+rSoxaReTYxOJYSoRKTQrSE+HFY9RV7dTvQ/9TCN/D35aEhbnORDUCFEBZJCv1XZ52DxcMzVajPiwjMUKhe+GhmKZxU5CVcIUbGk0G9FYS4sHo4uuMg/q71GeKoTM4e3o75vVaOTCSEqISn0m6U1fD8BzkXwc/O3mXfCnZf6N+fOJv5GJxNCVFJS6Dfr1w8hcgVJHV9gYlgt7m5ei8e6BRudSghRiclE7804+hNs/idFLQczPLoL3h7FvP9QG7nAsxDCUDJCv1FJUbBiHNRtz5tOTxKblsO/h7TFp6qr0cmEEJWcFPqNuJQK3w2FKtXY0u5jvt6XxPjuDena2M/oZEIIIYVeZsVFsGw0XEgi9W9z+ce6JFoHePHcPbKCohDCNsgceln9/CrE/Yp54Oc8s81EQZGZT4a2xdVZ/k0UQtgGaaOyOLgY9nwGnZ7ki6yO7I5NY8b9LWno72l0MiGEuEwK/XrORcAPk6F+Nw62eJ6PNh5nQJs6DL490OhkQgjxJ1Lof+VSKix5FDz8yP37HP6xPAr/alV45++t5RBFIYTNkTn0aykutHwIeikFxq7nX7+mEZtyiYWPd5KLVQghbJKM0K9lo+VDUP72CTtz6jF/Vxyj72gghygKIWyWFPrVHFwMez+HTk+RFfIgzy87SEP/qrzYr5nRyYQQ4pqk0K90/rDlQ9AGd0Kff/LGD1EkX8jno4fb4u5qMjqdEEJcU5kKXSnVTyl1TCkVo5Sado1tHlZKRSulopRSi6wbs4LkZlg+BHWvAQ/NZf2RVFbuT2DCXY1oW8/b6HRCCPGXrvuhqFLKBMwC7gHigX1KqTVa6+hS2zQBpgNdtdYZSqma5RW43JjNsOpJyEqAMWtJ0V68tGo7rQKqM1EuJSeEsANlGaF3BGK01rFa6wJgMTDwim3GAbO01hkAWutk68asADs+hOProe876MAOvLTqMBfzi/joYTkbVAhhH8rSVAHA2VL340seKy0ECFFK7VRK7VFK9bvaCymlxiulwpRSYSkpKTeXuDzEbILNb0Prh6HjOJaFx/NzdBIv9G1KSK1qRqcTQogysdbQ0xloAtwFDAO+Ukr9z6Sz1nq21jpUax3q728jV/bJPAMrHoOazeFvHxOXlsMba6LoFOzD2K5ywQohhP0oS6EnAPVK3Q8seay0eGCN1rpQa30KOI6l4G1bYR4sHQnmYhjyLYUmdyYvicDkpPj3kLY4OcnZoEII+1GWQt8HNFFKBSulXIGhwJortlmNZXSOUsoPyxRMrBVzlo/1L8K5A/DAF+DbiE9+OcHBs5m8O6gNdb3djU4nhBA35LqFrrUuAiYCG4AjwFKtdZRS6k2l1P0lm20A0pRS0cAWYKrWOq28QlvFwcUQPh+6PgvNBrA3No1ZW2MYfHsgA9rUMTqdEELcMKW1NuSNQ0NDdVhYmCHvTfIR+KoX1G0PI78nq0Bz78fbcXF24qdJd+JZRZa4EULYJqVUuNY69GrPVb7j8fIvWubNXavCQ3PQTiZeXnWY5Av5fDK0nZS5EMJuVa5C1xp+fBbSYuDBOVCtNiv2J/DjoUT+cU+InA0qhLBrlavQw+fB4WVw10vQsAen0y7x+veRdAz24ckejYxOJ4QQt6TyFPq5CFj3IjTqDXc+R0GRmUmL/zhE0SSHKAoh7FzlKPTcTMu8eVV/GPQVODnxwcZjlw9RDJBDFIUQDsDxPwHUGr6fANkJMHotVPVl89EkZm+P5dHOQXKIohDCYTh+oe/9Eo7+CH3ehqBOJGbl8tzSgzSrXY1XBrQwOp0QQliNY0+5JOyHja9A0/7QZQJFxWYmfxdBfpGZWY+0x81FLlghhHAcjlvoeVmwfAx41oKBs0ApPtl0gt/i0nnngdY08vc0OqEQQliVY065aG25jFzmWRizDjx82HEilZlbYng4NJC/t7ty9V8hhLB/jjlCD5sLUaug96sQ1InkC3k8uySCxv6ezLi/pdHphBCiXDjeCP38YVg/3XK8+R2TKTZrpiw5yMX8QhaN64SHq+PtshBCgKON0PMvwrLRlos8P/AlODnx+dYYdsSkMuNvLeXqQ0IIh+Y4w1Wt4acpkB4Lo34AT39+O5XORz8fZ2DbugzpUO/6ryGEEHbMcUboEYvg0BLoMQ0adCP9UgGTvjtAfd+qvP1Aa5SSU/uFEI7NMUboqSdg7VRocCd0fx6zWTNlaQTpOQWsGn2HLIkrhKgU7H+EXpQPy8eCcxUYNBucTPx3Ryxbj6Xw6oDmtKzrZXRCIYSoEPY/dP1lBpw/BMMWQ/W67D+Twfvrj9G/dW0e7Vzf6HRCCFFh7HuEfnwD7PkMOj4BTe8lK6eQZxYdoI63G+8OaiPz5kKISsV+R+jZibD6KajVGu55E601U5cfJPlCHsufvAMvdxejEwohRIWyzxG6uRhWjoPCXHhoLri4sWBXHBujk3ixXzNuk0vJCSEqIfscoe/4N8T9CvfPBP8QIhOyeGftUXo3q8lj3YKNTieEEIawvxH62d9gyzvQ6kFo9ygX84uYuGg/vp6ufDD4Npk3F0JUWvY3Qk89Dj4N4b5/o4FXVh3mTHoOi8d3oUZVV6PTCSGEYeyv0Ns9Cq0fBmdXloWdZXXEOZ67J4SOwT5GJxNCCEPZ35QLgLMrMckXeP37KO5o5MvTPRsbnUgIIQxnl4WeV1jMhIUH8HA18fGQtpicZN5cCCHsb8oFePPHaI4lXWDB2I7UrO5mdBwhhLAJdjdC/+lQIov2nuHJHo3oEeJvdBwhhLAZdlfoXu4u9GlRi+f6hBgdRQghbIrdTbl0a+JHtyZ+RscQQgibY3cjdCGEEFdXpkJXSvVTSh1TSsUopaZd5fnRSqkUpVREydfj1o8qhBDir1x3ykUpZQJmAfcA8cA+pdQarXX0FZsu0VpPLIeMQgghyqAsI/SOQIzWOlZrXQAsBgaWbywhhBA3qiyFHgCcLXU/vuSxKz2olDqklFqulKp3tRdSSo1XSoUppcJSUlJuIq4QQohrsdaHoj8ADbTWbYCfgQVX20hrPVtrHaq1DvX3l2PIhRDCmspS6AlA6RF3YMljl2mt07TW+SV3/wvcbp14Qgghyqoshb4PaKKUClZKuQJDgTWlN1BK1Sl1937giPUiCiGEKIvrHuWitS5SSk0ENgAmYK7WOkop9SYQprVeA0xSSt0PFAHpwOjrvW54eHiqUur0LaU3hh+QanQIA1TW/YbKu++y37ap/rWeUFrrigxi95RSYVrrUKNzVLTKut9Qefdd9tv+yJmiQgjhIKTQhRDCQUih37jZRgcwSGXdb6i8+y77bWdkDl0IIRyEjNCFEMJBSKELIYSDkEK/hjIsGRyklNqilDpQsoZNfyNyWlsZ9ru+UmpTyT5vVUoFGpHT2pRSc5VSyUqpyGs8r5RS/yn5czmklGpf0RnLQxn2u5lSardSKl8p9XxF5ysvZdjvR0q+z4eVUruUUrdVdMabIYV+FaWWDL4XaAEMU0q1uGKzV4ClWut2WM6e/axiU1pfGff7A+DrknV73gTerdiU5WY+0O8vnr8XaFLyNR74vAIyVYT5/PV+pwOTsHzfHcl8/nq/TwE9tNatgX9iJx+USqFfXVmWDNZA9ZLbXsC5CsxXXsqy3y2AzSW3t1zlebuktd6OpbyuZSCWf8i01noP4H3Fkhd26Xr7rbVO1lrvAworLlX5K8N+79JaZ5Tc3YNlDSubJ4V+dWVZMngG8KhSKh5YCzxTMdHKVVn2+yAwqOT2A0A1pZRvBWQzWlmXkRaO5zFgndEhykIK/eYNA+ZrrQOB/sA3SqnK8Of5PNBDKXUA6IFl5c1iYyMJUT6UUj2xFPqLRmcpi+suzlVJXXfJYCzf5H4AWuvdSik3LIv6JFdIwvJRlqWSz1EyQldKeQIPaq0zKyyhccryMyEciFKqDZblwO/VWqcZnacsKsOI8mZcd8lg4AzQG0Ap1RxwA+z9MkxlWSrZr9T/RKYDcys4o1HWACNLjnbpDGRprRONDiXKh1IqCFgJjNBaHzc6T1nJCP0qyrhk8HPAV0qpf2D5gHS0tvPTbsu433cB7yqlNLAdmGBYYCtSSn2HZd/8Sj4XeR1wAdBaf4Hlc5L+QAyQA4wxJql1XW+/lVK1gTAsBwCYlVLPAi201tkGRbaKMny/XwN8gc+UUgBF9rACo5z6L4QQDkKmXIQQwkFIoQshhIOQQhdCCAchhS6EEA5CCl0IIRyEFLoQQjgIKXQhhHAQ/w9TYOB6hWEJwAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7MfujMQ7oij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5f1b9314-e620-41f6-a012-1ff88fb80c14"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.225, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1.225, 1, 0.02, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}