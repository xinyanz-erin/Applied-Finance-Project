{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Lilian/European_Call_jax_1stock_v3_pricedelta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "f93a26d6-822a-4bee-aae5-583707e18184"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0807]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.2597*0.2597\n",
        "initial_stocks = jnp.array([0.7178]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.2106\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5234334\n",
            "[0.9998683]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d27b1a-e050-466e-c945-7e02af41ee42"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "# version 1, 2, 6\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(0.75 + np.random.random(self.N_STOCKS) * 0.5)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(0.15 + np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(0.25 + np.random.random(1) * 0.35), self.N_STOCKS)\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          K = 0.75 + np.random.random(1) * 0.5\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32) # remember to change this!\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "dbbcdd5d-809a-4fb6-a29c-f5526622a250"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.35, 0.35]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.25, 0.25]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "c285bc2e-046e-4cce-9e13-47c2ad2fd1a5"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.7)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "90b1bfaf-1068-4081-ce2d-0e2775d39ddb"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    adjusted_y2 = (1000 * y)**2\n",
        "    loss_weight = 1/adjusted_y2.mean(axis=0)\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "\n",
        "\n",
        "    # loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.2062716782093048 average time 0.10656352285004686 iter num 20\n",
            "loss 0.13432049751281738 average time 0.07056057957499888 iter num 40\n",
            "loss 0.12389954924583435 average time 0.059221007983326975 iter num 60\n",
            "loss 0.12991727888584137 average time 0.052957659499986674 iter num 80\n",
            "loss 0.11826185882091522 average time 0.049439921269977274 iter num 100\n",
            "loss 0.12849104404449463 average time 0.10857568890000949 iter num 20\n",
            "loss 0.1043272539973259 average time 0.07135178270003735 iter num 40\n",
            "loss 0.09696900099515915 average time 0.05963810035000279 iter num 60\n",
            "loss 0.10477063059806824 average time 0.053337341862504675 iter num 80\n",
            "loss 0.08568607270717621 average time 0.04955751443999816 iter num 100\n",
            "loss 0.06308209151029587 average time 0.10624584450004022 iter num 20\n",
            "loss 0.05310133844614029 average time 0.07026628094998841 iter num 40\n",
            "loss 0.03729735314846039 average time 0.05832075093332302 iter num 60\n",
            "loss 0.03695716708898544 average time 0.05219103124998128 iter num 80\n",
            "loss 0.037650104612112045 average time 0.0486288410099678 iter num 100\n",
            "loss 0.021685315296053886 average time 0.1092325990499603 iter num 20\n",
            "loss 0.013930954970419407 average time 0.0720939556499843 iter num 40\n",
            "loss 0.005525235086679459 average time 0.05975600963334576 iter num 60\n",
            "loss 0.008208892308175564 average time 0.05369903426249038 iter num 80\n",
            "loss 0.006563468370586634 average time 0.04982341622001513 iter num 100\n",
            "loss 0.0036605107598006725 average time 0.10862933045002592 iter num 20\n",
            "loss 0.004891116637736559 average time 0.0712139809750056 iter num 40\n",
            "loss 0.003826669417321682 average time 0.058587358900020564 iter num 60\n",
            "loss 0.0039012543857097626 average time 0.05245311981251462 iter num 80\n",
            "loss 0.0033299080096185207 average time 0.04892680731002201 iter num 100\n",
            "loss 0.0024102528113871813 average time 0.10826679525007421 iter num 20\n",
            "loss 0.0020650976803153753 average time 0.07176401972500343 iter num 40\n",
            "loss 0.0021130964159965515 average time 0.059459304483334564 iter num 60\n",
            "loss 0.004028231371194124 average time 0.05321640599998432 iter num 80\n",
            "loss 0.0018334273481741548 average time 0.04949795370998345 iter num 100\n",
            "loss 0.003406150732189417 average time 0.10545864435000567 iter num 20\n",
            "loss 0.0021095515694469213 average time 0.07031521437498896 iter num 40\n",
            "loss 0.0031767338514328003 average time 0.05813864206667555 iter num 60\n",
            "loss 0.0029199919663369656 average time 0.05220015313752242 iter num 80\n",
            "loss 0.003968837670981884 average time 0.04893921790001514 iter num 100\n",
            "loss 0.0033885412849485874 average time 0.11188742500003172 iter num 20\n",
            "loss 0.002990742912515998 average time 0.07279510125003981 iter num 40\n",
            "loss 0.00523750577121973 average time 0.05963690531671091 iter num 60\n",
            "loss 0.002790641039609909 average time 0.053007057775062094 iter num 80\n",
            "loss 0.0017648423090577126 average time 0.049422553850017724 iter num 100\n",
            "loss 0.0043786559253931046 average time 0.10524481365000611 iter num 20\n",
            "loss 0.003973654471337795 average time 0.07009247829998913 iter num 40\n",
            "loss 0.002345878165215254 average time 0.05800100264996218 iter num 60\n",
            "loss 0.002290021162480116 average time 0.05211091674995032 iter num 80\n",
            "loss 0.0023571124766021967 average time 0.04853559016995859 iter num 100\n",
            "loss 0.0017364781815558672 average time 0.1052141317499263 iter num 20\n",
            "loss 0.002719908719882369 average time 0.07008152537489423 iter num 40\n",
            "loss 0.0026545103173702955 average time 0.05877160171659549 iter num 60\n",
            "loss 0.003110316349193454 average time 0.05303492092496072 iter num 80\n",
            "loss 0.004311221651732922 average time 0.04928016483996544 iter num 100\n",
            "loss 0.002496632281690836 average time 0.10499156904998017 iter num 20\n",
            "loss 0.004380126018077135 average time 0.06950374615000783 iter num 40\n",
            "loss 0.0013701517600566149 average time 0.05804127463338773 iter num 60\n",
            "loss 0.002503449097275734 average time 0.05212340667504804 iter num 80\n",
            "loss 0.0015330059686675668 average time 0.04835453852001592 iter num 100\n",
            "loss 0.004166906699538231 average time 0.10747021305005547 iter num 20\n",
            "loss 0.002168473321944475 average time 0.07132619702506418 iter num 40\n",
            "loss 0.0012934613041579723 average time 0.05892900506675384 iter num 60\n",
            "loss 0.003218797268345952 average time 0.052929367650074256 iter num 80\n",
            "loss 0.0036511323414742947 average time 0.04893024373004664 iter num 100\n",
            "loss 0.002072197152301669 average time 0.10663701324992872 iter num 20\n",
            "loss 0.002978949574753642 average time 0.06966732594996757 iter num 40\n",
            "loss 0.0033028621692210436 average time 0.05790700276664514 iter num 60\n",
            "loss 0.0038157158996909857 average time 0.05221273771248889 iter num 80\n",
            "loss 0.0019617967773228884 average time 0.04850969422996968 iter num 100\n",
            "loss 0.002475403482094407 average time 0.10552561764998244 iter num 20\n",
            "loss 0.002307331655174494 average time 0.07031417154996689 iter num 40\n",
            "loss 0.0037989541888237 average time 0.05842501216660215 iter num 60\n",
            "loss 0.0037322910502552986 average time 0.05236785854996242 iter num 80\n",
            "loss 0.0026000184006989002 average time 0.048677854129973636 iter num 100\n",
            "loss 0.002215147949755192 average time 0.10792654885003686 iter num 20\n",
            "loss 0.0011928683379665017 average time 0.0717811944000232 iter num 40\n",
            "loss 0.002243942115455866 average time 0.059467410000024756 iter num 60\n",
            "loss 0.004119665827602148 average time 0.053208800587492534 iter num 80\n",
            "loss 0.004815917927771807 average time 0.04934606689999782 iter num 100\n",
            "loss 0.001754437922500074 average time 0.10473942879993955 iter num 20\n",
            "loss 0.0009923011530190706 average time 0.06996171247494658 iter num 40\n",
            "loss 0.0029045292176306248 average time 0.05793842383325985 iter num 60\n",
            "loss 0.0013684270670637488 average time 0.05192075713742952 iter num 80\n",
            "loss 0.0023011579178273678 average time 0.04868425481996383 iter num 100\n",
            "loss 0.0025779257994145155 average time 0.10786932589999196 iter num 20\n",
            "loss 0.0032565253786742687 average time 0.07106954577498073 iter num 40\n",
            "loss 0.002435786183923483 average time 0.05917513968327815 iter num 60\n",
            "loss 0.003089764155447483 average time 0.05279230942495587 iter num 80\n",
            "loss 0.0011860009981319308 average time 0.04885079564995976 iter num 100\n",
            "loss 0.002299020066857338 average time 0.10669633229999817 iter num 20\n",
            "loss 0.0008221960160881281 average time 0.07058050550001553 iter num 40\n",
            "loss 0.0023288794327527285 average time 0.05839571681667621 iter num 60\n",
            "loss 0.002809982979670167 average time 0.05247495630000003 iter num 80\n",
            "loss 0.0017393893795087934 average time 0.0487243759699777 iter num 100\n",
            "loss 0.0023043937981128693 average time 0.10501171775008515 iter num 20\n",
            "loss 0.0017211723607033491 average time 0.06983692059995975 iter num 40\n",
            "loss 0.0015233199810609221 average time 0.05794619691666109 iter num 60\n",
            "loss 0.0025426058564335108 average time 0.05203344027501089 iter num 80\n",
            "loss 0.0022070398554205894 average time 0.04853750683999351 iter num 100\n",
            "loss 0.0018411024939268827 average time 0.10888917360002778 iter num 20\n",
            "loss 0.0015677642077207565 average time 0.0707282084000326 iter num 40\n",
            "loss 0.0009587917011231184 average time 0.058624715900032244 iter num 60\n",
            "loss 0.0010138354264199734 average time 0.052300216862511205 iter num 80\n",
            "loss 0.0012499310541898012 average time 0.048848848970010295 iter num 100\n",
            "loss 0.002050220500677824 average time 0.10511797945000581 iter num 20\n",
            "loss 0.0013442402705550194 average time 0.0693625210500386 iter num 40\n",
            "loss 0.0010692240903154016 average time 0.05799072234998069 iter num 60\n",
            "loss 0.0013137373607605696 average time 0.05197276981247114 iter num 80\n",
            "loss 0.0016652082558721304 average time 0.04852576667997709 iter num 100\n",
            "loss 0.0004622959531843662 average time 0.10598395374995562 iter num 20\n",
            "loss 0.003756537800654769 average time 0.06956802397498904 iter num 40\n",
            "loss 0.001442264299839735 average time 0.057590450716679696 iter num 60\n",
            "loss 0.00124460703227669 average time 0.051500070862493884 iter num 80\n",
            "loss 0.0016462422208860517 average time 0.04832372433001183 iter num 100\n",
            "loss 0.0013576480560004711 average time 0.10655209865003598 iter num 20\n",
            "loss 0.0013090460561215878 average time 0.07015007415000127 iter num 40\n",
            "loss 0.0010177436051890254 average time 0.05820834451665178 iter num 60\n",
            "loss 0.000680448254570365 average time 0.05260502777496186 iter num 80\n",
            "loss 0.0007278565317392349 average time 0.049026380949972005 iter num 100\n",
            "loss 0.0006649686256423593 average time 0.10526044900002489 iter num 20\n",
            "loss 0.0009251249139197171 average time 0.06949136677499154 iter num 40\n",
            "loss 0.0018679939676076174 average time 0.05788487198333314 iter num 60\n",
            "loss 0.0017057708464562893 average time 0.05193293443753646 iter num 80\n",
            "loss 0.0011589829809963703 average time 0.04838750384003106 iter num 100\n",
            "loss 0.0011361438082531095 average time 0.10701054179999119 iter num 20\n",
            "loss 0.0010740433353930712 average time 0.0702532754000913 iter num 40\n",
            "loss 0.0004949399153701961 average time 0.05772721936673406 iter num 60\n",
            "loss 0.0013018893077969551 average time 0.05172023666256109 iter num 80\n",
            "loss 0.001354408566839993 average time 0.04812845656003446 iter num 100\n",
            "loss 0.0008593675447627902 average time 0.10691798385005313 iter num 20\n",
            "loss 0.00045717478496953845 average time 0.07059586712498458 iter num 40\n",
            "loss 0.0015298016369342804 average time 0.059026003666698065 iter num 60\n",
            "loss 0.0008477882947772741 average time 0.05277120506248138 iter num 80\n",
            "loss 0.00044021615758538246 average time 0.04904593692998788 iter num 100\n",
            "loss 0.0011612627422437072 average time 0.10427351415000885 iter num 20\n",
            "loss 0.0005508590256795287 average time 0.06925127079998675 iter num 40\n",
            "loss 0.0008696636068634689 average time 0.0574771551333015 iter num 60\n",
            "loss 0.0008103192085400224 average time 0.051821936674980404 iter num 80\n",
            "loss 0.0007791981333866715 average time 0.0483952193699497 iter num 100\n",
            "loss 0.0003914883127436042 average time 0.10880695404998733 iter num 20\n",
            "loss 0.000621327490080148 average time 0.07132984302500063 iter num 40\n",
            "loss 0.000582728476729244 average time 0.05905347869999484 iter num 60\n",
            "loss 0.0008177179261110723 average time 0.05298308275001773 iter num 80\n",
            "loss 0.0007173529011197388 average time 0.049811953560019905 iter num 100\n",
            "loss 0.0022098810877650976 average time 0.10631450219998442 iter num 20\n",
            "loss 0.000640102312900126 average time 0.07022762514995975 iter num 40\n",
            "loss 0.001488308422267437 average time 0.058096241783308265 iter num 60\n",
            "loss 0.0012411840725690126 average time 0.05211082918751799 iter num 80\n",
            "loss 0.0011423375690355897 average time 0.048627479620026864 iter num 100\n",
            "loss 0.0006819737609475851 average time 0.10815192059994842 iter num 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b20ab2470e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m           \u001b[0;31m################################################################################################### generate random input numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m           \u001b[0minitial_stocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.75\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m           \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# assume no correlation between stocks here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   3567\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected input type for array: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0;32m--> 461\u001b[0;31m                                        weak_type=new_weak_type)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbitcast_convert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    266\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b9437c-ce4d-4037-8b64-c61d37826d12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_1110_v1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Lilian/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d586624d-5725-4836-f930-ff8d0cf9606a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fad887-f4a2-4b08-cba4-6876602c5131"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_1110_v1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Lilian/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101584b7-19b7-40c9-fa08-cce11353a9b0"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43b79fd-1346-40ec-a2ca-3aeda7936783"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1000000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    adjusted_y2 = (1000 * y)**2\n",
        "    loss_weight = 1/adjusted_y2.mean(axis=0)\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "\n",
        "\n",
        "\n",
        "    # loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    # loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 10\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 30)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_1110_v1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Lilian/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.0006902356981299818 average time 0.26826680850008416 iter num 10\n",
            "loss 0.0005746527458541095 average time 0.1396888278001825 iter num 20\n",
            "loss 0.0004123586986679584 average time 0.09700819403357551 iter num 30\n",
            "loss 0.0003462013555690646 average time 0.07547847060013738 iter num 40\n",
            "loss 0.00019279090338386595 average time 0.06254587762010487 iter num 50\n",
            "loss 0.00034235749626532197 average time 0.05402904795006786 iter num 60\n",
            "loss 0.00015976096619851887 average time 0.047949026485808385 iter num 70\n",
            "loss 0.0007852234411984682 average time 0.04334835073757404 iter num 80\n",
            "loss 0.0002606419730000198 average time 0.039807441588972224 iter num 90\n",
            "loss 0.00011740873014787212 average time 0.03689482004008823 iter num 100\n",
            "loss 0.0008124981541186571 average time 0.17806545480016212 iter num 10\n",
            "loss 0.0001834932336350903 average time 0.09445950410008663 iter num 20\n",
            "loss 8.801493095234036e-05 average time 0.06669139303327636 iter num 30\n",
            "loss 0.0004120481316931546 average time 0.05293193344982683 iter num 40\n",
            "loss 0.0002763890952337533 average time 0.04474497773990151 iter num 50\n",
            "loss 0.00023447118292097002 average time 0.039119879199915886 iter num 60\n",
            "loss 0.00031656655482947826 average time 0.03520405068557011 iter num 70\n",
            "loss 0.00018800901307258755 average time 0.03219772743743761 iter num 80\n",
            "loss 0.0002611143863759935 average time 0.029931055377690224 iter num 90\n",
            "loss 0.00038751165266148746 average time 0.02805444945988711 iter num 100\n",
            "loss 7.443757203873247e-05 average time 0.17913029499977712 iter num 10\n",
            "loss 0.000503836665302515 average time 0.0950246206999509 iter num 20\n",
            "loss 0.00029045407427474856 average time 0.06705861233331234 iter num 30\n",
            "loss 0.00024992780527099967 average time 0.05306699960001424 iter num 40\n",
            "loss 0.00046368324547074735 average time 0.04463609987997188 iter num 50\n",
            "loss 0.0004185809812042862 average time 0.039044049316665524 iter num 60\n",
            "loss 0.00024174389545805752 average time 0.03511332218567986 iter num 70\n",
            "loss 0.00035104440758004785 average time 0.032231016762443686 iter num 80\n",
            "loss 0.00011401978554204106 average time 0.029939733099955547 iter num 90\n",
            "loss 0.0003571890702005476 average time 0.02819252774997949 iter num 100\n",
            "loss 0.00022354756947606802 average time 0.1784086371999365 iter num 10\n",
            "loss 0.0005826161941513419 average time 0.09504241019976689 iter num 20\n",
            "loss 0.0004764630284626037 average time 0.0670562471999195 iter num 30\n",
            "loss 0.00029512197943404317 average time 0.05327071054980479 iter num 40\n",
            "loss 0.00016463984502479434 average time 0.04471560877980665 iter num 50\n",
            "loss 0.0001141277389251627 average time 0.039040768883175286 iter num 60\n",
            "loss 0.000350325892213732 average time 0.03505080255698496 iter num 70\n",
            "loss 0.0001879154733614996 average time 0.03212749624985918 iter num 80\n",
            "loss 0.0003326187434140593 average time 0.029851724822098427 iter num 90\n",
            "loss 0.00019970376160927117 average time 0.027967195549899772 iter num 100\n",
            "loss 0.0020465124398469925 average time 0.17914217340003233 iter num 10\n",
            "loss 0.00015382218407467008 average time 0.09520583895027812 iter num 20\n",
            "loss 0.00027355083147995174 average time 0.06752704193349927 iter num 30\n",
            "loss 0.00021616832236759365 average time 0.05371263157521753 iter num 40\n",
            "loss 0.0003140220360364765 average time 0.04523438110016286 iter num 50\n",
            "loss 0.00059072783915326 average time 0.039570721466831556 iter num 60\n",
            "loss 9.832724754232913e-05 average time 0.03551145225732658 iter num 70\n",
            "loss 9.039165888680145e-05 average time 0.03243564130018513 iter num 80\n",
            "loss 0.00016632290498819202 average time 0.030109897855739594 iter num 90\n",
            "loss 0.00021640065824612975 average time 0.02818171118015016 iter num 100\n",
            "loss 0.00011774100858019665 average time 0.1778164156998173 iter num 10\n",
            "loss 0.000503423681948334 average time 0.09469256294987645 iter num 20\n",
            "loss 0.00026205461472272873 average time 0.06671501699996346 iter num 30\n",
            "loss 0.00020497755031101406 average time 0.0527396850250625 iter num 40\n",
            "loss 0.0008887185249477625 average time 0.044646778420064946 iter num 50\n",
            "loss 0.0001785052299965173 average time 0.03906962013343218 iter num 60\n",
            "loss 0.00010796642891364172 average time 0.035015327228653144 iter num 70\n",
            "loss 0.00011106471356470138 average time 0.03213001833757971 iter num 80\n",
            "loss 0.00015717445057816803 average time 0.029830835677906484 iter num 90\n",
            "loss 0.00020221428712829947 average time 0.027907942730125798 iter num 100\n",
            "loss 0.0001587029837537557 average time 0.1777896014000362 iter num 10\n",
            "loss 0.0003816764510702342 average time 0.09436019390004731 iter num 20\n",
            "loss 0.0014411057345569134 average time 0.06644826833332142 iter num 30\n",
            "loss 0.00020912219770252705 average time 0.052500950574949454 iter num 40\n",
            "loss 0.0005040926625952125 average time 0.04420605790004629 iter num 50\n",
            "loss 0.0023090261965990067 average time 0.038669105500018 iter num 60\n",
            "loss 6.226987898116931e-05 average time 0.03471750162856811 iter num 70\n",
            "loss 0.00010549540456850082 average time 0.03173961979998694 iter num 80\n",
            "loss 0.00027624209178611636 average time 0.02941379942221829 iter num 90\n",
            "loss 0.00017000315710902214 average time 0.02754088778003279 iter num 100\n",
            "loss 0.0018550546374171972 average time 0.17807008990002943 iter num 10\n",
            "loss 0.00047620589612051845 average time 0.0947152678000748 iter num 20\n",
            "loss 0.0001308208011323586 average time 0.0668737681999725 iter num 30\n",
            "loss 0.000285808666376397 average time 0.0529897470000833 iter num 40\n",
            "loss 0.00028948401450179517 average time 0.04452945834007551 iter num 50\n",
            "loss 0.0006130876136012375 average time 0.03889733628345008 iter num 60\n",
            "loss 0.0001378487650072202 average time 0.03490221280000177 iter num 70\n",
            "loss 0.00022254607756622136 average time 0.031949298687459306 iter num 80\n",
            "loss 0.00034026126377284527 average time 0.029561202877731476 iter num 90\n",
            "loss 0.00032545512658543885 average time 0.027723629779939075 iter num 100\n",
            "loss 0.00027227471582591534 average time 0.1781829731000471 iter num 10\n",
            "loss 0.00013960781507194042 average time 0.09508456240000669 iter num 20\n",
            "loss 0.000280268577625975 average time 0.0669248644999243 iter num 30\n",
            "loss 0.00026330462424084544 average time 0.05325123592492673 iter num 40\n",
            "loss 0.0002133758825948462 average time 0.04488377485995443 iter num 50\n",
            "loss 0.00032089470187202096 average time 0.03934611583335936 iter num 60\n",
            "loss 0.00027896647225134075 average time 0.03547888755718824 iter num 70\n",
            "loss 0.0001910532300826162 average time 0.03236934785004451 iter num 80\n",
            "loss 0.004132985603064299 average time 0.029973395833399585 iter num 90\n",
            "loss 0.00021409431064967066 average time 0.028218362540083034 iter num 100\n",
            "loss 0.00012036757834721357 average time 0.17804413460016805 iter num 10\n",
            "loss 0.00020059644884895533 average time 0.0948196487001951 iter num 20\n",
            "loss 0.00014247554645407945 average time 0.06699946626667952 iter num 30\n",
            "loss 0.00021107602515257895 average time 0.05308017984984872 iter num 40\n",
            "loss 0.0010242353891953826 average time 0.044744919080003456 iter num 50\n",
            "loss 0.00014330906560644507 average time 0.03910830434994447 iter num 60\n",
            "loss 0.00020163528097327799 average time 0.0352404836142471 iter num 70\n",
            "loss 0.0005109311314299703 average time 0.032284300724973035 iter num 80\n",
            "loss 0.00012354082718957216 average time 0.02995281285550138 iter num 90\n",
            "loss 0.0002647261426318437 average time 0.028084443559873763 iter num 100\n",
            "loss 0.0015161106130108237 average time 0.17904937530001916 iter num 10\n",
            "loss 0.00031296475208364427 average time 0.09558792084953893 iter num 20\n",
            "loss 0.00018900456780102104 average time 0.06724527406646909 iter num 30\n",
            "loss 0.0003204687964171171 average time 0.05312650522500917 iter num 40\n",
            "loss 0.0008328414405696094 average time 0.04466087095992407 iter num 50\n",
            "loss 0.00019242856069467962 average time 0.039161345383278486 iter num 60\n",
            "loss 0.00015040392463561147 average time 0.03511646725699913 iter num 70\n",
            "loss 0.00022481003543362021 average time 0.03211692834988753 iter num 80\n",
            "loss 0.0007198295788839459 average time 0.029744801988797715 iter num 90\n",
            "loss 0.0013658481184393167 average time 0.02789095394000469 iter num 100\n",
            "loss 0.00022088394325692207 average time 0.17844827379994968 iter num 10\n",
            "loss 0.00022751868527848274 average time 0.09471187289982481 iter num 20\n",
            "loss 0.00023387531109619886 average time 0.06685771289994591 iter num 30\n",
            "loss 0.0003596868773456663 average time 0.05275336969998534 iter num 40\n",
            "loss 0.00011638292926363647 average time 0.044302834700065435 iter num 50\n",
            "loss 0.00011580443242564797 average time 0.03871484215002662 iter num 60\n",
            "loss 0.00027711366419680417 average time 0.03482088734292899 iter num 70\n",
            "loss 0.000652438378892839 average time 0.03179715303758712 iter num 80\n",
            "loss 0.00011131335486425087 average time 0.029496563177922833 iter num 90\n",
            "loss 0.00019309151684865355 average time 0.02764254684007028 iter num 100\n",
            "loss 0.0001838773168856278 average time 0.17833673159984756 iter num 10\n",
            "loss 0.00028134844615124166 average time 0.0948345328504729 iter num 20\n",
            "loss 0.000130749904201366 average time 0.06694475756727722 iter num 30\n",
            "loss 8.777810580795631e-05 average time 0.05292886475062915 iter num 40\n",
            "loss 0.00022477276797872037 average time 0.044447181300565713 iter num 50\n",
            "loss 0.00019473581050988287 average time 0.038779439633860116 iter num 60\n",
            "loss 0.00030112621607258916 average time 0.034798347043137515 iter num 70\n",
            "loss 0.0001434377336408943 average time 0.03185579847522604 iter num 80\n",
            "loss 0.0003011317749042064 average time 0.029573388455780027 iter num 90\n",
            "loss 0.00045475707156583667 average time 0.027692680620202737 iter num 100\n",
            "loss 0.00019277870887890458 average time 0.17817695679987083 iter num 10\n",
            "loss 0.00045389338629320264 average time 0.0945233215998087 iter num 20\n",
            "loss 0.0006628325791098177 average time 0.06687010256682697 iter num 30\n",
            "loss 0.00010031391138909385 average time 0.053032844950030265 iter num 40\n",
            "loss 0.0010923901572823524 average time 0.04462340439997206 iter num 50\n",
            "loss 0.0001724804169498384 average time 0.039000710716694205 iter num 60\n",
            "loss 0.00012642837828025222 average time 0.034977187914150165 iter num 70\n",
            "loss 9.161543130176142e-05 average time 0.03204194928744073 iter num 80\n",
            "loss 0.0005512015777640045 average time 0.02964530127764091 iter num 90\n",
            "loss 0.00026825405075214803 average time 0.02776115286986169 iter num 100\n",
            "loss 0.00013510706776287407 average time 0.17846500870000453 iter num 10\n",
            "loss 0.00030797335784882307 average time 0.09481119359988952 iter num 20\n",
            "loss 0.00016030273400247097 average time 0.06688603799981745 iter num 30\n",
            "loss 0.00014467233268078417 average time 0.05294570047499292 iter num 40\n",
            "loss 0.0001487984845880419 average time 0.044545122220006304 iter num 50\n",
            "loss 0.0002125088358297944 average time 0.0389314462000281 iter num 60\n",
            "loss 0.0002098358527291566 average time 0.034970447642886676 iter num 70\n",
            "loss 0.0004547367279883474 average time 0.031972195375010413 iter num 80\n",
            "loss 0.00010313121310900897 average time 0.029584303633317984 iter num 90\n",
            "loss 0.00010639175889082253 average time 0.02778081527001632 iter num 100\n",
            "loss 0.0002067643217742443 average time 0.17808961260052455 iter num 10\n",
            "loss 0.00020603871962521225 average time 0.09497273250017316 iter num 20\n",
            "loss 0.0002320498606422916 average time 0.06694552676684301 iter num 30\n",
            "loss 0.00018417139654047787 average time 0.052938886175070365 iter num 40\n",
            "loss 0.00012532483378890902 average time 0.04458056065988785 iter num 50\n",
            "loss 0.0001607805461389944 average time 0.03900479941651914 iter num 60\n",
            "loss 0.0010185643332079053 average time 0.03521955527139653 iter num 70\n",
            "loss 9.558141027810052e-05 average time 0.03221267167505175 iter num 80\n",
            "loss 0.0007090366561897099 average time 0.029835844699967615 iter num 90\n",
            "loss 7.381101022474468e-05 average time 0.027974674119905105 iter num 100\n",
            "loss 0.00018752081086859107 average time 0.17861169050011086 iter num 10\n",
            "loss 0.0002550136996433139 average time 0.09510543604992563 iter num 20\n",
            "loss 0.00028286868473514915 average time 0.06709290503325367 iter num 30\n",
            "loss 0.00022334925597533584 average time 0.0532496495749001 iter num 40\n",
            "loss 0.00014561334683094174 average time 0.044810417459957536 iter num 50\n",
            "loss 0.0003808039764408022 average time 0.03914795148320991 iter num 60\n",
            "loss 0.00038519426016137004 average time 0.0351178177141784 iter num 70\n",
            "loss 0.00020634396059904248 average time 0.03213791983744159 iter num 80\n",
            "loss 0.0002487791352905333 average time 0.02980644984434346 iter num 90\n",
            "loss 0.00010631349869072437 average time 0.02789735069989547 iter num 100\n",
            "loss 0.0002603838802315295 average time 0.17729327119923255 iter num 10\n",
            "loss 0.000799102068413049 average time 0.094361631099855 iter num 20\n",
            "loss 0.0019375442061573267 average time 0.06644694813306463 iter num 30\n",
            "loss 0.0002325130917597562 average time 0.05254621332469469 iter num 40\n",
            "loss 0.00047353957779705524 average time 0.04421875073989213 iter num 50\n",
            "loss 4.75144297524821e-05 average time 0.038713056066505186 iter num 60\n",
            "loss 0.00014093046775087714 average time 0.03476461948562896 iter num 70\n",
            "loss 0.0003319245879538357 average time 0.03185139098745822 iter num 80\n",
            "loss 0.00016055963351391256 average time 0.029594703533237204 iter num 90\n",
            "loss 0.00019601700478233397 average time 0.027757733169855782 iter num 100\n",
            "loss 0.00013808759103994817 average time 0.17777564070056542 iter num 10\n",
            "loss 0.0004249243065714836 average time 0.09437774460056972 iter num 20\n",
            "loss 6.93373367539607e-05 average time 0.0664968693672563 iter num 30\n",
            "loss 0.0002602706372272223 average time 0.05271750080055426 iter num 40\n",
            "loss 0.00033614676794968545 average time 0.04424649760047032 iter num 50\n",
            "loss 6.98705916875042e-05 average time 0.038707348650495986 iter num 60\n",
            "loss 0.0002816296764649451 average time 0.03476159001475025 iter num 70\n",
            "loss 7.634351641172543e-05 average time 0.03178242868798407 iter num 80\n",
            "loss 0.00021696787734981626 average time 0.029517792378222416 iter num 90\n",
            "loss 0.00013382697943598032 average time 0.027642789970341256 iter num 100\n",
            "loss 0.000432920380262658 average time 0.17785607719888502 iter num 10\n",
            "loss 0.0002197756402892992 average time 0.09429546674928133 iter num 20\n",
            "loss 0.00012565619545057416 average time 0.06672333896603959 iter num 30\n",
            "loss 9.982792835216969e-05 average time 0.053160030374328926 iter num 40\n",
            "loss 7.451140118064359e-05 average time 0.04473059805939556 iter num 50\n",
            "loss 0.0003180531784892082 average time 0.03913567031619702 iter num 60\n",
            "loss 0.0009156091837212443 average time 0.03503656597100157 iter num 70\n",
            "loss 6.783376738894731e-05 average time 0.03209266609956103 iter num 80\n",
            "loss 0.0002535328094381839 average time 0.029780982766250318 iter num 90\n",
            "loss 0.00016270804917439818 average time 0.02789913701963087 iter num 100\n",
            "loss 0.00026710747624747455 average time 0.1777416094992077 iter num 10\n",
            "loss 0.0006794781074859202 average time 0.09430074764986784 iter num 20\n",
            "loss 0.0001973160105990246 average time 0.066533771666703 iter num 30\n",
            "loss 0.00025240686954930425 average time 0.05270840987477641 iter num 40\n",
            "loss 0.00018619984621182084 average time 0.04441138343972852 iter num 50\n",
            "loss 7.091530278557912e-05 average time 0.03889572436643599 iter num 60\n",
            "loss 0.0001693315280135721 average time 0.03497511808553619 iter num 70\n",
            "loss 0.0003534375864546746 average time 0.0319541146748179 iter num 80\n",
            "loss 0.0001590790634509176 average time 0.029630351733107494 iter num 90\n",
            "loss 6.013227175571956e-05 average time 0.02776612547972036 iter num 100\n",
            "loss 0.00026451615849509835 average time 0.17927099489934334 iter num 10\n",
            "loss 0.00028383213793858886 average time 0.0952775831492545 iter num 20\n",
            "loss 0.00016455129662062973 average time 0.06721325006607609 iter num 30\n",
            "loss 0.0001496101322118193 average time 0.05306549464939962 iter num 40\n",
            "loss 6.288442818913609e-05 average time 0.04463859169940406 iter num 50\n",
            "loss 0.00024699914501979947 average time 0.039036687032724636 iter num 60\n",
            "loss 0.00026082355179823935 average time 0.035107985613753306 iter num 70\n",
            "loss 8.522068674210459e-05 average time 0.03204682628702358 iter num 80\n",
            "loss 8.04060255177319e-05 average time 0.029755149810686513 iter num 90\n",
            "loss 0.00011060794349759817 average time 0.027882485339650886 iter num 100\n",
            "loss 0.00020759932522196323 average time 0.17860686640015047 iter num 10\n",
            "loss 0.0001977363572223112 average time 0.09480726394976954 iter num 20\n",
            "loss 0.0007664877339266241 average time 0.06673242259991335 iter num 30\n",
            "loss 0.00017826736439019442 average time 0.05279036044985332 iter num 40\n",
            "loss 0.00018849142361432314 average time 0.044428772179962835 iter num 50\n",
            "loss 0.00017617325647734106 average time 0.038859555916557535 iter num 60\n",
            "loss 0.0001813655107980594 average time 0.0349289964998856 iter num 70\n",
            "loss 9.376979141961783e-05 average time 0.03190461969998069 iter num 80\n",
            "loss 0.0002472331980243325 average time 0.029566138466624477 iter num 90\n",
            "loss 0.0001840269978856668 average time 0.027706913110050663 iter num 100\n",
            "loss 0.00017900620878208429 average time 0.177483677199416 iter num 10\n",
            "loss 0.0016933914739638567 average time 0.09400447499974689 iter num 20\n",
            "loss 0.00014489420573227108 average time 0.06628240339984283 iter num 30\n",
            "loss 0.0012443283339962363 average time 0.052609532124915856 iter num 40\n",
            "loss 0.00010355045378673822 average time 0.04424623361999693 iter num 50\n",
            "loss 6.385933374986053e-05 average time 0.038711345466860315 iter num 60\n",
            "loss 0.00012374116340652108 average time 0.03478690910014848 iter num 70\n",
            "loss 7.117202039808035e-05 average time 0.03183600637516974 iter num 80\n",
            "loss 0.0001848233223427087 average time 0.029543425000156276 iter num 90\n",
            "loss 0.00016345753101632 average time 0.027693913940165658 iter num 100\n",
            "loss 0.0002830038720276207 average time 0.17778001020014927 iter num 10\n",
            "loss 0.0003829400520771742 average time 0.09416611495016695 iter num 20\n",
            "loss 0.0003298160736449063 average time 0.06636300360017534 iter num 30\n",
            "loss 0.0006992710987105966 average time 0.05248856447533399 iter num 40\n",
            "loss 0.00013574821059592068 average time 0.04426956830022391 iter num 50\n",
            "loss 7.344334881054237e-05 average time 0.03895828576690595 iter num 60\n",
            "loss 0.00010135979391634464 average time 0.03491096772865733 iter num 70\n",
            "loss 0.00023593602236360312 average time 0.03191684302514659 iter num 80\n",
            "loss 9.793361823540181e-05 average time 0.02957461328907003 iter num 90\n",
            "loss 0.0001997561048483476 average time 0.02766156824021891 iter num 100\n",
            "loss 0.00022203696426004171 average time 0.1771590666998236 iter num 10\n",
            "loss 0.0009292394388467073 average time 0.09390920859987091 iter num 20\n",
            "loss 0.00022577606432605535 average time 0.0661708419998225 iter num 30\n",
            "loss 9.062891331268474e-05 average time 0.05234154209974804 iter num 40\n",
            "loss 0.00022144011745695025 average time 0.044073457139893434 iter num 50\n",
            "loss 0.0003367143508512527 average time 0.03860259469984158 iter num 60\n",
            "loss 0.00022433810227084905 average time 0.03460059899994771 iter num 70\n",
            "loss 5.6351487728534266e-05 average time 0.03163748456249778 iter num 80\n",
            "loss 0.00017620009020902216 average time 0.029369878077745347 iter num 90\n",
            "loss 0.0002136995317414403 average time 0.027502003939989663 iter num 100\n",
            "loss 0.00019220772082917392 average time 0.1781447543999093 iter num 10\n",
            "loss 0.0001697771076578647 average time 0.09440301804988849 iter num 20\n",
            "loss 7.153472688514739e-05 average time 0.06660458770008214 iter num 30\n",
            "loss 0.00016377586871385574 average time 0.052910100500139376 iter num 40\n",
            "loss 0.0003550277615431696 average time 0.04448407006020716 iter num 50\n",
            "loss 0.00017590235802344978 average time 0.03924047300018477 iter num 60\n",
            "loss 0.00010195837239734828 average time 0.03526638992869786 iter num 70\n",
            "loss 0.0002558093110565096 average time 0.03227581425007884 iter num 80\n",
            "loss 0.002368694404140115 average time 0.029899276600068938 iter num 90\n",
            "loss 0.0001220427657244727 average time 0.027965512670125463 iter num 100\n",
            "loss 0.00022335848188959062 average time 0.17740506230002212 iter num 10\n",
            "loss 8.645043999422342e-05 average time 0.09410607015033748 iter num 20\n",
            "loss 0.0006728514563292265 average time 0.06633477930020794 iter num 30\n",
            "loss 0.0002710979024413973 average time 0.052645046274938066 iter num 40\n",
            "loss 5.8289697335567325e-05 average time 0.04446719401996233 iter num 50\n",
            "loss 0.00011838346108561382 average time 0.038822205533263814 iter num 60\n",
            "loss 0.0001610722829354927 average time 0.03475685755698318 iter num 70\n",
            "loss 0.00018787579028867185 average time 0.03179565742484556 iter num 80\n",
            "loss 4.897154576610774e-05 average time 0.029439235166501022 iter num 90\n",
            "loss 0.00013442951603792608 average time 0.027607380659901537 iter num 100\n",
            "loss 0.00015660384087823331 average time 0.17768148000031941 iter num 10\n",
            "loss 0.00022838670702185482 average time 0.09427821579993179 iter num 20\n",
            "loss 0.0002307652757735923 average time 0.06654326159999376 iter num 30\n",
            "loss 0.00021325043053366244 average time 0.05253374827498192 iter num 40\n",
            "loss 0.00010066550021292642 average time 0.0441318517801119 iter num 50\n",
            "loss 0.00012474466348066926 average time 0.038611195649900766 iter num 60\n",
            "loss 0.00012693429016508162 average time 0.03463762614275245 iter num 70\n",
            "loss 0.0001393317070323974 average time 0.03159391279987176 iter num 80\n",
            "loss 7.988492143340409e-05 average time 0.02929037618871209 iter num 90\n",
            "loss 0.0004806411743629724 average time 0.027489609549775196 iter num 100\n",
            "loss 0.00028975046006962657 average time 0.17690180900026461 iter num 10\n",
            "loss 0.00018555193673819304 average time 0.09413525145009771 iter num 20\n",
            "loss 0.0002726188104134053 average time 0.06633008720006425 iter num 30\n",
            "loss 0.00015866351895965636 average time 0.0525396661000741 iter num 40\n",
            "loss 8.811919542495161e-05 average time 0.04416634470013378 iter num 50\n",
            "loss 0.0002090418856823817 average time 0.03861602158334184 iter num 60\n",
            "loss 9.893305832520127e-05 average time 0.0345879450571374 iter num 70\n",
            "loss 6.110104004619643e-05 average time 0.03160105944998577 iter num 80\n",
            "loss 0.00029449089197441936 average time 0.02927622182218733 iter num 90\n",
            "loss 0.0002009834279306233 average time 0.027432951749906353 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF4hB0bN788M"
      },
      "source": [
        "model_save_name = 'jax_european_1stock_1110_v1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/Lilian/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019500b3-4640-4902-e1d8-b78ba87bfde1"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.3, 0.3]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.27130044, 0.90763223)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.2729]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9024], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2AXrPt7bNj",
        "outputId": "4d65482a-51f3-427f-de4d-6eff289036c2"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.3]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.0]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27108762\n",
            "[0.9071957]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lovJwXo3-YEu",
        "outputId": "b3e6a6c1-6aa8-4b6c-be1b-4d4149cea5ec"
      },
      "source": [
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "BS_call_prices = []\n",
        "for p in prices:\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    BS_call_prices.append(bs_call(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, BS_call_prices, label = \"BS_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(BS_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lwApH0GT9bBK",
        "outputId": "433ab148-3844-41ff-a07e-e2784be03a70"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVxU1f/H8ddh2EQ2AXdEcRd3w30ptdzLNM201LQ9M7PdFjV/tn6rb4u2WJllaq6Z5lZuuaeoiIobKiqIoiCg7Myc3x+X/KKiLA4MDJ/n4zEPZuZe5n4u0Lvjueecq7TWCCGEKP0cbF2AEEII65BAF0IIOyGBLoQQdkICXQgh7IQEuhBC2AlHWx3Yz89P16pVy1aHF0KIUmn37t0XtdYVc9tms0CvVasWISEhtjq8EEKUSkqpUzfblmeXi1JqplIqVil14CbblVLqC6VUhFIqTCnV6naKFUIIUTj56UOfBfS6xfbeQL3sx5PA17dflhBCiILKM9C11puA+Fvs0h/4WRt2AN5KqarWKlAIIUT+WKMPvTpwJsfrqOz3Yq7fUSn1JEYrnoCAgBs+KDMzk6ioKNLS0qxQlijpXF1d8ff3x8nJydalCGEXivWiqNZ6BjADIDg4+IZFZKKiovDw8KBWrVoopYqzNFHMtNbExcURFRVFYGCgrcsRwi5YYxx6NFAjx2v/7PcKLC0tDV9fXwnzMkApha+vr/xrTAgrskagLwNGZI92aQckaq1v6G7JLwnzskN+10JYV55dLkqpecBdgJ9SKgqYBDgBaK2/AVYCfYAIIAUYVVTFCiHKDq01KRlmLqdlkZSWSVJqJqmZZjLNFjKyNJlmS/ZzC5kWjdlsIcuiybJozBZju8WS3bOrFA4KFAqlQAEmk8LRQWFycMCkwGRyyH6tMCmFo0nhoIx9HBwUbs4mvMs54+3mhLebE+4ujiWuUZJnoGuth+axXQNjrFaREMJuWSyai8npnEtMIyYxjQuX07l4JftxOePq88TUTJLSsjBbSu79GkwOCu9yTvi5u+BfoRw1fNwI8HHL8bUcbs7FO3fTZjNFy4J/Z8P6+fnd1j75NWvWLEJCQpg2bRqTJ0/G3d2dl19+Oc/vi4yMpF+/fhw4kOvcsRv2CQ0N5ezZs/Tp0+e2axb2xWLRnEtK41RcCqfjkzkdn8KZ+FRiElOJSUzjfFIameYbQ7qCmxGMfu4uNPX3xrucE57lHPF0dcLD1Xju4epEeWcTTiYHnEwOODsqnE0mnBwVjg4OOJmM1rWjg0P2V6Nl/S+tNRZtfNWAObsln3X1q+Xqe9c8tCbLrEnNNJOQksmllAwSUzJJSM3gUkomFy6ncyY+hR0n4kjOMAOaClymskqgXvkUGrqnEOiajL/TZSqqBLzMl3DqOAanoL5W//lLoIsCCw0NJSQkRAK9DEvLNBMRe4VjsZc5dv4KR89f4cTFK0TFp5Jhtlzdz9FBUdXblWpe5QiuWYGq3uWo6uVKVa9yVPF0paKHC77uzjiZin6dQKUUJgVGhws4mQrxIZmpkHQ2xyMaXGLAJQbtfg5LYgwqORYHS4axfxaQYDxN1c7Eam8O4UXCkXN0D7LCSV2nxAb6O8sPEn42yaqfGVTNk0n3Nr7lPpGRkfTq1Yt27dqxbds2WrduzahRo5g0aRKxsbHMmTOHunXrMnr0aE6cOIGbmxszZsygWbNmxMXFMXToUKKjo2nfvj05b+/3yy+/8MUXX5CRkUHbtm356quvMJny/ov6+eef+fjjj1FK0axZM2bPns3y5cuZOnUqGRkZ+Pr6MmfOHCpXrlygn8Xu3bsZPXo0AD169Lj6vtls5vXXX2fjxo2kp6czZswYnnrqqavbMzIymDhxIqmpqWzZsoUJEyYQGBjIuHHjSEtLo1y5cvz44480aNCAgwcPMmrUKDIyMrBYLCxevJh69eoVqE5hW1przielc/BsIgeikzh4NpEj5y9zOj6Ff/+8HR0UgX7lqV/Jg3saVSbA142aPuUJ8HGjmrcrjsUQ1lZhscCV85B4JvsRleNxBhKjITWXOZYuXuBZFeVRBVPtzuBeGTyqgkdlcK8C7pXAvRIZWS5cik/hzMVkmvl7FckplNhAt6WIiAgWLlzIzJkzad26NXPnzmXLli0sW7aM9957jxo1atCyZUuWLl3K+vXrGTFiBKGhobzzzjt06tSJiRMnsmLFCn744QcADh06xPz589m6dStOTk48++yzzJkzhxEjRtyyjoMHDzJ16lS2bduGn58f8fHGH1OnTp3YsWMHSim+//57PvroIz755JMCneOoUaOYNm0aXbp04ZVXXrn6/g8//ICXlxe7du0iPT2djh070qNHj6sXf5ydnZkyZcrVrh2ApKQkNm/ejKOjI2vXruWNN95g8eLFfPPNN4wbN46HH36YjIwMzGZzgWoUxS/TbGHPqUtsPnaRsOhEDkYnEpdstDaVgkC/8jSu5sn9LapTv7IH9Su7U9O3PM6OpSS0U+Lh0km4FAmXTkHCKUg4bTxPPAPmjGv3d/UCrxrg5Q/+bcCzGnhWz/G1KjiXz9ehvVygRXlnWtTwtv55ZSuxgZ5XS7ooBQYG0rRpUwAaN25M9+7dUUrRtGlTIiMjOXXqFIsXLwagW7duxMXFkZSUxKZNm1iyZAkAffv2pUKFCgCsW7eO3bt307p1awBSU1OpVKlSnnWsX7+ewYMHX+1f9/HxAYwJWEOGDCEmJoaMjIwCT8xJSEggISGBLl26ADB8+HBWrVoFwJ9//klYWBiLFi0CIDExkWPHjlG/fv2bfl5iYiIjR47k2LFjKKXIzMwEoH379rz77rtERUUxcOBAaZ2XUNEJqfx95AJ/H41la0QcV9KzMDko6lf2oFvDSjSu5kmT6l40qupJeZcSGxkGrSElDuKOQ1wExB+H+BMQf9II8rTEa/d38wPvAKjSFBr1M557BYB3DSOwXT1tcx6FVMJ/O7bh4uJy9bmDg8PV1w4ODmRlZRV4qrrWmpEjR/L+++9bpb6xY8fy4osvct9997Fx40YmT55slc8Fo9Yvv/ySnj17XvN+ZGTkTb/n7bffpmvXrvz2229ERkZy1113ATBs2DDatm3LihUr6NOnD99++y3dunWzWq2icLTWHIhOYtWBGP4KP8+x2CsAVPNy5d7m1bizfkU61PXF07UEL8mQlWEE9cUjcOEoxB0zAjwu4trQdnA0Wtg+tcE/GCoEgk+g8dU7AFzcbXcORUACvRA6d+7MnDlzePvtt9m4cSN+fn54enrSpUsX5s6dy1tvvcWqVau4dOkSAN27d6d///6MHz+eSpUqER8fz+XLl6lZs+Ytj9OtWzcGDBjAiy++iK+vL/Hx8fj4+JCYmEj16tUB+Omnnwpcv7e3N97e3mzZsoVOnToxZ86cq9t69uzJ119/Tbdu3XBycuLo0aNXj/UvDw8PLl++fPV1znpmzZp19f0TJ05Qu3Ztnn/+eU6fPk1YWJgEuo1YLJo9py+x6sA5Vh84R3RCKiYHRdtAH4a0rsGd9StSt5J7iRtXTVY6XDwGsYfgwiGIPWyEePxJ0Dm68Dz9wbcONBkEvnWzH3XAuyaYyk7MlZ0ztaLJkyczevRomjVrhpub29VQnTRpEkOHDqVx48Z06NDh6gJkQUFBTJ06lR49emCxWHBycmL69Ol5Bnrjxo158803ufPOOzGZTLRs2ZJZs2YxefJkBg8eTIUKFejWrRsnT54s8Dn8+OOPjB49GqXUNRdFH3/8cSIjI2nVqhVaaypWrMjSpUuv+d6uXbvywQcf0KJFCyZMmMCrr77KyJEjmTp1Kn37/m8o1oIFC5g9ezZOTk5UqVKFN954o8B1ittz8Gwii3ZH8UdYDBcup+NscqBTPT/G3V2PuxtVxqe8s61LNGht9GGf2w/nDkBsuBHicRH/C24HR/CpA5UaQdD94FcfKtYH33p219IuLJVzJEZxCg4O1tffsejQoUM0atTIJvUI25DfufVdvJLO0r3RLN4TzaGYJJxNDnRtWJE+TavSrWElPGzdlWLOhAuHIWbf/wL8/P4cXSUKKtSCSkFGeFdqZDz3rQuOJeR/QDaklNqttQ7ObZu00IWwA1prNh65wJx/TrHxyAWyLJrm/l78X//G3Nu8Gt5uNgpCc6bR0o4JhbOhxtdzB8Ccbmx3Kg+Vg6DJA1C5CVRpZgS4tLgLRQK9BIiLi6N79+43vL9u3Tp8fX1v67PHjBnD1q1br3lv3LhxjBolS+7YA4tF82f4eaZtOMaB6CQqebjwWOdABrXyp15lj+ItRmtjCGD0buMRFWK0wrNSje0unlC1ObR5Aqq1NJ771AGHUjLksRSQQC8BfH19CQ0NLZLPnj59epF8rrAts0Wzcn8M09ZHcOT8ZWr5uvHRoGYMaFm9WGZdAsYFy7OhcGYHnP4HonZBcqyxzdHVCOzgUVD9DiPAKwRKeBcxCXQhShGLRbM87CxfrDvG8QvJ1K3kzmdDWtCvWdWin5GZmgCnt2c//oGze//XdeJTG+p2N8LbP9joPjGV4GGPdkoCXYhSYs/pS0xZHk7omQQaVvFg+rBW9GpSBZNDEQ01TImHU9vg1FaI3Gz0faPBwclocbd5AgLaQY22xvR2YXMS6EKUcOcS0/hw9WF+2xtNJQ8XPh7cnIEtq1+zkqBVpF8xWt8nNhqP89mrbzq6Qo02cNcEqNXRaIU7lbPusYVVSKALUUKlZZqZsekEX288jllrxnStw7N31bXe9HtzFpzd878AP7MTLJlgcoGAttD1LajVCaq3AkeXvD5NlAAS6NcxmUw0bdoUrTUmk4lp06bRoUMHUlJSeOKJJwgLC0Nrjbe3N6tXr8bd/faHV8k65uJ6WyMu8uqiMKITUundpApv9GlEDR+32//gy+chYi1E/AXHN0BaAqCMC5jtx0Dtu4xuFGmBl0oS6NcpV67c1REna9asYcKECfz99998/vnnVK5cmf379wNw5MiRAq/pYmuyjnnJl5Zp5sPVh/lxayS1/coz74l2tK9zG0NXLWZjCOHR1XDsLzgXZrzvXhka9jMuZNa+C9x8rFG+sLGSG+irXjdmkVlTlabQ+4N8756UlHR1xcSYmJhrpuo3aNDglt8r65iLggqLSmD8/FCOX0jm0Q61eK1XQ8o5F+IuDBkpRhfKkRVwdA0kXwBlMvrBu70N9XoY/y2UtHVbxG0ruYFuI6mpqbRo0YK0tDRiYmJYv349AKNHj6ZHjx4sWrSI7t27M3LkyJuGnKxjLgoi02zhqw3H+XL9MfzcXZj9WBs616tYsA9JiYcjK+HQH3BiA2SlGRN56t0DDfoYLfFyFYrmBESJUXIDvQAtaWvK2eWyfft2RowYwYEDB2jRogUnTpzgzz//ZO3atbRu3Zrt27fnug6JrGMu8utUXDLPz9vLvqhE7m9RjXfua4KXWz678q5cgMN/QPjvcHKTsYiVVw1oNRIa9IaaHWXtkzKm5AZ6CdC+fXsuXrzIhQsXqFSpEu7u7gwcOJCBAwfi4ODAypUrC7SwlKxjLnJafSCGVxaG4eCgmD6sFX2bVc37m5IvQvhSOLjUGB+uLcakno7jIOg+qNpCulLKMJmHewuHDx/GbDbj6+vL1q1br65vnpGRQXh4+E2Xv+3WrRsLFy4kLi4O4GqXizXXMQdyXcf831b20aNHSU5Ovub7C7OOef/+/QkLCytwreLmMrIsTFkeztO/7KF2JXdWPN/p1mGefhn2zYdfBsHH9WHFS8a9Lzu/DE9vhbF74O5JxmQfCfMyTVro1/m3Dx2MVu9PP/2EyWTi+PHjPPPMM2itsVgs9O3blwceeCDXz5B1zMXNnE1IZczcPew9ncCjHWrxRp9Gud+PMyvDGFq4fyEcWW0scOUVAB2fN27iULmxhLe4gayHLmyqLP3ONxyJ5cX5oWSaNR8Nakafprm0ymPCIHQu7F9g3BvTzRcaD4Smg4ybFMviVmWerIcuhA2ZLZr//nWUaRsiaFTVk68ebkWgX447xV+5YLTEQ+caN3owORsjU1oMgzrdZJErkW8S6LdB1jEXeYm9nMa4eaFsPxHHkOAavNO/Ma5OJrBY4ORG2D0LDq8ASxZUawV9PjZu9iATfUQhlLhA11qXvBvV3oSsY357bNXdV1x2nIhj7Ly9XE7L5OPBzRl0h78x9X7HHNjzE1yKhHI+0PZpaPGwceceIW5DiQp0V1dX4uLi8PX1LTWhLgpHa01cXByurq62LsXqLBbN138f55M/j1DLtzyzR7emYWooLHjjf63xWp2NWZsN+4GT/f0MhG2UqED39/cnKiqKCxcu2LoUUQxcXV3x9/e3dRlWlZCSwYsL9rH+cCwDm3jzXt1DuC55HS4cMmZqtn0a7ngU/GSylrC+EhXoTk5OBZ49KURJse9MAs/O2YPz5dOsaLiHoKilqIhE48bH/b8y+salNS6KUIkKdCFKI601v+w4xR8rlvKe80q6OO1EnXIwZm62ecpYjla6EEUxkEAX4jYkp2Uwb/Y3tDgzm+GOR7E4e6OCx0Prx8Gruq3LE2WMBLoQhZGZRuyWH0nf/CWPW6JJKlcNS9cPcWg1HJzL5/39QhQBCXQhCiL9CoT8QNqmz6mUHkc4tTnS6XMadH0ETPKfk7At+QsUIj/SEuGfGegd01Gpl9hpbsp6v1d4euSjVPGW27WJkkECXYhbSYmHHV/BPzMgPZEQp9a8l96PNp178mbPBjiZZG0VUXJIoAuRm9QE2D4NdnwNGVeIrd6D56K7cygzkE8eaU6PxlVsXaEQN8hXoCulegGfAybge631B9dtrwnMBCoC8cAjWusoK9cqRNFLvwL/fAPbvoC0RCyN+vOD44O8u0vRpLonK4bdQYCvm62rFCJXeQa6UsoETAfuAaKAXUqpZVrr8By7fQz8rLX+SSnVDXgfGF4UBQtRJDLTIGQmbPnUuKlyvZ7EtX2FZ9ZlsfNkPMPaBjCxX5CxsJYQJVR+WuhtgAit9QkApdSvQH8gZ6AHAS9mP98AXHtnBSFKKovZWLZ24/uQFA2BXaDbXDanBfLCvFBSMsz8d0hzBrS0ryUKhH3KzxWd6sCZHK+jst/LaR8wMPv5AMBDKXXD+rFKqSeVUiFKqRBZr0XYlNZwbC180xmWPQceVWHEMszDl/HpIS9GzNyJr7szy8d2lDAXpYa1Loq+DExTSj0KbAKiAfP1O2mtZwAzwLhjkZWOLUTBxITBX2/DiY1QoRYMngVB9xN7JZ1x3//D9hNxDLrDnyn9G+PmLOMGROmRn7/WaKBGjtf+2e9dpbU+S3YLXSnlDjygtU6wVpFCWEViNKyfCvvmQTlv6Pk+tH4MHF3YFnGR538N5Up6Jv8Z1IzBwTXy/jwhSpj8BPouoJ5SKhAjyB8ChuXcQSnlB8RrrS3ABIwRL0KUDJlpxhDEzZ8Ya5F3GAudX4RyFTBbNNPWHuPzdUcJ9CvPnMfb0qCKh60rFqJQ8gx0rXWWUuo5YA3GsMWZWuuDSqkpQIjWehlwF/C+UkpjdLmMKcKahcgfreHIKlgzwbg7UMN+0PNdo5sFiE1K44X5oWw7HseAltWZen8TyrtIF4sovZStbgMWHBysQ0JCbHJsUQZcPAarX4eIteDXAHp/YNxwOdvmYxcYPz+UK+lZTOnfhMF3+MtdskSpoJTarbUOzm2bNEeEfclIho0fGNP1ndyMfvI2T4DJCYAss4XP1h5j+sYI6lZ0Z+4T7ahfWbpYhH2QQBf248hqWPkyJJ6BFo/A3ZPAvdLVzTGJqYybF8rOyHgeDPbnnfuaUM5ZJgoJ+yGBLkq/pLOw6lU4tBwqNoRRq6Bmh2t2WX/4PC8t2Ed6lkUmCgm7JYEuSi+LGXZ+ZwxFtGRC94nQfiw4Ol/dJSPLwkerD/P9lpM0qurJtGEtqVPR3YZFC1F0JNBF6XT+IPw+Bs7uhTrdoe/H4FP7ml1Ox6Uwdt4e9kUlMqJ9Td7o00jWYhF2TQJdlC7mTNj8KWz6D7h6wQM/QJMHbrgJ84qwGF5fHAYKvnmkFb2aVLVRwUIUHwl0UXrE7IOlY+D8fmgyCHp/BOWvXTIoLdPM1BXh/LLjNC1qePPl0JbU8JHlbkXZIIEuSr6sdPj7I9jyXyjvBw/NhYZ9b9jtVFwyz87Zw8GzSTzZpTavyB2FRBkjgS5Ktph9sOQpuHAImg+Fnu+Bm88Nu60+EMMrC8NwcFD8MDKY7o0q26BYIWxLAl2UTBYzbP0cNrwHbr4wbAHU73nDbhlZFj5YdZiZW0/SvIY304e1xL+CdLGIskkCXZQ8l07Bb0/D6W0Q1B/6fZZrqzzqUgrPzd1L6JkERnWsxYTejXB2lC4WUXZJoIuSQ2tjaduVrxqjVgZ8C82G3DCCBWBt+HleWrgPi0Xz9cOt6N1URrEIIYEuSoaUePjjBQj/HWp2hAHfgHfADbtlZFn4cPVhfthykibVPZk2tBW1/MrboGAhSh4JdGF7p3fAosfgynm4+x1jvXKHGycAnYlP4bm5xkShRzvUYkKfhrg4ykQhIf4lgS5sx2KBrf+F9e8arfHH/4JqLXPdddX+GF5dHAbIRCEhbkYCXdjGlVhY8iSc2GDM9Oz3Gbh63rBbWqaZ91Ye4uftp2hew5tpMlFIiJuSQBfF7/gGI8zTk+DeL6DViFwvfEbEXmHsvL0cikni8U6BvNqroYxiEeIWJNBF8bGY4e8PjVmfFRvAiN+hctANu2mtWbQ7iom/H6Scs4mZjwbTraFMFBIiLxLoonikxMPix+H4OmjxMPT5DzjfODrlSnoWb/22n6WhZ2lX24fPhrSkiperDQoWovSRQBdF72woLBgOl8/BvZ9Dq5G5drHsj0pk7Lw9nI5P4aV76vNs17qYHOQ+n0LklwS6KFp7f4E/XoTyFWHUavC/44ZdtNbM3BrJB6sO4efuwq9PtqdN4I0zQ4UQtyaBLopGVrpxW7jdsyDwThg001gp8TqXkjN4eeE+1h2O5Z6gyvxnUDO83Zxv/DwhRJ4k0IX1JcXA/EcgOgQ6jYeub4Hpxj+1nSfjeX7eXuKTM5h8bxAjO9RC5dIVI4TIHwl0YV3Re+DXYZCWBA/OhqD7btjFbNFM3xDBZ2uPEuDjxpJnO9CkupcNihXCvkigC+vZv8i4z2f5SvDYn1ClyQ27xCal8cL8ULYdj6N/i2q8O6Ap7i7yZyiENch/SeL2WSyw4V3Y/DEEdIAhs3PtL99wOJaXFu4jJSOLjwY1Y/Ad/tLFIoQVSaCL25N+BX57Cg7/AS2HQ99PwfHai5rpWWY+WHWYH7dG0rCKB18ObUe9yh42KlgI+yWBLgovMQrmDoHYcOj1AbR9+obx5RGxV3h+3l7CY5J4tEMtXu/dEFcnWSFRiKIggS4KJ2YfzHkQMlPg4YVQ9+5rNmutWRByhsnLwinnbJL7fApRDCTQRcEd+wsWPgqu3jB6NVRufM3mxNRM3vhtPyvCYuhY15dPH2xBZU+Zvi9EUZNAFwUT8iOseMlYVGvYQvC8dl3y3acuMe7XvZxLTOPVXg14uksdHGT6vhDFQgJd5I/FAuunwJb/Qt17YPCP4OKRY7Pm67+P8+lfR6nq5crCp9vTMqCCDQsWouyRQBd5y0qHpc/AgcVwxyjo8/E1Mz9jk9IYvyCUrRFx9GtWlfcGNsXT1cmGBQtRNkmgi1tLv2zM/Dy5ybjfZ8dx14xk2XgklpcW7CM5I4sPH2jKg8E1ZGy5EDYigS5u7soFmDMIzh+AATOg+ZCrmzLNFj5ec4RvN52gYRUPfpWx5ULYnAS6yN2lUzB7ACSdhYfmQf0eVzdFJ6Qydu4e9pxOYFjbACb2C5Kx5UKUABLo4kbnw+GXgcYY8xFLIaDd1U1rw8/z8qJ9ZJk1XwxtyX3Nq9mwUCFEThLo4lqn/4G5g8HJzbghRfY9PzPNFj5afZjvNp8kqKon0x9uRaDfjbeQE0LYTr5uoa6U6qWUOqKUilBKvZ7L9gCl1Aal1F6lVJhSqo/1SxVF7tha+Lk/uPnB6DVXw/xMfAoPfrud7zafZHi7mix5toOEuRAlUJ4tdKWUCZgO3ANEAbuUUsu01uE5dnsLWKC1/lopFQSsBGoVQb2iqBxaDgtHQaVG8MgScK8IwJqD53hl4T60hunDWtG3WdU8PkgIYSv56XJpA0RorU8AKKV+BfoDOQNdA57Zz72As9YsUhSx/YtgyZNQvRU8vAjKeZOeZeb9lYeZtS2SptW9mDasJTV9pVUuREmWn0CvDpzJ8ToKaHvdPpOBP5VSY4HywN3kQin1JPAkQEBAQEFrFUVh7y/w+3NQswMMmw8uHkReTOa5eXs4EJ3E6I6BvNa7AS6OMopFiJIuX33o+TAUmKW19gf6ALOVUjd8ttZ6htY6WGsdXLFiRSsdWhTazu+MOwzVvstombt4sHzfWfp9uYUz8anMGH4HE+8NkjAXopTITws9GqiR47V/9ns5PQb0AtBab1dKuQJ+QKw1ihRFYNs0+PNNqN8bBs8iVTsxZcl+5u08TasAb74Y2hL/Cm62rlIIUQD5CfRdQD2lVCBGkD8EDLtun9NAd2CWUqoR4ApcsGahwoo2fQzr/w+C7oeB33H4Yhpj5/7DsdgrPH1nHV7qUR8nk7X+8SaEKC55BrrWOksp9RywBjABM7XWB5VSU4AQrfUy4CXgO6XUeIwLpI9qrXVRFi4KafOnRpg3fRB9/1f8sussU/8Ix8PVidmPtaFzPekKE6K0ytfEIq31SoyhiDnfm5jjeTjQ0bqlCavb8hmseweaDiah5xe8Nncfaw6ep0v9inwyuDkVPVxsXaEQ4jbITNGyYtuXsHYSNHmAXS3fY9yX24i9nM4bfRryeKfachMKIeyABHpZsH06/PkWlqABfOn1Mp9/twv/Cm4sfqYDzWt427o6IYSVSKDbux3fwJo3SK3Xj1Fxj7Njz0kGtKzOlP6N8ZCbUAhhVyTQ7dnO72D1a5yrdg99jg0j3XKFTx9szsBW/rauTAhRBCTQ7dWe2bDyZQ56dqb/ieE09vfi84daUksW1RLCbkmg26ODv6GXP0+IY0sejn2cx+9swIv31MfZUcaWC2HPJNDtzdE/sSx+gr2WBoxTLx605PgAABFuSURBVPPDY21lbLkQZYQEuh2xnNiM5ddHOJRVnQ99J7NgRBeZvi9EGSKBbieunPgHx9mDOW32Y0HDz/l5cGe5z6cQZYwEuh04fWgn3vMHEqs92Hvnj0zp1galZKKQEGWNBHopt3H7ThqvHkyaciZ+4EKGNG9h65KEEDYiwx5KqSyzhc9+30rNVY/g4mBGjfiNFhLmQpRpEuil0IXL6Tz+3Qa67R5DdVMCriMXUbG2hLkQZZ10uZQyu09d4vlfdvBRxrs0MZ3GYeg8qNXO1mUJIUoACfRSQmvNz9tP8e6KA0x3/ZaOaj/cNx3q97R1aUKIEkICvRS4kp7FG0v2s2zfWb6t9Bv3JG2C7pOg5SO2Lk0IUYJIoJdwR85d5pk5u4m8mMycRv/Q8eQiaPs0dBpv69KEECWMXBQtwZbsiaL/9C0kpWaxuvt5Op78HBoPhJ7vg4wzF0JcR1roJVBappl3lh9k3s4ztA304ZvOaVRY/BrU6gwDvgEH+f+wEOJGEuglTOTFZJ6ds4fwmCTGdK3D+BYKxx97QIVaMGQ2OMp9P4UQuZNAL0GW7zvLhCX7MTkoZj4aTLcaJvi+Ozg4wrAFUK6CrUsUQpRgEuglgNHFEs68nadpFeDNl8NaUb088NN9cPkcPLoCfAJtXaYQooSTQLexiNgrPDd3D4fPXebpO+vwUo/6OClg0aMQtQse/Bn8g21dphCiFJBAt6Ele6J4a+kBXJ1MzBrVmrsaVDI2/DURwn+HHlMh6D7bFimEKDUk0G0gOT2LScsOsmh3FG0CffjioZZU8XI1Nu6eBVs/h+DHoP1zNq1TCFG6SKAXs4NnExk7dy8n45IZ260u47rXw9GUPQzx5GZY8RLUvRt6fyRjzYUQBSKBXky01szaFsn7Kw9TobwTcx9vR/s6vv/bIf4kLBgOPrVh0Ewwya9GCFEwkhrFID45g1cW7mPd4VjublSJjwY1x6e88/92SEuCeQ8Zz4f+Cq5etilUCFGqSaAXse3H43hh/l4uJWcy6d4gHu1Q69rbw1nMsPgxiIuA4b+Bbx3bFSuEKNUk0ItIptnCZ2uP8tXG4wT6lueHka1pUj2XlvfaSXDsT+j7KQR2Kf5ChRB2QwK9CEReTGbc/FD2nUlgSHANJt4bRHmXXH7Ue+fAti+h9RPQ+rHiL1QIYVck0K1Ia83iPdFM+v0AjiYHvn64Fb2bVs1959M74I8XIPBO6PV+8RYqhLBLEuhWkpiayZu/7eePsBja1fbh0wdbUM273E12job5j4CXPwyeBSanYq1VCGGfJNCtYOfJeMbPD+V8Uhqv9GzA03fWweRwkzHkWenG8MTMVBj5B7j5FG+xQgi7JYF+G/698Pn1xuME+Lix6JkOtKjhffNv0NqYOBS9Gx6cDZUaFl+xQgi7J4FeSCcuXOGF+aGERSXe+sJnTrt/hL2zofPLskaLEMLqJNALSGvNr7vOMGV5OC5ODnzzSCt6NbnJhc+czuyEla9C3Xug6xtFX6gQosyRQC+A+OQMXlscxl/h5+lU149PHmxOZU/XvL/x8jmYP9y4CPrAd+BgKvpihRBlTr4CXSnVC/gcMAHfa60/uG77f4Gu2S/dgEpa61t0Jpc+6w6d57XF+0lKzeStvo0Y3TEQh5td+MwpKwMWjID0y8ZMULnrkBCiiOQZ6EopEzAduAeIAnYppZZprcP/3UdrPT7H/mOBlkVQq00kp2cxdUU483aeoWEVD355vA0Nq3jm/wNWvw5n/jGGJ1YOKrI6hRAiPy30NkCE1voEgFLqV6A/EH6T/YcCk6xTnm2FRMbz4oJ9nLmUwtN31mH8PfVwcSxAd8m+XyHkB+g4DhoPKLpChRCC/AV6deBMjtdRQNvcdlRK1QQCgfU32f4k8CRAQEBAgQotThlZxnDEb/4+TvUK5VjwVHta1yrgePHz4bD8BajVGbpNLJpChRAiB2tfFH0IWKS1Nue2UWs9A5gBEBwcrK18bKs4fC6JF+fvIzwmiSHBNXj73iDc8xqOeL30y8bkIVdPeOAHWdtcCFEs8pM00UCNHK/9s9/LzUPAmNstyhbMFs2MTSf49K8jeJVz4rsRwdwTVLngH6Q1LBtr3LBi5HLwKMRnCCFEIeQn0HcB9ZRSgRhB/hAw7PqdlFINgQrAdqtWWAwiLybz0sJ97D51id5NqjD1/ib4ursU7sN2zoCDv8Hd70CtjtYtVAghbiHPQNdaZymlngPWYAxbnKm1PqiUmgKEaK2XZe/6EPCr1rpEdqXkRmvNL/+c5r0Vh3AyKT4b0oL+LapdewOKgjizC9a8CQ36QIfnrVusEELkQdkqf4ODg3VISIhNjg1wNiGV1xaHsfnYRTrX8+OjQc2o6nWT1RHzIzkOvu1iTBp66m8Zby6EKBJKqd1a6+DctpW5q3VaaxaEnGHqH4cwa83/3d+ER9oGFL5VDmCxwJInIPkCPPanhLkQwibKVKDHJKby+uL9/H30Au1q+/CfQc2p4eN2+x+85VM4vg76fQbVWtz+5wkhRCGUiUDXWrNwdxT/90c4WWbNO/c1Zni7mvmbup+XU9thw7vQZBDc8ejtf54QQhSS3Qd6TGIqbyzZz4YjF2gT6MN/BjWjpm9563x4Sjwsfgy8a0K//8LtdNsIIcRtsttA11ozb+cZ3l95iCyLZmK/IB7tUMs6rXLjALD02ex+87+MSURCCGFDdhnop+NSeH1JGNuOx9G+ti8fPtCMAF8r9JXntONrOLoKen0o/eZCiBLBrgLdbNH8tC2S/6w5gslB8d6ApgxtU+P2RrDkJnoP/DURGvSFtk9Z97OFEKKQ7CbQI2Iv89ri/ew+dYm7GlTkvQFNqeZ9G+PKbyYtERaNAvfK0H+a9JsLIUqMUh/oGVkWvt54nOkbIijnbOLTB5szoGV167fKweg3Xz4OEs7AqJXgVsAVGIUQogiV6kDfc/oSry8O4+j5K9zbvBoT+wVR0aOQa7Dk64A/G+u0dJ8IAe2K7jhCCFEIpTLQk9Oz+PjPI8zaFkkVT1d+GBlM90ZFvKrhxWPG3YcC74SO4/PeXwghilmpC/Qtxy7y2uIwziamMrxdTV7p2QAPV6eiPWhWBix+HBxdYMA34OBQtMcTQohCKHWBHpecTjlnEwufak9wQe8iVFgbpkJMKAyZA57ViueYQghRQKUu0O9rXo3eTari7FhMreQTf8PWL4xp/Y36Fc8xhRCiEEpdoCulcHYspqGCKfHw21PgWxd6vlc8xxRCiEIqdYFebP69lVzyRRg2H5yttP6LEEIUEbm6dzN7foLDfxhDFKs2t3U1QgiRJwn03Fw8BqsnQO27oP1ztq5GCCHyRQL9euZM4+5Dji5wvwxRFEKUHtKHfr1N/4Gze+HBn8Gzqq2rEUKIfJPmZ05Ru2HTx9DsIQjqb+tqhBCiQCTQ/5WRbHS1eFaDPh/ZuhohhCgw6XL5118TIf44jFwOrl62rkYIIQpMWugAEWth1/fQbgwEdrF1NUIIUSgS6CnxsHQMVGxojDkXQohSSrpcVrwEKXHw8AJwcrV1NUIIUWhlu4W+fxEcXAJ3vS6zQYUQpV7ZDfSkGFjxIvi3gY4v2LoaIYS4bWUz0P+9N2hWBtz/NZik50kIUfqVzSQLnQvH1kCvD8Cvrq2rEUIIqyh7LfTEaOPeoAEdoM1Ttq5GCCGspmwF+r9rnFuy4P7psvCWEMKulK0ulz0/w/F10Odj8Klt62qEEMKqyk4TNeE0rHkTanWG4MdsXY0QQlhd2Qh0reH35wAN/aWrRQhhn8pGl0vITDj5N/T7L1SoaetqhBCiSNh/U/XSKfjzbeN2cneMsnU1QghRZOw70P+dQKQU3Pel8VUIIexUvgJdKdVLKXVEKRWhlHr9Jvs8qJQKV0odVErNtW6ZhbT3FzixAe6eDN4Btq5GCCGKVJ596EopEzAduAeIAnYppZZprcNz7FMPmAB01FpfUkpVKqqC8y0pxhjVUrOjjGoRQpQJ+WmhtwEitNYntNYZwK/A9TfcfAKYrrW+BKC1jrVumQWktbEsrjnd6GqRUS1CiDIgP0lXHTiT43VU9ns51QfqK6W2KqV2KKV65fZBSqknlVIhSqmQCxcuFK7i/Di4BI6sgK5vgm+dojuOEEKUINZqujoC9YC7gKHAd0op7+t30lrP0FoHa62DK1asaKVDXyf5Iqx8Baq1gnbPFs0xhBCiBMpPoEcDNXK89s9+L6coYJnWOlNrfRI4ihHwxW/Va5CWZEwgkmVxhRBlSH4CfRdQTykVqJRyBh4Cll23z1KM1jlKKT+MLpgTVqwzf46sggOLoMsrUDmo2A8vhBC2lGega62zgOeANcAhYIHW+qBSaopS6r7s3dYAcUqpcGAD8IrWOq6ois5VagL8MR4qNYZO44v10EIIURLkq09Ca70SWHndexNzPNfAi9kP21g7Ca6ch4fmgqOzzcoQQghbsY/xfJFbYfcs4yJo9Va2rkYIIWyi9Ad6Zpoxvd87ALq+YetqhBDCZkr/MJDNn0DcMXhkCTiXt3U1QghhM6W7hX4+HLZ8Cs2GQN3utq5GCCFsqvQGusUMy58HF0/o+Z6tqxFCCJsrvV0uu36AqF0w4Fso72fraoQQwuZKZws9MQrWvQN1uhndLUIIIUphoP+7kqK2GLeUk5tWCCEEUBoD/eBvcHS1MUSxQi1bVyOEECVG6Qt0V09o0BfaPmPrSoQQokQpfRdF695tPIQQQlyj9LXQhRBC5EoCXQgh7IQEuhBC2AkJdCGEsBMS6EIIYSck0IUQwk5IoAshhJ2QQBdCCDuhjNuB2uDASl0ATtnk4LfHD7ho6yJsoKyeN5Tdc5fzLplqaq0r5rbBZoFeWimlQrTWwbauo7iV1fOGsnvuct6lj3S5CCGEnZBAF0IIOyGBXnAzbF2AjZTV84aye+5y3qWM9KELIYSdkBa6EELYCQl0IYSwExLoN6GU6qWUOqKUilBKvZ7L9gCl1Aal1F6lVJhSqo8t6rS2fJx3TaXUuuxz3qiU8rdFndamlJqplIpVSh24yXallPoi++cSppRqVdw1FoV8nHdDpdR2pVS6Uurl4q6vqOTjvB/O/j3vV0ptU0o1L+4aC0MCPRdKKRMwHegNBAFDlVJB1+32FrBAa90SeAj4qnirtL58nvfHwM9a62bAFOD94q2yyMwCet1ie2+gXvbjSeDrYqipOMzi1ucdDzyP8Xu3J7O49XmfBO7UWjcF/o9ScqFUAj13bYAIrfUJrXUG8CvQ/7p9NOCZ/dwLOFuM9RWV/Jx3ELA++/mGXLaXSlrrTRjhdTP9Mf5HprXWOwBvpVTV4qmu6OR13lrrWK31LiCz+Koqevk4721a60vZL3cApeJfohLouasOnMnxOir7vZwmA48opaKAlcDY4imtSOXnvPcBA7OfDwA8lFK+xVCbreXnZyPs02PAKlsXkR8S6IU3FJiltfYH+gCzlVJl4ef5MnCnUmovcCcQDZhtW5IQRUMp1RUj0F+zdS354WjrAkqoaKBGjtf+2e/l9BjZfXBa6+1KKVeMRX1ii6XCopHneWutz5LdQldKuQMPaK0Tiq1C28nP34SwI0qpZsD3QG+tdZyt68mPstCiLIxdQD2lVKBSyhnjouey6/Y5DXQHUEo1AlyBC8VapfXled5KKb8c/xKZAMws5hptZRkwInu0SzsgUWsdY+uiRNFQSgUAS4DhWuujtq4nv6SFngutdZZS6jlgDWACZmqtDyqlpgAhWutlwEvAd0qp8RgXSB/VpXzabT7P+y7gfaWUBjYBY2xWsBUppeZhnJtf9nWRSYATgNb6G4zrJH2ACCAFGGWbSq0rr/NWSlUBQjAGAFiUUi8AQVrrJBuVbBX5+H1PBHyBr5RSAFmlYQVGmfovhBB2QrpchBDCTkigCyGEnZBAF0IIOyGBLoQQdkICXQgh7IQEuhBC2AkJdCGEsBP/D9z+vyMlfgUfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}