{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Lilian/European_Call_jax_1stock_v3_pricedelta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "2546e73c-278d-4a09-bda4-b60c3fdae84f"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0807]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.2597*0.2597\n",
        "initial_stocks = jnp.array([0.7178]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.2106\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5235104\n",
            "[0.99997556]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cba24e-1625-4f87-fb9d-b7badb0a0d30"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "# version 1, 2, 6\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(0.75 + np.random.random(self.N_STOCKS) * 0.5)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(0.15 + np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(0.25 + np.random.random(1) * 0.35), self.N_STOCKS)\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          K = 0.75 + np.random.random(1) * 0.5\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32) # remember to change this!\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "31241227-eaee-457c-80a4-026945d0b22b"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.35, 0.35]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.25, 0.25]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "d2d7d551-2ae2-4de5-81c1-61ce464fd7e6"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "07879890-5298-487d-f516-5dc881b75b17"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    adjusted_y2 = (1000 * y)**2\n",
        "    loss_weight = 1/adjusted_y2.mean(axis=0)\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "\n",
        "\n",
        "    # loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8473, 0.1527], device='cuda:0')\n",
            "tensor([[0.6847, 0.9997],\n",
            "        [0.5227, 0.9575],\n",
            "        [0.2927, 0.8537],\n",
            "        [0.6174, 0.9821],\n",
            "        [0.1815, 0.8362],\n",
            "        [0.2563, 0.8437],\n",
            "        [0.1856, 0.7066],\n",
            "        [0.0830, 0.5544],\n",
            "        [0.4647, 0.9768],\n",
            "        [0.0770, 0.5582],\n",
            "        [0.1886, 0.7705],\n",
            "        [0.4030, 0.9719],\n",
            "        [0.0684, 0.5283],\n",
            "        [0.2164, 0.9235],\n",
            "        [0.0784, 0.7096],\n",
            "        [0.2460, 0.8260],\n",
            "        [0.4926, 0.9991],\n",
            "        [0.0828, 0.5073],\n",
            "        [0.4423, 0.9062],\n",
            "        [0.3287, 0.8692],\n",
            "        [0.3821, 0.9974],\n",
            "        [0.3457, 0.9100],\n",
            "        [0.5485, 0.9779],\n",
            "        [0.4038, 0.8773],\n",
            "        [0.2793, 0.9669],\n",
            "        [0.3809, 0.9915],\n",
            "        [0.3834, 0.9975],\n",
            "        [0.1653, 0.6972],\n",
            "        [0.3826, 0.9286],\n",
            "        [0.4001, 0.9906],\n",
            "        [0.6476, 0.9813],\n",
            "        [0.1018, 0.5858]], device='cuda:0')\n",
            "tensor([0.8040, 0.1960], device='cuda:0')\n",
            "tensor([[0.6071, 0.9927],\n",
            "        [0.5466, 1.0003],\n",
            "        [0.5668, 0.9744],\n",
            "        [0.6091, 1.0003],\n",
            "        [0.5351, 0.9997],\n",
            "        [0.2879, 0.8796],\n",
            "        [0.5620, 0.9915],\n",
            "        [0.5666, 0.9759],\n",
            "        [0.7102, 0.9900],\n",
            "        [0.4158, 0.9885],\n",
            "        [0.3426, 0.8967],\n",
            "        [0.2524, 0.9043],\n",
            "        [0.3568, 0.9139],\n",
            "        [0.6371, 0.9987],\n",
            "        [0.4980, 0.9964],\n",
            "        [0.7883, 0.9992],\n",
            "        [0.3858, 0.9998],\n",
            "        [0.3832, 0.9227],\n",
            "        [0.2413, 0.9234],\n",
            "        [0.5935, 0.9815],\n",
            "        [0.5653, 0.9944],\n",
            "        [0.4128, 0.8974],\n",
            "        [0.3915, 0.9548],\n",
            "        [0.3078, 0.9227],\n",
            "        [0.1441, 0.6154],\n",
            "        [0.4267, 0.8956],\n",
            "        [0.2654, 0.8021],\n",
            "        [0.2326, 0.8645],\n",
            "        [0.2885, 0.8864],\n",
            "        [0.4076, 0.9999],\n",
            "        [0.4212, 0.9849],\n",
            "        [0.1660, 0.7216]], device='cuda:0')\n",
            "tensor([0.8352, 0.1648], device='cuda:0')\n",
            "tensor([[0.4643, 0.9521],\n",
            "        [0.3899, 0.9447],\n",
            "        [0.3904, 0.9642],\n",
            "        [0.2046, 0.7085],\n",
            "        [0.3224, 0.9064],\n",
            "        [0.2739, 0.7924],\n",
            "        [0.1776, 0.7937],\n",
            "        [0.4026, 0.9782],\n",
            "        [0.1847, 0.8018],\n",
            "        [0.5277, 0.9540],\n",
            "        [0.4581, 0.9975],\n",
            "        [0.6047, 0.9836],\n",
            "        [0.2319, 0.7828],\n",
            "        [0.2084, 0.7385],\n",
            "        [0.1107, 0.6534],\n",
            "        [0.4136, 0.9027],\n",
            "        [0.3330, 0.8963],\n",
            "        [0.0855, 0.6768],\n",
            "        [0.4996, 0.9997],\n",
            "        [0.5873, 0.9844],\n",
            "        [0.2344, 0.7821],\n",
            "        [0.2099, 0.8398],\n",
            "        [0.4171, 0.9640],\n",
            "        [0.3978, 0.9711],\n",
            "        [0.6147, 0.9940],\n",
            "        [0.6116, 0.9813],\n",
            "        [0.1871, 0.8082],\n",
            "        [0.4971, 0.9395],\n",
            "        [0.4003, 0.9774],\n",
            "        [0.5016, 0.9937],\n",
            "        [0.4933, 0.9939],\n",
            "        [0.4345, 0.9812]], device='cuda:0')\n",
            "tensor([0.8378, 0.1622], device='cuda:0')\n",
            "tensor([[0.3791, 0.9892],\n",
            "        [0.1375, 0.7651],\n",
            "        [0.4714, 0.9223],\n",
            "        [0.7920, 1.0004],\n",
            "        [0.2508, 0.9496],\n",
            "        [0.3679, 0.9957],\n",
            "        [0.3341, 0.8667],\n",
            "        [0.2188, 0.9671],\n",
            "        [0.4214, 0.9253],\n",
            "        [0.4668, 0.9313],\n",
            "        [0.2956, 0.8123],\n",
            "        [0.6158, 0.9931],\n",
            "        [0.4495, 0.8866],\n",
            "        [0.5432, 0.9986],\n",
            "        [0.3173, 0.9472],\n",
            "        [0.6542, 1.0000],\n",
            "        [0.1050, 0.6141],\n",
            "        [0.4031, 0.8612],\n",
            "        [0.3743, 0.9289],\n",
            "        [0.2362, 0.8530],\n",
            "        [0.5932, 0.9687],\n",
            "        [0.3504, 0.8238],\n",
            "        [0.1610, 0.7357],\n",
            "        [0.3525, 0.9814],\n",
            "        [0.2355, 0.7057],\n",
            "        [0.3255, 0.9974],\n",
            "        [0.1554, 0.6027],\n",
            "        [0.1977, 0.8310],\n",
            "        [0.3536, 0.9398],\n",
            "        [0.1288, 0.5923],\n",
            "        [0.3343, 0.9632],\n",
            "        [0.4731, 0.9999]], device='cuda:0')\n",
            "tensor([0.8279, 0.1721], device='cuda:0')\n",
            "tensor([[0.4302, 0.9543],\n",
            "        [0.3580, 0.9412],\n",
            "        [0.1395, 0.7734],\n",
            "        [0.2245, 0.9624],\n",
            "        [0.4724, 0.9678],\n",
            "        [0.6241, 0.9988],\n",
            "        [0.3159, 0.9986],\n",
            "        [0.0986, 0.5279],\n",
            "        [0.4750, 1.0002],\n",
            "        [0.1612, 0.7354],\n",
            "        [0.2136, 0.9635],\n",
            "        [0.6431, 0.9851],\n",
            "        [0.2027, 0.7550],\n",
            "        [0.0975, 0.7898],\n",
            "        [0.5330, 0.9940],\n",
            "        [0.4539, 0.9126],\n",
            "        [0.5575, 0.9874],\n",
            "        [0.4909, 0.9997],\n",
            "        [0.5725, 1.0002],\n",
            "        [0.3903, 0.9608],\n",
            "        [0.5078, 0.9386],\n",
            "        [0.3679, 0.9094],\n",
            "        [0.2124, 0.7018],\n",
            "        [0.2198, 0.7733],\n",
            "        [0.2650, 0.8947],\n",
            "        [0.6112, 0.9942],\n",
            "        [0.3273, 0.7794],\n",
            "        [0.3956, 0.8536],\n",
            "        [0.2486, 0.9852],\n",
            "        [0.6154, 0.9907],\n",
            "        [0.6072, 0.9592],\n",
            "        [0.1586, 0.6202]], device='cuda:0')\n",
            "tensor([0.8325, 0.1675], device='cuda:0')\n",
            "tensor([[0.4210, 0.9700],\n",
            "        [0.1910, 0.8398],\n",
            "        [0.6131, 0.9998],\n",
            "        [0.4320, 0.9850],\n",
            "        [0.3825, 0.9862],\n",
            "        [0.6691, 0.9993],\n",
            "        [0.5696, 0.9951],\n",
            "        [0.1258, 0.6311],\n",
            "        [0.4611, 0.9022],\n",
            "        [0.2589, 0.8119],\n",
            "        [0.2437, 0.8606],\n",
            "        [0.5096, 0.9989],\n",
            "        [0.2595, 0.9564],\n",
            "        [0.3128, 0.9652],\n",
            "        [0.2995, 0.9629],\n",
            "        [0.2748, 0.8362],\n",
            "        [0.0887, 0.6050],\n",
            "        [0.0542, 0.4335],\n",
            "        [0.6077, 0.9608],\n",
            "        [0.4200, 0.9511],\n",
            "        [0.2771, 0.8048],\n",
            "        [0.4079, 0.9703],\n",
            "        [0.7564, 1.0003],\n",
            "        [0.6979, 0.9989],\n",
            "        [0.4767, 0.9532],\n",
            "        [0.1617, 0.8242],\n",
            "        [0.1105, 0.5788],\n",
            "        [0.2651, 0.9549],\n",
            "        [0.2995, 0.9874],\n",
            "        [0.1480, 0.6589],\n",
            "        [0.3426, 0.9848],\n",
            "        [0.1863, 0.6992]], device='cuda:0')\n",
            "tensor([0.8536, 0.1464], device='cuda:0')\n",
            "tensor([[0.4373, 0.9965],\n",
            "        [0.4651, 0.9766],\n",
            "        [0.2189, 0.7028],\n",
            "        [0.1606, 0.6007],\n",
            "        [0.2245, 0.9556],\n",
            "        [0.4000, 0.9682],\n",
            "        [0.5490, 0.9906],\n",
            "        [0.1471, 0.7557],\n",
            "        [0.3128, 0.8535],\n",
            "        [0.3475, 0.8352],\n",
            "        [0.5134, 0.9787],\n",
            "        [0.1351, 0.7404],\n",
            "        [0.3016, 0.8470],\n",
            "        [0.4343, 0.9986],\n",
            "        [0.2979, 0.9138],\n",
            "        [0.3340, 0.9745],\n",
            "        [0.1142, 0.5726],\n",
            "        [0.2035, 0.8993],\n",
            "        [0.2263, 0.8637],\n",
            "        [0.2886, 0.9590],\n",
            "        [0.0591, 0.5347],\n",
            "        [0.1460, 0.6216],\n",
            "        [0.1874, 0.7974],\n",
            "        [0.7294, 0.9912],\n",
            "        [0.2525, 0.9922],\n",
            "        [0.3551, 0.9751],\n",
            "        [0.6020, 0.9792],\n",
            "        [0.7159, 1.0004],\n",
            "        [0.0877, 0.6983],\n",
            "        [0.3044, 0.9920],\n",
            "        [0.4056, 0.9809],\n",
            "        [0.4240, 0.9650]], device='cuda:0')\n",
            "tensor([0.8387, 0.1613], device='cuda:0')\n",
            "tensor([[0.5649, 0.9988],\n",
            "        [0.2310, 0.7233],\n",
            "        [0.1421, 0.6226],\n",
            "        [0.5055, 0.9918],\n",
            "        [0.1562, 0.8382],\n",
            "        [0.2508, 0.9464],\n",
            "        [0.4423, 0.9529],\n",
            "        [0.5015, 0.9487],\n",
            "        [0.1665, 0.7261],\n",
            "        [0.4127, 0.9519],\n",
            "        [0.2651, 0.8287],\n",
            "        [0.1948, 0.8788],\n",
            "        [0.4871, 0.9505],\n",
            "        [0.2012, 0.9360],\n",
            "        [0.6920, 1.0003],\n",
            "        [0.5624, 0.9991],\n",
            "        [0.4845, 0.9978],\n",
            "        [0.1376, 0.8817],\n",
            "        [0.4134, 0.8788],\n",
            "        [0.3625, 0.9946],\n",
            "        [0.3902, 0.8932],\n",
            "        [0.5423, 1.0002],\n",
            "        [0.2760, 0.8959],\n",
            "        [0.5863, 0.9849],\n",
            "        [0.3343, 0.9199],\n",
            "        [0.2043, 0.8855],\n",
            "        [0.3423, 0.9231],\n",
            "        [0.1756, 0.8101],\n",
            "        [0.2001, 0.7286],\n",
            "        [0.2129, 0.7518],\n",
            "        [0.6488, 1.0003],\n",
            "        [0.5184, 0.9997]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bc7026cdfd3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m           \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m           \u001b[0mEuropean_Call_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m           \u001b[0mgooptionvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptionvalueavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m           \u001b[0mDeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooptionvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36moptionvalueavg\u001b[0;34m(key, initial_stocks, numsteps, drift, r, cov, K, T, keys)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m###################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m   5653\u001b[0m   \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_index_for_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5654\u001b[0m   return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[0;32m-> 5655\u001b[0;31m                  unique_indices)\n\u001b[0m\u001b[1;32m   5656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[0;31m# TODO(phawkins): re-enable jit after fixing excessive recompilation for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m   5661\u001b[0m             unique_indices):\n\u001b[1;32m   5662\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_static_and_dynamic_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5663\u001b[0;31m   \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_index_to_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shared with _scatter_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5664\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   5853\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"index is out of bounds for axis {x_axis} with size 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5854\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_axis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnormalize_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5855\u001b[0;31m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5856\u001b[0m       \u001b[0mgather_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgather_indices_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5857\u001b[0m       \u001b[0mcollapsed_slice_dims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mconvert_element_type\u001b[0;34m(operand, new_dtype)\u001b[0m\n\u001b[1;32m    429\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__jax_array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0moperand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__jax_array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m def _convert_element_type(operand: Array, new_dtype: Optional[DType] = None,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    441\u001b[0m   \u001b[0mnew_weak_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m   if (dtypes.issubdtype(old_dtype, np.complexfloating) and\n\u001b[0m\u001b[1;32m    444\u001b[0m       not dtypes.issubdtype(new_dtype, np.complexfloating)):\n\u001b[1;32m    445\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Casting complex values to real discards the imaginary part\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bfloat16\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a735ae-cc05-4efc-e677-3cd250ec19b3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_pricedelta_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3432ba72-25e3-4507-d373-38ed2f12b9d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73baacf1-0a85-4e65-ad42-6a7e6c4cf573"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_pricedelta_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39caa3c-449e-4bc4-dd00-dd705158b160"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6e233c-b19a-4a16-96a1-de53fdfa2194"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 2000000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 10\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 30)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_pricedelta_4.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.0003841693978756666 average time 0.5690565446002438 iter num 10\n",
            "loss 0.00019969901768490672 average time 0.2896439497004394 iter num 20\n",
            "loss 0.0003055417037103325 average time 0.19650686630059985 iter num 30\n",
            "loss 6.988467794144526e-05 average time 0.15006869707558507 iter num 40\n",
            "loss 0.00018562970217317343 average time 0.12208804602043528 iter num 50\n",
            "loss 0.00022086316312197596 average time 0.10345371651716656 iter num 60\n",
            "loss 5.261728074401617e-05 average time 0.0901659875576505 iter num 70\n",
            "loss 0.00037479648017324507 average time 0.0802129490754396 iter num 80\n",
            "loss 7.945597462821752e-05 average time 0.0724342811336909 iter num 90\n",
            "loss 0.0010689797345548868 average time 0.06623000957028126 iter num 100\n",
            "loss 5.190215597394854e-05 average time 0.33324962090009647 iter num 10\n",
            "loss 7.92388163972646e-05 average time 0.17187969159949718 iter num 20\n",
            "loss 0.0003374926745891571 average time 0.11799609506621588 iter num 30\n",
            "loss 0.0004288842319510877 average time 0.0911793219746869 iter num 40\n",
            "loss 0.00011017902579624206 average time 0.07503445659960561 iter num 50\n",
            "loss 0.0001543379039503634 average time 0.06437394141642774 iter num 60\n",
            "loss 2.4077429770841263e-05 average time 0.056692196528456404 iter num 70\n",
            "loss 0.00011039411765523255 average time 0.05091563643741211 iter num 80\n",
            "loss 0.00031052270787768066 average time 0.04639783087768592 iter num 90\n",
            "loss 0.00014698690210934728 average time 0.04278943102981429 iter num 100\n",
            "loss 0.0006281908717937768 average time 0.3329301739002403 iter num 10\n",
            "loss 0.00027013482758775353 average time 0.1716266841000106 iter num 20\n",
            "loss 5.816221528220922e-05 average time 0.11786270513330237 iter num 30\n",
            "loss 0.000316553603624925 average time 0.09103328202472767 iter num 40\n",
            "loss 0.0001290571817662567 average time 0.07490335513983154 iter num 50\n",
            "loss 3.1499253964284435e-05 average time 0.0641501840166408 iter num 60\n",
            "loss 0.00012451130896806717 average time 0.056478598214276386 iter num 70\n",
            "loss 7.449393888236955e-05 average time 0.05075547031251517 iter num 80\n",
            "loss 0.00010997534991474822 average time 0.04626073035558673 iter num 90\n",
            "loss 0.00010147372813662514 average time 0.0427062591300637 iter num 100\n",
            "loss 0.0001219960322487168 average time 0.33272324820027277 iter num 10\n",
            "loss 0.007115728687494993 average time 0.1715466166002443 iter num 20\n",
            "loss 0.00011620101577136666 average time 0.1178384548001001 iter num 30\n",
            "loss 0.0001732243108563125 average time 0.09096308095013228 iter num 40\n",
            "loss 0.00012466064072214067 average time 0.0749237682401872 iter num 50\n",
            "loss 0.0001297441776841879 average time 0.0641416351333343 iter num 60\n",
            "loss 5.89562114328146e-05 average time 0.05646575911414402 iter num 70\n",
            "loss 0.00019514829909894615 average time 0.05069424812486432 iter num 80\n",
            "loss 0.00034877052530646324 average time 0.04618810544436403 iter num 90\n",
            "loss 9.125406359089538e-05 average time 0.042653193009900864 iter num 100\n",
            "loss 0.0004070694267284125 average time 0.3321639033998508 iter num 10\n",
            "loss 0.00021937873680144548 average time 0.17119174520030356 iter num 20\n",
            "loss 6.494304398074746e-05 average time 0.11753125663356817 iter num 30\n",
            "loss 0.00033676333259791136 average time 0.09085204177508785 iter num 40\n",
            "loss 0.00027227698592469096 average time 0.07477910842011624 iter num 50\n",
            "loss 0.0002923593274317682 average time 0.06402444810009911 iter num 60\n",
            "loss 8.948515460360795e-05 average time 0.056338307257205344 iter num 70\n",
            "loss 6.415940879378468e-05 average time 0.05056545893748989 iter num 80\n",
            "loss 4.2136816773563623e-05 average time 0.04608831534444309 iter num 90\n",
            "loss 4.1832558054011315e-05 average time 0.04260076115995617 iter num 100\n",
            "loss 0.0003300870303064585 average time 0.33197374299998045 iter num 10\n",
            "loss 4.526353950495832e-05 average time 0.17115777995022655 iter num 20\n",
            "loss 0.00022253621136769652 average time 0.11761421270008819 iter num 30\n",
            "loss 0.000533956685103476 average time 0.09075913272499747 iter num 40\n",
            "loss 8.15759485703893e-05 average time 0.07463879356007964 iter num 50\n",
            "loss 0.0004468661791179329 average time 0.06396814198324137 iter num 60\n",
            "loss 0.00021772267064079642 average time 0.056278607799946 iter num 70\n",
            "loss 6.323423440335318e-05 average time 0.05053807408735338 iter num 80\n",
            "loss 6.481684977188706e-05 average time 0.046055085988822006 iter num 90\n",
            "loss 0.00014413893222808838 average time 0.04250457269001345 iter num 100\n",
            "loss 0.0008329971460625529 average time 0.3322057718003634 iter num 10\n",
            "loss 0.00020646635675802827 average time 0.17118162530005065 iter num 20\n",
            "loss 0.0001782181643648073 average time 0.1174883258666038 iter num 30\n",
            "loss 3.5028919228352606e-05 average time 0.09070389335001891 iter num 40\n",
            "loss 0.00012276721827220172 average time 0.0746578508400853 iter num 50\n",
            "loss 0.00018839850963559002 average time 0.06405191043334829 iter num 60\n",
            "loss 0.0001236679672729224 average time 0.05641366067133536 iter num 70\n",
            "loss 0.00017556951206643134 average time 0.050646016949895054 iter num 80\n",
            "loss 0.00010774606926133856 average time 0.04619345436658477 iter num 90\n",
            "loss 0.00029930437449365854 average time 0.04260443416995258 iter num 100\n",
            "loss 0.00022931456624064595 average time 0.33218257619955693 iter num 10\n",
            "loss 5.7889072195393965e-05 average time 0.1711803317999511 iter num 20\n",
            "loss 0.0002307177783222869 average time 0.11751327553332279 iter num 30\n",
            "loss 0.00019769923528656363 average time 0.09068450539989499 iter num 40\n",
            "loss 5.1411345339147374e-05 average time 0.0745811003799463 iter num 50\n",
            "loss 0.00012883283488918096 average time 0.0638443484832654 iter num 60\n",
            "loss 0.0004645709414035082 average time 0.05632771492834893 iter num 70\n",
            "loss 0.0003264989354647696 average time 0.050570801874800966 iter num 80\n",
            "loss 0.00013248219329398125 average time 0.046144948133183386 iter num 90\n",
            "loss 5.230564420344308e-05 average time 0.04257393034986308 iter num 100\n",
            "loss 0.0018632535357028246 average time 0.3324857105002593 iter num 10\n",
            "loss 0.00016350482474081218 average time 0.17140587095036608 iter num 20\n",
            "loss 6.341798143694177e-05 average time 0.11794368016708176 iter num 30\n",
            "loss 0.0005645215278491378 average time 0.09116394430029687 iter num 40\n",
            "loss 0.0002359650970902294 average time 0.07514468806039076 iter num 50\n",
            "loss 0.00019537089974619448 average time 0.06430850738373313 iter num 60\n",
            "loss 0.0001062173250829801 average time 0.056599588500258896 iter num 70\n",
            "loss 0.0010765244951471686 average time 0.050794759937707566 iter num 80\n",
            "loss 0.002827281365171075 average time 0.04630329433352421 iter num 90\n",
            "loss 2.649497582751792e-05 average time 0.042719774240104015 iter num 100\n",
            "loss 0.00015872875519562513 average time 0.33230996849997607 iter num 10\n",
            "loss 0.00013304682215675712 average time 0.17138246184968012 iter num 20\n",
            "loss 0.00015669366985093802 average time 0.11770587549945048 iter num 30\n",
            "loss 2.4448989279335365e-05 average time 0.09085454379946896 iter num 40\n",
            "loss 0.00020277651492506266 average time 0.07496493525941332 iter num 50\n",
            "loss 0.00027030293131247163 average time 0.06420732313284437 iter num 60\n",
            "loss 0.0004350280505605042 average time 0.05650474828513065 iter num 70\n",
            "loss 0.0002435491478536278 average time 0.05073616836193651 iter num 80\n",
            "loss 0.0001694865059107542 average time 0.046254524832854964 iter num 90\n",
            "loss 0.0001104584225686267 average time 0.0426694042595409 iter num 100\n",
            "loss 9.947914077201858e-05 average time 0.33296278800044093 iter num 10\n",
            "loss 0.00010812187247211114 average time 0.17189876210049987 iter num 20\n",
            "loss 3.4053606214001775e-05 average time 0.11808255706710043 iter num 30\n",
            "loss 0.00014114761142991483 average time 0.09115897905030579 iter num 40\n",
            "loss 0.00016340032743755728 average time 0.07496204402028525 iter num 50\n",
            "loss 0.00010578374349279329 average time 0.06418605123362794 iter num 60\n",
            "loss 0.00043470613309182227 average time 0.05649276655739024 iter num 70\n",
            "loss 0.00011340939090587199 average time 0.05073091846270472 iter num 80\n",
            "loss 0.0005272056441754103 average time 0.0462753285890762 iter num 90\n",
            "loss 6.476105045294389e-05 average time 0.042704529500151696 iter num 100\n",
            "loss 0.00031664909329265356 average time 0.3331431980008347 iter num 10\n",
            "loss 0.002873127581551671 average time 0.17183118970042416 iter num 20\n",
            "loss 5.895887079532258e-05 average time 0.1179774822670879 iter num 30\n",
            "loss 0.00017681824101600796 average time 0.09110085935035386 iter num 40\n",
            "loss 0.0009776586666703224 average time 0.07492701690011018 iter num 50\n",
            "loss 0.00010539479262661189 average time 0.06413937600009376 iter num 60\n",
            "loss 7.00719901942648e-05 average time 0.05656250270013905 iter num 70\n",
            "loss 0.0001643193536438048 average time 0.05076822573760183 iter num 80\n",
            "loss 0.00012017117842333391 average time 0.04624995894450371 iter num 90\n",
            "loss 9.644875535741448e-05 average time 0.042678426560123627 iter num 100\n",
            "loss 0.00011116136738564819 average time 0.3326686890999554 iter num 10\n",
            "loss 0.00016378356667701155 average time 0.171653435850385 iter num 20\n",
            "loss 0.00014189290232025087 average time 0.11788311486695117 iter num 30\n",
            "loss 1.7146227037301287e-05 average time 0.0910144064002452 iter num 40\n",
            "loss 0.00019670662004500628 average time 0.07486651052015077 iter num 50\n",
            "loss 9.28482404560782e-05 average time 0.0640993923167116 iter num 60\n",
            "loss 8.486178558086976e-05 average time 0.056417966128563944 iter num 70\n",
            "loss 0.00015736046771053225 average time 0.05067642283756868 iter num 80\n",
            "loss 0.00011479794193292037 average time 0.04629649313343786 iter num 90\n",
            "loss 0.00015242367226164788 average time 0.04269738391012652 iter num 100\n",
            "loss 0.00019399826123844832 average time 0.3326649119000649 iter num 10\n",
            "loss 8.273200364783406e-05 average time 0.17155767219974222 iter num 20\n",
            "loss 0.0009276021737605333 average time 0.1179747597330182 iter num 30\n",
            "loss 0.00012923571921419352 average time 0.09103086387476651 iter num 40\n",
            "loss 0.00010803606710396707 average time 0.07490609035972738 iter num 50\n",
            "loss 0.00016269584011752158 average time 0.06412955469980565 iter num 60\n",
            "loss 6.94588161422871e-05 average time 0.05643061627119356 iter num 70\n",
            "loss 0.00013420137111097574 average time 0.05066650181233854 iter num 80\n",
            "loss 8.204091864172369e-05 average time 0.04623765707765415 iter num 90\n",
            "loss 7.059689232846722e-05 average time 0.04263697868991585 iter num 100\n",
            "loss 0.00015557979349978268 average time 0.3325457267001184 iter num 10\n",
            "loss 0.00011361360520822927 average time 0.1714946758504084 iter num 20\n",
            "loss 0.0003020801814273 average time 0.11772527773367376 iter num 30\n",
            "loss 5.727449388359673e-05 average time 0.090931521125367 iter num 40\n",
            "loss 0.00022280325356405228 average time 0.07484248900036619 iter num 50\n",
            "loss 0.0001979709486477077 average time 0.06410816600030861 iter num 60\n",
            "loss 0.000190324368304573 average time 0.05644547698596268 iter num 70\n",
            "loss 0.00023068640439305454 average time 0.05067545131278166 iter num 80\n",
            "loss 5.266733205644414e-05 average time 0.04627482307797537 iter num 90\n",
            "loss 4.5648695959243923e-05 average time 0.04267967032017623 iter num 100\n",
            "loss 0.00014827547420281917 average time 0.33249949230048514 iter num 10\n",
            "loss 0.0001575692876940593 average time 0.17139513735037326 iter num 20\n",
            "loss 0.0006225252873264253 average time 0.1177096077670285 iter num 30\n",
            "loss 8.310128760058433e-05 average time 0.09082071200036808 iter num 40\n",
            "loss 0.0014736453304067254 average time 0.07479081574056182 iter num 50\n",
            "loss 0.00021919718710705638 average time 0.06405701616707422 iter num 60\n",
            "loss 0.00010508311970625073 average time 0.056381765686039996 iter num 70\n",
            "loss 0.0003555779403541237 average time 0.05062369110037253 iter num 80\n",
            "loss 6.225707329576835e-05 average time 0.04615270436689672 iter num 90\n",
            "loss 0.00013830045645590872 average time 0.04262886950014945 iter num 100\n",
            "loss 0.0001874270965345204 average time 0.333371326899578 iter num 10\n",
            "loss 0.0001044130403897725 average time 0.17198888439943402 iter num 20\n",
            "loss 0.0026430620346218348 average time 0.11807108999976966 iter num 30\n",
            "loss 7.690444908803329e-05 average time 0.09117624762466221 iter num 40\n",
            "loss 7.516227196902037e-05 average time 0.07501501807979366 iter num 50\n",
            "loss 0.0002769007405731827 average time 0.064276322116469 iter num 60\n",
            "loss 0.0008140706922858953 average time 0.05658977744268279 iter num 70\n",
            "loss 0.00011267434456385672 average time 0.05079537077476744 iter num 80\n",
            "loss 5.3601059335051104e-05 average time 0.046303870633063425 iter num 90\n",
            "loss 0.0003930538659915328 average time 0.04272213018968614 iter num 100\n",
            "loss 0.00033275503665208817 average time 0.3323144012996636 iter num 10\n",
            "loss 7.794988778186962e-05 average time 0.17139603904925024 iter num 20\n",
            "loss 0.00011021061800420284 average time 0.11771481543255505 iter num 30\n",
            "loss 0.00015359377721324563 average time 0.09088653904955209 iter num 40\n",
            "loss 5.616878843284212e-05 average time 0.07488888129970292 iter num 50\n",
            "loss 7.227199239423499e-05 average time 0.06415408201640578 iter num 60\n",
            "loss 0.0005075051449239254 average time 0.056565484256913934 iter num 70\n",
            "loss 0.00018400711996946484 average time 0.050779553362326625 iter num 80\n",
            "loss 0.0002146386686945334 average time 0.04630157472201972 iter num 90\n",
            "loss 0.0002582053712103516 average time 0.042694887219840896 iter num 100\n",
            "loss 0.007038837298750877 average time 0.33245454550014986 iter num 10\n",
            "loss 0.00014869746519252658 average time 0.17139983425022365 iter num 20\n",
            "loss 6.10811694059521e-05 average time 0.11770941143331584 iter num 30\n",
            "loss 0.00011689306847983971 average time 0.09089222879983935 iter num 40\n",
            "loss 0.00016198153025470674 average time 0.07478525979997358 iter num 50\n",
            "loss 0.0031606361735612154 average time 0.06412874333339763 iter num 60\n",
            "loss 0.00012645682727452368 average time 0.05649275085717298 iter num 70\n",
            "loss 0.00011163534509250894 average time 0.05070741085010013 iter num 80\n",
            "loss 7.304872269742191e-05 average time 0.04622273480013569 iter num 90\n",
            "loss 8.735703158890828e-05 average time 0.042620575780129004 iter num 100\n",
            "loss 0.00017137078975792974 average time 0.33213788109969755 iter num 10\n",
            "loss 7.940004434203729e-05 average time 0.17146513869938645 iter num 20\n",
            "loss 7.368795922957361e-05 average time 0.11774385806614494 iter num 30\n",
            "loss 0.00011084724974352866 average time 0.09085614869973142 iter num 40\n",
            "loss 0.0002464318822603673 average time 0.0747642569396703 iter num 50\n",
            "loss 0.00021812281920574605 average time 0.06408371504973426 iter num 60\n",
            "loss 0.0007967716082930565 average time 0.05644030512838591 iter num 70\n",
            "loss 0.00011938148963963613 average time 0.05068196742486179 iter num 80\n",
            "loss 3.347076562931761e-05 average time 0.04620067776664251 iter num 90\n",
            "loss 0.00013211170153226703 average time 0.04260091069998453 iter num 100\n",
            "loss 0.00010110424045706168 average time 0.3334102029002679 iter num 10\n",
            "loss 0.00012437700934242457 average time 0.1720184893500118 iter num 20\n",
            "loss 0.0003822635335382074 average time 0.11809182086666017 iter num 30\n",
            "loss 8.184799662558362e-05 average time 0.09115225594996446 iter num 40\n",
            "loss 0.00010578377987258136 average time 0.07496917046002637 iter num 50\n",
            "loss 0.00027027452597394586 average time 0.06422596608329817 iter num 60\n",
            "loss 0.00013838855375070125 average time 0.05651523471419101 iter num 70\n",
            "loss 9.399848204338923e-05 average time 0.05077200202499625 iter num 80\n",
            "loss 9.29669477045536e-05 average time 0.046336781055490266 iter num 90\n",
            "loss 0.0001184920547530055 average time 0.04273424855997291 iter num 100\n",
            "loss 0.00011412342428229749 average time 0.3318659448996186 iter num 10\n",
            "loss 0.0001874785084510222 average time 0.1710557490498104 iter num 20\n",
            "loss 0.00026912445900961757 average time 0.11740789230001004 iter num 30\n",
            "loss 0.0012628623517230153 average time 0.09061191127502752 iter num 40\n",
            "loss 7.498799823224545e-05 average time 0.07464534020000428 iter num 50\n",
            "loss 0.00013057925389148295 average time 0.06392413160016683 iter num 60\n",
            "loss 0.002554466715082526 average time 0.05625844001447799 iter num 70\n",
            "loss 8.992018410935998e-05 average time 0.050559010475080865 iter num 80\n",
            "loss 0.00010451729031046852 average time 0.04607749080011369 iter num 90\n",
            "loss 5.5574953876202926e-05 average time 0.04250593334007135 iter num 100\n",
            "loss 8.008869917830452e-05 average time 0.3327063287993951 iter num 10\n",
            "loss 0.0002449901949148625 average time 0.17148940009956276 iter num 20\n",
            "loss 0.0001692864898359403 average time 0.11782285696632849 iter num 30\n",
            "loss 9.423394658369943e-05 average time 0.09100580297481428 iter num 40\n",
            "loss 0.0009412358631379902 average time 0.07484220375976293 iter num 50\n",
            "loss 0.00011368731065886095 average time 0.06406759524985925 iter num 60\n",
            "loss 9.759494423633441e-05 average time 0.05641855835690097 iter num 70\n",
            "loss 0.00029806719976477325 average time 0.05072914676225082 iter num 80\n",
            "loss 9.373115608468652e-05 average time 0.04625476351086238 iter num 90\n",
            "loss 4.5924818550702184e-05 average time 0.04264654058977612 iter num 100\n",
            "loss 0.00021007480972912163 average time 0.3326673851999658 iter num 10\n",
            "loss 0.00013597976067103446 average time 0.17179009084993596 iter num 20\n",
            "loss 0.00018095145060215145 average time 0.11805689986661795 iter num 30\n",
            "loss 9.941403550328687e-05 average time 0.09111115922487442 iter num 40\n",
            "loss 8.863744733389467e-05 average time 0.07496415769986925 iter num 50\n",
            "loss 6.632791337324306e-05 average time 0.06430037108317871 iter num 60\n",
            "loss 0.00012094796693418175 average time 0.056600124728460544 iter num 70\n",
            "loss 0.00012475399125833064 average time 0.05086723453741797 iter num 80\n",
            "loss 0.00011451298632891849 average time 0.04640262649999285 iter num 90\n",
            "loss 0.00017579091945663095 average time 0.042793888240012165 iter num 100\n",
            "loss 0.0023456704802811146 average time 0.3328514324995922 iter num 10\n",
            "loss 0.00019269665062893182 average time 0.17162826729963854 iter num 20\n",
            "loss 0.0001288155181100592 average time 0.1178359521995541 iter num 30\n",
            "loss 0.00015442018047906458 average time 0.09099022774971673 iter num 40\n",
            "loss 0.00039035125519149005 average time 0.0749131864598894 iter num 50\n",
            "loss 0.0014573506778106093 average time 0.06412822998318006 iter num 60\n",
            "loss 0.00023060709645505995 average time 0.05642602017131659 iter num 70\n",
            "loss 0.001241308986209333 average time 0.050652355687361705 iter num 80\n",
            "loss 0.0012836286332458258 average time 0.04617851003325389 iter num 90\n",
            "loss 0.0006702245445922017 average time 0.04260022875987488 iter num 100\n",
            "loss 0.0007238308317027986 average time 0.33281966609974917 iter num 10\n",
            "loss 6.665058026555926e-05 average time 0.17157935989998804 iter num 20\n",
            "loss 8.176871051546186e-05 average time 0.11776565080011399 iter num 30\n",
            "loss 0.0010711122304201126 average time 0.09087780652498623 iter num 40\n",
            "loss 5.932537897024304e-05 average time 0.07478740448015742 iter num 50\n",
            "loss 0.00014562525029759854 average time 0.06406847145014277 iter num 60\n",
            "loss 0.00010057629697257653 average time 0.056390263442985346 iter num 70\n",
            "loss 0.00011091115447925404 average time 0.05061508666253758 iter num 80\n",
            "loss 2.9777038434986025e-05 average time 0.046122652788916536 iter num 90\n",
            "loss 0.00011340209312038496 average time 0.042534536100065454 iter num 100\n",
            "loss 0.0006987371016293764 average time 0.3324551837002218 iter num 10\n",
            "loss 0.0001916658366099 average time 0.1714235715502582 iter num 20\n",
            "loss 0.0001163780179922469 average time 0.11776245740014323 iter num 30\n",
            "loss 6.500523159047589e-05 average time 0.09092814632522277 iter num 40\n",
            "loss 5.223476546234451e-05 average time 0.0748110587202973 iter num 50\n",
            "loss 8.529167098458856e-05 average time 0.0640467876335606 iter num 60\n",
            "loss 8.21987196104601e-05 average time 0.05638935480024624 iter num 70\n",
            "loss 0.00016376223356928676 average time 0.05060822623772765 iter num 80\n",
            "loss 0.0027246284298598766 average time 0.046124266833552 iter num 90\n",
            "loss 0.00016041698108892888 average time 0.042527430040208855 iter num 100\n",
            "loss 0.00018568923405837268 average time 0.33345759389994784 iter num 10\n",
            "loss 6.27714252914302e-05 average time 0.17192291440042026 iter num 20\n",
            "loss 0.0004989223671145737 average time 0.11802094733381334 iter num 30\n",
            "loss 4.915601311950013e-05 average time 0.09116079135046676 iter num 40\n",
            "loss 0.00014368980191648006 average time 0.07497178552031983 iter num 50\n",
            "loss 0.0001466793764848262 average time 0.06417600705020353 iter num 60\n",
            "loss 0.00012112239346606657 average time 0.05646392734301376 iter num 70\n",
            "loss 0.00028012756956741214 average time 0.05068719147516276 iter num 80\n",
            "loss 2.7973646865575574e-05 average time 0.046250115511267925 iter num 90\n",
            "loss 0.0019821322057396173 average time 0.042636937070165006 iter num 100\n",
            "loss 0.00027496254188008606 average time 0.33240518719976536 iter num 10\n",
            "loss 0.0002876846119761467 average time 0.17131020774977515 iter num 20\n",
            "loss 0.00019820714078377932 average time 0.11763783890006986 iter num 30\n",
            "loss 0.0004356939753051847 average time 0.09082463527502113 iter num 40\n",
            "loss 8.178395364666358e-05 average time 0.07477780026012625 iter num 50\n",
            "loss 0.00012319590314291418 average time 0.06402490798354847 iter num 60\n",
            "loss 8.607323979958892e-05 average time 0.05633751835734334 iter num 70\n",
            "loss 0.00023476530623156577 average time 0.05058099352513636 iter num 80\n",
            "loss 4.4537580834003165e-05 average time 0.04610461746682025 iter num 90\n",
            "loss 0.00012472343223635107 average time 0.042556148800067604 iter num 100\n",
            "loss 9.678290371084586e-05 average time 0.33296917610059606 iter num 10\n",
            "loss 0.002691784640774131 average time 0.17165713445065195 iter num 20\n",
            "loss 7.608981832163408e-05 average time 0.11789881073321642 iter num 30\n",
            "loss 0.000150742256664671 average time 0.09107277472467104 iter num 40\n",
            "loss 0.00014714057033415884 average time 0.07489438149976195 iter num 50\n",
            "loss 0.0003306929429527372 average time 0.06415091088335127 iter num 60\n",
            "loss 0.0002651878457982093 average time 0.056477578014268406 iter num 70\n",
            "loss 0.0005914033390581608 average time 0.05069485398753386 iter num 80\n",
            "loss 0.0005551830399781466 average time 0.046334287966682396 iter num 90\n",
            "loss 0.00010990789451170713 average time 0.04278590371010069 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36be0999-4f57-41eb-a340-078ed9b796d9"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.3, 0.3]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.27130044, 0.90763223)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.2709]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9074], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2AXrPt7bNj",
        "outputId": "4afb485a-3fc3-498f-87d7-ccfc558530cd"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.3]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.0]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27120972\n",
            "[0.9076326]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lovJwXo3-YEu",
        "outputId": "27f1d486-e481-4f9d-c9e6-7e9f49abb3a4"
      },
      "source": [
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "BS_call_prices = []\n",
        "for p in prices:\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    BS_call_prices.append(bs_call(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, BS_call_prices, label = \"BS_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(BS_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lwApH0GT9bBK",
        "outputId": "07b30ba6-9fbb-4236-b3b3-0338a3a1880c"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}