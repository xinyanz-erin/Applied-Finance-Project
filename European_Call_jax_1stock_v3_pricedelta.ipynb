{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/European_Call_jax_1stock_v3_pricedelta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "14a70623-8740-4be2-ab28-da12c98fc27f"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0807]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.2597*0.2597\n",
        "initial_stocks = jnp.array([0.7178]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.2106\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5235387\n",
            "[1.0000153]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083f34f5-70c3-421e-a2a6-fab01c207d49"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "# version 1, 2, 6\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(0.75 + np.random.random(self.N_STOCKS) * 0.5)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(0.15 + np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(0.25 + np.random.random(1) * 0.35), self.N_STOCKS)\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          K = 0.75 + np.random.random(1) * 0.5\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32) # remember to change this!\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "52c4f306-5d94-4394-b9a9-a57c022c31f1"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.35, 0.35]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.25, 0.25]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "b0475211-558c-4b98-9fd5-7f058eb93954"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 36.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "4677ea62-a615-4a11-941d-b046c628a969"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.4798091948032379 average time 0.4904599884999925 iter num 20\n",
            "loss 0.37511003017425537 average time 0.2614305457499995 iter num 40\n",
            "loss 0.31043532490730286 average time 0.18509542631666326 iter num 60\n",
            "loss 0.3056291937828064 average time 0.14704274772499987 iter num 80\n",
            "loss 0.31630778312683105 average time 0.12403802296000321 iter num 100\n",
            "loss 0.24472853541374207 average time 0.10155521650001446 iter num 20\n",
            "loss 0.22874873876571655 average time 0.06671665587500968 iter num 40\n",
            "loss 0.24113500118255615 average time 0.055318749250005794 iter num 60\n",
            "loss 0.2216084748506546 average time 0.0494726686250047 iter num 80\n",
            "loss 0.2223050445318222 average time 0.04597564263000208 iter num 100\n",
            "loss 0.16626408696174622 average time 0.10130310964999581 iter num 20\n",
            "loss 0.15633787214756012 average time 0.06677647415000365 iter num 40\n",
            "loss 0.14141887426376343 average time 0.055149057116667184 iter num 60\n",
            "loss 0.14296317100524902 average time 0.04932300701249517 iter num 80\n",
            "loss 0.13729698956012726 average time 0.04592182069000046 iter num 100\n",
            "loss 0.10950792580842972 average time 0.10266233485002658 iter num 20\n",
            "loss 0.08538923412561417 average time 0.06731484930002125 iter num 40\n",
            "loss 0.06847448647022247 average time 0.0557217011166775 iter num 60\n",
            "loss 0.08181855827569962 average time 0.049739619475008115 iter num 80\n",
            "loss 0.07414538413286209 average time 0.04621309270999973 iter num 100\n",
            "loss 0.05243433639407158 average time 0.10194426070003146 iter num 20\n",
            "loss 0.03327015042304993 average time 0.06717612000001623 iter num 40\n",
            "loss 0.029417160898447037 average time 0.055510688583342474 iter num 60\n",
            "loss 0.024736009538173676 average time 0.0495875045000048 iter num 80\n",
            "loss 0.027336422353982925 average time 0.04611453917000517 iter num 100\n",
            "loss 0.022463269531726837 average time 0.10033071360001031 iter num 20\n",
            "loss 0.024819135665893555 average time 0.06636481310000023 iter num 40\n",
            "loss 0.01237371377646923 average time 0.05505146088332822 iter num 60\n",
            "loss 0.014563067816197872 average time 0.0494084716374914 iter num 80\n",
            "loss 0.016970869153738022 average time 0.04592361393999227 iter num 100\n",
            "loss 0.01611987315118313 average time 0.1005955386999858 iter num 20\n",
            "loss 0.01059641968458891 average time 0.0664564749749843 iter num 40\n",
            "loss 0.01459028385579586 average time 0.054986477099987496 iter num 60\n",
            "loss 0.01423064898699522 average time 0.0492182334124891 iter num 80\n",
            "loss 0.014295610599219799 average time 0.045798679139991236 iter num 100\n",
            "loss 0.012367111630737782 average time 0.10101114325000253 iter num 20\n",
            "loss 0.011303868144750595 average time 0.06677896259998875 iter num 40\n",
            "loss 0.024652231484651566 average time 0.055460484966662685 iter num 60\n",
            "loss 0.01650618202984333 average time 0.04961902767500419 iter num 80\n",
            "loss 0.01620488613843918 average time 0.046274385150006765 iter num 100\n",
            "loss 0.010719998739659786 average time 0.10069764050000458 iter num 20\n",
            "loss 0.015796106308698654 average time 0.06642255482499308 iter num 40\n",
            "loss 0.016518453136086464 average time 0.05482928890000191 iter num 60\n",
            "loss 0.011373303830623627 average time 0.04924074693750526 iter num 80\n",
            "loss 0.016549838706851006 average time 0.04593281706000198 iter num 100\n",
            "loss 0.012174999341368675 average time 0.10186141335004777 iter num 20\n",
            "loss 0.012833500280976295 average time 0.06743284760002552 iter num 40\n",
            "loss 0.011965063400566578 average time 0.055562402016706376 iter num 60\n",
            "loss 0.01526025403290987 average time 0.04964113408752837 iter num 80\n",
            "loss 0.016663677990436554 average time 0.04619489835002241 iter num 100\n",
            "loss 0.012336243875324726 average time 0.09982989964998978 iter num 20\n",
            "loss 0.00796782411634922 average time 0.06564898102500365 iter num 40\n",
            "loss 0.01791241019964218 average time 0.054327027150005355 iter num 60\n",
            "loss 0.02923065796494484 average time 0.04872956092499976 iter num 80\n",
            "loss 0.01146649569272995 average time 0.045415482919997884 iter num 100\n",
            "loss 0.011138269677758217 average time 0.10218988110002555 iter num 20\n",
            "loss 0.020509319379925728 average time 0.06688479862503982 iter num 40\n",
            "loss 0.02184423618018627 average time 0.055520296433322375 iter num 60\n",
            "loss 0.02227596379816532 average time 0.049606161812505434 iter num 80\n",
            "loss 0.014242637902498245 average time 0.046163850530001584 iter num 100\n",
            "loss 0.01766926236450672 average time 0.10162396749997242 iter num 20\n",
            "loss 0.009068924002349377 average time 0.06698264142499966 iter num 40\n",
            "loss 0.0091010183095932 average time 0.05523569015000097 iter num 60\n",
            "loss 0.018215235322713852 average time 0.049389523074989936 iter num 80\n",
            "loss 0.011075455695390701 average time 0.046000636249978015 iter num 100\n",
            "loss 0.015143418684601784 average time 0.10104450104995522 iter num 20\n",
            "loss 0.008894586935639381 average time 0.0672656201999871 iter num 40\n",
            "loss 0.012472030706703663 average time 0.05571562420000191 iter num 60\n",
            "loss 0.010671744123101234 average time 0.04981553757500592 iter num 80\n",
            "loss 0.011249744333326817 average time 0.04627427310999792 iter num 100\n",
            "loss 0.016611294820904732 average time 0.10023539114997675 iter num 20\n",
            "loss 0.016311589628458023 average time 0.06632833702494736 iter num 40\n",
            "loss 0.02577597089111805 average time 0.05485315838329067 iter num 60\n",
            "loss 0.019420113414525986 average time 0.0490826878749715 iter num 80\n",
            "loss 0.010302169248461723 average time 0.045751464559984925 iter num 100\n",
            "loss 0.015675721690058708 average time 0.10064891430001807 iter num 20\n",
            "loss 0.009218974970281124 average time 0.06623806092501354 iter num 40\n",
            "loss 0.00492606358602643 average time 0.05490621998332396 iter num 60\n",
            "loss 0.01681305654346943 average time 0.04925613244998885 iter num 80\n",
            "loss 0.01106277946382761 average time 0.045846544480000376 iter num 100\n",
            "loss 0.012822452932596207 average time 0.09911448175005262 iter num 20\n",
            "loss 0.008425530046224594 average time 0.06551846050001586 iter num 40\n",
            "loss 0.012856746092438698 average time 0.054403946099985966 iter num 60\n",
            "loss 0.016429565846920013 average time 0.048721460787487555 iter num 80\n",
            "loss 0.012041992507874966 average time 0.045421684899988574 iter num 100\n",
            "loss 0.013850565999746323 average time 0.10017665874997875 iter num 20\n",
            "loss 0.010493087582290173 average time 0.06601870517496308 iter num 40\n",
            "loss 0.014901658520102501 average time 0.0547078343333169 iter num 60\n",
            "loss 0.009502994827926159 average time 0.049056299612482236 iter num 80\n",
            "loss 0.010012433864176273 average time 0.04571895151998888 iter num 100\n",
            "loss 0.009897473268210888 average time 0.10080401865000113 iter num 20\n",
            "loss 0.006386664230376482 average time 0.06639723987500475 iter num 40\n",
            "loss 0.010221344418823719 average time 0.05502385641668752 iter num 60\n",
            "loss 0.009492974728345871 average time 0.049379569375025766 iter num 80\n",
            "loss 0.0095734354108572 average time 0.04593150647001494 iter num 100\n",
            "loss 0.009614164009690285 average time 0.10081683474998045 iter num 20\n",
            "loss 0.010477868840098381 average time 0.06641396910000594 iter num 40\n",
            "loss 0.01715029776096344 average time 0.054854675750016214 iter num 60\n",
            "loss 0.00949757918715477 average time 0.049299778587510446 iter num 80\n",
            "loss 0.02491573430597782 average time 0.045885085300001266 iter num 100\n",
            "loss 0.01596089079976082 average time 0.10025194845006809 iter num 20\n",
            "loss 0.011815963312983513 average time 0.06600359625000465 iter num 40\n",
            "loss 0.009635001420974731 average time 0.054882305766659555 iter num 60\n",
            "loss 0.010741905309259892 average time 0.04906610219998697 iter num 80\n",
            "loss 0.008504046127200127 average time 0.04558300889999373 iter num 100\n",
            "loss 0.00553332781419158 average time 0.1002270811000244 iter num 20\n",
            "loss 0.01078308466821909 average time 0.06630163482499256 iter num 40\n",
            "loss 0.004511043895035982 average time 0.0548020881833357 iter num 60\n",
            "loss 0.008707663044333458 average time 0.04916098039999497 iter num 80\n",
            "loss 0.008457188494503498 average time 0.04580799297998965 iter num 100\n",
            "loss 0.01669173873960972 average time 0.10053513099999237 iter num 20\n",
            "loss 0.010087719187140465 average time 0.06631816569999956 iter num 40\n",
            "loss 0.013365697115659714 average time 0.054965990016648904 iter num 60\n",
            "loss 0.013978509232401848 average time 0.04913344877496684 iter num 80\n",
            "loss 0.009345858357846737 average time 0.045687929129971966 iter num 100\n",
            "loss 0.006038840394467115 average time 0.10058217685000273 iter num 20\n",
            "loss 0.009455021470785141 average time 0.06633599595000987 iter num 40\n",
            "loss 0.009025055915117264 average time 0.05480186361668681 iter num 60\n",
            "loss 0.0056161596439778805 average time 0.049071942075005384 iter num 80\n",
            "loss 0.013281695544719696 average time 0.04566474311000548 iter num 100\n",
            "loss 0.01027604192495346 average time 0.09999172660000114 iter num 20\n",
            "loss 0.008228443562984467 average time 0.06622142114993039 iter num 40\n",
            "loss 0.0069446987472474575 average time 0.05477808548330358 iter num 60\n",
            "loss 0.006388634443283081 average time 0.049139083287479936 iter num 80\n",
            "loss 0.005743412300944328 average time 0.04574104429997533 iter num 100\n",
            "loss 0.005407633259892464 average time 0.09984629209993727 iter num 20\n",
            "loss 0.014427684247493744 average time 0.06596895029992993 iter num 40\n",
            "loss 0.007518533151596785 average time 0.05473581859993525 iter num 60\n",
            "loss 0.011673839762806892 average time 0.049110366174966204 iter num 80\n",
            "loss 0.006406066473573446 average time 0.0457919816699723 iter num 100\n",
            "loss 0.015147695317864418 average time 0.10111904814993977 iter num 20\n",
            "loss 0.005053841974586248 average time 0.06661035817501215 iter num 40\n",
            "loss 0.012520176358520985 average time 0.05514779685004214 iter num 60\n",
            "loss 0.009673530235886574 average time 0.04941754303752077 iter num 80\n",
            "loss 0.0070275673642754555 average time 0.04600422080001408 iter num 100\n",
            "loss 0.008330058306455612 average time 0.10046422905002146 iter num 20\n",
            "loss 0.007079727482050657 average time 0.06603445395001017 iter num 40\n",
            "loss 0.012861830182373524 average time 0.0546507874499639 iter num 60\n",
            "loss 0.010322102345526218 average time 0.04891766439997127 iter num 80\n",
            "loss 0.009097064845263958 average time 0.04549878491999152 iter num 100\n",
            "loss 0.01077330857515335 average time 0.10033680240003377 iter num 20\n",
            "loss 0.005797857418656349 average time 0.06635654462502316 iter num 40\n",
            "loss 0.0072224559262394905 average time 0.054907678883364494 iter num 60\n",
            "loss 0.006255855783820152 average time 0.04931463822500746 iter num 80\n",
            "loss 0.008082999847829342 average time 0.045904128699994544 iter num 100\n",
            "loss 0.005058096721768379 average time 0.09999494399999094 iter num 20\n",
            "loss 0.005322772543877363 average time 0.06602284672496808 iter num 40\n",
            "loss 0.004690075293183327 average time 0.05479812246664248 iter num 60\n",
            "loss 0.0057524205185472965 average time 0.04900908439996101 iter num 80\n",
            "loss 0.01503397524356842 average time 0.04569890247997137 iter num 100\n",
            "loss 0.00455668568611145 average time 0.10085402580002664 iter num 20\n",
            "loss 0.004122441168874502 average time 0.06650006197498896 iter num 40\n",
            "loss 0.004310918040573597 average time 0.054950713416640914 iter num 60\n",
            "loss 0.005190777126699686 average time 0.049229656437512405 iter num 80\n",
            "loss 0.007542652543634176 average time 0.04578407724002318 iter num 100\n",
            "loss 0.003684533527120948 average time 0.10244543440003326 iter num 20\n",
            "loss 0.004770385567098856 average time 0.0670230589999619 iter num 40\n",
            "loss 0.00718903262168169 average time 0.05538432219994623 iter num 60\n",
            "loss 0.005310482811182737 average time 0.049585693124913635 iter num 80\n",
            "loss 0.0088002635166049 average time 0.04605738007995569 iter num 100\n",
            "loss 0.004996718838810921 average time 0.10087371090012312 iter num 20\n",
            "loss 0.004076423589140177 average time 0.06625858327511196 iter num 40\n",
            "loss 0.0032008993439376354 average time 0.05494437106678258 iter num 60\n",
            "loss 0.009446063078939915 average time 0.049226207462561435 iter num 80\n",
            "loss 0.009149083867669106 average time 0.04582705483004247 iter num 100\n",
            "loss 0.003734632395207882 average time 0.10011965595008405 iter num 20\n",
            "loss 0.0022047634702175856 average time 0.06612167822506762 iter num 40\n",
            "loss 0.00785467866808176 average time 0.05470748413340516 iter num 60\n",
            "loss 0.009747246280312538 average time 0.04903531171253235 iter num 80\n",
            "loss 0.005847002845257521 average time 0.04557486695003718 iter num 100\n",
            "loss 0.003092854516580701 average time 0.10105532544998823 iter num 20\n",
            "loss 0.002721350872889161 average time 0.06650523690002501 iter num 40\n",
            "loss 0.003829387715086341 average time 0.05520911336667875 iter num 60\n",
            "loss 0.009800623171031475 average time 0.04947368096253513 iter num 80\n",
            "loss 0.0025216194335371256 average time 0.04591576790007821 iter num 100\n",
            "loss 0.012186415493488312 average time 0.10050118830004066 iter num 20\n",
            "loss 0.009310519322752953 average time 0.06647069922507853 iter num 40\n",
            "loss 0.004237260669469833 average time 0.05507790218343871 iter num 60\n",
            "loss 0.005645947530865669 average time 0.04915658495006028 iter num 80\n",
            "loss 0.007377550937235355 average time 0.04576495739003803 iter num 100\n",
            "loss 0.010189387947320938 average time 0.10026114184997822 iter num 20\n",
            "loss 0.0030077698174864054 average time 0.06604857569996056 iter num 40\n",
            "loss 0.005281390622258186 average time 0.054731084816664104 iter num 60\n",
            "loss 0.0038240717258304358 average time 0.04903203742500182 iter num 80\n",
            "loss 0.006299483589828014 average time 0.045686244739999896 iter num 100\n",
            "loss 0.003517161589115858 average time 0.10112348675002067 iter num 20\n",
            "loss 0.005053790286183357 average time 0.06663848487501127 iter num 40\n",
            "loss 0.009574693627655506 average time 0.055278051700012536 iter num 60\n",
            "loss 0.004172423854470253 average time 0.04946322652498338 iter num 80\n",
            "loss 0.002462600124999881 average time 0.04592176370998459 iter num 100\n",
            "loss 0.004314950201660395 average time 0.1002149129999907 iter num 20\n",
            "loss 0.004691495560109615 average time 0.06602939914996568 iter num 40\n",
            "loss 0.0033618956804275513 average time 0.05471927333334558 iter num 60\n",
            "loss 0.0034758339170366526 average time 0.049042812612515266 iter num 80\n",
            "loss 0.003469177521765232 average time 0.04563931349000086 iter num 100\n",
            "loss 0.0037904910277575254 average time 0.10165776735016152 iter num 20\n",
            "loss 0.006999982986599207 average time 0.06676324335010123 iter num 40\n",
            "loss 0.0028144652023911476 average time 0.055063710100042344 iter num 60\n",
            "loss 0.001977145904675126 average time 0.04932429413754562 iter num 80\n",
            "loss 0.005492159631103277 average time 0.04590238227003283 iter num 100\n",
            "loss 0.00998265203088522 average time 0.10080975800001397 iter num 20\n",
            "loss 0.008345923386514187 average time 0.06653964170004656 iter num 40\n",
            "loss 0.0017949315952137113 average time 0.05478331711666821 iter num 60\n",
            "loss 0.0047702062875032425 average time 0.049034049325018716 iter num 80\n",
            "loss 0.0011435789056122303 average time 0.04556396993002636 iter num 100\n",
            "loss 0.002681691199541092 average time 0.09919965630006118 iter num 20\n",
            "loss 0.003009143518283963 average time 0.065636493875013 iter num 40\n",
            "loss 0.00333226565271616 average time 0.05454366526666187 iter num 60\n",
            "loss 0.006391574628651142 average time 0.04885977498747707 iter num 80\n",
            "loss 0.006321253255009651 average time 0.04554428467996331 iter num 100\n",
            "loss 0.0033346146810799837 average time 0.10097882385020966 iter num 20\n",
            "loss 0.003152798395603895 average time 0.06654691645017011 iter num 40\n",
            "loss 0.0013024656800553203 average time 0.05515154865009511 iter num 60\n",
            "loss 0.002005611779168248 average time 0.04961772761259908 iter num 80\n",
            "loss 0.004188257735222578 average time 0.046057311000085976 iter num 100\n",
            "loss 0.001474678167141974 average time 0.10262670904994593 iter num 20\n",
            "loss 0.002115904586389661 average time 0.06766786449995835 iter num 40\n",
            "loss 0.0026145540177822113 average time 0.05603309954994984 iter num 60\n",
            "loss 0.005362995900213718 average time 0.05010111343746075 iter num 80\n",
            "loss 0.004607621114701033 average time 0.046635173129989195 iter num 100\n",
            "loss 0.0034305774606764317 average time 0.10070600765011477 iter num 20\n",
            "loss 0.006028620060533285 average time 0.0669233371251039 iter num 40\n",
            "loss 0.001443754998035729 average time 0.055184001416697964 iter num 60\n",
            "loss 0.00301358918659389 average time 0.04943604808750024 iter num 80\n",
            "loss 0.006427624728530645 average time 0.04604012828000123 iter num 100\n",
            "loss 0.003926299046725035 average time 0.10135501579998163 iter num 20\n",
            "loss 0.0018773393239825964 average time 0.06688169725002809 iter num 40\n",
            "loss 0.003071678103879094 average time 0.055339744666692545 iter num 60\n",
            "loss 0.0027870323974639177 average time 0.04950181301250041 iter num 80\n",
            "loss 0.0028677056543529034 average time 0.04600701291001315 iter num 100\n",
            "loss 0.0015479957219213247 average time 0.10194610594985534 iter num 20\n",
            "loss 0.006826965603977442 average time 0.06742145339992475 iter num 40\n",
            "loss 0.00763015216216445 average time 0.0556388057166411 iter num 60\n",
            "loss 0.0025425220374017954 average time 0.04984892921245319 iter num 80\n",
            "loss 0.00262389425188303 average time 0.04649275168995701 iter num 100\n",
            "loss 0.00155974505469203 average time 0.10299186929996722 iter num 20\n",
            "loss 0.004014056175947189 average time 0.06779067007494177 iter num 40\n",
            "loss 0.0010417315643280745 average time 0.05581698921666126 iter num 60\n",
            "loss 0.0018903265008702874 average time 0.049870927262520584 iter num 80\n",
            "loss 0.001306025544181466 average time 0.04641290552002829 iter num 100\n",
            "loss 0.0006644995301030576 average time 0.10044350619991746 iter num 20\n",
            "loss 0.0011002480750903487 average time 0.06660374612497436 iter num 40\n",
            "loss 0.005089154001325369 average time 0.05511005773329695 iter num 60\n",
            "loss 0.0012941573513671756 average time 0.049361697312440354 iter num 80\n",
            "loss 0.0028025468345731497 average time 0.04604522561997328 iter num 100\n",
            "loss 0.0031393333338201046 average time 0.10121968174985341 iter num 20\n",
            "loss 0.002163894474506378 average time 0.06664020254984279 iter num 40\n",
            "loss 0.0016309077618643641 average time 0.055157240616578444 iter num 60\n",
            "loss 0.0005788148264400661 average time 0.0493173739124245 iter num 80\n",
            "loss 0.0011893629562109709 average time 0.045907076869934824 iter num 100\n",
            "loss 0.00338631309568882 average time 0.10116589610001939 iter num 20\n",
            "loss 0.0033975590486079454 average time 0.06665634802504883 iter num 40\n",
            "loss 0.0011727462988346815 average time 0.05526468453334322 iter num 60\n",
            "loss 0.0038061889354139566 average time 0.04944234462498116 iter num 80\n",
            "loss 0.009135710075497627 average time 0.045992288699962955 iter num 100\n",
            "loss 0.0020405109971761703 average time 0.10135749034989203 iter num 20\n",
            "loss 0.0009348623570986092 average time 0.06658788639997511 iter num 40\n",
            "loss 0.00039486808236688375 average time 0.05523068663334622 iter num 60\n",
            "loss 0.0015877829864621162 average time 0.04953865665000876 iter num 80\n",
            "loss 0.0036471476778388023 average time 0.046105696800022994 iter num 100\n",
            "loss 0.0021732195746153593 average time 0.10074675585001387 iter num 20\n",
            "loss 0.0028319114353507757 average time 0.06645356210003683 iter num 40\n",
            "loss 0.001059538684785366 average time 0.05493584066666699 iter num 60\n",
            "loss 0.0006980508333072066 average time 0.04936632661249405 iter num 80\n",
            "loss 0.00250932015478611 average time 0.045967843760017786 iter num 100\n",
            "loss 0.0014541279524564743 average time 0.10042599294997671 iter num 20\n",
            "loss 0.0005855067865923047 average time 0.06631078819998493 iter num 40\n",
            "loss 0.002688679378479719 average time 0.05503589788321127 iter num 60\n",
            "loss 0.001365806208923459 average time 0.049162128037448835 iter num 80\n",
            "loss 0.0007106802077032626 average time 0.04565695179991053 iter num 100\n",
            "loss 0.0034426867496222258 average time 0.10050582664998728 iter num 20\n",
            "loss 0.0007163072586990893 average time 0.06631382484988535 iter num 40\n",
            "loss 0.0003939528833143413 average time 0.054888002766589734 iter num 60\n",
            "loss 0.0016285269521176815 average time 0.04917481242491704 iter num 80\n",
            "loss 0.0038183045107871294 average time 0.045705099109964065 iter num 100\n",
            "loss 0.0015009647468104959 average time 0.10119408359996669 iter num 20\n",
            "loss 0.0025922786444425583 average time 0.06677091350020418 iter num 40\n",
            "loss 0.0011960839619860053 average time 0.05514425593340396 iter num 60\n",
            "loss 0.0025380749721080065 average time 0.049351232175035877 iter num 80\n",
            "loss 0.0005040796822868288 average time 0.04587930845007577 iter num 100\n",
            "loss 0.00044867894030176103 average time 0.10291038430023036 iter num 20\n",
            "loss 0.0013819618616253138 average time 0.06737086075004299 iter num 40\n",
            "loss 0.0006890805088914931 average time 0.05568164949997178 iter num 60\n",
            "loss 0.00331916194409132 average time 0.04984065253747758 iter num 80\n",
            "loss 0.0006961310282349586 average time 0.04633621392998975 iter num 100\n",
            "loss 0.0014977568062022328 average time 0.10095578630034652 iter num 20\n",
            "loss 0.002354256110265851 average time 0.06644774895012233 iter num 40\n",
            "loss 0.0010096647311002016 average time 0.05495614195003024 iter num 60\n",
            "loss 0.00038227945333346725 average time 0.0491917169625367 iter num 80\n",
            "loss 0.0011365710524842143 average time 0.045774444850012516 iter num 100\n",
            "loss 0.0006718073855154216 average time 0.10237015630018505 iter num 20\n",
            "loss 0.0015168000245466828 average time 0.06736937115024375 iter num 40\n",
            "loss 0.0006270240410231054 average time 0.05574797500027368 iter num 60\n",
            "loss 0.0006682134116999805 average time 0.04975712110024233 iter num 80\n",
            "loss 0.001887895748950541 average time 0.046224468830187104 iter num 100\n",
            "loss 0.005106065422296524 average time 0.10081208260035054 iter num 20\n",
            "loss 0.0013040858320891857 average time 0.06656891757529593 iter num 40\n",
            "loss 0.0015807639574632049 average time 0.0550407851167923 iter num 60\n",
            "loss 0.0007849512039683759 average time 0.04934351016263463 iter num 80\n",
            "loss 0.002235138788819313 average time 0.045840775620163185 iter num 100\n",
            "loss 0.002500983653590083 average time 0.10166455000007772 iter num 20\n",
            "loss 0.00046536888112314045 average time 0.06700810689994796 iter num 40\n",
            "loss 0.002009972231462598 average time 0.0553745566999775 iter num 60\n",
            "loss 0.00023789569968357682 average time 0.04949750396244781 iter num 80\n",
            "loss 0.0006371106137521565 average time 0.046021883199937295 iter num 100\n",
            "loss 0.000578048056922853 average time 0.10145590675001585 iter num 20\n",
            "loss 0.0005035906797274947 average time 0.06682342777498888 iter num 40\n",
            "loss 0.003372806590050459 average time 0.05541533046677311 iter num 60\n",
            "loss 0.0009913798421621323 average time 0.04973234927508656 iter num 80\n",
            "loss 0.002080329693853855 average time 0.046226500230077364 iter num 100\n",
            "loss 0.000964111415669322 average time 0.1007324230501581 iter num 20\n",
            "loss 0.004505363758653402 average time 0.06654335792504754 iter num 40\n",
            "loss 0.0008702410268597305 average time 0.05503971051669699 iter num 60\n",
            "loss 0.005169246345758438 average time 0.04940686367503986 iter num 80\n",
            "loss 0.000948005763348192 average time 0.04595951791998232 iter num 100\n",
            "loss 0.0024800098035484552 average time 0.10086304419965017 iter num 20\n",
            "loss 0.0002882837725337595 average time 0.06651438394997058 iter num 40\n",
            "loss 0.0002441771503072232 average time 0.054947256516607014 iter num 60\n",
            "loss 0.0021450382191687822 average time 0.049261856000021 iter num 80\n",
            "loss 0.00502606388181448 average time 0.04582636712997555 iter num 100\n",
            "loss 0.0008751553250476718 average time 0.10236194375020205 iter num 20\n",
            "loss 0.0016818871954455972 average time 0.0671592276251431 iter num 40\n",
            "loss 0.002694904338568449 average time 0.05545333055006267 iter num 60\n",
            "loss 0.0018825294682756066 average time 0.049561430662606654 iter num 80\n",
            "loss 0.0009482096065767109 average time 0.04613573021011689 iter num 100\n",
            "loss 0.0007437358144670725 average time 0.1020170157499706 iter num 20\n",
            "loss 0.00072825001552701 average time 0.06691003212508803 iter num 40\n",
            "loss 0.0010089876595884562 average time 0.05518457646664198 iter num 60\n",
            "loss 0.0006804349250160158 average time 0.04935622408747804 iter num 80\n",
            "loss 0.0008808679413050413 average time 0.04596701401998871 iter num 100\n",
            "loss 0.0005310251726768911 average time 0.10215408145004404 iter num 20\n",
            "loss 0.0005196539568714797 average time 0.06746139024990043 iter num 40\n",
            "loss 0.00033465807791799307 average time 0.05560128111659045 iter num 60\n",
            "loss 0.0007511673611588776 average time 0.04977072551246238 iter num 80\n",
            "loss 0.0005094861844554543 average time 0.0462951389599948 iter num 100\n",
            "loss 0.0007975329062901437 average time 0.10131831685021098 iter num 20\n",
            "loss 0.003073637606576085 average time 0.0666370680500222 iter num 40\n",
            "loss 0.0016870504477992654 average time 0.05527022085010079 iter num 60\n",
            "loss 0.0013208008604124188 average time 0.04949338627507131 iter num 80\n",
            "loss 0.0005361459916457534 average time 0.046030445330052315 iter num 100\n",
            "loss 0.0002617260324768722 average time 0.10168605764993117 iter num 20\n",
            "loss 0.00047495460603386164 average time 0.06687760077497842 iter num 40\n",
            "loss 0.00038070636219345033 average time 0.055808781616724444 iter num 60\n",
            "loss 0.0006015623221173882 average time 0.050002953812531815 iter num 80\n",
            "loss 0.0016220617108047009 average time 0.046374555980055444 iter num 100\n",
            "loss 0.0008836586493998766 average time 0.10275563594987033 iter num 20\n",
            "loss 0.0006987075903452933 average time 0.06739764599983573 iter num 40\n",
            "loss 0.0019152893219143152 average time 0.05553316189992377 iter num 60\n",
            "loss 0.0007952884188853204 average time 0.04968429298744468 iter num 80\n",
            "loss 0.001839994452893734 average time 0.04608437690998471 iter num 100\n",
            "loss 0.0014077648520469666 average time 0.10093625704985243 iter num 20\n",
            "loss 0.0007734211394563317 average time 0.0664990995998778 iter num 40\n",
            "loss 0.0008971459465101361 average time 0.05498084148324172 iter num 60\n",
            "loss 0.0006059347651898861 average time 0.049202612174963176 iter num 80\n",
            "loss 0.0005833883187733591 average time 0.04592086846994789 iter num 100\n",
            "loss 0.0002522805007174611 average time 0.10135735254962128 iter num 20\n",
            "loss 0.0011846693232655525 average time 0.06669775302489142 iter num 40\n",
            "loss 0.0015987384831532836 average time 0.055203758583259814 iter num 60\n",
            "loss 0.0009209754643961787 average time 0.049469863062472544 iter num 80\n",
            "loss 0.000837061321362853 average time 0.045986324079949556 iter num 100\n",
            "loss 0.000215431849937886 average time 0.10121068520029439 iter num 20\n",
            "loss 0.0007012435817159712 average time 0.06706112327515257 iter num 40\n",
            "loss 0.0021064034663140774 average time 0.055554436950084586 iter num 60\n",
            "loss 0.0013093664310872555 average time 0.049802582812503715 iter num 80\n",
            "loss 0.0002617043210193515 average time 0.04623907606001012 iter num 100\n",
            "loss 0.0010988324647769332 average time 0.10085913400007485 iter num 20\n",
            "loss 0.0011901879915967584 average time 0.06646400877502856 iter num 40\n",
            "loss 0.0005223745829425752 average time 0.05508107735016286 iter num 60\n",
            "loss 0.0005844809347763658 average time 0.04930585838762909 iter num 80\n",
            "loss 0.0017355665331706405 average time 0.04591225955011396 iter num 100\n",
            "loss 0.000717317103408277 average time 0.10236443814974336 iter num 20\n",
            "loss 0.000403939513489604 average time 0.06707453927483584 iter num 40\n",
            "loss 0.0007937898044474423 average time 0.055280648783294355 iter num 60\n",
            "loss 0.0006150552653707564 average time 0.049429626725032 iter num 80\n",
            "loss 0.0002399938239250332 average time 0.04600979152006403 iter num 100\n",
            "loss 0.00034716216032393277 average time 0.10071381009984179 iter num 20\n",
            "loss 0.0005306689999997616 average time 0.06662930579987006 iter num 40\n",
            "loss 0.0003119602333754301 average time 0.05516751498328934 iter num 60\n",
            "loss 0.0006743957637809217 average time 0.04938504999993256 iter num 80\n",
            "loss 0.0007377432193607092 average time 0.04583294298998226 iter num 100\n",
            "loss 0.0006070506642572582 average time 0.10184373644997322 iter num 20\n",
            "loss 0.0005043373676016927 average time 0.06706572977495853 iter num 40\n",
            "loss 0.0001606600999366492 average time 0.05526431658327056 iter num 60\n",
            "loss 0.0019431565888226032 average time 0.04956113393741361 iter num 80\n",
            "loss 0.0008773035369813442 average time 0.04607053735990121 iter num 100\n",
            "loss 0.0005826577544212341 average time 0.10262325189996772 iter num 20\n",
            "loss 0.00032501594978384674 average time 0.06742922437501875 iter num 40\n",
            "loss 0.0012292260071262717 average time 0.055442701516767556 iter num 60\n",
            "loss 0.0005097586545161903 average time 0.04959412425007485 iter num 80\n",
            "loss 0.0006305219139903784 average time 0.04599874355009888 iter num 100\n",
            "loss 0.0004793455882463604 average time 0.10143749875005596 iter num 20\n",
            "loss 0.002286328002810478 average time 0.066716238825029 iter num 40\n",
            "loss 0.00038581297849304974 average time 0.05516789078337751 iter num 60\n",
            "loss 0.0015875542303547263 average time 0.04942361632508892 iter num 80\n",
            "loss 0.00044094386976212263 average time 0.045985138140076744 iter num 100\n",
            "loss 0.0004907455877400935 average time 0.10109763075015507 iter num 20\n",
            "loss 0.00043140596244484186 average time 0.06661888332500894 iter num 40\n",
            "loss 0.0003551848349161446 average time 0.05517691981670699 iter num 60\n",
            "loss 0.0011804844252765179 average time 0.04936039467502269 iter num 80\n",
            "loss 0.0013071313733235002 average time 0.045856526589977874 iter num 100\n",
            "loss 0.00024587384541518986 average time 0.1004731157000606 iter num 20\n",
            "loss 0.0010315371910110116 average time 0.06606966509998528 iter num 40\n",
            "loss 0.0006611288408748806 average time 0.05472562061668214 iter num 60\n",
            "loss 0.00033917761174961925 average time 0.04914793046250452 iter num 80\n",
            "loss 0.0019151864107698202 average time 0.04577475341002355 iter num 100\n",
            "loss 0.0005269410903565586 average time 0.1014429198498874 iter num 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9251a825839c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m           \u001b[0;31m################################################################################################### generate random input numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m           \u001b[0minitial_stocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.75\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m           \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STOCKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# assume no correlation between stocks here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   3567\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected input type for array: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0;32m--> 461\u001b[0;31m                                        weak_type=new_weak_type)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbitcast_convert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    266\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a735ae-cc05-4efc-e677-3cd250ec19b3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_pricedelta_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3432ba72-25e3-4507-d373-38ed2f12b9d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73baacf1-0a85-4e65-ad42-6a7e6c4cf573"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_pricedelta_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39caa3c-449e-4bc4-dd00-dd705158b160"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6e233c-b19a-4a16-96a1-de53fdfa2194"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 2000000, batch = 8, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 10\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 30)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_pricedelta_4.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.0003841693978756666 average time 0.5690565446002438 iter num 10\n",
            "loss 0.00019969901768490672 average time 0.2896439497004394 iter num 20\n",
            "loss 0.0003055417037103325 average time 0.19650686630059985 iter num 30\n",
            "loss 6.988467794144526e-05 average time 0.15006869707558507 iter num 40\n",
            "loss 0.00018562970217317343 average time 0.12208804602043528 iter num 50\n",
            "loss 0.00022086316312197596 average time 0.10345371651716656 iter num 60\n",
            "loss 5.261728074401617e-05 average time 0.0901659875576505 iter num 70\n",
            "loss 0.00037479648017324507 average time 0.0802129490754396 iter num 80\n",
            "loss 7.945597462821752e-05 average time 0.0724342811336909 iter num 90\n",
            "loss 0.0010689797345548868 average time 0.06623000957028126 iter num 100\n",
            "loss 5.190215597394854e-05 average time 0.33324962090009647 iter num 10\n",
            "loss 7.92388163972646e-05 average time 0.17187969159949718 iter num 20\n",
            "loss 0.0003374926745891571 average time 0.11799609506621588 iter num 30\n",
            "loss 0.0004288842319510877 average time 0.0911793219746869 iter num 40\n",
            "loss 0.00011017902579624206 average time 0.07503445659960561 iter num 50\n",
            "loss 0.0001543379039503634 average time 0.06437394141642774 iter num 60\n",
            "loss 2.4077429770841263e-05 average time 0.056692196528456404 iter num 70\n",
            "loss 0.00011039411765523255 average time 0.05091563643741211 iter num 80\n",
            "loss 0.00031052270787768066 average time 0.04639783087768592 iter num 90\n",
            "loss 0.00014698690210934728 average time 0.04278943102981429 iter num 100\n",
            "loss 0.0006281908717937768 average time 0.3329301739002403 iter num 10\n",
            "loss 0.00027013482758775353 average time 0.1716266841000106 iter num 20\n",
            "loss 5.816221528220922e-05 average time 0.11786270513330237 iter num 30\n",
            "loss 0.000316553603624925 average time 0.09103328202472767 iter num 40\n",
            "loss 0.0001290571817662567 average time 0.07490335513983154 iter num 50\n",
            "loss 3.1499253964284435e-05 average time 0.0641501840166408 iter num 60\n",
            "loss 0.00012451130896806717 average time 0.056478598214276386 iter num 70\n",
            "loss 7.449393888236955e-05 average time 0.05075547031251517 iter num 80\n",
            "loss 0.00010997534991474822 average time 0.04626073035558673 iter num 90\n",
            "loss 0.00010147372813662514 average time 0.0427062591300637 iter num 100\n",
            "loss 0.0001219960322487168 average time 0.33272324820027277 iter num 10\n",
            "loss 0.007115728687494993 average time 0.1715466166002443 iter num 20\n",
            "loss 0.00011620101577136666 average time 0.1178384548001001 iter num 30\n",
            "loss 0.0001732243108563125 average time 0.09096308095013228 iter num 40\n",
            "loss 0.00012466064072214067 average time 0.0749237682401872 iter num 50\n",
            "loss 0.0001297441776841879 average time 0.0641416351333343 iter num 60\n",
            "loss 5.89562114328146e-05 average time 0.05646575911414402 iter num 70\n",
            "loss 0.00019514829909894615 average time 0.05069424812486432 iter num 80\n",
            "loss 0.00034877052530646324 average time 0.04618810544436403 iter num 90\n",
            "loss 9.125406359089538e-05 average time 0.042653193009900864 iter num 100\n",
            "loss 0.0004070694267284125 average time 0.3321639033998508 iter num 10\n",
            "loss 0.00021937873680144548 average time 0.17119174520030356 iter num 20\n",
            "loss 6.494304398074746e-05 average time 0.11753125663356817 iter num 30\n",
            "loss 0.00033676333259791136 average time 0.09085204177508785 iter num 40\n",
            "loss 0.00027227698592469096 average time 0.07477910842011624 iter num 50\n",
            "loss 0.0002923593274317682 average time 0.06402444810009911 iter num 60\n",
            "loss 8.948515460360795e-05 average time 0.056338307257205344 iter num 70\n",
            "loss 6.415940879378468e-05 average time 0.05056545893748989 iter num 80\n",
            "loss 4.2136816773563623e-05 average time 0.04608831534444309 iter num 90\n",
            "loss 4.1832558054011315e-05 average time 0.04260076115995617 iter num 100\n",
            "loss 0.0003300870303064585 average time 0.33197374299998045 iter num 10\n",
            "loss 4.526353950495832e-05 average time 0.17115777995022655 iter num 20\n",
            "loss 0.00022253621136769652 average time 0.11761421270008819 iter num 30\n",
            "loss 0.000533956685103476 average time 0.09075913272499747 iter num 40\n",
            "loss 8.15759485703893e-05 average time 0.07463879356007964 iter num 50\n",
            "loss 0.0004468661791179329 average time 0.06396814198324137 iter num 60\n",
            "loss 0.00021772267064079642 average time 0.056278607799946 iter num 70\n",
            "loss 6.323423440335318e-05 average time 0.05053807408735338 iter num 80\n",
            "loss 6.481684977188706e-05 average time 0.046055085988822006 iter num 90\n",
            "loss 0.00014413893222808838 average time 0.04250457269001345 iter num 100\n",
            "loss 0.0008329971460625529 average time 0.3322057718003634 iter num 10\n",
            "loss 0.00020646635675802827 average time 0.17118162530005065 iter num 20\n",
            "loss 0.0001782181643648073 average time 0.1174883258666038 iter num 30\n",
            "loss 3.5028919228352606e-05 average time 0.09070389335001891 iter num 40\n",
            "loss 0.00012276721827220172 average time 0.0746578508400853 iter num 50\n",
            "loss 0.00018839850963559002 average time 0.06405191043334829 iter num 60\n",
            "loss 0.0001236679672729224 average time 0.05641366067133536 iter num 70\n",
            "loss 0.00017556951206643134 average time 0.050646016949895054 iter num 80\n",
            "loss 0.00010774606926133856 average time 0.04619345436658477 iter num 90\n",
            "loss 0.00029930437449365854 average time 0.04260443416995258 iter num 100\n",
            "loss 0.00022931456624064595 average time 0.33218257619955693 iter num 10\n",
            "loss 5.7889072195393965e-05 average time 0.1711803317999511 iter num 20\n",
            "loss 0.0002307177783222869 average time 0.11751327553332279 iter num 30\n",
            "loss 0.00019769923528656363 average time 0.09068450539989499 iter num 40\n",
            "loss 5.1411345339147374e-05 average time 0.0745811003799463 iter num 50\n",
            "loss 0.00012883283488918096 average time 0.0638443484832654 iter num 60\n",
            "loss 0.0004645709414035082 average time 0.05632771492834893 iter num 70\n",
            "loss 0.0003264989354647696 average time 0.050570801874800966 iter num 80\n",
            "loss 0.00013248219329398125 average time 0.046144948133183386 iter num 90\n",
            "loss 5.230564420344308e-05 average time 0.04257393034986308 iter num 100\n",
            "loss 0.0018632535357028246 average time 0.3324857105002593 iter num 10\n",
            "loss 0.00016350482474081218 average time 0.17140587095036608 iter num 20\n",
            "loss 6.341798143694177e-05 average time 0.11794368016708176 iter num 30\n",
            "loss 0.0005645215278491378 average time 0.09116394430029687 iter num 40\n",
            "loss 0.0002359650970902294 average time 0.07514468806039076 iter num 50\n",
            "loss 0.00019537089974619448 average time 0.06430850738373313 iter num 60\n",
            "loss 0.0001062173250829801 average time 0.056599588500258896 iter num 70\n",
            "loss 0.0010765244951471686 average time 0.050794759937707566 iter num 80\n",
            "loss 0.002827281365171075 average time 0.04630329433352421 iter num 90\n",
            "loss 2.649497582751792e-05 average time 0.042719774240104015 iter num 100\n",
            "loss 0.00015872875519562513 average time 0.33230996849997607 iter num 10\n",
            "loss 0.00013304682215675712 average time 0.17138246184968012 iter num 20\n",
            "loss 0.00015669366985093802 average time 0.11770587549945048 iter num 30\n",
            "loss 2.4448989279335365e-05 average time 0.09085454379946896 iter num 40\n",
            "loss 0.00020277651492506266 average time 0.07496493525941332 iter num 50\n",
            "loss 0.00027030293131247163 average time 0.06420732313284437 iter num 60\n",
            "loss 0.0004350280505605042 average time 0.05650474828513065 iter num 70\n",
            "loss 0.0002435491478536278 average time 0.05073616836193651 iter num 80\n",
            "loss 0.0001694865059107542 average time 0.046254524832854964 iter num 90\n",
            "loss 0.0001104584225686267 average time 0.0426694042595409 iter num 100\n",
            "loss 9.947914077201858e-05 average time 0.33296278800044093 iter num 10\n",
            "loss 0.00010812187247211114 average time 0.17189876210049987 iter num 20\n",
            "loss 3.4053606214001775e-05 average time 0.11808255706710043 iter num 30\n",
            "loss 0.00014114761142991483 average time 0.09115897905030579 iter num 40\n",
            "loss 0.00016340032743755728 average time 0.07496204402028525 iter num 50\n",
            "loss 0.00010578374349279329 average time 0.06418605123362794 iter num 60\n",
            "loss 0.00043470613309182227 average time 0.05649276655739024 iter num 70\n",
            "loss 0.00011340939090587199 average time 0.05073091846270472 iter num 80\n",
            "loss 0.0005272056441754103 average time 0.0462753285890762 iter num 90\n",
            "loss 6.476105045294389e-05 average time 0.042704529500151696 iter num 100\n",
            "loss 0.00031664909329265356 average time 0.3331431980008347 iter num 10\n",
            "loss 0.002873127581551671 average time 0.17183118970042416 iter num 20\n",
            "loss 5.895887079532258e-05 average time 0.1179774822670879 iter num 30\n",
            "loss 0.00017681824101600796 average time 0.09110085935035386 iter num 40\n",
            "loss 0.0009776586666703224 average time 0.07492701690011018 iter num 50\n",
            "loss 0.00010539479262661189 average time 0.06413937600009376 iter num 60\n",
            "loss 7.00719901942648e-05 average time 0.05656250270013905 iter num 70\n",
            "loss 0.0001643193536438048 average time 0.05076822573760183 iter num 80\n",
            "loss 0.00012017117842333391 average time 0.04624995894450371 iter num 90\n",
            "loss 9.644875535741448e-05 average time 0.042678426560123627 iter num 100\n",
            "loss 0.00011116136738564819 average time 0.3326686890999554 iter num 10\n",
            "loss 0.00016378356667701155 average time 0.171653435850385 iter num 20\n",
            "loss 0.00014189290232025087 average time 0.11788311486695117 iter num 30\n",
            "loss 1.7146227037301287e-05 average time 0.0910144064002452 iter num 40\n",
            "loss 0.00019670662004500628 average time 0.07486651052015077 iter num 50\n",
            "loss 9.28482404560782e-05 average time 0.0640993923167116 iter num 60\n",
            "loss 8.486178558086976e-05 average time 0.056417966128563944 iter num 70\n",
            "loss 0.00015736046771053225 average time 0.05067642283756868 iter num 80\n",
            "loss 0.00011479794193292037 average time 0.04629649313343786 iter num 90\n",
            "loss 0.00015242367226164788 average time 0.04269738391012652 iter num 100\n",
            "loss 0.00019399826123844832 average time 0.3326649119000649 iter num 10\n",
            "loss 8.273200364783406e-05 average time 0.17155767219974222 iter num 20\n",
            "loss 0.0009276021737605333 average time 0.1179747597330182 iter num 30\n",
            "loss 0.00012923571921419352 average time 0.09103086387476651 iter num 40\n",
            "loss 0.00010803606710396707 average time 0.07490609035972738 iter num 50\n",
            "loss 0.00016269584011752158 average time 0.06412955469980565 iter num 60\n",
            "loss 6.94588161422871e-05 average time 0.05643061627119356 iter num 70\n",
            "loss 0.00013420137111097574 average time 0.05066650181233854 iter num 80\n",
            "loss 8.204091864172369e-05 average time 0.04623765707765415 iter num 90\n",
            "loss 7.059689232846722e-05 average time 0.04263697868991585 iter num 100\n",
            "loss 0.00015557979349978268 average time 0.3325457267001184 iter num 10\n",
            "loss 0.00011361360520822927 average time 0.1714946758504084 iter num 20\n",
            "loss 0.0003020801814273 average time 0.11772527773367376 iter num 30\n",
            "loss 5.727449388359673e-05 average time 0.090931521125367 iter num 40\n",
            "loss 0.00022280325356405228 average time 0.07484248900036619 iter num 50\n",
            "loss 0.0001979709486477077 average time 0.06410816600030861 iter num 60\n",
            "loss 0.000190324368304573 average time 0.05644547698596268 iter num 70\n",
            "loss 0.00023068640439305454 average time 0.05067545131278166 iter num 80\n",
            "loss 5.266733205644414e-05 average time 0.04627482307797537 iter num 90\n",
            "loss 4.5648695959243923e-05 average time 0.04267967032017623 iter num 100\n",
            "loss 0.00014827547420281917 average time 0.33249949230048514 iter num 10\n",
            "loss 0.0001575692876940593 average time 0.17139513735037326 iter num 20\n",
            "loss 0.0006225252873264253 average time 0.1177096077670285 iter num 30\n",
            "loss 8.310128760058433e-05 average time 0.09082071200036808 iter num 40\n",
            "loss 0.0014736453304067254 average time 0.07479081574056182 iter num 50\n",
            "loss 0.00021919718710705638 average time 0.06405701616707422 iter num 60\n",
            "loss 0.00010508311970625073 average time 0.056381765686039996 iter num 70\n",
            "loss 0.0003555779403541237 average time 0.05062369110037253 iter num 80\n",
            "loss 6.225707329576835e-05 average time 0.04615270436689672 iter num 90\n",
            "loss 0.00013830045645590872 average time 0.04262886950014945 iter num 100\n",
            "loss 0.0001874270965345204 average time 0.333371326899578 iter num 10\n",
            "loss 0.0001044130403897725 average time 0.17198888439943402 iter num 20\n",
            "loss 0.0026430620346218348 average time 0.11807108999976966 iter num 30\n",
            "loss 7.690444908803329e-05 average time 0.09117624762466221 iter num 40\n",
            "loss 7.516227196902037e-05 average time 0.07501501807979366 iter num 50\n",
            "loss 0.0002769007405731827 average time 0.064276322116469 iter num 60\n",
            "loss 0.0008140706922858953 average time 0.05658977744268279 iter num 70\n",
            "loss 0.00011267434456385672 average time 0.05079537077476744 iter num 80\n",
            "loss 5.3601059335051104e-05 average time 0.046303870633063425 iter num 90\n",
            "loss 0.0003930538659915328 average time 0.04272213018968614 iter num 100\n",
            "loss 0.00033275503665208817 average time 0.3323144012996636 iter num 10\n",
            "loss 7.794988778186962e-05 average time 0.17139603904925024 iter num 20\n",
            "loss 0.00011021061800420284 average time 0.11771481543255505 iter num 30\n",
            "loss 0.00015359377721324563 average time 0.09088653904955209 iter num 40\n",
            "loss 5.616878843284212e-05 average time 0.07488888129970292 iter num 50\n",
            "loss 7.227199239423499e-05 average time 0.06415408201640578 iter num 60\n",
            "loss 0.0005075051449239254 average time 0.056565484256913934 iter num 70\n",
            "loss 0.00018400711996946484 average time 0.050779553362326625 iter num 80\n",
            "loss 0.0002146386686945334 average time 0.04630157472201972 iter num 90\n",
            "loss 0.0002582053712103516 average time 0.042694887219840896 iter num 100\n",
            "loss 0.007038837298750877 average time 0.33245454550014986 iter num 10\n",
            "loss 0.00014869746519252658 average time 0.17139983425022365 iter num 20\n",
            "loss 6.10811694059521e-05 average time 0.11770941143331584 iter num 30\n",
            "loss 0.00011689306847983971 average time 0.09089222879983935 iter num 40\n",
            "loss 0.00016198153025470674 average time 0.07478525979997358 iter num 50\n",
            "loss 0.0031606361735612154 average time 0.06412874333339763 iter num 60\n",
            "loss 0.00012645682727452368 average time 0.05649275085717298 iter num 70\n",
            "loss 0.00011163534509250894 average time 0.05070741085010013 iter num 80\n",
            "loss 7.304872269742191e-05 average time 0.04622273480013569 iter num 90\n",
            "loss 8.735703158890828e-05 average time 0.042620575780129004 iter num 100\n",
            "loss 0.00017137078975792974 average time 0.33213788109969755 iter num 10\n",
            "loss 7.940004434203729e-05 average time 0.17146513869938645 iter num 20\n",
            "loss 7.368795922957361e-05 average time 0.11774385806614494 iter num 30\n",
            "loss 0.00011084724974352866 average time 0.09085614869973142 iter num 40\n",
            "loss 0.0002464318822603673 average time 0.0747642569396703 iter num 50\n",
            "loss 0.00021812281920574605 average time 0.06408371504973426 iter num 60\n",
            "loss 0.0007967716082930565 average time 0.05644030512838591 iter num 70\n",
            "loss 0.00011938148963963613 average time 0.05068196742486179 iter num 80\n",
            "loss 3.347076562931761e-05 average time 0.04620067776664251 iter num 90\n",
            "loss 0.00013211170153226703 average time 0.04260091069998453 iter num 100\n",
            "loss 0.00010110424045706168 average time 0.3334102029002679 iter num 10\n",
            "loss 0.00012437700934242457 average time 0.1720184893500118 iter num 20\n",
            "loss 0.0003822635335382074 average time 0.11809182086666017 iter num 30\n",
            "loss 8.184799662558362e-05 average time 0.09115225594996446 iter num 40\n",
            "loss 0.00010578377987258136 average time 0.07496917046002637 iter num 50\n",
            "loss 0.00027027452597394586 average time 0.06422596608329817 iter num 60\n",
            "loss 0.00013838855375070125 average time 0.05651523471419101 iter num 70\n",
            "loss 9.399848204338923e-05 average time 0.05077200202499625 iter num 80\n",
            "loss 9.29669477045536e-05 average time 0.046336781055490266 iter num 90\n",
            "loss 0.0001184920547530055 average time 0.04273424855997291 iter num 100\n",
            "loss 0.00011412342428229749 average time 0.3318659448996186 iter num 10\n",
            "loss 0.0001874785084510222 average time 0.1710557490498104 iter num 20\n",
            "loss 0.00026912445900961757 average time 0.11740789230001004 iter num 30\n",
            "loss 0.0012628623517230153 average time 0.09061191127502752 iter num 40\n",
            "loss 7.498799823224545e-05 average time 0.07464534020000428 iter num 50\n",
            "loss 0.00013057925389148295 average time 0.06392413160016683 iter num 60\n",
            "loss 0.002554466715082526 average time 0.05625844001447799 iter num 70\n",
            "loss 8.992018410935998e-05 average time 0.050559010475080865 iter num 80\n",
            "loss 0.00010451729031046852 average time 0.04607749080011369 iter num 90\n",
            "loss 5.5574953876202926e-05 average time 0.04250593334007135 iter num 100\n",
            "loss 8.008869917830452e-05 average time 0.3327063287993951 iter num 10\n",
            "loss 0.0002449901949148625 average time 0.17148940009956276 iter num 20\n",
            "loss 0.0001692864898359403 average time 0.11782285696632849 iter num 30\n",
            "loss 9.423394658369943e-05 average time 0.09100580297481428 iter num 40\n",
            "loss 0.0009412358631379902 average time 0.07484220375976293 iter num 50\n",
            "loss 0.00011368731065886095 average time 0.06406759524985925 iter num 60\n",
            "loss 9.759494423633441e-05 average time 0.05641855835690097 iter num 70\n",
            "loss 0.00029806719976477325 average time 0.05072914676225082 iter num 80\n",
            "loss 9.373115608468652e-05 average time 0.04625476351086238 iter num 90\n",
            "loss 4.5924818550702184e-05 average time 0.04264654058977612 iter num 100\n",
            "loss 0.00021007480972912163 average time 0.3326673851999658 iter num 10\n",
            "loss 0.00013597976067103446 average time 0.17179009084993596 iter num 20\n",
            "loss 0.00018095145060215145 average time 0.11805689986661795 iter num 30\n",
            "loss 9.941403550328687e-05 average time 0.09111115922487442 iter num 40\n",
            "loss 8.863744733389467e-05 average time 0.07496415769986925 iter num 50\n",
            "loss 6.632791337324306e-05 average time 0.06430037108317871 iter num 60\n",
            "loss 0.00012094796693418175 average time 0.056600124728460544 iter num 70\n",
            "loss 0.00012475399125833064 average time 0.05086723453741797 iter num 80\n",
            "loss 0.00011451298632891849 average time 0.04640262649999285 iter num 90\n",
            "loss 0.00017579091945663095 average time 0.042793888240012165 iter num 100\n",
            "loss 0.0023456704802811146 average time 0.3328514324995922 iter num 10\n",
            "loss 0.00019269665062893182 average time 0.17162826729963854 iter num 20\n",
            "loss 0.0001288155181100592 average time 0.1178359521995541 iter num 30\n",
            "loss 0.00015442018047906458 average time 0.09099022774971673 iter num 40\n",
            "loss 0.00039035125519149005 average time 0.0749131864598894 iter num 50\n",
            "loss 0.0014573506778106093 average time 0.06412822998318006 iter num 60\n",
            "loss 0.00023060709645505995 average time 0.05642602017131659 iter num 70\n",
            "loss 0.001241308986209333 average time 0.050652355687361705 iter num 80\n",
            "loss 0.0012836286332458258 average time 0.04617851003325389 iter num 90\n",
            "loss 0.0006702245445922017 average time 0.04260022875987488 iter num 100\n",
            "loss 0.0007238308317027986 average time 0.33281966609974917 iter num 10\n",
            "loss 6.665058026555926e-05 average time 0.17157935989998804 iter num 20\n",
            "loss 8.176871051546186e-05 average time 0.11776565080011399 iter num 30\n",
            "loss 0.0010711122304201126 average time 0.09087780652498623 iter num 40\n",
            "loss 5.932537897024304e-05 average time 0.07478740448015742 iter num 50\n",
            "loss 0.00014562525029759854 average time 0.06406847145014277 iter num 60\n",
            "loss 0.00010057629697257653 average time 0.056390263442985346 iter num 70\n",
            "loss 0.00011091115447925404 average time 0.05061508666253758 iter num 80\n",
            "loss 2.9777038434986025e-05 average time 0.046122652788916536 iter num 90\n",
            "loss 0.00011340209312038496 average time 0.042534536100065454 iter num 100\n",
            "loss 0.0006987371016293764 average time 0.3324551837002218 iter num 10\n",
            "loss 0.0001916658366099 average time 0.1714235715502582 iter num 20\n",
            "loss 0.0001163780179922469 average time 0.11776245740014323 iter num 30\n",
            "loss 6.500523159047589e-05 average time 0.09092814632522277 iter num 40\n",
            "loss 5.223476546234451e-05 average time 0.0748110587202973 iter num 50\n",
            "loss 8.529167098458856e-05 average time 0.0640467876335606 iter num 60\n",
            "loss 8.21987196104601e-05 average time 0.05638935480024624 iter num 70\n",
            "loss 0.00016376223356928676 average time 0.05060822623772765 iter num 80\n",
            "loss 0.0027246284298598766 average time 0.046124266833552 iter num 90\n",
            "loss 0.00016041698108892888 average time 0.042527430040208855 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e484fcd8-6e11-4dcc-d9c2-715902a847bc"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.3, 0.3]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.27130044, 0.90763223)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.2708]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9065], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2AXrPt7bNj",
        "outputId": "4afb485a-3fc3-498f-87d7-ccfc558530cd"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.3]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.0]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27120972\n",
            "[0.9076326]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lovJwXo3-YEu",
        "outputId": "0fcb3f35-14df-405e-f84d-724ac79edb95"
      },
      "source": [
        "from scipy.stats import norm\n",
        "from math import log, sqrt, pi, exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d1(S,K,T,r,sigma):\n",
        "    return(log(S/K)+(r+sigma**2/2.)*T)/(sigma*sqrt(T))\n",
        "def d2(S,K,T,r,sigma):\n",
        "    return d1(S,K,T,r,sigma)-sigma*sqrt(T)    \n",
        "def bs_call(S,K,T,r,sigma):\n",
        "    return S*norm.cdf(d1(S,K,T,r,sigma))-K*exp(-r*T)*norm.cdf(d2(S,K,T,r,sigma))\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "BS_call_prices = []\n",
        "for p in prices:\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    BS_call_prices.append(bs_call(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, BS_call_prices, label = \"BS_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(BS_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lwApH0GT9bBK",
        "outputId": "e911e6f7-1982-4169-8090-33e6d37c173a"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, S, 0.25, 0.3, 0.3]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "\n",
        "def bs_delta(S,K,T,r,sigma):\n",
        "  return norm.cdf(d1(S,K,T,r,sigma))\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "BS_call_deltas = []\n",
        "for p in prices:\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    BS_call_deltas.append(bs_delta(p, 1, 1, 0.3, 0.25))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, BS_call_deltas, label = \"BS_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(BS_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}