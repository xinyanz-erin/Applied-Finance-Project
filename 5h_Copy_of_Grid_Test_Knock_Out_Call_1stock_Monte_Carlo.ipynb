{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Grid Test_Knock Out Call 1stock Monte Carlo",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/5h_Copy_of_Grid_Test_Knock_Out_Call_1stock_Monte_Carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYigDkiy0HU9",
        "outputId": "01c4c40a-2d86-4bec-f6ff-c0d12da258b7"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "S_range = jnp.linspace(0.75, 1.25, 8)\n",
        "K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "B_range = jnp.linspace(0.5, 1.0, 5)\n",
        "sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "\n",
        "print(S_range)\n",
        "print(K_range)\n",
        "print(B_range)\n",
        "print(sigma_range)\n",
        "print(r_range)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75       0.82142854 0.89285713 0.9642857  1.0357143  1.1071429\n",
            " 1.1785713  1.25      ]\n",
            "[0.75  0.875 1.    1.125 1.25 ]\n",
            "[0.5   0.625 0.75  0.875 1.   ]\n",
            "[0.15       0.25       0.35000002 0.45      ]\n",
            "[0.01  0.025 0.04 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQxpJqK6OZr"
      },
      "source": [
        "# import cupy\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "# from jax import random\n",
        "# from jax import jit\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "#     stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "#     stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "#                             jax.ops.index[0],         # initialization of stock prices\n",
        "#                             initial_stocks)\n",
        "#     noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "#     sigma = jnp.diag(cov) ** 0.5\n",
        "#     dt = T / numsteps\n",
        "#     def time_step(t, val):\n",
        "#         dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "#         val = jax.ops.index_update(val,\n",
        "#                             jax.ops.index[t],\n",
        "#                             val[t-1] * dx)\n",
        "#         return val\n",
        "#     return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "# def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "#     return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "#                                 (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "#                     jnp.exp(-r[0] * T))\n",
        "\n",
        "# goptionvalueavg = jax.grad(optionvalueavg, argnums=1)\n",
        "\n",
        "# #################################################################### Adjust all parameters here (not inside class)\n",
        "# numstocks = 1\n",
        "# numsteps = 50\n",
        "# numpaths = 2000000\n",
        "\n",
        "# rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "# rng, key = jax.random.split(rng)\n",
        "# keys = jax.random.split(key, numpaths)\n",
        "\n",
        "# S_range = jnp.linspace(0.75, 1.25, 8)\n",
        "# K_range = jnp.linspace(0.75, 1.25, 5)\n",
        "# B_range = jnp.linspace(0.5, 1.0, 5)\n",
        "# sigma_range = jnp.linspace(0.15, 0.45, 4)\n",
        "# r_range = jnp.linspace(0.01, 0.04, 3)\n",
        "# T = 1.0\n",
        "\n",
        "# fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "# batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "# ####################################################################\n",
        "\n",
        "# call = []\n",
        "# count = 0\n",
        "\n",
        "# for S in S_range:\n",
        "#   for K in K_range:\n",
        "#     for B in B_range:\n",
        "#       for r in r_range:\n",
        "#         for sigma in sigma_range:    \n",
        "\n",
        "#           initial_stocks = jnp.array([S]*numstocks) # must be float\n",
        "#           r_tmp = jnp.array([r]*numstocks)\n",
        "#           drift = r_tmp\n",
        "#           cov = jnp.identity(numstocks)*sigma*sigma\n",
        "\n",
        "#           European_Call_price = optionvalueavg(key, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "#           Deltas = goptionvalueavg(keys, initial_stocks, numsteps, drift, r_tmp, cov, K, B, T)\n",
        "#           call.append([T, K, B, S, sigma, r, r, European_Call_price] + list(Deltas)) #T, K, B, S, sigma, mu, r, price, delta\n",
        "          \n",
        "#           count += 1\n",
        "#           print(count)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_OUtP8GUwj5"
      },
      "source": [
        "# Thedataset = pd.DataFrame(call)\n",
        "# Thedataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQKnflf6peX"
      },
      "source": [
        "# # save to csv\n",
        "# Thedataset.to_csv('Knock_Out_Call_1stock_MC_Datset_v2.csv', index=False, header=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGxiVAOjc-oM",
        "outputId": "218ad8c6-bfb8-4e25-f2bd-fc0bfd6abaef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "skGWSSsG8TGG",
        "outputId": "b40d05a1-b442-4de8-f4af-108d766f1686"
      },
      "source": [
        "# read csv\n",
        "import pandas as pd\n",
        "\n",
        "Thedataset = pd.read_csv('/content/drive/MyDrive/AFP/Save_Models/Knock_Out_Call_1stock_MC_Datset.csv', header=None)\n",
        "Thedataset"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.048510</td>\n",
              "      <td>0.556664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.078121</td>\n",
              "      <td>0.565542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.107045</td>\n",
              "      <td>0.574232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.133747</td>\n",
              "      <td>0.572047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.054222</td>\n",
              "      <td>0.595631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.200693</td>\n",
              "      <td>0.473159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.100417</td>\n",
              "      <td>0.632826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.144892</td>\n",
              "      <td>0.584785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.180591</td>\n",
              "      <td>0.531644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.207978</td>\n",
              "      <td>0.484591</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1536 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1    2     3     4      5      6         7         8\n",
              "0     1.0  0.75  0.5  0.75  0.15  0.010  0.010  0.048510  0.556664\n",
              "1     1.0  0.75  0.5  0.75  0.25  0.010  0.010  0.078121  0.565542\n",
              "2     1.0  0.75  0.5  0.75  0.35  0.010  0.010  0.107045  0.574232\n",
              "3     1.0  0.75  0.5  0.75  0.45  0.010  0.010  0.133747  0.572047\n",
              "4     1.0  0.75  0.5  0.75  0.15  0.025  0.025  0.054222  0.595631\n",
              "...   ...   ...  ...   ...   ...    ...    ...       ...       ...\n",
              "1531  1.0  1.25  1.0  1.25  0.45  0.025  0.025  0.200693  0.473159\n",
              "1532  1.0  1.25  1.0  1.25  0.15  0.040  0.040  0.100417  0.632826\n",
              "1533  1.0  1.25  1.0  1.25  0.25  0.040  0.040  0.144892  0.584785\n",
              "1534  1.0  1.25  1.0  1.25  0.35  0.040  0.040  0.180591  0.531644\n",
              "1535  1.0  1.25  1.0  1.25  0.45  0.040  0.040  0.207978  0.484591\n",
              "\n",
              "[1536 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4HV-G44th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3f4813-78bf-4c20-a6ab-8f247f37279c"
      },
      "source": [
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch\n",
        "torch.set_printoptions(precision=6)\n",
        "\n",
        "Thedataset_X = Thedataset.iloc[:,:7]\n",
        "Thedataset_Y = Thedataset.iloc[:,7:]\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.X = cupy.array(Thedataset_X)\n",
        "        self.Y = cupy.array(Thedataset_Y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(self.X.toDlpack()), from_dlpack(self.Y.toDlpack()))\n",
        "\n",
        "# print\n",
        "ds = OptionDataSet(max_len = 1)\n",
        "for i in ds:\n",
        "    print(i[0])\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    print(i[1].shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.000000, 0.750000, 0.500000,  ..., 0.150000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.500000,  ..., 0.250000, 0.010000, 0.010000],\n",
            "        [1.000000, 0.750000, 0.500000,  ..., 0.350000, 0.010000, 0.010000],\n",
            "        ...,\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.250000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.350000, 0.040000, 0.040000],\n",
            "        [1.000000, 1.250000, 1.000000,  ..., 0.450000, 0.040000, 0.040000]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([1536, 7])\n",
            "tensor([[0.048510, 0.556664],\n",
            "        [0.078121, 0.565542],\n",
            "        [0.107045, 0.574232],\n",
            "        ...,\n",
            "        [0.144892, 0.584785],\n",
            "        [0.180591, 0.531644],\n",
            "        [0.207978, 0.484591]], device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([1536, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "54285d4b-a610-4173-d925-73ff3912da31"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(7*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.5, 0.3, 0.03, 0.03]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, B, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.5, 0.75, 0.15, 0.01, 0.01]*1).cuda()) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "0edf9467-4de2-4e67-b34d-817b237efac2"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeLVZiiaDS4y",
        "outputId": "94818f37-0c19-48e6-ff89-ca44fd0d7e1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CyULkENYKb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bae2966-c3c8-41e9-ea52-a8d15676d568"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-8, amsgrad=True) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    # print(x)\n",
        "    # print(x.shape)\n",
        "    y = batch[1]\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "    y_pred = model(x.float())\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]  # Now index 3 is stock price, not 2\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: /usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/param_scheduler.py has been moved to /ignite/handlers/param_scheduler.py and will be removed in version 0.6.0.\n",
            " Please refer to the documentation for more details.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.15670578552133896 average time 1.436817291749992 iter num 20\n",
            "loss 0.1383454558589478 average time 1.4363197635499945 iter num 40\n",
            "loss 0.12600189929901684 average time 1.4310929691999983 iter num 60\n",
            "loss 0.11966505056143828 average time 1.4308221041874958 iter num 80\n",
            "loss 0.11829480862158442 average time 1.4318045984699972 iter num 100\n",
            "loss 0.09085479755883127 average time 1.430046758699973 iter num 20\n",
            "loss 0.06749213995575129 average time 1.4385407956999756 iter num 40\n",
            "loss 0.057814775413421414 average time 1.4443787627333184 iter num 60\n",
            "loss 0.05571530584220339 average time 1.4357170898249876 iter num 80\n",
            "loss 0.05544088352151598 average time 1.4363410768899871 iter num 100\n",
            "loss 0.05677707922855679 average time 1.4383795010999734 iter num 20\n",
            "loss 0.061178429798782914 average time 1.43299095509999 iter num 40\n",
            "loss 0.06273961051390334 average time 1.4312060333333154 iter num 60\n",
            "loss 0.06316362681784912 average time 1.4307426851749796 iter num 80\n",
            "loss 0.06323122267376739 average time 1.4299967252199781 iter num 100\n",
            "loss 0.06414597386853918 average time 1.4254903448499476 iter num 20\n",
            "loss 0.06412262030364659 average time 1.419126923174963 iter num 40\n",
            "loss 0.06377292324600121 average time 1.4205287660166315 iter num 60\n",
            "loss 0.06357515136170566 average time 1.4240350162249626 iter num 80\n",
            "loss 0.0635323651104467 average time 1.419425829679967 iter num 100\n",
            "loss 0.06272305186948775 average time 1.396918416550011 iter num 20\n",
            "loss 0.06202928548139733 average time 1.389560577224978 iter num 40\n",
            "loss 0.061580288390044036 average time 1.3913408787166532 iter num 60\n",
            "loss 0.0613765689491404 average time 1.3899123464749863 iter num 80\n",
            "loss 0.0613337775013167 average time 1.3895176964599887 iter num 100\n",
            "loss 0.060518804261789405 average time 1.3881276885999796 iter num 20\n",
            "loss 0.05979798720083246 average time 1.3907104212999912 iter num 40\n",
            "loss 0.05932479305621108 average time 1.3912455602499942 iter num 60\n",
            "loss 0.05911025291763346 average time 1.3922130503624828 iter num 80\n",
            "loss 0.059065275426915434 average time 1.392233918049999 iter num 100\n",
            "loss 0.058208703223721986 average time 1.3786874187500076 iter num 20\n",
            "loss 0.0574533043553089 average time 1.3775678690000006 iter num 40\n",
            "loss 0.05695856416384126 average time 1.3799167177166622 iter num 60\n",
            "loss 0.05673445182321655 average time 1.3785201703250038 iter num 80\n",
            "loss 0.05668757392828284 average time 1.3777955344300061 iter num 100\n",
            "loss 0.05579222018435885 average time 1.3855783714500376 iter num 20\n",
            "loss 0.055002204192534425 average time 1.381341244750007 iter num 40\n",
            "loss 0.05448337241465023 average time 1.376631231533338 iter num 60\n",
            "loss 0.054248536556849046 average time 1.3779698307624955 iter num 80\n",
            "loss 0.0541994735556833 average time 1.3768557837899926 iter num 100\n",
            "loss 0.05326127663447134 average time 1.3961090062499806 iter num 20\n",
            "loss 0.05243696641930609 average time 1.3837436973999615 iter num 40\n",
            "loss 0.05189769703455659 average time 1.3763168484833008 iter num 60\n",
            "loss 0.05165498023069244 average time 1.3793209248249696 iter num 80\n",
            "loss 0.051604442793357866 average time 1.3773591038499717 iter num 100\n",
            "loss 0.05063876594713843 average time 1.3771655584499514 iter num 20\n",
            "loss 0.04980224919925291 average time 1.373431781425006 iter num 40\n",
            "loss 0.049261130802073595 average time 1.3744967845833647 iter num 60\n",
            "loss 0.04901990650562957 average time 1.3714789406875183 iter num 80\n",
            "loss 0.0489699213028836 average time 1.3736503546300127 iter num 100\n",
            "loss 0.04802379787248465 average time 1.3684941723999828 iter num 20\n",
            "loss 0.04723113682167435 average time 1.3749175105500058 iter num 40\n",
            "loss 0.04673343663264802 average time 1.37431789885001 iter num 60\n",
            "loss 0.04651649875209158 average time 1.375487818612521 iter num 80\n",
            "loss 0.046471936309533605 average time 1.3747318388500163 iter num 100\n",
            "loss 0.04564581823653194 average time 1.3677418336999836 iter num 20\n",
            "loss 0.04498909480721417 average time 1.3687411278250239 iter num 40\n",
            "loss 0.04460016621151153 average time 1.3683099923833653 iter num 60\n",
            "loss 0.044436153815935985 average time 1.3678498682500162 iter num 80\n",
            "loss 0.04440302142430451 average time 1.3692426764000174 iter num 100\n",
            "loss 0.04381614309630265 average time 1.3607312526999522 iter num 20\n",
            "loss 0.04340021814021408 average time 1.3648136541249642 iter num 40\n",
            "loss 0.04318242605781949 average time 1.3689810684166333 iter num 60\n",
            "loss 0.04309726679869488 average time 1.3694572662249698 iter num 80\n",
            "loss 0.04308071025860187 average time 1.3690537917899883 iter num 100\n",
            "loss 0.04281643257024748 average time 1.3836056462499982 iter num 20\n",
            "loss 0.042683803046926594 average time 1.3766680369499908 iter num 40\n",
            "loss 0.042641497342235525 average time 1.3774268453166543 iter num 60\n",
            "loss 0.042631286744067506 average time 1.3731252457874974 iter num 80\n",
            "loss 0.042629782724239555 average time 1.3694485710800017 iter num 100\n",
            "loss 0.04263627909014141 average time 1.381906421050053 iter num 20\n",
            "loss 0.04269507608860612 average time 1.3776956513750407 iter num 40\n",
            "loss 0.04274577863180166 average time 1.3671392603666996 iter num 60\n",
            "loss 0.04276850879699469 average time 1.3673683621875283 iter num 80\n",
            "loss 0.04277300153488238 average time 1.3702655465100089 iter num 100\n",
            "loss 0.04286507320603008 average time 1.3548535736499616 iter num 20\n",
            "loss 0.042946292754505455 average time 1.3599990585249544 iter num 40\n",
            "loss 0.04298747881470441 average time 1.3596297592666435 iter num 60\n",
            "loss 0.04300066197249566 average time 1.3614994213624982 iter num 80\n",
            "loss 0.043002832484271246 average time 1.3635360193899941 iter num 100\n",
            "loss 0.04303875454337941 average time 1.365641891500013 iter num 20\n",
            "loss 0.04305053106867658 average time 1.3702194778249919 iter num 40\n",
            "loss 0.04304089978185525 average time 1.365491381583335 iter num 60\n",
            "loss 0.04303184309815103 average time 1.366077061775013 iter num 80\n",
            "loss 0.04302946311256071 average time 1.3680425307699988 iter num 100\n",
            "loss 0.042976743566760095 average time 1.396384443800025 iter num 20\n",
            "loss 0.042914075044898116 average time 1.3864650921500357 iter num 40\n",
            "loss 0.042862770436392285 average time 1.3752330670333246 iter num 60\n",
            "loss 0.04283678503689698 average time 1.3716322217874848 iter num 80\n",
            "loss 0.04283110165583769 average time 1.3671070071200029 iter num 100\n",
            "loss 0.04272027785701176 average time 1.3591267876499615 iter num 20\n",
            "loss 0.042616136729314635 average time 1.365263747924996 iter num 40\n",
            "loss 0.04254424079506497 average time 1.3696023211166675 iter num 60\n",
            "loss 0.04251058152044022 average time 1.3702843573999985 iter num 80\n",
            "loss 0.04250348809480704 average time 1.370361000380003 iter num 100\n",
            "loss 0.0423670185482402 average time 1.359520983099992 iter num 20\n",
            "loss 0.04224488476809931 average time 1.358862891574995 iter num 40\n",
            "loss 0.04216352447758513 average time 1.3657588542666492 iter num 60\n",
            "loss 0.0421267614070622 average time 1.3681377497249856 iter num 80\n",
            "loss 0.04211909469462692 average time 1.3655823681899846 iter num 100\n",
            "loss 0.0419750395538057 average time 1.3859510404000048 iter num 20\n",
            "loss 0.04184789519924677 average time 1.3828548035749577 iter num 40\n",
            "loss 0.04176489469016631 average time 1.3765980690332904 iter num 60\n",
            "loss 0.04172785848105763 average time 1.3757367453624851 iter num 80\n",
            "loss 0.04172013037448794 average time 1.3723365190299728 iter num 100\n",
            "loss 0.04157646951401572 average time 1.3510007680499712 iter num 20\n",
            "loss 0.0414531798632769 average time 1.3677033643250183 iter num 40\n",
            "loss 0.041374140129913625 average time 1.36606328778336 iter num 60\n",
            "loss 0.041338605225181464 average time 1.3687156387875234 iter num 80\n",
            "loss 0.04133125710869661 average time 1.3683350908400098 iter num 100\n",
            "loss 0.041194766499040594 average time 1.3658253529499689 iter num 20\n",
            "loss 0.041079618394480606 average time 1.366182167899956 iter num 40\n",
            "loss 0.041006253237357126 average time 1.3647029642166293 iter num 60\n",
            "loss 0.040973736918162515 average time 1.3679100204999712 iter num 80\n",
            "loss 0.04096701149685676 average time 1.366988773819985 iter num 100\n",
            "loss 0.040842894472601 average time 1.3646490680499028 iter num 20\n",
            "loss 0.04073798999279507 average time 1.3607795939749168 iter num 40\n",
            "loss 0.04067152664462404 average time 1.3581436206332758 iter num 60\n",
            "loss 0.04064225914797801 average time 1.3600393810374727 iter num 80\n",
            "loss 0.040636199479133266 average time 1.366068676729974 iter num 100\n",
            "loss 0.040524458041688215 average time 1.3710081400499803 iter num 20\n",
            "loss 0.04043006320086012 average time 1.3590253485749826 iter num 40\n",
            "loss 0.04037038826351567 average time 1.3604790225499679 iter num 60\n",
            "loss 0.04034411262466654 average time 1.3623930583124775 iter num 80\n",
            "loss 0.04033868240783963 average time 1.3633430135699927 iter num 100\n",
            "loss 0.04023864378255897 average time 1.362870013949987 iter num 20\n",
            "loss 0.04015461430138302 average time 1.3570117582500187 iter num 40\n",
            "loss 0.040101542779557464 average time 1.354953198333351 iter num 60\n",
            "loss 0.04007809716574126 average time 1.3526919608000412 iter num 80\n",
            "loss 0.040073276549438326 average time 1.3543149395100318 iter num 100\n",
            "loss 0.0399832271974054 average time 1.3684621998001147 iter num 20\n",
            "loss 0.03990639602371622 average time 1.3700425392000852 iter num 40\n",
            "loss 0.039857529285449486 average time 1.3650207490334347 iter num 60\n",
            "loss 0.039836005399996266 average time 1.3643737122000856 iter num 80\n",
            "loss 0.039831572049064325 average time 1.36387118835004 iter num 100\n",
            "loss 0.03974958539377725 average time 1.3463910309500533 iter num 20\n",
            "loss 0.0396804680481815 average time 1.351566035899964 iter num 40\n",
            "loss 0.03963687712681088 average time 1.3583514249166 iter num 60\n",
            "loss 0.03961764890812682 average time 1.3576749626999116 iter num 80\n",
            "loss 0.03961370250137166 average time 1.3604180495999207 iter num 100\n",
            "loss 0.03954044500694058 average time 1.3710052334000011 iter num 20\n",
            "loss 0.039478533184874716 average time 1.3681274445499638 iter num 40\n",
            "loss 0.03943933704216844 average time 1.3751657139999527 iter num 60\n",
            "loss 0.0394219859009059 average time 1.3750804123499734 iter num 80\n",
            "loss 0.03941839464846055 average time 1.3746209977999753 iter num 100\n",
            "loss 0.039352272094343006 average time 1.3575747997000236 iter num 20\n",
            "loss 0.039296548460484616 average time 1.3674536108500432 iter num 40\n",
            "loss 0.03926116241230098 average time 1.3679472951499974 iter num 60\n",
            "loss 0.03924557576091322 average time 1.365262193775004 iter num 80\n",
            "loss 0.03924234517944157 average time 1.3647315917799825 iter num 100\n",
            "loss 0.039183301989681105 average time 1.3817177518499648 iter num 20\n",
            "loss 0.039133781761199096 average time 1.3687206361999642 iter num 40\n",
            "loss 0.03910278836792257 average time 1.3653727078166866 iter num 60\n",
            "loss 0.03908912829128573 average time 1.3705917442499982 iter num 80\n",
            "loss 0.039086330340612215 average time 1.3661893409499952 iter num 100\n",
            "loss 0.03903426000970255 average time 1.3656725629000903 iter num 20\n",
            "loss 0.03898969735435702 average time 1.3624557573999483 iter num 40\n",
            "loss 0.038961533851034263 average time 1.3644988189333012 iter num 60\n",
            "loss 0.03894923391023773 average time 1.364912964374969 iter num 80\n",
            "loss 0.03894670772394639 average time 1.3651719245699587 iter num 100\n",
            "loss 0.03890010449528554 average time 1.3630680679000762 iter num 20\n",
            "loss 0.03886094970016666 average time 1.3658373174250529 iter num 40\n",
            "loss 0.03883622719792476 average time 1.3713555946666323 iter num 60\n",
            "loss 0.038825382574860935 average time 1.3738653369374447 iter num 80\n",
            "loss 0.03882317395985825 average time 1.374357189589973 iter num 100\n",
            "loss 0.03878245231548412 average time 1.3810470622500817 iter num 20\n",
            "loss 0.038748214122855236 average time 1.3775156332000733 iter num 40\n",
            "loss 0.038726800965858475 average time 1.3704770787834302 iter num 60\n",
            "loss 0.03871745984696303 average time 1.3738307590250998 iter num 80\n",
            "loss 0.0387155345244067 average time 1.3745716411600732 iter num 100\n",
            "loss 0.03868015602774744 average time 1.3763573010000982 iter num 20\n",
            "loss 0.03865048859665057 average time 1.3884040798250907 iter num 40\n",
            "loss 0.038632084674554396 average time 1.3860042015500766 iter num 60\n",
            "loss 0.03862402852446045 average time 1.385147925825072 iter num 80\n",
            "loss 0.03862236232356845 average time 1.384227807930074 iter num 100\n",
            "loss 0.03859082906204042 average time 1.3820244220499263 iter num 20\n",
            "loss 0.03856381581706398 average time 1.395775532349853 iter num 40\n",
            "loss 0.03854717557199355 average time 1.3972892679666227 iter num 60\n",
            "loss 0.03853991658295702 average time 1.398628093474929 iter num 80\n",
            "loss 0.038538416140400426 average time 1.3984964207699249 iter num 100\n",
            "loss 0.038511052246632385 average time 1.4095296103500914 iter num 20\n",
            "loss 0.038488577762015586 average time 1.3988604076249884 iter num 40\n",
            "loss 0.03847456611513608 average time 1.4003001546833123 iter num 60\n",
            "loss 0.03846848405929258 average time 1.4013107897875217 iter num 80\n",
            "loss 0.038467211839032615 average time 1.4010080729100354 iter num 100\n",
            "loss 0.038444334724832156 average time 1.4083990755000286 iter num 20\n",
            "loss 0.038424824573587556 average time 1.3985069044249712 iter num 40\n",
            "loss 0.03841257265663696 average time 1.4018671959999969 iter num 60\n",
            "loss 0.038407241794134316 average time 1.4064907993125075 iter num 80\n",
            "loss 0.0384061241863387 average time 1.4122911354700136 iter num 100\n",
            "loss 0.03838627354918159 average time 1.3877507464500922 iter num 20\n",
            "loss 0.038369962364218 average time 1.3933865955750206 iter num 40\n",
            "loss 0.03835983970695472 average time 1.3978561859166954 iter num 60\n",
            "loss 0.038355497711321726 average time 1.39368278901253 iter num 80\n",
            "loss 0.03835459217510532 average time 1.4005082397299975 iter num 100\n",
            "loss 0.03833825974455792 average time 1.4012363566000203 iter num 20\n",
            "loss 0.03832446051289374 average time 1.399570446100006 iter num 40\n",
            "loss 0.038315714823322584 average time 1.399714769716714 iter num 60\n",
            "loss 0.038311927754311886 average time 1.4103382502000499 iter num 80\n",
            "loss 0.038311158589704714 average time 1.412117836630032 iter num 100\n",
            "loss 0.038296961096701686 average time 1.4175603760499598 iter num 20\n",
            "loss 0.0382852781643728 average time 1.410343093699953 iter num 40\n",
            "loss 0.0382780628331246 average time 1.4067320089166364 iter num 60\n",
            "loss 0.03827498164353727 average time 1.4052776696624847 iter num 80\n",
            "loss 0.03827435760941129 average time 1.4046130876099596 iter num 100\n",
            "loss 0.03826268392409254 average time 1.394266334799886 iter num 20\n",
            "loss 0.038253054493369056 average time 1.389221153874928 iter num 40\n",
            "loss 0.03824730017080577 average time 1.387457523099935 iter num 60\n",
            "loss 0.03824484775494997 average time 1.3912056503749795 iter num 80\n",
            "loss 0.03824433352746269 average time 1.40057026432999 iter num 100\n",
            "loss 0.03823543894892664 average time 1.41341985199997 iter num 20\n",
            "loss 0.03822839386014822 average time 1.41404562537507 iter num 40\n",
            "loss 0.038224188422381 average time 1.409898506216708 iter num 60\n",
            "loss 0.03822233076963288 average time 1.406386052775042 iter num 80\n",
            "loss 0.038221925468432214 average time 1.4053869890400166 iter num 100\n",
            "loss 0.038214842044880126 average time 1.3912179366499458 iter num 20\n",
            "loss 0.038209103793735044 average time 1.3940965945000017 iter num 40\n",
            "loss 0.0382058348978169 average time 1.3981227244500285 iter num 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0fb181a15148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jax_knock_out_1stock_MC_oneDS_1.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0fb181a15148>\u001b[0m in \u001b[0;36mtrain_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# print(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0fb181a15148>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# print(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0fb181a15148>\u001b[0m in \u001b[0;36mcompute_deltas\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mfirst_order_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_order_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now index 3 is stock price, not 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_fEzULvKwR-"
      },
      "source": [
        "# 1h40min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5dd139-81bc-416a-8a17-9719c32fd62b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f52681-c5dd-4224-9e29-21fb5e98a975"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82c3ae5-2c4d-4b89-bdb7-9d578b09fb72"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51eff137-a8fe-4291-e3b2-fe5f54c7a447"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=7, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25af52f4-5a04-4a6b-f3b3-f93700ded6d1"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-8, amsgrad=True) # try using higher epsilon and amsgrad\n",
        "dataset = OptionDataSet(max_len = 100) # Use max_len to adjust\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x.float())\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x.float()\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[3]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 1]).cuda()\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)\n",
        "\n",
        "model_save_name = 'jax_knock_out_1stock_MC_oneDS_2.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.03823499856759156 average time 1.4380569248000028 iter num 20\n",
            "loss 0.03826933483486532 average time 1.442350504450087 iter num 40\n",
            "loss 0.03793840726989856 average time 1.4364470465667636 iter num 60\n",
            "loss 0.03801178822451784 average time 1.4372949136625608 iter num 80\n",
            "loss 0.038008898496197506 average time 1.4429273199300587 iter num 100\n",
            "loss 0.03799483083479593 average time 1.4385973977999584 iter num 20\n",
            "loss 0.037985698499196745 average time 1.4353089575749665 iter num 40\n",
            "loss 0.03797081115415898 average time 1.4383826662666253 iter num 60\n",
            "loss 0.0379645168793717 average time 1.441134507662491 iter num 80\n",
            "loss 0.03796315684401265 average time 1.4419675139399897 iter num 100\n",
            "loss 0.037939454704466365 average time 1.430039216799969 iter num 20\n",
            "loss 0.037919522503045436 average time 1.4352119462500468 iter num 40\n",
            "loss 0.03790758298863803 average time 1.4368393656500607 iter num 60\n",
            "loss 0.037902791260368676 average time 1.4401474252125581 iter num 80\n",
            "loss 0.03790190968415248 average time 1.4380914637500246 iter num 100\n",
            "loss 0.037885370896185586 average time 1.4721415860499747 iter num 20\n",
            "loss 0.03787380308636307 average time 1.462436872049966 iter num 40\n",
            "loss 0.03786835077401612 average time 1.45173859641662 iter num 60\n",
            "loss 0.03786657721292622 average time 1.4498197009499676 iter num 80\n",
            "loss 0.03786628145148125 average time 1.4425290879299792 iter num 100\n",
            "loss 0.03786323183259027 average time 1.4282720776000133 iter num 20\n",
            "loss 0.03786521032788272 average time 1.431745377600032 iter num 40\n",
            "loss 0.03786943543502372 average time 1.4299400090000138 iter num 60\n",
            "loss 0.037872026712842156 average time 1.4303532043375184 iter num 80\n",
            "loss 0.03787267215382014 average time 1.4307302551200065 iter num 100\n",
            "loss 0.0378865004320879 average time 1.4237766738501705 iter num 20\n",
            "loss 0.03790249215773159 average time 1.4288652162000972 iter num 40\n",
            "loss 0.037914369502047206 average time 1.4271847137333529 iter num 60\n",
            "loss 0.03792015012471781 average time 1.4248272065625087 iter num 80\n",
            "loss 0.0379213970378108 average time 1.4279654424199817 iter num 100\n",
            "loss 0.03794734246459131 average time 1.4267709476499022 iter num 20\n",
            "loss 0.037974050564855415 average time 1.430470006125006 iter num 40\n",
            "loss 0.03799256629580074 average time 1.430371120083358 iter num 60\n",
            "loss 0.03800092378300257 average time 1.4303929535000293 iter num 80\n",
            "loss 0.038002652579091245 average time 1.4256855573000211 iter num 100\n",
            "loss 0.03803788579685344 average time 1.4378475396998964 iter num 20\n",
            "loss 0.03807330313382847 average time 1.4269819242250605 iter num 40\n",
            "loss 0.03809811896520567 average time 1.4336855575668324 iter num 60\n",
            "loss 0.03810943255760832 average time 1.4284596175625779 iter num 80\n",
            "loss 0.03811179371018991 average time 1.4267692887400427 iter num 100\n",
            "loss 0.03815958689441833 average time 1.4088081624500775 iter num 20\n",
            "loss 0.038205467149877693 average time 1.4126484756248374 iter num 40\n",
            "loss 0.03823666609887887 average time 1.4090410040498378 iter num 60\n",
            "loss 0.03825080798117848 average time 1.4115447728623622 iter num 80\n",
            "loss 0.038253765848250784 average time 1.4117986043199016 iter num 100\n",
            "loss 0.0383125349028348 average time 1.4186030076499265 iter num 20\n",
            "loss 0.03836711788484927 average time 1.424958066624822 iter num 40\n",
            "loss 0.03840277744291178 average time 1.4360587863832128 iter num 60\n",
            "loss 0.03841851843381162 average time 1.436536680637437 iter num 80\n",
            "loss 0.03842171987772994 average time 1.4355693385699668 iter num 100\n",
            "loss 0.03848481510939899 average time 1.4339460544500071 iter num 20\n",
            "loss 0.03854165405906456 average time 1.4366835989499123 iter num 40\n",
            "loss 0.038578764626895884 average time 1.4319588389667237 iter num 60\n",
            "loss 0.03859534527059464 average time 1.4314441733000423 iter num 80\n",
            "loss 0.03859869963393445 average time 1.4346621687100378 iter num 100\n",
            "loss 0.03866662617900096 average time 1.4357918221498038 iter num 20\n",
            "loss 0.038728210882560454 average time 1.432153370974902 iter num 40\n",
            "loss 0.03876830476468784 average time 1.4297174424666081 iter num 60\n",
            "loss 0.03878608762904424 average time 1.440314316674926 iter num 80\n",
            "loss 0.038789755486198355 average time 1.4402753439698972 iter num 100\n",
            "loss 0.03886330382290129 average time 1.443461830850174 iter num 20\n",
            "loss 0.03892922673600979 average time 1.4411065605499516 iter num 40\n",
            "loss 0.03897195103681686 average time 1.440117273083312 iter num 60\n",
            "loss 0.038991083729660994 average time 1.4426004333625086 iter num 80\n",
            "loss 0.038994988834460266 average time 1.439921056479925 iter num 100\n",
            "loss 0.03907355311768816 average time 1.4386690638997606 iter num 20\n",
            "loss 0.03914578176743254 average time 1.4389756954748918 iter num 40\n",
            "loss 0.03919339477376802 average time 1.441261213966512 iter num 60\n",
            "loss 0.03921433622367274 average time 1.4393584786623705 iter num 80\n",
            "loss 0.039218583018810434 average time 1.4449107426098453 iter num 100\n",
            "loss 0.03933338131852284 average time 1.4569890058997772 iter num 20\n",
            "loss 0.03938929233542116 average time 1.4483358469247378 iter num 40\n",
            "loss 0.03943487469599098 average time 1.4473838750497634 iter num 60\n",
            "loss 0.03946064308287894 average time 1.4438989776497693 iter num 80\n",
            "loss 0.0394652198574024 average time 1.4427207686198198 iter num 100\n",
            "loss 0.03952047035133059 average time 1.4211417341499328 iter num 20\n",
            "loss 0.03971935107310991 average time 1.4306859432249894 iter num 40\n",
            "loss 0.0396437948320484 average time 1.4328455788666967 iter num 60\n",
            "loss 0.03966267600377152 average time 1.4268037849750272 iter num 80\n",
            "loss 0.03967008416345144 average time 1.4321029584799725 iter num 100\n",
            "loss 0.039749885658107456 average time 1.4474423010001374 iter num 20\n",
            "loss 0.03982553677445296 average time 1.440694144174995 iter num 40\n",
            "loss 0.03987910208179941 average time 1.4359900995499022 iter num 60\n",
            "loss 0.03990392739376995 average time 1.4364067667125255 iter num 80\n",
            "loss 0.039909202432915035 average time 1.436044321759946 iter num 100\n",
            "loss 0.040010884203022075 average time 1.441315126799782 iter num 20\n",
            "loss 0.040106082012203906 average time 1.4346641553249355 iter num 40\n",
            "loss 0.040170711479858824 average time 1.4332164785166546 iter num 60\n",
            "loss 0.040200164136172944 average time 1.4295681275874812 iter num 80\n",
            "loss 0.04020633423370252 average time 1.428831829510018 iter num 100\n",
            "loss 0.04032571839428421 average time 1.4566653294999923 iter num 20\n",
            "loss 0.04043592290933293 average time 1.4490471947749028 iter num 40\n",
            "loss 0.0405096165859207 average time 1.4422977827833467 iter num 60\n",
            "loss 0.04054313684981437 average time 1.435733042962579 iter num 80\n",
            "loss 0.040550192514217945 average time 1.4358095644600508 iter num 100\n",
            "loss 0.04066483014290426 average time 1.4165684108001186 iter num 20\n",
            "loss 0.04080705848253645 average time 1.4284954521752753 iter num 40\n",
            "loss 0.040886730211989936 average time 1.424527422850224 iter num 60\n",
            "loss 0.04092255252397957 average time 1.4237434749127034 iter num 80\n",
            "loss 0.040929738989418485 average time 1.4234295915301665 iter num 100\n",
            "loss 0.04023339216166099 average time 1.451547472800121 iter num 20\n",
            "loss 0.041072182449616856 average time 1.4546336540750873 iter num 40\n",
            "loss 0.041114372939578765 average time 1.446219682583281 iter num 60\n",
            "loss 0.04117208165061612 average time 1.4441028860874667 iter num 80\n",
            "loss 0.04117768021329127 average time 1.4409856378599215 iter num 100\n",
            "loss 0.04126626612292756 average time 1.4269025800001145 iter num 20\n",
            "loss 0.04135443473558313 average time 1.4280298242751086 iter num 40\n",
            "loss 0.041405712433078744 average time 1.4296217598834118 iter num 60\n",
            "loss 0.041429609829905595 average time 1.4300698024375151 iter num 80\n",
            "loss 0.04143463510148957 average time 1.4310694615600186 iter num 100\n",
            "loss 0.04152784301477467 average time 1.4306898587997239 iter num 20\n",
            "loss 0.0416088454094737 average time 1.4425750875999257 iter num 40\n",
            "loss 0.04166225893648714 average time 1.447057816116679 iter num 60\n",
            "loss 0.041686355300063316 average time 1.4439154878875116 iter num 80\n",
            "loss 0.04169136035533588 average time 1.4426097667499926 iter num 100\n",
            "loss 0.04178426621778538 average time 1.4408205196498785 iter num 20\n",
            "loss 0.04186497736293569 average time 1.4427950912497636 iter num 40\n",
            "loss 0.04191584742855578 average time 1.4406208403832048 iter num 60\n",
            "loss 0.04193839371832378 average time 1.4362243894749553 iter num 80\n",
            "loss 0.04194307312143064 average time 1.4374513799800115 iter num 100\n",
            "loss 0.0420303583766484 average time 1.4383329636497364 iter num 20\n",
            "loss 0.042106040621135155 average time 1.4359160506498028 iter num 40\n",
            "loss 0.04215495247952861 average time 1.4374208064831995 iter num 60\n",
            "loss 0.04217651220288683 average time 1.444934594487404 iter num 80\n",
            "loss 0.04218093489705813 average time 1.4423753667499295 iter num 100\n",
            "loss 0.04226543780760923 average time 1.4344141110002055 iter num 20\n",
            "loss 0.0423394802387776 average time 1.435384289975127 iter num 40\n",
            "loss 0.04238673771926152 average time 1.4326345891001135 iter num 60\n",
            "loss 0.042407623724812785 average time 1.4344726994001575 iter num 80\n",
            "loss 0.04241195544165889 average time 1.436114294080162 iter num 100\n",
            "loss 0.04249288296349837 average time 1.4544413398998586 iter num 20\n",
            "loss 0.042561695951171666 average time 1.445170392650016 iter num 40\n",
            "loss 0.04260567962684915 average time 1.4412381916666466 iter num 60\n",
            "loss 0.042625234559746525 average time 1.4385064119500157 iter num 80\n",
            "loss 0.042629274550253456 average time 1.442638362839989 iter num 100\n",
            "loss 0.04270414909962607 average time 1.4339762342999165 iter num 20\n",
            "loss 0.042770866993479525 average time 1.434624103674969 iter num 40\n",
            "loss 0.04281278308301072 average time 1.435931696899964 iter num 60\n",
            "loss 0.04283159000927798 average time 1.4325590196998974 iter num 80\n",
            "loss 0.04283552971051918 average time 1.4325421656498838 iter num 100\n",
            "loss 0.042785693373962555 average time 1.435964844749924 iter num 20\n",
            "loss 0.04296931819948662 average time 1.43798682202505 iter num 40\n",
            "loss 0.04301901337133469 average time 1.4405934447166753 iter num 60\n",
            "loss 0.04303919689827334 average time 1.4373021637874444 iter num 80\n",
            "loss 0.04304168203412401 average time 1.4378084405599294 iter num 100\n",
            "loss 0.04192750810719597 average time 1.4409636960497665 iter num 20\n",
            "loss 0.04287244242671583 average time 1.4366730497750724 iter num 40\n",
            "loss 0.04314628252027731 average time 1.4363451002834153 iter num 60\n",
            "loss 0.04314363366312991 average time 1.4357195082750878 iter num 80\n",
            "loss 0.04316302663194872 average time 1.435651070360036 iter num 100\n",
            "loss 0.04322963975586769 average time 1.4361157458999514 iter num 20\n",
            "loss 0.04325482395045229 average time 1.436412530399957 iter num 40\n",
            "loss 0.043283503203439246 average time 1.4318280745999497 iter num 60\n",
            "loss 0.04329947997381853 average time 1.4409692675874566 iter num 80\n",
            "loss 0.04330216445314347 average time 1.4408090814099523 iter num 100\n",
            "loss 0.04336109637729148 average time 1.4540919044500697 iter num 20\n",
            "loss 0.04341312204263979 average time 1.449646421224952 iter num 40\n",
            "loss 0.04344621062146468 average time 1.4460629251498782 iter num 60\n",
            "loss 0.043460861824800974 average time 1.4420678530124404 iter num 80\n",
            "loss 0.04346388359843937 average time 1.4453228758500154 iter num 100\n",
            "loss 0.043519882810863206 average time 1.4394988844997898 iter num 20\n",
            "loss 0.04356700782579931 average time 1.4361788896998404 iter num 40\n",
            "loss 0.04359657746157744 average time 1.4411208052999427 iter num 60\n",
            "loss 0.04360969370438928 average time 1.4386475046374472 iter num 80\n",
            "loss 0.043612448257401584 average time 1.4398474981899927 iter num 100\n",
            "loss 0.043663670968920695 average time 1.4098398694000025 iter num 20\n",
            "loss 0.043708398639050586 average time 1.4339384110999618 iter num 40\n",
            "loss 0.04373625205953282 average time 1.4371684500499518 iter num 60\n",
            "loss 0.04374845956285205 average time 1.448636379100003 iter num 80\n",
            "loss 0.043750948359166084 average time 1.45938264770999 iter num 100\n",
            "loss 0.04379818077640603 average time 1.4439499072001127 iter num 20\n",
            "loss 0.04383995684998966 average time 1.4401798784000222 iter num 40\n",
            "loss 0.043866343367827104 average time 1.437436188983429 iter num 60\n",
            "loss 0.04387812261573898 average time 1.438496031900081 iter num 80\n",
            "loss 0.04388052830471036 average time 1.4359411982800885 iter num 100\n",
            "loss 0.04392657390993762 average time 1.432355416449991 iter num 20\n",
            "loss 0.043966801042342275 average time 1.4450695790249939 iter num 40\n",
            "loss 0.043992755849207844 average time 1.4538246709665752 iter num 60\n",
            "loss 0.04400445846533536 average time 1.45109806939995 iter num 80\n",
            "loss 0.04400687138269354 average time 1.4478068068399625 iter num 100\n",
            "loss 0.04405253114995022 average time 1.4460781565501748 iter num 20\n",
            "loss 0.04409210522517459 average time 1.4470389232000798 iter num 40\n",
            "loss 0.04411761370844246 average time 1.4495305329833779 iter num 60\n",
            "loss 0.04412907406415184 average time 1.4509835592250284 iter num 80\n",
            "loss 0.044131452671154904 average time 1.4490488754899524 iter num 100\n",
            "loss 0.04417602171648643 average time 1.4504358055998636 iter num 20\n",
            "loss 0.04421494171266348 average time 1.4420052981747176 iter num 40\n",
            "loss 0.04424008143434597 average time 1.448777682133156 iter num 60\n",
            "loss 0.044251306092810264 average time 1.4470768150998992 iter num 80\n",
            "loss 0.044253643830697305 average time 1.448005793659886 iter num 100\n",
            "loss 0.04429702944166392 average time 1.4318927287501537 iter num 20\n",
            "loss 0.044334609481401596 average time 1.4358389309249104 iter num 40\n",
            "loss 0.044358645919299874 average time 1.4383874561166219 iter num 60\n",
            "loss 0.044369232265746614 average time 1.4381032295875003 iter num 80\n",
            "loss 0.0443714110398217 average time 1.4391681566500119 iter num 100\n",
            "loss 0.044412230470238526 average time 1.4427206554001715 iter num 20\n",
            "loss 0.044446793312645366 average time 1.4385880894750698 iter num 40\n",
            "loss 0.04446883147823541 average time 1.4417479001167217 iter num 60\n",
            "loss 0.04447864951515732 average time 1.4481040058125927 iter num 80\n",
            "loss 0.04448070287916561 average time 1.4442592770700322 iter num 100\n",
            "loss 0.04451748431480731 average time 1.4266262417500002 iter num 20\n",
            "loss 0.044547394032659865 average time 1.4337437620500624 iter num 40\n",
            "loss 0.04456575266929667 average time 1.4310092038833622 iter num 60\n",
            "loss 0.044573841873038825 average time 1.4292748341625157 iter num 80\n",
            "loss 0.04457550567590729 average time 1.426200621140033 iter num 100\n",
            "loss 0.04460648370723632 average time 1.4438608994998503 iter num 20\n",
            "loss 0.04463281635159344 average time 1.435723007474826 iter num 40\n",
            "loss 0.044649391235566424 average time 1.4383377859665112 iter num 60\n",
            "loss 0.04465662667001258 average time 1.4372057531374367 iter num 80\n",
            "loss 0.04465812012245381 average time 1.4429335572299715 iter num 100\n",
            "loss 0.04468580201551903 average time 1.4408096899997873 iter num 20\n",
            "loss 0.04470948242706124 average time 1.427198547525086 iter num 40\n",
            "loss 0.04472461021816584 average time 1.4338672202834155 iter num 60\n",
            "loss 0.044731263227468114 average time 1.4321956379250878 iter num 80\n",
            "loss 0.044732640087263895 average time 1.424467362820069 iter num 100\n",
            "loss 0.04475834708973217 average time 1.4495393287998923 iter num 20\n",
            "loss 0.0447803801921075 average time 1.4459864186750564 iter num 40\n",
            "loss 0.044794348290363266 average time 1.443194260866676 iter num 60\n",
            "loss 0.044800475947544355 average time 1.4450243455750524 iter num 80\n",
            "loss 0.04480172158288688 average time 1.4419589135600837 iter num 100\n",
            "loss 0.04482477055866399 average time 1.4662209300499853 iter num 20\n",
            "loss 0.044844541595600605 average time 1.4589210802250363 iter num 40\n",
            "loss 0.044857080358839896 average time 1.4496063042333844 iter num 60\n",
            "loss 0.04486262698143937 average time 1.4466678949499965 iter num 80\n",
            "loss 0.044863757612768826 average time 1.4453199113100528 iter num 100\n",
            "loss 0.04488501691043899 average time 1.4354985024997404 iter num 20\n",
            "loss 0.04490265557708304 average time 1.4342037535997405 iter num 40\n",
            "loss 0.04491383774661719 average time 1.4329904221164422 iter num 60\n",
            "loss 0.044918665845433796 average time 1.434436820587348 iter num 80\n",
            "loss 0.044919660148090707 average time 1.4370803687097942 iter num 100\n",
            "loss 0.0449382284129683 average time 1.4450135056499676 iter num 20\n",
            "loss 0.04495415785259599 average time 1.4469487561249934 iter num 40\n",
            "loss 0.04496419845633495 average time 1.447094320566642 iter num 60\n",
            "loss 0.04496868661431054 average time 1.4437743446374725 iter num 80\n",
            "loss 0.04496958157435884 average time 1.44152765756 iter num 100\n",
            "loss 0.04498653950835082 average time 1.4442796933499267 iter num 20\n",
            "loss 0.04500088019395685 average time 1.467821009374984 iter num 40\n",
            "loss 0.045010230678547355 average time 1.4668434740833922 iter num 60\n",
            "loss 0.045014362698289505 average time 1.457940847174973 iter num 80\n",
            "loss 0.045015178704292384 average time 1.454087032719999 iter num 100\n",
            "loss 0.04503107524633371 average time 1.4378750893502001 iter num 20\n",
            "loss 0.045044425284461606 average time 1.4480958687252496 iter num 40\n",
            "loss 0.045053065126851054 average time 1.444095454216828 iter num 60\n",
            "loss 0.045056905560584386 average time 1.4431918495751461 iter num 80\n",
            "loss 0.045057702184534004 average time 1.4400546927000868 iter num 100\n",
            "loss 0.0450724658191573 average time 1.4302145761001157 iter num 20\n",
            "loss 0.045084929442693204 average time 1.4302266035000684 iter num 40\n",
            "loss 0.045092886132338014 average time 1.4314997783000762 iter num 60\n",
            "loss 0.0450963487672687 average time 1.4353401449375496 iter num 80\n",
            "loss 0.0450970362173284 average time 1.4376334805400075 iter num 100\n",
            "loss 0.04511009180967542 average time 1.4196521886001392 iter num 20\n",
            "loss 0.04512139863674043 average time 1.4265920779751469 iter num 40\n",
            "loss 0.04512884193010651 average time 1.4366991324334473 iter num 60\n",
            "loss 0.045132160748128536 average time 1.4319124368000984 iter num 80\n",
            "loss 0.04513281770386293 average time 1.4330393369600825 iter num 100\n",
            "loss 0.045145645644675776 average time 1.4256843604999632 iter num 20\n",
            "loss 0.04515665771115553 average time 1.4345711604498774 iter num 40\n",
            "loss 0.045163678459505384 average time 1.4324321428164997 iter num 60\n",
            "loss 0.04516675892833626 average time 1.433183638774949 iter num 80\n",
            "loss 0.04516736590310938 average time 1.43097566001994 iter num 100\n",
            "loss 0.04517931698212253 average time 1.42858525690026 iter num 20\n",
            "loss 0.04518942958142943 average time 1.4276217434751288 iter num 40\n",
            "loss 0.04519586966498985 average time 1.429807968733409 iter num 60\n",
            "loss 0.04519867907790314 average time 1.4350647494125952 iter num 80\n",
            "loss 0.04519921316317815 average time 1.4341064799501146 iter num 100\n",
            "loss 0.04521007870836477 average time 1.4227104897998288 iter num 20\n",
            "loss 0.04521941697266953 average time 1.4264565480498732 iter num 40\n",
            "loss 0.045225479279684115 average time 1.4307155263833011 iter num 60\n",
            "loss 0.045228085267303465 average time 1.4290181681624516 iter num 80\n",
            "loss 0.04522857864077931 average time 1.4296623352500137 iter num 100\n",
            "loss 0.04523858938603243 average time 1.421521551549904 iter num 20\n",
            "loss 0.0452464472215764 average time 1.4191021172249747 iter num 40\n",
            "loss 0.04525123546366743 average time 1.4227829602165911 iter num 60\n",
            "loss 0.045253301505983945 average time 1.424855596024986 iter num 80\n",
            "loss 0.0452536614652859 average time 1.4303421794600217 iter num 100\n",
            "loss 0.04526137631902508 average time 1.4113009245503236 iter num 20\n",
            "loss 0.0452679103680437 average time 1.4210264761001326 iter num 40\n",
            "loss 0.04527228958151991 average time 1.4313158107501294 iter num 60\n",
            "loss 0.04527430517710407 average time 1.4324622763376964 iter num 80\n",
            "loss 0.045274665853032595 average time 1.4321414212001582 iter num 100\n",
            "loss 0.04528232622614388 average time 1.4160875985499843 iter num 20\n",
            "loss 0.0452885299847539 average time 1.4279319360248792 iter num 40\n",
            "loss 0.045292672723802656 average time 1.4306575445998533 iter num 60\n",
            "loss 0.045294502885834594 average time 1.43278496336236 iter num 80\n",
            "loss 0.045294785229107286 average time 1.4351721947298575 iter num 100\n",
            "loss 0.04530084280646192 average time 1.4645457857500332 iter num 20\n",
            "loss 0.04530543495930758 average time 1.4452549538250878 iter num 40\n",
            "loss 0.045308182718225974 average time 1.440819407083412 iter num 60\n",
            "loss 0.04530943321475218 average time 1.4420374800250557 iter num 80\n",
            "loss 0.04530965543751025 average time 1.4433950119199834 iter num 100\n",
            "loss 0.04531399614962016 average time 1.4317102591498951 iter num 20\n",
            "loss 0.04531715645168611 average time 1.434157670599916 iter num 40\n",
            "loss 0.04531907790714897 average time 1.430320886866745 iter num 60\n",
            "loss 0.045319973364038174 average time 1.4327983657375625 iter num 80\n",
            "loss 0.04532013753950412 average time 1.434678773380092 iter num 100\n",
            "loss 0.04532358798710987 average time 1.4471514504498373 iter num 20\n",
            "loss 0.04532636667250844 average time 1.4445158089247798 iter num 40\n",
            "loss 0.04532820071126215 average time 1.4414291287665946 iter num 60\n",
            "loss 0.045328920290480024 average time 1.446751993224916 iter num 80\n",
            "loss 0.04532904650403253 average time 1.4462803873299344 iter num 100\n",
            "loss 0.0453320504974522 average time 1.4297490975000984 iter num 20\n",
            "loss 0.04533468994134846 average time 1.4268991668001036 iter num 40\n",
            "loss 0.045336390273929106 average time 1.4346556810500866 iter num 60\n",
            "loss 0.045337136151487996 average time 1.4372759457125994 iter num 80\n",
            "loss 0.04533726022091578 average time 1.4359700212500683 iter num 100\n",
            "loss 0.045340565568973916 average time 1.4334808322500976 iter num 20\n",
            "loss 0.04534350387296999 average time 1.4468979905000197 iter num 40\n",
            "loss 0.04534542632229683 average time 1.436896159716677 iter num 60\n",
            "loss 0.04534634369868079 average time 1.4347327378374986 iter num 80\n",
            "loss 0.04534650110541718 average time 1.4332974797299902 iter num 100\n",
            "loss 0.0453499162900224 average time 1.4257502561001274 iter num 20\n",
            "loss 0.04535289964504399 average time 1.430850633800219 iter num 40\n",
            "loss 0.045354803405861246 average time 1.4258592959501282 iter num 60\n",
            "loss 0.045355656393365015 average time 1.4284893989125749 iter num 80\n",
            "loss 0.04535581311539918 average time 1.4296436227200866 iter num 100\n",
            "loss 0.04535931008368967 average time 1.428977385000053 iter num 20\n",
            "loss 0.04536245805451555 average time 1.4345966738750122 iter num 40\n",
            "loss 0.045364473868363946 average time 1.4401589233333045 iter num 60\n",
            "loss 0.04536531784554838 average time 1.439531673249985 iter num 80\n",
            "loss 0.04536542880914654 average time 1.4424939214499135 iter num 100\n",
            "loss 0.045368811229889394 average time 1.4356484778002596 iter num 20\n",
            "loss 0.0453713553346142 average time 1.4332653053748345 iter num 40\n",
            "loss 0.04537273572651793 average time 1.4336808567665078 iter num 60\n",
            "loss 0.04537337743530449 average time 1.4305171672498773 iter num 80\n",
            "loss 0.0453734638857296 average time 1.4330829160998655 iter num 100\n",
            "loss 0.045376063989632086 average time 1.4274982902503326 iter num 20\n",
            "loss 0.045378512385721666 average time 1.424088138125171 iter num 40\n",
            "loss 0.0453803643369942 average time 1.4292623511836913 iter num 60\n",
            "loss 0.04538125746959001 average time 1.438615672262904 iter num 80\n",
            "loss 0.04538138935127407 average time 1.4393438870003228 iter num 100\n",
            "loss 0.045385049186401745 average time 1.4431365992497376 iter num 20\n",
            "loss 0.04538820286727063 average time 1.4406699414248578 iter num 40\n",
            "loss 0.04539035322947477 average time 1.4354945433664397 iter num 60\n",
            "loss 0.04539134094602214 average time 1.4410483005497554 iter num 80\n",
            "loss 0.04539148851901111 average time 1.4387224883897942 iter num 100\n",
            "loss 0.045395503414812986 average time 1.4415327326998522 iter num 20\n",
            "loss 0.04539912716286642 average time 1.4406838204748964 iter num 40\n",
            "loss 0.04540138633300939 average time 1.4323891880165927 iter num 60\n",
            "loss 0.04540252767799432 average time 1.4364872793998529 iter num 80\n",
            "loss 0.04540267977343993 average time 1.438941394639951 iter num 100\n",
            "loss 0.04540701731678904 average time 1.4337220864501432 iter num 20\n",
            "loss 0.04541087109554331 average time 1.4409544770003777 iter num 40\n",
            "loss 0.045413338344490586 average time 1.4376191810004457 iter num 60\n",
            "loss 0.04541452544849773 average time 1.4380664664754932 iter num 80\n",
            "loss 0.04541471773766957 average time 1.4412241350203476 iter num 100\n",
            "loss 0.04541964573462007 average time 1.4451301625997075 iter num 20\n",
            "loss 0.04542408504448476 average time 1.44099038309987 iter num 40\n",
            "loss 0.04542705908178708 average time 1.4417050836664809 iter num 60\n",
            "loss 0.04542849285002748 average time 1.4356146478372467 iter num 80\n",
            "loss 0.045428709265177385 average time 1.4359874121397298 iter num 100\n",
            "loss 0.04543397157980871 average time 1.4306985467999767 iter num 20\n",
            "loss 0.045438275076099856 average time 1.427316946349856 iter num 40\n",
            "loss 0.04544093525759465 average time 1.4275516008167566 iter num 60\n",
            "loss 0.04544197616315415 average time 1.4323758415875545 iter num 80\n",
            "loss 0.0454421160650266 average time 1.431490101680065 iter num 100\n",
            "loss 0.045445815077900874 average time 1.4208399292994727 iter num 20\n",
            "loss 0.045449217606134185 average time 1.4353292339245853 iter num 40\n",
            "loss 0.04545145658556032 average time 1.4288918203662737 iter num 60\n",
            "loss 0.04545246936801872 average time 1.4309090706246024 iter num 80\n",
            "loss 0.045452641256922284 average time 1.4320737798897607 iter num 100\n",
            "loss 0.045456739811532566 average time 1.4577436121999199 iter num 20\n",
            "loss 0.045460531693531106 average time 1.449921129924951 iter num 40\n",
            "loss 0.0454627823017232 average time 1.443411975766685 iter num 60\n",
            "loss 0.04546367646926203 average time 1.4425429630251074 iter num 80\n",
            "loss 0.04546379662342122 average time 1.4409978500102079 iter num 100\n",
            "loss 0.04546763531666405 average time 1.434903350600689 iter num 20\n",
            "loss 0.04547066965844526 average time 1.4245462627252892 iter num 40\n",
            "loss 0.0454727327571204 average time 1.4331893945500875 iter num 60\n",
            "loss 0.04547371693456516 average time 1.4347309004125237 iter num 80\n",
            "loss 0.04547381683571485 average time 1.436370546710059 iter num 100\n",
            "loss 0.045477462627339545 average time 1.4466834408507565 iter num 20\n",
            "loss 0.045480426704960594 average time 1.45325720200035 iter num 40\n",
            "loss 0.04548213426769454 average time 1.4486657619170122 iter num 60\n",
            "loss 0.04548289942385277 average time 1.4428454633251477 iter num 80\n",
            "loss 0.045482983417276746 average time 1.4367241854100938 iter num 100\n",
            "loss 0.045486276544745845 average time 1.4381606389493755 iter num 20\n",
            "loss 0.045488854532257456 average time 1.4351138968494523 iter num 40\n",
            "loss 0.04549032086079903 average time 1.4401227111831152 iter num 60\n",
            "loss 0.04549096823927595 average time 1.4408879373622767 iter num 80\n",
            "loss 0.0454910384026663 average time 1.4400868756998535 iter num 100\n",
            "loss 0.04549344136476907 average time 1.4354514622000352 iter num 20\n",
            "loss 0.04549551169861648 average time 1.4394903928251552 iter num 40\n",
            "loss 0.04549705385515485 average time 1.4444855608334668 iter num 60\n",
            "loss 0.04549782076714217 average time 1.4425348688626856 iter num 80\n",
            "loss 0.0454979058497128 average time 1.4444258372502372 iter num 100\n",
            "loss 0.045500859356095 average time 1.4468811267001 iter num 20\n",
            "loss 0.0455034344713574 average time 1.4375980503250503 iter num 40\n",
            "loss 0.04550522576707527 average time 1.437083770916676 iter num 60\n",
            "loss 0.04550613736369298 average time 1.438631311562449 iter num 80\n",
            "loss 0.04550627345334443 average time 1.4368451356899459 iter num 100\n",
            "loss 0.04550977113858896 average time 1.4348686596002154 iter num 20\n",
            "loss 0.04551250932970981 average time 1.4263308307251463 iter num 40\n",
            "loss 0.04551428588535948 average time 1.428649424033574 iter num 60\n",
            "loss 0.04551511737828995 average time 1.4310158476252126 iter num 80\n",
            "loss 0.04551524473790904 average time 1.427020910230167 iter num 100\n",
            "loss 0.04551853819275267 average time 1.4291149103501084 iter num 20\n",
            "loss 0.045520863614059895 average time 1.4220156289750776 iter num 40\n",
            "loss 0.04552229439631191 average time 1.4210619756498999 iter num 60\n",
            "loss 0.04552294425247279 average time 1.4177904215249781 iter num 80\n",
            "loss 0.0455230032142867 average time 1.4175448148699434 iter num 100\n",
            "loss 0.04552573696224386 average time 1.4215881721000188 iter num 20\n",
            "loss 0.04552806548161248 average time 1.4280050604497774 iter num 40\n",
            "loss 0.04552978601708392 average time 1.4262425392499305 iter num 60\n",
            "loss 0.045530669358694584 average time 1.431613143887398 iter num 80\n",
            "loss 0.04553081722636215 average time 1.4337117437898268 iter num 100\n",
            "loss 0.0455343965968527 average time 1.4236678686504092 iter num 20\n",
            "loss 0.04553710119889795 average time 1.428032655050265 iter num 40\n",
            "loss 0.04553898234705834 average time 1.4321096736667944 iter num 60\n",
            "loss 0.045539880004416444 average time 1.4336096978499882 iter num 80\n",
            "loss 0.045540015725442855 average time 1.4300334225500047 iter num 100\n",
            "loss 0.04554346127696122 average time 1.4435985976495431 iter num 20\n",
            "loss 0.045546289468289224 average time 1.4302373153495864 iter num 40\n",
            "loss 0.04554802146356944 average time 1.43480040648307 iter num 60\n",
            "loss 0.04554884471428285 average time 1.4361551471622989 iter num 80\n",
            "loss 0.0455489752216638 average time 1.4393991900098626 iter num 100\n",
            "loss 0.04555226210405117 average time 1.4387060529004885 iter num 20\n",
            "loss 0.045554766011485816 average time 1.4313716882752487 iter num 40\n",
            "loss 0.04555637575802749 average time 1.4292082179335315 iter num 60\n",
            "loss 0.04555704373301852 average time 1.430623059212803 iter num 80\n",
            "loss 0.04555713001448899 average time 1.4314467568702458 iter num 100\n",
            "loss 0.0455597293335147 average time 1.4287704407001 iter num 20\n",
            "loss 0.04556154392101214 average time 1.4374389574250928 iter num 40\n",
            "loss 0.045562877273188224 average time 1.443555201316667 iter num 60\n",
            "loss 0.0455635661197362 average time 1.445940363487398 iter num 80\n",
            "loss 0.04556365084648088 average time 1.441758149349953 iter num 100\n",
            "loss 0.045566595157818364 average time 1.4583722846000455 iter num 20\n",
            "loss 0.04556883090237816 average time 1.4440879089502232 iter num 40\n",
            "loss 0.045570445482298244 average time 1.4381910004167973 iter num 60\n",
            "loss 0.045571166348451035 average time 1.4351057304376353 iter num 80\n",
            "loss 0.04557126791697726 average time 1.4345604581799853 iter num 100\n",
            "loss 0.0455742565302496 average time 1.4268414113003018 iter num 20\n",
            "loss 0.04557680458291653 average time 1.4304220759751842 iter num 40\n",
            "loss 0.045578486715858485 average time 1.4292343625334374 iter num 60\n",
            "loss 0.045579199175477245 average time 1.4287246800875437 iter num 80\n",
            "loss 0.045579274206073425 average time 1.4276011751599436 iter num 100\n",
            "loss 0.04558227093118822 average time 1.427053365050233 iter num 20\n",
            "loss 0.04558483956607408 average time 1.4329868835251545 iter num 40\n",
            "loss 0.04558643573915154 average time 1.4290065832001346 iter num 60\n",
            "loss 0.045587151529546024 average time 1.4257801227874551 iter num 80\n",
            "loss 0.0455872385228049 average time 1.4226091427500434 iter num 100\n",
            "loss 0.045589966230140604 average time 1.4357072713501111 iter num 20\n",
            "loss 0.04559227188627508 average time 1.4287876189498092 iter num 40\n",
            "loss 0.045593887934686377 average time 1.4235388927832295 iter num 60\n",
            "loss 0.045594699639984644 average time 1.4201726797500214 iter num 80\n",
            "loss 0.045594795445185576 average time 1.4193963346900638 iter num 100\n",
            "loss 0.04559809594370766 average time 1.432150652600103 iter num 20\n",
            "loss 0.04560066447058074 average time 1.4448420129753685 iter num 40\n",
            "loss 0.04560245960633859 average time 1.4479084259335044 iter num 60\n",
            "loss 0.0456033105237064 average time 1.4455995815375444 iter num 80\n",
            "loss 0.04560342268231474 average time 1.4450284654001007 iter num 100\n",
            "loss 0.04560659391054997 average time 1.4360374312005662 iter num 20\n",
            "loss 0.04560939978755271 average time 1.4375317953502418 iter num 40\n",
            "loss 0.04561131207974471 average time 1.4414668300668079 iter num 60\n",
            "loss 0.04561222558872887 average time 1.44055537607519 iter num 80\n",
            "loss 0.045612344228071054 average time 1.4369598340002268 iter num 100\n",
            "loss 0.0456159515115674 average time 1.4385284033998686 iter num 20\n",
            "loss 0.045618794578931365 average time 1.4430184479499986 iter num 40\n",
            "loss 0.045620751244936704 average time 1.4464359892166612 iter num 60\n",
            "loss 0.04562169778005665 average time 1.4457801446748362 iter num 80\n",
            "loss 0.04562181405246738 average time 1.4446555056299621 iter num 100\n",
            "loss 0.04562558131963589 average time 1.452722857199842 iter num 20\n",
            "loss 0.0456285592071877 average time 1.4504108951498893 iter num 40\n",
            "loss 0.0456305794350913 average time 1.4464198671997412 iter num 60\n",
            "loss 0.04563153829398767 average time 1.4439455473122962 iter num 80\n",
            "loss 0.04563164869444306 average time 1.4415497847798542 iter num 100\n",
            "loss 0.04563510599773504 average time 1.4220289379994937 iter num 20\n",
            "loss 0.04563824287066583 average time 1.4393506172496928 iter num 40\n",
            "loss 0.04564021163852751 average time 1.4340208698329056 iter num 60\n",
            "loss 0.0456411435157272 average time 1.4374129107245608 iter num 80\n",
            "loss 0.04564128213381697 average time 1.4325736454898652 iter num 100\n",
            "loss 0.04564509879982095 average time 1.42247329180027 iter num 20\n",
            "loss 0.04564794749026989 average time 1.4208687237000959 iter num 40\n",
            "loss 0.045650093717916576 average time 1.4219699136000297 iter num 60\n",
            "loss 0.04565106131759182 average time 1.4196592960874113 iter num 80\n",
            "loss 0.04565118508138988 average time 1.4196796522899968 iter num 100\n",
            "loss 0.04565498908706909 average time 1.4569707529002698 iter num 20\n",
            "loss 0.04565819387074441 average time 1.443728419625404 iter num 40\n",
            "loss 0.04566027272047679 average time 1.4306659713503904 iter num 60\n",
            "loss 0.04566129089994165 average time 1.428158488637837 iter num 80\n",
            "loss 0.04566144484387484 average time 1.4323747305301366 iter num 100\n",
            "loss 0.04566550848426597 average time 1.42101864594988 iter num 20\n",
            "loss 0.045668484540794384 average time 1.4217669080499036 iter num 40\n",
            "loss 0.04567064174526818 average time 1.4158857237998745 iter num 60\n",
            "loss 0.04567167523171779 average time 1.4199774566249288 iter num 80\n",
            "loss 0.04567182415348648 average time 1.4196353126098984 iter num 100\n",
            "loss 0.04567622321098616 average time 1.4446442136497353 iter num 20\n",
            "loss 0.04567934030526912 average time 1.4304235847249402 iter num 40\n",
            "loss 0.04568176391844456 average time 1.4237516518497917 iter num 60\n",
            "loss 0.04568286902570865 average time 1.426401183149801 iter num 80\n",
            "loss 0.04568302119832419 average time 1.425232183609769 iter num 100\n",
            "loss 0.04568759888140832 average time 1.4466177377003988 iter num 20\n",
            "loss 0.04569071986785595 average time 1.4340240478250963 iter num 40\n",
            "loss 0.0456933853873718 average time 1.4298941691332099 iter num 60\n",
            "loss 0.04569454803218864 average time 1.42644275512489 iter num 80\n",
            "loss 0.045694727292403974 average time 1.4240927036899302 iter num 100\n",
            "loss 0.045699718839996455 average time 1.4116381560499576 iter num 20\n",
            "loss 0.04570296668396005 average time 1.4095172976502 iter num 40\n",
            "loss 0.04570574480872706 average time 1.4163293810335744 iter num 60\n",
            "loss 0.045706874542979746 average time 1.4172395533126292 iter num 80\n",
            "loss 0.04570703182263262 average time 1.4192556615001741 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQs-OZHGEwac"
      },
      "source": [
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec7ec7f-5275-4a52-bff3-d8ad65eaf807"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 0.8, 1, 0.25, 0.02, 0.02]]).cuda() # T, K, B, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[3]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.10632345, 0.5543747)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[0.098601]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.591849], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqpasxVi0hx3",
        "outputId": "408279ef-7704-4355-9a3d-c5f3a97982c0"
      },
      "source": [
        "# Knock out call\n",
        "\n",
        "# now change code such that 'numsteps' does not represent year\n",
        "# make dt = year / numsteps\n",
        "# Add r, and notice that noise must have mean 0, not drift, or else it'll give large option prices\n",
        "# (done)\n",
        "# after making the changes, the values are still correct\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        #dx =  drift + noise[t,:] # no need to multiply by sigma here because noise generated by cov not corr\n",
        "        dx2 = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx2)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T): # down-and-out call\n",
        "    return jnp.mean(jnp.maximum((1 - jnp.any(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2) < B, axis=1).astype(int))* \n",
        "                                (jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T), axis=2))[:,-1]-K, 0) *\n",
        "                    jnp.exp(-r[0] * T))\n",
        "    # must use '-1' not 'numsteps', or else grad will be 0\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 2000000\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.02]*numstocks)\n",
        "r = drift # let r = drift to match B-S\n",
        "\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.]*numstocks) # must be float\n",
        "\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "B = 0.8 # if B is set to 0, equivalent to European call\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T\n",
        "\n",
        "# delta\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, B, T)) # here numsteps different from T"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10632345\n",
            "[0.5543747]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovJwXo3-YEu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "baf7adc0-c67e-4ef2-dee4-384c3268a28d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def compute_price(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    return model(inputs.float())\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_prices = []\n",
        "correct_call_prices = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_prices.append(compute_price(p).item())\n",
        "    correct_call_prices.append(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, B, T))\n",
        "\n",
        "#plt.plot(prices, model_call_prices, label = \"model_call_prices\")\n",
        "#plt.plot(prices, correct_call_prices, label = \"correct_call_prices\")\n",
        "plt.plot(prices, np.array(model_call_prices)-np.array(correct_call_prices), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwApH0GT9bBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "061ba88d-d97e-4287-89e1-156a567a5d10"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.0, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgD1NjP7DOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "0663e404-c2c1-42af-e5f5-477a579c1eb7"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.775, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 0.775, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7MfujMQ7oij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "c26e5838-0ae8-4007-a469-432480d98c08"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 1.225, 0.8, S, 0.25, 0.02, 0.02]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][3]\n",
        "\n",
        "prices = np.arange(0.75, 1.25, 0.01)\n",
        "model_call_deltas = []\n",
        "correct_call_deltas = []\n",
        "for p in prices:\n",
        "    initial_stocks = jnp.array([p]*numstocks) # must be float\n",
        "    model_call_deltas.append(compute_delta(p).item())\n",
        "    correct_call_deltas.append(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, 1.225, B, T))\n",
        "\n",
        "plt.plot(prices, model_call_deltas, label = \"model_call_deltas\")\n",
        "plt.plot(prices, correct_call_deltas, label = \"correct_call_deltas\")\n",
        "#plt.plot(prices, np.array(model_call_deltas)-np.array(correct_call_deltas), label = \"Differences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLWb0Ilm-ouZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}