{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "European_Call_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Pui/European_Call_jax_1stock_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RYKgBifCYw"
      },
      "source": [
        "# Test (Skip this if not trying to test, to make sure that functions are defined correctly in cells below without running this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWYfON_marpj",
        "outputId": "4e76cd73-4520-4643-f2b7-5e9bc6aa8587"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T):\n",
        "  return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.0807]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.2597*0.2597\n",
        "initial_stocks = jnp.array([0.7178]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 0.2106\n",
        "\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "keys = jax.random.split(key, numpaths)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T)\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "#%timeit goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5235912\n",
            "[1.0000885]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2YUJ9cfEiF"
      },
      "source": [
        "# Construct Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxFUnc_iBVcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66adad4-928a-4494-a762-e0c1b11c673a"
      },
      "source": [
        "%%writefile cupy_dataset.py\n",
        "# version 1, 2, 6\n",
        "import cupy\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit\n",
        "import numpy as np\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "def Brownian_motion(key, initial_stocks, numsteps, drift, cov, T):\n",
        "    stocks_init = jnp.zeros((numsteps + 1, initial_stocks.shape[0]))\n",
        "    stocks_init = jax.ops.index_update(stocks_init,   # jax.ops.index_update(x, idx, y) <-> Pure equivalent of x[idx] = y\n",
        "                            jax.ops.index[0],         # initialization of stock prices\n",
        "                            initial_stocks)\n",
        "    noise = jax.random.multivariate_normal(key,  jnp.array([0]*initial_stocks.shape[0]), cov, (numsteps+1,)) # noise must have mean 0\n",
        "    sigma = jnp.diag(cov) ** 0.5\n",
        "    dt = T / numsteps\n",
        "    def time_step(t, val):\n",
        "        dx = jnp.exp((drift - sigma ** 2. / 2.) * dt + jnp.sqrt(dt) * noise[t,:])\n",
        "        val = jax.ops.index_update(val,\n",
        "                            jax.ops.index[t],\n",
        "                            val[t-1] * dx)\n",
        "        return val\n",
        "    return jax.lax.fori_loop(1, numsteps+1, time_step, stocks_init)[1:] # jax.lax.fori_loop(lower, upper, body_fun, init_val)\n",
        "\n",
        "def optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T, keys): # need to pass 'keys'\n",
        "    return jnp.mean((jnp.maximum(jnp.mean(batch_simple(keys, initial_stocks, numsteps, drift, cov, T)[:,-1,:], axis=1)-K,0)) * jnp.exp(-r[0] * T))\n",
        "\n",
        "###################################################################################################\n",
        "# these 2 functions must be defined outside class in order to be used in 'optionvalueavg' function\n",
        "fast_simple = jax.jit(Brownian_motion, static_argnums=2)\n",
        "batch_simple = jax.vmap(fast_simple, in_axes=(0, None, None, None, None, None))\n",
        "###################################################################################################\n",
        "\n",
        "class OptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len, number_path, batch, seed, stocks):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 50\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        self.T = 1.0 # assume T = 1, use float here\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num >= self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros((self.N_BATCH, 1 + self.N_STOCKS), dtype=cupy.float32) # output: price, delta1, delta2, delta3\n",
        "        X = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          rng = jax.random.PRNGKey(self.seed)\n",
        "          rng, key = jax.random.split(rng)\n",
        "\n",
        "          ################################################################################################### generate random input numbers\n",
        "\n",
        "          initial_stocks = jnp.array(0.75 + np.random.random(self.N_STOCKS) * 0.5)\n",
        "\n",
        "          corr = jnp.diag(jnp.array([1]*self.N_STOCKS)) # assume no correlation between stocks here\n",
        "          sigma = jnp.array(0.15 + np.random.random(self.N_STOCKS) * 0.3)\n",
        "          cov = (jnp.diag(sigma)).dot(corr).dot(jnp.diag(sigma))\n",
        "\n",
        "          r = jnp.repeat(jnp.array(0.25 + np.random.random(1) * 0.35), self.N_STOCKS)\n",
        "          drift = r\n",
        "\n",
        "          T = self.T\n",
        "          K = 0.75 + np.random.random(1) * 0.5\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### apply functions to compute price and deltas\n",
        "          \n",
        "          keys = jax.random.split(key, self.N_PATHS)\n",
        "\n",
        "          European_Call_price = optionvalueavg(key, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "          gooptionvalue = jax.grad(optionvalueavg, argnums=1)\n",
        "          Deltas = gooptionvalue(keys, initial_stocks, self.N_STEPS, drift, r, cov, K, T, keys) # need to pass 'keys'\n",
        "\n",
        "          ###################################################################################################\n",
        "          ################################################################################################### store input and output numbers in X and Y\n",
        "\n",
        "          Y[op, 0] = European_Call_price\n",
        "          Y[op, 1:] = cupy.array(Deltas, dtype=cupy.float32) # remember to change this!\n",
        "\n",
        "          # T, K, S, sigma, mu, r\n",
        "          paras = (jnp.repeat(jnp.array(T), self.N_STOCKS), jnp.repeat(jnp.array(K), self.N_STOCKS), initial_stocks, sigma, drift, r)\n",
        "          paras = np.column_stack(paras).reshape(1,-1)[0]\n",
        "          X[op,] = cupy.array(paras)\n",
        "\n",
        "          ###################################################################################################\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "# ds = OptionDataSet(max_len = 2, number_path = 1000000, batch = 2, seed = np.random.randint(10000), stocks=1) # for testing purpose, use constant seed. When training, change to random seed\n",
        "# for i in ds:\n",
        "#     print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cupy_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6JO9OBHdvv",
        "outputId": "0ca96951-5c1a-4efe-a317-d13b7db42214"
      },
      "source": [
        "%%writefile model.py\n",
        "# version 1,2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6*1, 32) # remember to change this!\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1) # 1 outputs: price\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1.0, 0.5, 0.5, 0.3, 0.35, 0.35]*1)) # don't use numpy here - will give error later\n",
        "                                                                               # T, K, S, sigma, mu, r\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = (x - torch.tensor([0.0, 0.75, 0.75, 0.15, 0.25, 0.25]*1)) / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSrICMvyZX0"
      },
      "source": [
        "# Train Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlXD80xPNVc6",
        "outputId": "77c0cc7e-61f4-4a2a-a6fd-c5df435b1ffb"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 30.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3CyULkENYKb",
        "outputId": "e575b4c7-44b1-4dd2-986e-cf652e726c89"
      },
      "source": [
        "# version 2, 7\n",
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 100000, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 2408.8115234375 average time 0.21569903545000102 iter num 20\n",
            "loss 2126.212890625 average time 0.12378502192499923 iter num 40\n",
            "loss 2588.671142578125 average time 0.09295705038333797 iter num 60\n",
            "loss 921.111572265625 average time 0.07758783802500915 iter num 80\n",
            "loss 1434.099853515625 average time 0.0684344519600097 iter num 100\n",
            "loss 89.68195343017578 average time 0.10293295500001705 iter num 20\n",
            "loss 25.545438766479492 average time 0.06765549510001279 iter num 40\n",
            "loss 18.984630584716797 average time 0.055631484150004934 iter num 60\n",
            "loss 12.278401374816895 average time 0.04941242690000536 iter num 80\n",
            "loss 15.778316497802734 average time 0.04580528509999795 iter num 100\n",
            "loss 10.017118453979492 average time 0.10074550609999733 iter num 20\n",
            "loss 16.154296875 average time 0.06602838105000046 iter num 40\n",
            "loss 10.468088150024414 average time 0.05471895543333479 iter num 60\n",
            "loss 3.5554184913635254 average time 0.04910922859998834 iter num 80\n",
            "loss 5.760096549987793 average time 0.04569119162999641 iter num 100\n",
            "loss 3.7533280849456787 average time 0.10315763224999727 iter num 20\n",
            "loss 4.0251994132995605 average time 0.06724684820000562 iter num 40\n",
            "loss 6.518115997314453 average time 0.05541306915000632 iter num 60\n",
            "loss 2.7385900020599365 average time 0.04954311079999911 iter num 80\n",
            "loss 5.745238304138184 average time 0.045935153099997024 iter num 100\n",
            "loss 5.634224891662598 average time 0.09997176645000536 iter num 20\n",
            "loss 8.081476211547852 average time 0.06619191722498385 iter num 40\n",
            "loss 8.868260383605957 average time 0.05451630423331532 iter num 60\n",
            "loss 3.5334880352020264 average time 0.04882748766248426 iter num 80\n",
            "loss 2.043426990509033 average time 0.045319097309986775 iter num 100\n",
            "loss 2.8792247772216797 average time 0.09864614854999446 iter num 20\n",
            "loss 3.3329906463623047 average time 0.06514913862498588 iter num 40\n",
            "loss 3.641904592514038 average time 0.05386342301665081 iter num 60\n",
            "loss 1.74638032913208 average time 0.04837392039999884 iter num 80\n",
            "loss 2.013693332672119 average time 0.04496669601999656 iter num 100\n",
            "loss 3.709444284439087 average time 0.10119774029998325 iter num 20\n",
            "loss 1.7237167358398438 average time 0.06654081819999647 iter num 40\n",
            "loss 5.891289710998535 average time 0.054942200233320665 iter num 60\n",
            "loss 2.253864288330078 average time 0.049170678699991296 iter num 80\n",
            "loss 6.265925407409668 average time 0.045743250599991826 iter num 100\n",
            "loss 9.2235107421875 average time 0.09987013529998875 iter num 20\n",
            "loss 2.883808135986328 average time 0.06547284630000831 iter num 40\n",
            "loss 6.697634696960449 average time 0.05424017251668071 iter num 60\n",
            "loss 2.788149833679199 average time 0.0485613023625092 iter num 80\n",
            "loss 4.0608110427856445 average time 0.04513127452001754 iter num 100\n",
            "loss 5.362698078155518 average time 0.1010740105499508 iter num 20\n",
            "loss 4.302915573120117 average time 0.06612449799996512 iter num 40\n",
            "loss 4.836935520172119 average time 0.05446568604997992 iter num 60\n",
            "loss 3.178679943084717 average time 0.04869696849998491 iter num 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dc8e31c55796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iter num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cupy_dataset.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m           \u001b[0mEuropean_Call_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptionvalueavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0mgooptionvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptionvalueavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m           \u001b[0mDeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooptionvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to pass 'keys'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m           \u001b[0;31m###################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mgrad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_output_dtype_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholomorphic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_vjp_pullback_wrapper\u001b[0;34m(cotangent_dtypes, cotangent_shapes, io_tree, fun, py_args)\u001b[0m\n\u001b[1;32m   2050\u001b[0m           \u001b[0;34m\"must be the same as the shape of corresponding primal input \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m           f\"{ct_shape}.\")\n\u001b[0;32m-> 2052\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2053\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36munbound_vjp\u001b[0;34m(pvals, jaxpr, consts, *cts)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mcts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_consts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mdummy_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mUndefinedPrimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0marg_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_cts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(jaxpr, reduce_axes, consts, primals_in, cotangents_in)\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         cts_out = get_primitive_transpose(eqn.primitive)(cts_in, *invals,\n\u001b[0;32m--> 229\u001b[0;31m                                                          **eqn.params)\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0mcts_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcts_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mZero\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcts_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# FIXME: Some invars correspond to primals!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_gather_transpose_rule\u001b[0;34m(t, operand, indices, dimension_numbers, slice_sizes, unique_indices, indices_are_sorted)\u001b[0m\n\u001b[1;32m   4600\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mad_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4601\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4602\u001b[0;31m     \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4603\u001b[0m     scatter_dnums = ScatterDimensionNumbers(\n\u001b[1;32m   4604\u001b[0m       \u001b[0mupdate_window_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension_numbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mfull_like\u001b[0;34m(x, fill_value, dtype, shape)\u001b[0m\n\u001b[1;32m   1895\u001b[0m   \u001b[0mweak_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_weakly_typed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1897\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0;32m--> 461\u001b[0;31m                                        weak_type=new_weak_type)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbitcast_convert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    266\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxUYnbSVTnl"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_GDLdvU9ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39569314-e018-4481-91f5-740fc4586dd3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEAqIJAVa-6"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_test1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXkmXlAVdEh"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjERL5bcVcXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58206fe0-aa56-4fab-d958-de126cadc0e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJoo5Z7VjAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1106fd84-ad99-44b2-e245-e10d0e926348"
      },
      "source": [
        "import torch\n",
        "model_save_name = 'jax_european_1stock_test1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['norm', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias', 'fc6.weight', 'fc6.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkrjknWVlL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93312a3-4e05-4c8e-9851-f4a556f754e8"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net().cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc6): Linear(in_features=1024, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MbsAwEVmff"
      },
      "source": [
        "**Continue to train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYxo1IXVl57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f6e292-e5f9-43bc-d58a-cf46da83757a"
      },
      "source": [
        "# If memory is not enough, try changing parameters and restarting session\n",
        "# loss will converge\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "timer = Timer(average=True)\n",
        "#model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "#dataset = OptionDataSet(max_len = 100, number_path = 1024, batch = 32, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "dataset = OptionDataSet(max_len = 100, number_path = 1000000, batch = 16, seed = np.random.randint(10000), stocks = 1) # must have random seed\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "\n",
        "    def compute_deltas(x):\n",
        "      inputs = x\n",
        "      inputs.requires_grad = True\n",
        "      first_order_gradient = grad(model(inputs), inputs, create_graph=False)\n",
        "      return first_order_gradient[0][[2]]\n",
        "\n",
        "    deltas = torch.stack([compute_deltas(x) for x in torch.unbind(x)], dim=0)\n",
        "    y_pred = torch.cat((y_pred, deltas), 1)\n",
        "\n",
        "    loss_weight = torch.tensor([1, 0]).cuda()  # let delta weight = 0 for testing\n",
        "    loss_weight_normalized = loss_weight/loss_weight.sum()\n",
        "    loss = ((y_pred - y) ** 2 * loss_weight_normalized).mean(axis=0).sum() # compute weighted MSE between the 2 arrays\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 10\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter) # print by multiplying 10000 -> easier to read (actual loss function isn't amplified)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 20)\n",
        "\n",
        "model_save_name = 'jax_european_1stock_test1.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP Project/PUI/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 13.031719207763672 average time 0.3516769881000073 iter num 10\n",
            "loss 2.8753252029418945 average time 0.1843993639002292 iter num 20\n",
            "loss 6.898102283477783 average time 0.1290628192336347 iter num 30\n",
            "loss 0.5391082167625427 average time 0.10116679775028388 iter num 40\n",
            "loss 0.7777198553085327 average time 0.08436501544019848 iter num 50\n",
            "loss 0.3032364845275879 average time 0.07326824943350706 iter num 60\n",
            "loss 0.3129631280899048 average time 0.06527927104299514 iter num 70\n",
            "loss 0.04416505992412567 average time 0.05930497996255326 iter num 80\n",
            "loss 0.188725084066391 average time 0.05463837472238487 iter num 90\n",
            "loss 0.11835906654596329 average time 0.05103864909018739 iter num 100\n",
            "loss 0.1419709175825119 average time 0.35059288259944876 iter num 10\n",
            "loss 0.2971019148826599 average time 0.1840592314494643 iter num 20\n",
            "loss 0.39013251662254333 average time 0.12847000879968012 iter num 30\n",
            "loss 0.12671756744384766 average time 0.10075981384970874 iter num 40\n",
            "loss 0.38977205753326416 average time 0.08409988429979422 iter num 50\n",
            "loss 0.18107745051383972 average time 0.07297496138301843 iter num 60\n",
            "loss 0.15708260238170624 average time 0.06509447944247637 iter num 70\n",
            "loss 0.23784999549388885 average time 0.05915240023714432 iter num 80\n",
            "loss 0.17468073964118958 average time 0.05449868203302483 iter num 90\n",
            "loss 0.045542534440755844 average time 0.05084125269979268 iter num 100\n",
            "loss 0.41773316264152527 average time 0.351460353798393 iter num 10\n",
            "loss 0.23758958280086517 average time 0.18458086019909387 iter num 20\n",
            "loss 0.2140710949897766 average time 0.12884307376601403 iter num 30\n",
            "loss 0.1845245510339737 average time 0.10093711844947392 iter num 40\n",
            "loss 0.21972604095935822 average time 0.08433170589945803 iter num 50\n",
            "loss 0.22079816460609436 average time 0.07318439559967374 iter num 60\n",
            "loss 0.14310792088508606 average time 0.06518488918534006 iter num 70\n",
            "loss 0.16029880940914154 average time 0.0592316778246186 iter num 80\n",
            "loss 0.36059314012527466 average time 0.05457422372180897 iter num 90\n",
            "loss 0.19454920291900635 average time 0.050837973679699645 iter num 100\n",
            "loss 0.4428374469280243 average time 0.3496781525998813 iter num 10\n",
            "loss 0.2741026282310486 average time 0.1836413201504911 iter num 20\n",
            "loss 1.0623512268066406 average time 0.12842321486677974 iter num 30\n",
            "loss 0.21877238154411316 average time 0.10079887437514116 iter num 40\n",
            "loss 0.0706307590007782 average time 0.08404365196016443 iter num 50\n",
            "loss 0.2405414581298828 average time 0.07303312675018485 iter num 60\n",
            "loss 0.18261957168579102 average time 0.06516769521440438 iter num 70\n",
            "loss 0.2735801041126251 average time 0.05928038077504425 iter num 80\n",
            "loss 0.20828348398208618 average time 0.05466873871118878 iter num 90\n",
            "loss 0.3489322066307068 average time 0.05098976614004641 iter num 100\n",
            "loss 0.7461527585983276 average time 0.34996019379941573 iter num 10\n",
            "loss 0.28981536626815796 average time 0.1836115359001269 iter num 20\n",
            "loss 0.80437171459198 average time 0.12823603156660587 iter num 30\n",
            "loss 0.2783163785934448 average time 0.10052808972477578 iter num 40\n",
            "loss 0.28165513277053833 average time 0.08387244207980984 iter num 50\n",
            "loss 0.23249031603336334 average time 0.0729141086666156 iter num 60\n",
            "loss 0.11953118443489075 average time 0.06495438947136531 iter num 70\n",
            "loss 0.1766463965177536 average time 0.059086884650105274 iter num 80\n",
            "loss 0.05976049602031708 average time 0.05446770955564312 iter num 90\n",
            "loss 0.16907528042793274 average time 0.0507714610500625 iter num 100\n",
            "loss 0.048642635345458984 average time 0.35032369349937653 iter num 10\n",
            "loss 0.4226280450820923 average time 0.18412806850010383 iter num 20\n",
            "loss 0.6908438801765442 average time 0.12844041766657027 iter num 30\n",
            "loss 0.16076552867889404 average time 0.1008291582500533 iter num 40\n",
            "loss 0.4253615736961365 average time 0.08410869448001904 iter num 50\n",
            "loss 0.43754035234451294 average time 0.07312294561664506 iter num 60\n",
            "loss 0.23985958099365234 average time 0.06518614994279882 iter num 70\n",
            "loss 0.10419552028179169 average time 0.059235167887527494 iter num 80\n",
            "loss 0.10535185784101486 average time 0.054562228777730426 iter num 90\n",
            "loss 0.10315419733524323 average time 0.05086663637990568 iter num 100\n",
            "loss 0.3942822515964508 average time 0.3505451625002024 iter num 10\n",
            "loss 0.3942877948284149 average time 0.1837951844505369 iter num 20\n",
            "loss 0.6513512134552002 average time 0.1283334080336014 iter num 30\n",
            "loss 0.3897257447242737 average time 0.10062506597532775 iter num 40\n",
            "loss 0.6121179461479187 average time 0.08403003012041153 iter num 50\n",
            "loss 0.13251610100269318 average time 0.07301995958368934 iter num 60\n",
            "loss 0.22402355074882507 average time 0.06508615302892784 iter num 70\n",
            "loss 0.09030671417713165 average time 0.059198553512578654 iter num 80\n",
            "loss 0.13250313699245453 average time 0.05456016553341922 iter num 90\n",
            "loss 0.18194231390953064 average time 0.05084969407005701 iter num 100\n",
            "loss 0.05539378523826599 average time 0.34985279530010305 iter num 10\n",
            "loss 0.8910642266273499 average time 0.18369232895001913 iter num 20\n",
            "loss 0.6047148704528809 average time 0.12839921343341607 iter num 30\n",
            "loss 5.075118541717529 average time 0.10065249567533101 iter num 40\n",
            "loss 1.0136717557907104 average time 0.08415267104035593 iter num 50\n",
            "loss 0.6688827872276306 average time 0.07297807955031506 iter num 60\n",
            "loss 0.4249623417854309 average time 0.06521044087172154 iter num 70\n",
            "loss 0.21151871979236603 average time 0.05925823132524784 iter num 80\n",
            "loss 0.2589077353477478 average time 0.054622662133624544 iter num 90\n",
            "loss 0.32540881633758545 average time 0.05093561384026543 iter num 100\n",
            "loss 0.08163855969905853 average time 0.35014776330062886 iter num 10\n",
            "loss 1.6070239543914795 average time 0.1838905322001665 iter num 20\n",
            "loss 0.6117766499519348 average time 0.12838611656673796 iter num 30\n",
            "loss 0.4604716897010803 average time 0.1006308625001111 iter num 40\n",
            "loss 0.44343674182891846 average time 0.0841155577400059 iter num 50\n",
            "loss 0.32004356384277344 average time 0.07301006631672256 iter num 60\n",
            "loss 0.5346200466156006 average time 0.06512043721418845 iter num 70\n",
            "loss 0.049853138625621796 average time 0.05919197266243827 iter num 80\n",
            "loss 0.23837816715240479 average time 0.054553046844375785 iter num 90\n",
            "loss 0.20702703297138214 average time 0.05085115932997723 iter num 100\n",
            "loss 0.9454748034477234 average time 0.3501252745005331 iter num 10\n",
            "loss 2.967975616455078 average time 0.1839204725500167 iter num 20\n",
            "loss 7.7200727462768555 average time 0.12842863956660341 iter num 30\n",
            "loss 0.6860002279281616 average time 0.10067633155003933 iter num 40\n",
            "loss 0.8521842360496521 average time 0.08402899177999643 iter num 50\n",
            "loss 0.16481918096542358 average time 0.07290996536661017 iter num 60\n",
            "loss 0.34028714895248413 average time 0.06498365675704137 iter num 70\n",
            "loss 0.1585959792137146 average time 0.05901770976238367 iter num 80\n",
            "loss 0.07592305541038513 average time 0.0543817002776551 iter num 90\n",
            "loss 0.07492899149656296 average time 0.05068055573992751 iter num 100\n",
            "loss 0.4048931300640106 average time 0.3506223589000001 iter num 10\n",
            "loss 0.9472426176071167 average time 0.184089171800224 iter num 20\n",
            "loss 0.28325146436691284 average time 0.12878152586696767 iter num 30\n",
            "loss 0.1827509105205536 average time 0.10112106572523771 iter num 40\n",
            "loss 0.15673580765724182 average time 0.08433892188011669 iter num 50\n",
            "loss 0.07184480130672455 average time 0.07327783231667126 iter num 60\n",
            "loss 0.22419732809066772 average time 0.06530792742851191 iter num 70\n",
            "loss 0.06600610166788101 average time 0.05934020144995884 iter num 80\n",
            "loss 0.06317481398582458 average time 0.05472539441115158 iter num 90\n",
            "loss 0.0646345317363739 average time 0.05101232522003556 iter num 100\n",
            "loss 0.12871110439300537 average time 0.3505478976010636 iter num 10\n",
            "loss 0.5994843244552612 average time 0.18394678100066814 iter num 20\n",
            "loss 0.13812988996505737 average time 0.1286913624338922 iter num 30\n",
            "loss 0.26685816049575806 average time 0.10090721920023497 iter num 40\n",
            "loss 0.42929890751838684 average time 0.08417975874006516 iter num 50\n",
            "loss 0.09255129098892212 average time 0.0729981181666517 iter num 60\n",
            "loss 0.12080544233322144 average time 0.06503097958588373 iter num 70\n",
            "loss 0.1905462145805359 average time 0.059061331275142945 iter num 80\n",
            "loss 0.2492639124393463 average time 0.05446338434453032 iter num 90\n",
            "loss 0.09610819816589355 average time 0.050798849360180615 iter num 100\n",
            "loss 0.5001543760299683 average time 0.3509194584996294 iter num 10\n",
            "loss 0.4628395140171051 average time 0.18413342664989613 iter num 20\n",
            "loss 0.34395459294319153 average time 0.12862264499999582 iter num 30\n",
            "loss 0.34733104705810547 average time 0.10092229684987615 iter num 40\n",
            "loss 0.15990109741687775 average time 0.08421694139986358 iter num 50\n",
            "loss 0.538243293762207 average time 0.07305757768329689 iter num 60\n",
            "loss 0.43496450781822205 average time 0.0651248796427743 iter num 70\n",
            "loss 0.4782372713088989 average time 0.05915562807485912 iter num 80\n",
            "loss 0.37424352765083313 average time 0.054502266744303696 iter num 90\n",
            "loss 0.06517710536718369 average time 0.05083873301984568 iter num 100\n",
            "loss 0.08846993744373322 average time 0.3510280705002515 iter num 10\n",
            "loss 0.2976366877555847 average time 0.18441562224979863 iter num 20\n",
            "loss 0.505929708480835 average time 0.1291088892332482 iter num 30\n",
            "loss 0.90855473279953 average time 0.10120131897492683 iter num 40\n",
            "loss 0.9731789827346802 average time 0.08454349953979545 iter num 50\n",
            "loss 0.5743523836135864 average time 0.07336818068324646 iter num 60\n",
            "loss 0.2072528600692749 average time 0.06539854122851725 iter num 70\n",
            "loss 0.14149785041809082 average time 0.05942715107494223 iter num 80\n",
            "loss 0.12213779985904694 average time 0.0547369961110639 iter num 90\n",
            "loss 0.14182116091251373 average time 0.05100504512000043 iter num 100\n",
            "loss 0.6627664566040039 average time 0.3503958504003094 iter num 10\n",
            "loss 0.48501312732696533 average time 0.18385207840001386 iter num 20\n",
            "loss 0.16497382521629333 average time 0.1283050297667311 iter num 30\n",
            "loss 0.2567806541919708 average time 0.10058298090016252 iter num 40\n",
            "loss 0.048337917774915695 average time 0.08400677152021671 iter num 50\n",
            "loss 0.7114816904067993 average time 0.07292550451693387 iter num 60\n",
            "loss 0.14604556560516357 average time 0.06500866704295082 iter num 70\n",
            "loss 0.9186713695526123 average time 0.05905450582517915 iter num 80\n",
            "loss 0.12851157784461975 average time 0.054491941322420544 iter num 90\n",
            "loss 0.07749789953231812 average time 0.05076311542015901 iter num 100\n",
            "loss 0.9957321882247925 average time 0.3508332667999639 iter num 10\n",
            "loss 0.14694133400917053 average time 0.18444882335024887 iter num 20\n",
            "loss 0.2330334633588791 average time 0.1287547048667572 iter num 30\n",
            "loss 0.7809557318687439 average time 0.10098197432516827 iter num 40\n",
            "loss 1.009750485420227 average time 0.0842593818201567 iter num 50\n",
            "loss 0.08803081512451172 average time 0.07311000313341841 iter num 60\n",
            "loss 0.2986123263835907 average time 0.06525995365710904 iter num 70\n",
            "loss 0.04769906774163246 average time 0.05938332294990687 iter num 80\n",
            "loss 0.2398948222398758 average time 0.05483310486658108 iter num 90\n",
            "loss 0.13857437670230865 average time 0.05112775400990358 iter num 100\n",
            "loss 0.13248400390148163 average time 0.3511019841000234 iter num 10\n",
            "loss 0.14839012920856476 average time 0.18440822644988658 iter num 20\n",
            "loss 1.1528314352035522 average time 0.12905564070000158 iter num 30\n",
            "loss 0.4497174918651581 average time 0.10133010277513677 iter num 40\n",
            "loss 0.44982922077178955 average time 0.08454602136014727 iter num 50\n",
            "loss 0.16945910453796387 average time 0.07339258806678117 iter num 60\n",
            "loss 0.10308021306991577 average time 0.06541289505725477 iter num 70\n",
            "loss 0.07563406974077225 average time 0.05944944318748639 iter num 80\n",
            "loss 0.07294097542762756 average time 0.05474967172218991 iter num 90\n",
            "loss 0.3216913342475891 average time 0.05102345144998253 iter num 100\n",
            "loss 0.49033409357070923 average time 0.34951742859957446 iter num 10\n",
            "loss 0.1093893051147461 average time 0.1838581570496899 iter num 20\n",
            "loss 0.39524537324905396 average time 0.12855406733312216 iter num 30\n",
            "loss 0.09133954346179962 average time 0.10071979069980444 iter num 40\n",
            "loss 0.23443837463855743 average time 0.08408276803980698 iter num 50\n",
            "loss 0.19042551517486572 average time 0.07292989444983201 iter num 60\n",
            "loss 0.08592016994953156 average time 0.06510621351418584 iter num 70\n",
            "loss 0.0542297437787056 average time 0.0592572038498929 iter num 80\n",
            "loss 0.07086948305368423 average time 0.05461318977771346 iter num 90\n",
            "loss 0.07168100029230118 average time 0.05089016243990045 iter num 100\n",
            "loss 0.519108235836029 average time 0.35087501449997943 iter num 10\n",
            "loss 0.3422030210494995 average time 0.18412670714988053 iter num 20\n",
            "loss 0.6420333385467529 average time 0.12857644683341884 iter num 30\n",
            "loss 0.8344279527664185 average time 0.10092996847506583 iter num 40\n",
            "loss 1.8405628204345703 average time 0.0842488352201326 iter num 50\n",
            "loss 1.3307898044586182 average time 0.07310191128356867 iter num 60\n",
            "loss 0.6809667944908142 average time 0.06513606660015024 iter num 70\n",
            "loss 0.18457835912704468 average time 0.05913208060014767 iter num 80\n",
            "loss 0.22176982462406158 average time 0.05450176298901271 iter num 90\n",
            "loss 0.24122348427772522 average time 0.0508442024701435 iter num 100\n",
            "loss 0.2565596103668213 average time 0.3501701026005321 iter num 10\n",
            "loss 0.13529154658317566 average time 0.18405281815030322 iter num 20\n",
            "loss 1.485055923461914 average time 0.12848122760018063 iter num 30\n",
            "loss 1.4425125122070312 average time 0.10074921930008714 iter num 40\n",
            "loss 0.9958144426345825 average time 0.08405319925994263 iter num 50\n",
            "loss 0.3208935260772705 average time 0.07300058956661815 iter num 60\n",
            "loss 0.44576790928840637 average time 0.06513936694287363 iter num 70\n",
            "loss 0.16507382690906525 average time 0.05915140095003153 iter num 80\n",
            "loss 0.05523232743144035 average time 0.05450857443340485 iter num 90\n",
            "loss 0.06211776286363602 average time 0.05080126456006837 iter num 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLZ1zpgV2Zv"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hhVHEVV06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7012fdc-c6db-4f5a-de8a-ab583bb43c22"
      },
      "source": [
        "import torch\n",
        "inputs = torch.tensor([[1, 1, 1, 0.25, 0.3, 0.3]]).cuda() # T, K, S, sigma, mu, r\n",
        "print('price: ' + str(model(inputs.float())))\n",
        "\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient[0][[2]]\n",
        "\n",
        "# price, delta\n",
        "# should be around (0.098688, 0.627409)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: tensor([[1.6141]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3334], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2AXrPt7bNj",
        "outputId": "ee26aa7a-8d97-44b4-d494-2ddaaf05e0cf"
      },
      "source": [
        "numstocks = 1\n",
        "numsteps = 50\n",
        "numpaths = 1000000\n",
        "\n",
        "rng = jax.random.PRNGKey(np.random.randint(10000))\n",
        "rng, key = jax.random.split(rng)\n",
        "\n",
        "drift = jnp.array([0.3]*numstocks)\n",
        "r = drift\n",
        "cov = jnp.identity(numstocks)*0.25*0.25\n",
        "initial_stocks = jnp.array([1.0]*numstocks) # must be float\n",
        "T = 1.0\n",
        "K = 1.0\n",
        "\n",
        "# option price\n",
        "print(optionvalueavg(key, initial_stocks, numsteps, drift, r, cov, K, T))\n",
        "\n",
        "# delta test\n",
        "goptionvalueavg = jax.grad(optionvalueavg,argnums=1)\n",
        "print(goptionvalueavg(keys, initial_stocks, numsteps, drift, r, cov, K, T))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27124467\n",
            "[0.90766466]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVR9u56Uu3X"
      },
      "source": [
        "# Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "lwApH0GT9bBK",
        "outputId": "40d6b4de-6475-48db-c73a-cd5cbf61c3b1"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    inputs = torch.tensor([[1, 0.5, S, 0.25, 0.05, 0.05]]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]\n",
        "prices = np.arange(0, 1, 0.01)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efe2bbdc190>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}