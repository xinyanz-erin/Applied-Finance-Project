{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "TEST_European_Call_nstock_auto.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NwN6aLFDnwiy",
        "TY_9g3tbdLiY",
        "JIa4c_aHz15a",
        "u2_89jOknwjH",
        "NMHqzJycx8XH",
        "kSWrm4LrhUMa",
        "rXT4Bg0wdL7l",
        "ehmhDw8BUtLi"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyanz-erin/Applied-Finance-Project/blob/Erin/TEST_European_Call_nstock_auto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xk52L4czj5x"
      },
      "source": [
        "nstock = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCR6hhw5Xq_R"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSxOZk3ls2XQ"
      },
      "source": [
        "!curl https://colab.chainer.org/install |sh -\n",
        "import cupy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN6aLFDnwiy"
      },
      "source": [
        "### Deep Learning Barrier Option\n",
        "\n",
        "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the Asian Barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. [Ryan et al (2018)](https://arxiv.org/abs/1809.02233) showed that a deep learning model can be trained to value derivatives. The deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. In the this notebook, we will use a fully connected network to learn the pricing mode of the Asian Barrier option. Monte Carlo simulation is used as pricing ground truth for the training. We use the same Asian Barrier Option model as last notebook with parameters listed as following:\n",
        "\n",
        "```\n",
        "T - Maturity (yrs.)\n",
        "S - Spot (usd)\n",
        "K - Strike (usd)\n",
        "sigma - Volatility (per.)\n",
        "r - Risk Free Rate (per.)\n",
        "mu - Stock Drift Rate (per.)\n",
        "B - Barrier (usd)\n",
        "```\n",
        "\n",
        "### Batched Data generation\n",
        "\n",
        "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYrh4iYfP-n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "###Test: Judy's new X code\n",
        "#N_STOCKS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy7qGwT0jv4A",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = cupy.array([])\n",
        "#for i in range(0,N_STOCKS):\n",
        "  #X =  cupy.concatenate((X,cupy.array([1,1]), cupy.random.rand(3),cupy.array([1])))\n",
        "#X = X.reshape(N_STOCKS,6)\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OHtAXC8hVae",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#X = X * ((cupy.array([200.0, 0, 200.0, 0.4, 0.2, 0.2] * N_STOCKS, dtype = cupy.float32)).reshape(N_STOCKS, 6))\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_9g3tbdLiY"
      },
      "source": [
        "### Train(Erin Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxT9Eida-c_"
      },
      "source": [
        "# ################################# TEST ########################################\n",
        "# %%writefile cupy_dataset.py\n",
        "\n",
        "# import numba\n",
        "# from numba import cuda\n",
        "# import random\n",
        "# import cupy\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import torch\n",
        "# cupy.cuda.set_allocator(None)\n",
        "# from torch.utils.dlpack import from_dlpack\n",
        "\n",
        "# @cuda.jit\n",
        "# def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
        "#     # ii - overall thread index\n",
        "#     ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "#     stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "#     tmp3 = math.sqrt(T/N_STEPS)\n",
        "#     for i in range(ii, N_PATHS * N_BATCH, stride):\n",
        "#         batch_id = i // N_PATHS\n",
        "#         path_id = i % N_PATHS\n",
        "#         tmp1 = mu[batch_id]*T/N_STEPS\n",
        "#         tmp2 = math.exp(-r[batch_id]*T)\n",
        "#         running_average = 0.0\n",
        "#         s_curr = S0[batch_id]\n",
        "#         for n in range(N_STEPS):\n",
        "#             s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "#             running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "#             if i==0 and batch_id == 2:\n",
        "#                 print(s_curr)\n",
        "#             if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "#                 break\n",
        "#         payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "#         d_s[i] = tmp2 * payoff\n",
        "\n",
        "# class NumbaOptionDataSet(object):\n",
        "    \n",
        "#     def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "#         self.num = 0\n",
        "#         self.max_length = max_len\n",
        "#         self.N_PATHS = number_path\n",
        "#         self.N_STEPS = 365\n",
        "#         self.N_BATCH = batch\n",
        "#         self.N_STOCKS = stocks\n",
        "#         self.T = np.float32(1.0)\n",
        "#         self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32) \n",
        "#         self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "#         self.number_of_threads = threads\n",
        "#         cupy.random.seed(seed)\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.max_length\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         self.num = 0\n",
        "#         return self\n",
        "    \n",
        "#     def __next__(self):\n",
        "#         if self.num > self.max_length:\n",
        "#             raise StopIteration\n",
        "        \n",
        "#         Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "#         paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "#         for op in range(self.N_BATCH):\n",
        "          \n",
        "#           X = cupy.array([])\n",
        "#           K_rand = cupy.random.rand(1)[0]\n",
        "#           B_rand = cupy.random.rand(1)[0]\n",
        "#           r_rand = cupy.random.rand(1)[0]\n",
        "#           for i in range(0,self.N_STOCKS):\n",
        "#             X =  cupy.concatenate((X,cupy.array([K_rand,B_rand]), cupy.random.rand(3),cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "#           X = X.reshape(self.N_STOCKS,6)\n",
        "#           X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "#           #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "#           #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "#           # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "#           #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "#           # make sure the Barrier is smaller than the Strike price\n",
        "#           # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "#           for i in range(self.N_STOCKS):\n",
        "#             paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "\n",
        "#           stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "#           rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "#           #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "#           #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "#           #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "#           stocks_randoms_cov = cupy.array([1] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "#           cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "#           num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "#           randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                                         num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "#           b1_r = randoms_gpu[:,0]\n",
        "#           b2_r = randoms_gpu[:,1]\n",
        "#           randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "#           for i in range(interval):\n",
        "#             if i % 2 == 0:\n",
        "#                 ind = int(i/2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "#             else:\n",
        "#                 ind = int(i//2)\n",
        "#                 randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "\n",
        "#           randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "#           batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
        "#                                 X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "#           o = self.output.reshape(self.N_STOCKS, self.N_PATHS)\n",
        "#           Y[op] = o.mean(axis = 0).mean()\n",
        "\n",
        "#         self.num += 1\n",
        "#         return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\n",
        "\n",
        "\n",
        "# # ds = NumbaOptionDataSet(10, number_path=100000, batch=3, seed=random.randint(0,100), stocks=5)\n",
        "# # for i in ds:\n",
        "# #     print(i[0])\n",
        "# ################################# TEST ########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dZnWTTfbf1"
      },
      "source": [
        "### Train (European Call option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5idhIf0axvbR"
      },
      "source": [
        "tuple(float(i) for i in cupy.array([1, 2, 3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeREuPw0fguQ"
      },
      "source": [
        "################################# TEST ########################################\n",
        "#%%writefile cupy_dataset.py\n",
        "\n",
        "import numba\n",
        "from numba import cuda\n",
        "import random\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "@cuda.jit\n",
        "def European_call_option(d_s, T, K, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_STOCKS):\n",
        "    # ii - overall thread index\n",
        "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "    # tmp1 = r[0]*T[0]/N_STEPS\n",
        "    # tmp1 = np.array([[1, 2, 3]]\n",
        "    tmp3 = math.sqrt(T[0]/N_STEPS)\n",
        "    \n",
        "    for i in range(ii, N_PATHS, stride):\n",
        "\n",
        "      # s_curr = tuple(float(i) for i in S0)\n",
        "      # s_curr = [1, 2, 3]\n",
        "      for j in range(N_STOCKS):\n",
        "        s_curr[j] = S0[j]\n",
        "\n",
        "      # for n in range(N_STEPS):\n",
        "\n",
        "      #   for k in range(N_STOCKS):\n",
        "\n",
        "\n",
        "\n",
        "        # + sigma * s_curr * tmp3 * d_normals[0:N_STOCKS] # stock price\n",
        "\n",
        "    #tmp3 = math.sqrt(T/N_STEPS)\n",
        "    # count = 0\n",
        "    # for i in range(ii, N_PATHS * N_STOCKS, stride): \n",
        "    # # i from 0 - N_PATHS*N_STOCKS\n",
        "    # # for i in range(0, N_PATHS*N_STEPS*N_STOCKS):\n",
        "    #   stock_id = i // N_PATHS\n",
        "    #   path_id = i % N_PATHS\n",
        "    #   # h = T[stock_id] / N_STEPS\n",
        "    #   tmp1 = r[stock_id]*T[stock_id]/N_STEPS \n",
        "    #   # tmp2 = math.exp(-r[stock_id]*T[stock_id]) # discount\n",
        "    #   tmp3 = math.sqrt(T[stock_id]/N_STEPS)\n",
        "    #   #running_average = 0.0\n",
        "    #   s_curr = S0[stock_id]\n",
        "    #   for n in range(N_STEPS):\n",
        "    #     s_curr += tmp1 * s_curr + sigma[stock_id]*s_curr*tmp3*d_normals[count] # stock price\n",
        "    #     count += 1\n",
        "    #     #s_curr += tmp1 * s_curr + sigma[stock_id]*s_curr*tmp3*d_normals[path_id + stock_id * N_PATHS + n * N_PATHS * N_BATCH] # stock price\n",
        "    #     #running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average) # average of the path\n",
        "    #       #if i==0 and batch_id == 2:\n",
        "    #       #    print(s_curr)\n",
        "    #       #if running_average <= B[batch_id]: # if reach barrier, drop out the path\n",
        "    #       #    break\n",
        "    #     #payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
        "    #     #payoff = s_curr - K[batch_id] if s_curr > K[batch_id] else 0\n",
        "    #     #d_s[i] = tmp2 * payoff\n",
        "        \n",
        "    #   d_s[i] = s_curr\n",
        "\n",
        "\n",
        "\n",
        "class NumbaOptionDataSet(object):\n",
        "    \n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15, stocks=3):  # 3 stocks\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        #self.N_STEPS = 365\n",
        "        self.N_STEPS = 10000\n",
        "        self.N_BATCH = batch\n",
        "        self.N_STOCKS = stocks\n",
        "        #self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_STOCKS*self.N_PATHS, dtype=cupy.float32)\n",
        "        # self.number_of_blocks = (self.N_PATHS * self.N_STOCKS - 1) // threads + 1\n",
        "        self.number_of_blocks = (self.N_PATHS - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        \n",
        "        Y = cupy.zeros(self.N_BATCH, dtype=cupy.float32)\n",
        "        paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 6), dtype = cupy.float32)\n",
        "        #paras = cupy.zeros((self.N_BATCH, self.N_STOCKS * 5), dtype = cupy.float32)\n",
        "\n",
        "        for op in range(self.N_BATCH):\n",
        "          \n",
        "          X = cupy.array([])\n",
        "          #T_rand = cupy.random.rand(1)[0]\n",
        "          K_rand = cupy.random.rand(1)\n",
        "          #B_rand = cupy.random.rand(1)[0]\n",
        "          r_rand = cupy.random.rand(1)\n",
        "          for i in range(0, self.N_STOCKS):\n",
        "            #X =  cupy.concatenate((X, cupy.array([K_rand,B_rand]), cupy.random.rand(3), cupy.array([r_rand]))) #[K,B,S0,sigma,mu,r], K B r are shared\n",
        "            X = cupy.concatenate((X, cupy.array([1.0]), K_rand, cupy.random.rand(3), r_rand))\n",
        "          \n",
        "          X = X.reshape(self.N_STOCKS, 6)\n",
        "          #X = X * ((cupy.array([200.0, 0.1, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #[T, K, S0, sigma, mu, r]\n",
        "          X = X * ((cupy.array([1, 150.0, 150.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6))\n",
        "          #X = cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)\n",
        "          #X = 0.9 + cupy.random.rand(6 * self.N_STOCKS, dtype=cupy.float32)*0.1\n",
        "          # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "          #X = (X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2] * self.N_STOCKS, dtype = cupy.float32)).reshape(self.N_STOCKS, 6)\n",
        "\n",
        "          # make sure the Barrier is smaller than the Strike price\n",
        "          # X[:, 1] = X[:, 0] * X[:, 1]\n",
        "          for i in range(self.N_STOCKS):\n",
        "            paras[op,i*6:(i+1)*6] = X[i,:]\n",
        "            #paras[op, i*5:(i+1)*5] = X[i,:]\n",
        "\n",
        "          stocks_randoms_mean = cupy.zeros(self.N_STOCKS, dtype = cupy.float32)\n",
        "          # rho = cupy.random.normal(0, 1, self.N_STOCKS, dtype = cupy.float32)\n",
        "\n",
        "          #stocks_randoms_cov = cupy.ones((self.N_STOCKS, self.N_STOCKS), dtype = cupy.float32)\n",
        "          #cupy.fill_diagonal(stocks_randoms_cov, rho)\n",
        "\n",
        "          #stocks_randoms_cov = (-0.99 + cupy.random.rand(self.N_STOCKS*self.N_STOCKS, dtype=cupy.float32)*2*0.99).reshape(self.N_STOCKS,self.N_STOCKS)\n",
        "          if self.N_STOCKS != 1:\n",
        "            # randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "\n",
        "            stocks_randoms_cov = cupy.array([0.8] * self.N_STOCKS*self.N_STOCKS, dtype = cupy.float32).reshape(self.N_STOCKS,self.N_STOCKS)  #Covariance\n",
        "            cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "            num_of_randoms_each_stock = self.N_PATHS * self.N_STEPS\n",
        "            randoms_gpu = cupy.asarray(np.random.multivariate_normal(stocks_randoms_mean.get(), stocks_randoms_cov.get(), num_of_randoms_each_stock))\n",
        "            \n",
        "\n",
        "\n",
        "            # randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "            #                                               num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "            \n",
        "            randoms = randoms_gpu.T.reshape((1, -1))[0]\n",
        "\n",
        "            # b1_r = randoms_gpu[:,0]\n",
        "            # b2_r = randoms_gpu[:,1]\n",
        "            # randoms = cupy.zeros(self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "            # interval = int((self.N_PATHS * self.N_STEPS * self.N_STOCKS) / self.N_PATHS)\n",
        "            # for i in range(interval):\n",
        "            #   if i % 2 == 0:\n",
        "            #       ind = int(i/2)\n",
        "            #       randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b1_r[ind:(ind+self.N_PATHS)]\n",
        "            #   else:\n",
        "            #       ind = int(i//2)\n",
        "            #       randoms[i*self.N_PATHS:(i+1)*self.N_PATHS] = b2_r[ind:(ind+self.N_PATHS)]\n",
        "          if self.N_STOCKS == 1:\n",
        "            randoms = cupy.random.normal(0, 1, self.N_STOCKS * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "          \n",
        "          European_call_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, X[:, 0], \n",
        "                                X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_STOCKS)\n",
        "          \n",
        "          # np.set_printoptions(threshold=np.inf)\n",
        "          # print(self.output)\n",
        "          o = self.output.reshape(self.N_STOCKS, self.N_PATHS) # this contains prices for each stock for each path at time T\n",
        "          K = K_rand * 150\n",
        "          r = r_rand * 0.2\n",
        "          o = o.mean(axis = 0) # average across stocks end prices\n",
        "          payoff = np.maximum(o - K, 0) # compute payoff\n",
        "          payoff = payoff * np.exp(-r*1) # T=1, discount\n",
        "          Y[op] = payoff.mean()\n",
        "\n",
        "        self.num += 1\n",
        "        return (from_dlpack(paras.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "s\n",
        "\n",
        "ds = NumbaOptionDataSet(1, number_path = 10000, batch = 1, seed = random.randint(0,100), stocks=nstock)\n",
        "for i in ds:\n",
        "    print(i)\n",
        "################################# TEST ########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiU-bJvEhhoK"
      },
      "source": [
        "N_STOCKS = 3\n",
        "N_PATHS = 2\n",
        "N_STEPS = 4\n",
        "stocks_randoms_mean = cupy.zeros(N_STOCKS, dtype = cupy.float32)\n",
        "stocks_randoms_cov = cupy.array([0.2] * N_STOCKS*N_STOCKS, dtype = cupy.float32).reshape(N_STOCKS,N_STOCKS)  #Covariance\n",
        "cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "num_of_randoms_each_stock = N_PATHS * N_STEPS\n",
        "randoms_gpu = cupy.asarray(np.random.multivariate_normal(stocks_randoms_mean.get(), stocks_randoms_cov.get(), num_of_randoms_each_stock))\n",
        "\n",
        "randoms_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkoSXaJ9hhfw"
      },
      "source": [
        "a = randoms_gpu.reshape(1,-1)\n",
        "a[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIcHeQht0P5U"
      },
      "source": [
        "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(a.T)\n",
        "a.T.reshape(1,-1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_0dEUWCtNCM"
      },
      "source": [
        "# batch_id = i // N_PATHS\n",
        "# path_id = i % N_PATHS\n",
        "\n",
        "for i in range(N_PATHS*N_STOCKS):\n",
        "  print(i // N_PATHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqa8B75ktP0g"
      },
      "source": [
        "for i in range(N_PATHS*N_STOCKS):\n",
        "  print(i % N_PATHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIa4c_aHz15a"
      },
      "source": [
        "### For verification (Monte Carlo for n-stocks basket)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DTuXR1ANZPC"
      },
      "source": [
        "# def test(array):\n",
        "#   S1 = array[2]\n",
        "#   S2 = array[8]\n",
        "#   S3 = array[14]\n",
        "#   sigma1 = array[3]\n",
        "#   sigma2 = array[9]\n",
        "#   sigma3 = array[15]\n",
        "#   T = 1  \n",
        "#   K = array[1]\n",
        "#   r = array[5]\n",
        "\n",
        "#   dt = 1/1000\n",
        "#   N = round(T/dt)\n",
        "#   t = np.linspace(0, T, N)\n",
        "\n",
        "#   out = []\n",
        "\n",
        "#   for i in range(10000):\n",
        "#     W1 = np.random.standard_normal(size = N)\n",
        "#     W1 = np.cumsum(W1) * np.sqrt(dt)\n",
        "#     W2 = np.random.standard_normal(size = N)\n",
        "#     W2 = np.cumsum(W2) * np.sqrt(dt)\n",
        "#     W3 = np.random.standard_normal(size = N)\n",
        "#     W3 = np.cumsum(W3) * np.sqrt(dt)\n",
        "\n",
        "#     # W = np.random.standard_normal(size = N)\n",
        "#     # W = np.cumsum(W) * np.sqrt(dt)\n",
        "    \n",
        "#     P1_T = (S1 * np.exp((r - 0.5*sigma1**2) *t + sigma1 * W1))[-1]\n",
        "#     P2_T = (S2 * np.exp((r - 0.5*sigma2**2) *t + sigma2 * W2))[-1]\n",
        "#     P3_T = (S3 * np.exp((r - 0.5*sigma3**2) *t + sigma3 * W3))[-1]\n",
        "#     payoff = (P1_T+P2_T+P3_T)/3 - K if (P1_T+P2_T+P3_T)/3 > K else 0\n",
        "#     out.append(payoff * np.exp(-r*T))\n",
        "  \n",
        "#   return(np.array(out).mean())\n",
        "\n",
        "# print(test(np.array([1.0000e+00, 5.8549e+00, 3.6871e+01, 2.6866e-01, 1.9026e-01, 1.8305e-01,\n",
        "#          1.0000e+00, 5.8549e+00, 4.1110e+00, 2.3714e-01, 5.9547e-02, 1.8305e-01,\n",
        "#          1.0000e+00, 5.8549e+00, 1.6189e+01, 7.3936e-02, 1.1713e-01, 1.8305e-01])))\n",
        "# print(test(np.array([1.0000e+00, 1.1296e+02, 7.4318e+01, 2.0409e-01, 1.9133e-01, 5.3904e-02,\n",
        "#          1.0000e+00, 1.1296e+02, 5.3453e+01, 4.8439e-02, 5.8557e-02, 5.3904e-02,\n",
        "#          1.0000e+00, 1.1296e+02, 3.8721e+01, 2.0259e-01, 6.0423e-02, 5.3904e-02])))\n",
        "# print(test(np.array([1.0000e+00, 3.2307e+01, 3.8874e+01, 2.5820e-01, 1.9522e-01, 8.6353e-02,\n",
        "#          1.0000e+00, 3.2307e+01, 3.6077e+01, 3.7123e-01, 1.8810e-01, 8.6353e-02,\n",
        "#          1.0000e+00, 3.2307e+01, 6.9274e+01, 2.4100e-01, 1.9839e-01, 8.6353e-02])))\n",
        "# print(test(np.array([1.0000e+00, 2.4764e+01, 6.6956e+01, 2.0769e-01, 9.8127e-02, 1.7302e-01,\n",
        "#          1.0000e+00, 2.4764e+01, 3.6483e+01, 8.9075e-02, 1.9375e-01, 1.7302e-01,\n",
        "#          1.0000e+00, 2.4764e+01, 9.4748e+01, 2.6420e-01, 1.2015e-01, 1.7302e-01])))\n",
        "# print(test(np.array([1.0000e+00, 8.4591e+01, 1.2730e+02, 2.8949e-02, 1.0954e-01, 1.3706e-01,\n",
        "#          1.0000e+00, 8.4591e+01, 1.2859e+02, 1.5262e-01, 1.4064e-03, 1.3706e-01,\n",
        "#          1.0000e+00, 8.4591e+01, 4.6679e+01, 2.6688e-01, 1.5895e-01, 1.3706e-01])))\n",
        "# print(test(np.array([1.0000e+00, 6.8433e+01, 3.5593e+01, 6.5870e-02, 6.8068e-02, 1.3578e-02,\n",
        "#          1.0000e+00, 6.8433e+01, 2.3864e+01, 9.5815e-02, 1.6075e-01, 1.3578e-02,\n",
        "#          1.0000e+00, 6.8433e+01, 1.1863e+02, 3.0400e-01, 5.6958e-02, 1.3578e-02])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqx6hNuucM5U"
      },
      "source": [
        "# array = np.array([1.0000e+00, 8.3754e+01, 1.2787e+02, 2.7420e-01, 1.2507e-01, 1.7700e-01,\n",
        "#          1.0000e+00, 8.3754e+01, 4.0869e+01, 3.3302e-01, 1.7439e-01, 1.7700e-01,\n",
        "#          1.0000e+00, 8.3754e+01, 1.1031e+02, 3.5945e-01, 1.5382e-01, 1.7700e-01])\n",
        "\n",
        "# S1 = array[2]\n",
        "# S2 = array[8]\n",
        "# S3 = array[14]\n",
        "# sigma1 = array[3]\n",
        "# sigma2 = array[9]\n",
        "# sigma3 = array[15]\n",
        "# T = 1  \n",
        "# K = array[1]\n",
        "# r = array[5]\n",
        "\n",
        "# dt = 1/100\n",
        "# N = round(T/dt)\n",
        "# t = np.linspace(0, T, N)\n",
        "\n",
        "# out = []\n",
        "\n",
        "# #for i in range(1000):\n",
        "# W = np.random.standard_normal(size = N)\n",
        "# W = np.cumsum(W) * np.sqrt(dt)\n",
        "# P1_T = (S1 * np.exp((r - 0.5*sigma1**2) *t + sigma1 * W))\n",
        "# P2_T = (S2 * np.exp((r - 0.5*sigma2**2) *t + sigma2 * W))[-1]\n",
        "# P3_T = (S3 * np.exp((r - 0.5*sigma3**2) *t + sigma3 * W))[-1]\n",
        "# #ayoff = (P1_T+P2_T+P3_T)/3 - K if (P1_T+P2_T+P3_T)/3 > K else 0\n",
        "# #out.append(payoff * np.exp(-r*T))\n",
        "\n",
        "# #return(np.array(out).mean())\n",
        "# #P1_T\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(t, P1_T)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tpLw9WDhx8b"
      },
      "source": [
        "# stocks_randoms_cov = cupy.array([0.99999] * 3 * 3, dtype = cupy.float32).reshape(3,3)  #Covariance\n",
        "# cupy.fill_diagonal(stocks_randoms_cov, 1)\n",
        "\n",
        "# stocks_randoms_mean = cupy.zeros(3, dtype = cupy.float32)\n",
        "\n",
        "# num_of_randoms_each_stock = 2 * 3\n",
        "# randoms_gpu = cupy.random.multivariate_normal(stocks_randoms_mean, stocks_randoms_cov,\n",
        "#                                               num_of_randoms_each_stock, dtype=cupy.float32)\n",
        "# randoms_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_89jOknwjH"
      },
      "source": [
        "### Model\n",
        "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this DL price model into a file `model.py`:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHqzJycx8XH"
      },
      "source": [
        "### Modified Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTn7iJQryAIH"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024, nstock=3):\n",
        "        super(Net, self).__init__()\n",
        "        self.nstock = nstock\n",
        "        self.fc1 = nn.Linear(6 * self.nstock, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([1, 150.0, 150.0, 0.4, 0.2, 0.2] * self.nstock)) # don't use numpy here - will give error later\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1] \n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPRFqyznwjI"
      },
      "source": [
        "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ELu` activation function. `ELu` is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero. The last layer is a linear layer that maps the hidden dimension to the predicted option price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8J2liPnwjJ"
      },
      "source": [
        "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler. The following code is feeding the random option data to the pricing model to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACi4ge13_rd"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TyZT8_AH35M"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ej82G8nwjJ"
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import NumbaOptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net(nstock = nstock).cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# dataset = NumbaOptionDataSet(max_len = 10000, number_path = 1024, batch = 4800)\n",
        "# dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = 3)\n",
        "dataset = NumbaOptionDataSet(max_len = 100, number_path = 1024, batch = 32, stocks = nstock)\n",
        "\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 20\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)    \n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "trainer.run(dataset, max_epochs = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1EpGuInwjJ"
      },
      "source": [
        "$2365$ seconds The loss is keeping decreasing which means the pricing model can predict the option prices better. It takes about $12ms$ to compute one mini-batch in average, In the following sections, we will try to expore the full potentials of the GPU to accelerate the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSWrm4LrhUMa"
      },
      "source": [
        "### Save and load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8McNtejRNFT"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtOr1XIPOvF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndftly2yPEaM"
      },
      "source": [
        "import torch\n",
        "model_save_name = f'EuCall_{str(nstock)}.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DRO9K2RQoJ"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGXZSV_YRT8v"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ntY-N5bOqdq"
      },
      "source": [
        "import torch\n",
        "model_save_name = f'EuCall_{str(nstock)}.pth'\n",
        "path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "state_dict = torch.load(path)\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GAGPAgPmgh"
      },
      "source": [
        "# need to run 'Writing cupy_dataset.py' and 'Writing model.py' above before this\n",
        "from model import Net\n",
        "model = Net(nstock = nstock).cuda()\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXT4Bg0wdL7l"
      },
      "source": [
        "### Continue to train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa9cp6CdG8T"
      },
      "source": [
        "# from ignite.engine import Engine, Events\n",
        "# from ignite.handlers import Timer\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.optim import Adam\n",
        "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "# from ignite.handlers import ModelCheckpoint\n",
        "# from model import Net\n",
        "# from cupy_dataset import NumbaOptionDataSet\n",
        "# timer = Timer(average=True)\n",
        "# #model = Net().cuda()\n",
        "# loss_fn = MSELoss()\n",
        "# optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# # dataset = NumbaOptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "# dataset = NumbaOptionDataSet(max_len=500, number_path = 1024, batch=32, stocks=3)\n",
        "\n",
        "# def train_update(engine, batch):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     x = batch[0]\n",
        "#     y = batch[1]\n",
        "#     y_pred = model(x)\n",
        "#     loss = loss_fn(y_pred[:,0], y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# trainer = Engine(train_update)\n",
        "# log_interval = 20\n",
        "\n",
        "# scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "# trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "# timer.attach(trainer,\n",
        "#              start=Events.EPOCH_STARTED,\n",
        "#              resume=Events.ITERATION_STARTED,\n",
        "#              pause=Events.ITERATION_COMPLETED,\n",
        "#              step=Events.ITERATION_COMPLETED)    \n",
        "# @trainer.on(Events.ITERATION_COMPLETED)\n",
        "# def log_training_loss(engine):\n",
        "#     iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "#     if iter % log_interval == 0:\n",
        "#         print('loss', engine.state.output, 'average time', timer.value(), 'iter num', iter)\n",
        "        \n",
        "# trainer.run(dataset, max_epochs=20)\n",
        "\n",
        "# model_save_name = 'checkpoint15.pth'\n",
        "# path = F\"/content/drive/MyDrive/AFP/Save_Models/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmhDw8BUtLi"
      },
      "source": [
        "## Inference and Greeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiro43mOU0Ro"
      },
      "source": [
        "We can load the model parameters and use it to do inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlu6tGTRx1F"
      },
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05] * nstock]).cuda()\n",
        "model(inputs.float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Iy-9pWVRDO"
      },
      "source": [
        "One of the benefits of building a deep learning model is that the [Greeks](<https://en.wikipedia.org/wiki/Greeks_(finance)#First-order_Greeks>) can be easily computed. \n",
        "We just need to take advantage of the auto-grad feature in Pytorch. We can use `grad` function to compute the first order differentiation for parameters 'K, B, S0, sigma, mu, r'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBZaYHKSnDu"
      },
      "source": [
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05] * nstock]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs.float())\n",
        "x.backward()\n",
        "first_order_gradient = inputs.grad\n",
        "first_order_gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeijaDDVZGd"
      },
      "source": [
        "Here we are going to plot the Delta graph:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwgeVDsA_Mr"
      },
      "source": [
        "### Delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi_2Dqnx3_oP"
      },
      "source": [
        "#### Using gradient, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USh3qaADSYQp"
      },
      "source": [
        "#### Using gradient, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "from torch.autograd import grad\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_delta(S, ith):\n",
        "    inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs.requires_grad = True\n",
        "    x = model(inputs.float())\n",
        "    #x = model(inputs)\n",
        "    x.backward()\n",
        "    first_order_gradient = inputs.grad\n",
        "    return first_order_gradient[0][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MfZKJlv13SC"
      },
      "source": [
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 200, 0.1)\n",
        "  deltas = []\n",
        "  for p in prices:\n",
        "      deltas.append(compute_delta(p, i).item())\n",
        "  fig = pylab.plot(prices, deltas)\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Delta')\n",
        "  fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXixHQic35iI"
      },
      "source": [
        "#### Using Finite Difference, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGk5Hw64fMdh"
      },
      "source": [
        "## Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S, ith):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S + epsilon, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "\n",
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 200, 0.1)\n",
        "  deltas = []\n",
        "  for p in prices:\n",
        "      deltas.append(compute_delta(p, i).item())\n",
        "  fig = pylab.plot(prices, deltas)\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Delta')\n",
        "  fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIJe36fJ4JFO"
      },
      "source": [
        "#### Using Finite Difference, Change 3 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1I8COnUxnz"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_delta(S):\n",
        "    epsilon = 0.01\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    delta = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return delta\n",
        "\n",
        "\n",
        "prices = np.arange(0, 200, 0.1)\n",
        "deltas = []\n",
        "for p in prices:\n",
        "    deltas.append(compute_delta(p).item())\n",
        "fig = pylab.plot(prices, deltas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Delta')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB8LPwfBMHQ"
      },
      "source": [
        "### Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO_5nEGVcEc"
      },
      "source": [
        "Calculating the second order derivative is easy in PyTorch too. We just need to apply the `grad` function twice. Use this mechanism, we can calculate the second order derivatives $\\frac{\\partial^2 P}{\\partial K \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial B \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial S_0^2}$, $\\frac{\\partial^2 P}{\\partial \\sigma \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial \\mu \\partial S_0}$, $\\frac{\\partial^2 P}{\\partial r \\partial S_0}$ in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzj7A3sThZK"
      },
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "from torch import nn\n",
        "\n",
        "inputs = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "inputs.requires_grad = True\n",
        "x = model(inputs)\n",
        "\n",
        "# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n",
        "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "loss_grads = grad(x, inputs, create_graph=True)\n",
        "drv = grad(loss_grads[0][0][2], inputs)\n",
        "drv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbZYtvhVmSo"
      },
      "source": [
        "Gamma is the second order differenation of `S`. We can plot the the Gamma curve as a function of the stock price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlXVePiK4UH5"
      },
      "source": [
        "#### Using gradient, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpQa3EJToA0"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "import pylab\n",
        "import numpy as np\n",
        "def compute_gamma(S, ith):\n",
        "  inputs1 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "  # inputs = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05] + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "  inputs.requires_grad = True\n",
        "  x = model(inputs.float())\n",
        "  #x = model(inputs)\n",
        "  loss_grads = grad(x, inputs, create_graph=True)\n",
        "  drv = grad(loss_grads[0][0][2], inputs)\n",
        "  return drv[0][0][2]\n",
        "\n",
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 200, 0.1)\n",
        "  gammas = []\n",
        "  for p in prices:\n",
        "      gammas.append(compute_gamma(p, i).item())\n",
        "  fig2 = pylab.plot(prices, gammas)\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Gamma')\n",
        "  fig2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDEmn97t4xxJ"
      },
      "source": [
        "#### Using Finite Difference, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoaOyCDxQy0"
      },
      "source": [
        "##Using Finite Difference, Change only 1 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S, ith):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S+epsilon, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs3 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S-epsilon, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    # inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]  + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    # inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]  + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    # inputs3 = torch.tensor([[1, 110.0, S - epsilon, 0.35, 0.1, 0.05]  + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "\n",
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 200, 0.1)\n",
        "  gammas = []\n",
        "  for p in prices:\n",
        "      gammas.append(compute_gamma(p, i).item())\n",
        "  fig = pylab.plot(prices, gammas)\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Gamma')\n",
        "  fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnRaSM685NCq"
      },
      "source": [
        "#### Using Finite Difference, Change 3 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsXgOwWZ_ru"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_gamma(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S + epsilon, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    inputs3 = torch.tensor([[1, 110.0, S - epsilon, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    gamma = (model(inputs2.float()) - 2*model(inputs1.float()) + model(inputs3.float()))/(epsilon**2)\n",
        "    return gamma\n",
        "\n",
        "prices = np.arange(0, 200, 0.1)\n",
        "gammas = []\n",
        "for p in prices:\n",
        "    gammas.append(compute_gamma(p).item())\n",
        "fig = pylab.plot(prices, gammas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Gamma')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lca2xrBh9a"
      },
      "source": [
        "### Vega"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuETeg-e53wz"
      },
      "source": [
        "#### Using gradient, Change only 1 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muozc-hzhSGA"
      },
      "source": [
        "##Using gradient, Change only 1 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S, ith):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05] + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S, 0.35 + epsilon, 0.1, 0.05] + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*2)]).cuda()\n",
        "\n",
        "    inputs1 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, 110.0, 0.35, 0.1, 0.05]*(ith-1)\\\n",
        "                           + [1, 110.0, S, 0.35+epsilon, 0.1, 0.05]\\\n",
        "                           + ([1, 110.0, 110.0, 0.35, 0.1, 0.05]*(nstock-ith))]).cuda()\n",
        "\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "\n",
        "for i in range(1, nstock+1):\n",
        "  prices = np.arange(0, 200, 0.1)\n",
        "  vegas = []\n",
        "  for p in prices:\n",
        "      vegas.append(compute_vega(p, i).item())\n",
        "  fig = pylab.plot(prices, vegas)\n",
        "  pylab.xlabel('prices')\n",
        "  pylab.ylabel('Vega')\n",
        "  fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_BYyL9d584w"
      },
      "source": [
        "#### Using Finite Difference, Change 3 S0 at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KATxBCAdlFt"
      },
      "source": [
        "##Using Finite Difference, Change 3 S0 at a time\n",
        "# vega\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "def compute_vega(S):\n",
        "    epsilon = 0.5\n",
        "    inputs1 = torch.tensor([[1, 110.0, S, 0.35, 0.1, 0.05]*nstock]).cuda()\n",
        "    inputs2 = torch.tensor([[1, 110.0, S, 0.35 + epsilon, 0.1, 0.05]*nstock]).cuda()\n",
        "    vega = (model(inputs2.float()) - model(inputs1.float()))/epsilon\n",
        "    return vega\n",
        "\n",
        "\n",
        "prices = np.arange(0, 200, 0.1)\n",
        "vegas = []\n",
        "for p in prices:\n",
        "    vegas.append(compute_vega(p).item())\n",
        "fig = pylab.plot(prices, vegas)\n",
        "pylab.xlabel('prices')\n",
        "pylab.ylabel('Vega')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7NlW6GVqSA"
      },
      "source": [
        "[Implied volatility](https://en.wikipedia.org/wiki/Implied_volatility) is the forecasted volatility of the underlying asset based on the quoted prices of the option. It is the reverse mapping of price to the option parameter given the model which is hard to do with the Monte Carlo simulation approach. But if we have the deep learning pricing model, it is an easy task. We can first plot the relationship between volatility and the option price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrCw5UNT07t"
      },
      "source": [
        "# import pylab\n",
        "# import numpy as np\n",
        "# def compute_price(sigma):\n",
        "#     inputs = torch.tensor([[1, 110.0, 110.0, sigma, 0.1, 0.05]]).cuda()\n",
        "#     x = model(inputs.float())\n",
        "#     #x = model(inputs)\n",
        "#     return x.item()\n",
        "# sigmas = np.arange(0, 0.5, 0.1)\n",
        "# prices = []\n",
        "# for s in sigmas:\n",
        "#     prices.append(compute_price(s))\n",
        "# fig3 = pylab.plot(sigmas, prices)\n",
        "# pylab.xlabel('Sigma')\n",
        "# pylab.ylabel('Price')\n",
        "# fig3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU25Cj29VtCa"
      },
      "source": [
        "Given the prices `P`, the implied volatility is the root of the function `compute_price`. We can use bisection to find the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHnwm_zUBYD"
      },
      "source": [
        "# def bisection_root(small, large, fun, target, EPS=1e-6):\n",
        "#     if fun(large) - target < 0:\n",
        "#         print('upper bound is too small')\n",
        "#         return None\n",
        "#     if fun(small) - target > 0:\n",
        "#         print('lower bound is too large')\n",
        "#         return None\n",
        "#     while large - small > EPS:\n",
        "#         mid = (large + small) / 2.0\n",
        "#         if fun(mid) - target >= 0:\n",
        "#             large = mid\n",
        "#         else:\n",
        "#             small = mid\n",
        "#     mid = (large + small) / 2.0\n",
        "#     return mid, abs(fun(mid) - target)\n",
        "# quoted_price = 16.0\n",
        "# sigma, err = bisection_root(0, 0.5, compute_price, quoted_price)\n",
        "# print('implied volativity', sigma, 'error', err)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0AAVB08dqM-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}